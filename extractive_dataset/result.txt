p1:
Extractive Summary:
there were no significant differences in the demographic characteristics (table 1), sleep quality (table 2), and fatigue level (table 3) between the two groups at baseline.after four and 8 weeks of aerobic exercise, the total scores for sleep quality and its components (except for sleep duration after 4 weeks intervention) were significantly lower than the control group (table 2).aerobic exercise resulted in a significant reduction of the total score of fatigue level and its dimensions in weeks four and eight compared to the control group (table 3).the results of the friedman test indicated significant improvement in all components of sleep quality and fatigue level in the treatment group after completion of the intervention (tables 2 and 3).moreover, we observed a significant increase in subjective sleep quality, daytime dysfunction, and the global psqi scores in the control group over time (table 2).discussion our results demonstrated that 8 weeks of aerobic exercise was able to improve all sleep components, although improvement in sleep duration was not significant after 4 weeks of intervention.it seems that increasing the intensity of exercise from mild to moderate enhanced all aspects of sleep quality.also, after four and 8 weeks of intervention, an aerobic exercise decreased the total fatigue score and its components.studies show that exercise significantly improved sleep quality among students [23, 24].however in an iranian quasi-experimental study, at least three sessions per week for 3 months of intervention in 48 students (24 female and 24 male) had no significant influence on sleep quality in either gender [20].the difference in findings can be attributed to confounding factors (i.e., marital and employment status, household type, intake of herbal medicine, and exercise type which were not disclosed in the study.the varying intensity levels of physical activity may affect sleep quality differently.moderate-intensity exercise is generally defined at 65–70% of maximal heart rate [36].our results revealed that increasing the intensity of exercise from mild to moderate after 4 weeks could improve sleep quality.according to consultation with sports science expert, the intervention should be designed from mild to moderate activity for participants who did not exercise regularly.our results revealed that increasing the intensity of exercise from mild to moderate after 4 weeks could improve sleep quality.consistent with our results, several studies report the benefits of moderate physical activity on sleep [18, 23, 37–40].a systematic review also showed that moderate exercise has promising outcomes on sleep quality [41].thus, examining exercise intensity is imperative to understanding the linkage between physical activity and sleep.time of day is also a critical components of exercise [39].in the present study, early evening exercise increased sleep quality.morin et al. similarly found that physical activity early in the evening improves the quality of sleep [42].results from a systematic review and meta-analysis showed that moderate evening exercise may have positive impact on sleep [43].further studies are required to examine the effects of exercise at different times of day on sleep quality.concerning the frequency of physical activity, consistent with our results, some studies found that regular physical activity can lead to a more efficient sleep period [44, 45].a survey reported a positive association between increased frequency of exercise and sleep in young adults [43].wu et al. did not observe an association between physical activity and better sleep quality [46].in a study surveying athletes, 1 day without physical activity had harmed subsequent sleep [47], concluding the benefits of frequent engagement in physical activity on sleep.according to our findings, four and 8 weeks of aerobic exercises also improved fatigue level and its components in university students.de.veris’s study revealed significant beneficial effects of aerobic exercises on fatigue level of university students [48].in one study of multiple sclerosis patients, aerobic exercises significantly reduced fatigue level compared the control group [28].physical activity in individuals with pulmonary arterial hypertension could also reduce fatigue severity [14].generally, the results of various surveys show that physical activity may decrease fatigue level and promote sleep quality through various mechanisms.perhaps the most possible mechanism could be attributed to anxiety reduction through exercise [49].
number of words= 660
[{'rouge-1': {'f': 0.4159042960547444, 'p': 0.7455555555555555,'r': 0.28839080459770117}, 'rouge-2': {'f': 0.2538495376412644, 'p': 0.4182142857142857,'r': 0.18223021582733812}, 'rouge-l': {'f': 0.3888724290650148, 'p': 0.5903252032520325,'r': 0.28993127147766323}}]
-----------------------------------------------------------------------------------------------------------------------------------
p2:
Extractive Summary:
background active adults are among the group of people who always have been interested in ergogenic aids, including caffeine effect on sports performance [1].those participants aged between 20 and 50 years exercise routinely 3–5 times per week [3].madden et al. [9] also found that the rating of perceived exertion and physical performance was higher in the group of individuals ingesting caffeine.our exclusion criteria include gyms with a population fewer than 30 individuals as well as single-sex gyms.accordingly, two gyms were excluded for not fitting these criteria.moreover, one gym’s administration refused to participate in the study.as such, a total of 7 gyms meeting these criteria were included.full lists of customers were provided by each gym and we randomly picked a sample from each.one gym refused to provide a list of its customers, therefore, we estimated the total number based on the size and importance of the gym in the area, and the population size of similar gyms.the population size of all individuals who use these gyms is 1750.a total of 31 individuals from each gym were invited to participate in the study.exclusion and inclusion criteria of the sample all individuals who consume products containing caffeine and perform aerobic and/or anaerobic exercise in different gyms in tripoli- lebanon were included in our sample.data collection data was collected inside the targeted gyms by the main student.the student visited gyms for 8 days between 5 and 8 pm, which were indicated as the busiest hours by the gym administrations.in each gym, the student stayed at the reception desk and randomly invited participants who had finished their exercise session to fill in the survey in the english language.there were 206 gym goers who filled the survey, giving a response rate of 94.9%.data analysis survey data were entered into spss, version 22, by the main student.moreover, various types of statistical analysis were done using spss, depending on the nature and content of the questions and answers.the kmo test assesses how suitable the data is for factor analysis and it is a measure of the proportion of variance among items that might have common variance.the larger the proportion, the more suitable the data is for factor analysis.second, the aerobic duration of exercise was assessed asking respondents to describe their aerobic duration with a time range of less than 30 min to more than 90 min (< 30 min, 30–60 min, 61–90 min and > 90 min).across these diverse exercise types, 54.8% of the sample has been working for a duration of more than 6 months, indicating that the sample is generally experienced with athletic performance and physical activity.as shown in fig. 1, 30.7–31.8% practice anaerobic exercise 5–6 days respectively.additionally, 29.2–31.2% practice aerobic exercise 5–6 days respectively (fig. 2).the resulting cronbach’s alpha was equal to 0.901 which is above 0.7 demonstrating a good level of reliability.scores demonstrated an acceptable level of reliability.factor analysis we found that kmo is 0.887 implying that the sampling is adequate and factor analysis is meritorious or marvelous.the bartlett’s test was significant (p < 0.0001) indicating that the items were correlated and therefore suitable for data dimension reduction and hence factor analysis may be useful.therefore, we conclude that the data may be reduced to three factors only which are factor 1 related to physical performance, factor 2 related to work durability, and factor 3 related to productivity.the first three factors were included, and we excluded the fourth one since it reflects a small number of items.the second group of items with values of vectors 2 and 3 as negative include: 1 (caffeine picks me up when i’m feeling tired), 5 (caffeine improves my mood), 11 (caffeine improves my concentration), 12 (caffeine helps me work over a long period of time), 14 (caffeine improves my attention) and 16 (caffeine makes me feel more energetic).the fourth group of items with all values positive include 2 (i am easily stressed after having caffeine) and 9 (caffeine makes me feel nervous).correlation analyses these were carried out to show the association of the three dependent variables; aerobic exercise frequency, aerobic exercise duration, and anaerobic exercise frequency with the three independent variables based on the caffeine questions; physical performance, work durability, and productivity.in addition, no association has been shown between aerobic exercise and caffeine when perceived in different aspects: physical performance enhancer, work durability enhancer, or productivity enhancer.most of our sample were male (73.3%) active exercisers.the majority of our sample (76.1%) were between the ages of 18 and 35, 14.1% of it were below 18, and the remaining 9.8% were 36 and above.the finding that caffeine consumption was very high (92.2%) among the gym goers in this study is similar to a study done on different university students in lebanon which indicated that 97% of 800 students used caffeine products [4].despite the literature showing a positive association between caffeine consumption and both aerobic and anaerobic activities [2], our results showed the absence of significant association between aerobic exercises and the three factors for the caffeine consumption variable: (1) caffeine as a physical performance enhancer, (2) caffeine as work durability enhancer and (3) caffeine as a productivity enhancer.in addition, our data on caffeine consumption and exercise (aerobic and anaerobic) were collected based on past exposure and thus our findings could be affected by recall bias.on the other hand, randomized experimental studies have found that the intake of a moderate dose of caffeine has an ergogenic effect on male ice hockey players [9].our statistical results are based on analyzing subjective answers from participants regarding perception towards caffeine consumption as a physical performance enhancer, work durability enhancer, and productivity enhancer, which can be influenced by reporting bias.madden et al. [9] revealed that perceived effort is greater in the caffeine group compared to the placebo suggesting that caffeine could increase overall physical performance in anaerobic activities.furthermore, our results have shown that caffeine when perceived as a work durability enhancer is found to be positively associated with anaerobic exercise which is coincident with the american alliance for health theory that caffeine has a direct effect on the central nervous system to increase alertness and decrease fatigue [2].also, a randomized double-blind study revealed that caffeine-containing supplements have a positive effect on anaerobic exercises especially in delaying fatigue and increasing strength [7].nevertheless, this research suffers from some limitations, mainly due to not assessing dose-exposure relationships between caffeine uptake and aerobic and anaerobic exercises, its observational nature in terms of study design and reliance on self-report.also, the sample may not be representative of the target population as the recruitment was limited to only 7 gyms where our target population of active lebanese participants in tripoli were mostly found.this could help us to build scientific guidelines pertaining to caffeine’s association with sports performan
number of words= 1114
[{'rouge-1': {'f': 0.34136933632080774, 'p': 0.808197424892704,'r': 0.21638297872340426}, 'rouge-2': {'f': 0.2057669609006423, 'p': 0.41051724137931034,'r': 0.13729131175468484}, 'rouge-l': {'f': 0.3565127113879264, 'p': 0.6713986013986013,'r': 0.24269076305220885}}]
-----------------------------------------------------------------------------------------------------------------------------------
p3:
Extractive Summary:
in soccer, speed is considered a main determinant of performance [2, 3] and it is the most dominant action when scoring goals in young players from 13 to 16 years [4].therefore, the measure of straight sprinting could in part detect if someone continue a career as a professional or amateur player in adulthood.despite increasing the popularity of soccer among female adolescents, there is a little information about the possible effect of age on sprint performance in this population.the increase in circulating hormonal levels improves neural function, coordination andpower and also produces both oxidative and non-oxidative changes in the metabolism [21, 22].any inappropriate stimulus could cause some alteration in the development of the adolescent and that is why, the training loads should adapt to the speed of subject’s maturation [23].participants were healthy and they did not suffer any disease or injury that could affect the results at the moment of the study.procedures participants were assessed within a single testing session on linear sprinting at the end of the second third of the competitive season (march).a portable body composition analyser with a 200 kg maximum capacity and a ± 0.1 kg error margin (tanita bc-418 ma; tanita corp., tokyo, japan) was used to measure body mass.bmi was calculated by dividing weight (kg) by squared height ( m2).thus, a maturity age of -1.0 indicates that the player was measured 1 year before this peak velocity; a maturity of 0 indicates that the participant was measured 1 year after this peak velocity.sprint testing running speed was assessed by 40 m sprint time with 10 m, 20 m and 30 m split times.players commenced each sprint from a standing start with their front foot 0.5 m behind the first timing gate.two trials were separated by at least 3 min of recovery with the best performance used as the final result [30].between-group differences according to age category in sprint performance were assessed using analysis of covariance (one-way ancova).cohen’s d effect sizes (es) were also used to compare magnitude differences in sprint variables and covariates between ages categories.pearson correlation analyses (r) were applied to determine the relationships between age, anthropometrics (height, body mass, bmi), maturation and sprint variables.the significance level was set at p < 0.05. results descriptive statistics for anthropometric characteristics of the 80 adolescent female soccer players of the u14 to u18 age categories are presented in table 1. age and aphv except for bmi, showed significant age differences (u18 > u16 > u14; p < 0.05).there were significant differences in height among the u18, u16 and u14 (p < 0.05) and body mass between the u14 and u18 age groups (p < 0.05).posthoc analysis revealed better performance in all split sprint times of older soccer players (u18 and u16) compared with younger category (f: 3.38 to 6.17; p < 0.05; es: 0.64 to 1.33).furthermore, 20 m flying showed no age-related differences (p > 0.05); es: 0.30 to 0.92).an ancova was applied to account for the influence of maturation, body mass and bmi on between group differences in the sprint variables (table 2).bmi were significantly related to 40 m (p < 0.05; r = − 0.27).conversely, differences between group did not disappear in sprint running performances when adjusted for aphv.an analysis of the differences between groups revealed improvements in anthropometric characteristics and sprint performance (table 1 and 2).the current findings demonstrate that these characteristics are developed as age increases.the greatest significant changes in speed were observed within u14 and u16/ u18.previous studies have evaluated the possible role of growth (i.e. weight, height or body mass) and maturation on different running speed qualities relevant to soccer performance in soccer players [16, 19, 36].in the present study, all between groups differences in sprint performance disappeared when the body mass and bmi was statiscally controlled.these results demonstrated that sprint differences between groups do not exis when the body mass and bmi effect is removed, which can mean that sprint differences are related to anthropometric characteristics.therefore, caution must be taken into account when comparing both methods.the results indicated that bmi and body mass were significantly correlated with 40 m sprint time (p < 0.05) and 20 m flying (p < 0.01).therefore, our results can be explained by antropometric characteristics, which might suggest that sprint performance is related to variation in body mass and bmi.a possible limitation of the present study was that the results have a cross-sectional character and concern a number of age groups with a relatively small size.second, it is possible that resistance training can affect speed development, however, resistance training has not been controlled during the training courses.in addition, female players were grouped by chronological age and comparisons among groups did not take into account the biological maturity of female players.future studies should analyse possible differences in sprint and other performance capacities in regard to maturational status of players, tactical position and playing positions in female soccer players.conclusions to conclude, this study emphasized that players may improve their anthropometric characteristics and their results in sprint tests as older they are.also, significant differences between younger and older age categories were observed because younger players were far to their age of peak competitive performance.
number of words= 856
[{'rouge-1': {'f': 0.4327123800913631, 'p': 0.8255555555555556,'r': 0.29319474835886217}, 'rouge-2': {'f': 0.27255884938758634, 'p': 0.47892193308550185,'r': 0.19048192771084338}, 'rouge-l': {'f': 0.43017804634634726, 'p': 0.7081578947368421,'r': 0.3089162561576355}}]
-----------------------------------------------------------------------------------------------------------------------------------
p4:
Extractive Summary:
an inadequate number of studies exploring the home-based rehabilitation model for patients with pah have been published to date.a total of 46 patients with a diagnosis of pah (group 1 in the comprehensive clinical classification of pulmonary hypertension according to the ers/esc guidelines [8]) based on right-heart catheterization (mean pulmonary artery pressure (mpap) ≥25 mmhg, pulmonary artery wedge pressure (pawp) ≤15 mmhg) were enrolled in the study.the speed and incline were increased gradually every 20 s. patients were monitored three minutes before, during the test, and three minutes after exercise (electrocardiography, pulse oximetry).patients were advised to be assisted during training sessions by a family member or a caregiver.patients were instructed to record all changes in symptoms, or situations which may have impacted the quality of performed exercises as well as adverse reactions not associated with training.patients from the intervention group had appointments in the department of rehabilitation one month and three months after starting the training program in order for the research team to check if the exercises were performed properly, to monitor diary completion, and to make potential modifications to the training program.adherence regularity of exercise in cardiac rehabilitation plays an important role in the final outcome of a training program undertaken by the patient.therefore, in the present study, repeated weekly telephone calls were made to verify adherence to the minimum recommended five training sessions per week.adherence was defined as a percentage of training sessions performed per week over the period of six months of intervention (minimum 120 sessions in a 24-week period).during the observational phase, patients decided independently about continuation or discontinuation of the recommended training program and intensity of exercises.those who decided to continue the training program were offered the support of the research team.followup investigations included a medical history, physical examination, anthropometric measurements, assessment of who-fc, 6mwt, measurements of respiratory muscle and handgrip strength, body composition analysis (bia), and completed sf-36 and fss questionnaires.statistical analysis data analysis was performed with use of statistical environment r 3.6.2 and additional packages dedicated to particular types of analysis and data visualization.for comparisons of multiple independent means (intergroup comparisons), welch’s anova was performed based on type 2 sum of squares.the kruskall-wallis test was used as the non-parametric equivalent of welch’s anova.for comparisons of multiple dependent means (comparisons of the effects of intervention in the study group), a one-way analysis of variance for repeated measures was used.if the assumption of sphericity was violated, tested with the mauchly test, the greenhousegeisser correction was applied.the friedmann test was used as the nonparametric equivalent of this analysis.strength of effect for analyzes of variance was measured using the eta squared.the bonferroni correction was used for post-hoc analysis.for comparisons of two independent groups, welch’s t tests not assuming homogeneity of variance were used.as the nonparametric equivalent of this test, the mann-whitney test was applied.the t-test for paired data and the non-parametric wilcoxon test were used to compare two paired groups.measurement of the effect size for t-tests was cohen’s d, and for non-parametric tests, pearson’s r. when comparisons of two groups were performed as post-hoc analyses, p-values were corrected using the bonferroni correction for multiple comparisons.p-values < 0.05 were considered statistically significant.results prior to the commencement of the rehabilitation program, there were statistically significant differences between the intervention group and the healthy reference group in parameters such as pimax (49.62 ± 20.89 and 81.96 ± 27.9 cmh2o, respectively, p < 0.001) and pemax (76 ± 32.77 and 113.16 ± 36.55 cmh2o, respectively, p < 0.001), measurement of respiratory muscle strength, and scores of sf-36 (pcs: 40.11 ± 11.35 and 51.5 ± 7, respectively, p = 0.008) and fss questionnaires (4.08 ± 1.6 and 1.87 ± 1.07, respectively, p < 0.001).patients with pah who completed the exercise training program formed the intervention group (16 individuals, age 48.9 ± 18.6 years, 7 female (44%), 9 male (56%)) and the control group (23 individuals, age 53.7 ± 12.8 years, 13 female (57%), 10 male (43%)).these parameters returned to baseline levels after training discontinuation during the observation period (table 2).in post hoc analysis, a significant effect of intervals at which the measurements were taken noted (p < 0.05).a statistically significant improvement of 71.38 ± 83.4 m in the 6mwt distance was obtained(p = 0.004) which was maintained at the level of 57.12 ± 103.31 m (p = 0.043) after the observation period in comparison to the baseline value (table 2, fig. 2c).in post hoc analysis, a significant effect of intervals at which the measurements were taken was noted (p = 0.005).the level of oxygen saturation (sao2) before 6mwt was statistically significantly higher after the intervention period (p = 0.029), and returned to initial values after the observation period (table 2).following the intervention phase and the observation period, study participants reported decreased dyspnea before 6mwt (p = 0.011 and p = 0.006, respectively, table 2).the statistically significant improvement in quality of life achieved after the completion of the exercise training program measured using the sf-36 questionnaire (physical functioning, p < 0.001; role physical, p = 0.015; vitality, p = 0.022; role emotional, p = 0.029; physical component summary, p = 0.005) did not persist beyond completion of the study.results of the one-way analysis of variance for repeated measurements of the evaluated parameters are presented in table 2.no statistically significant changes in who-fc were observed.a limited number of adverse events were reported, including excessive fatigue (n = 1), and a single incident of a sudden increase in hr during training, which aroused anxiety in the patient (n = 1) as the recommended maximum training heart rate was exceeded.there were no cases of death in either pah group.adherence to the recommended exercise protocol, which included a minimum of 120 sessions in 24 weeks, was on average 91.88 ± 14.1%.the effectiveness of the program is comparable or greater than that reported in previously published interventions based mainly on closely supervised rehabilitation in medical facilities.the impact of physical training on functional parameters has been revealed in meta-analyses [12, 25, 26] which demonstrated, inter alia, improvements in 6mwt (6mwd) by 53–72 m, peak vo2/kg in the range of 1.5–2.2 ml/min/kg, and workload by 14.9 w [9].our study, compared with these analyses, showed an improvement in 6mwd by 71.4 m after the intervention period, revealing equally high effectiveness.in other studies [27–29] evaluating the effectiveness of home-based rehabilitation programs, a significantly less marked effect was achieved (39–48.5 m).the 6-month observation period, which followed the intervention phase, during which the majority of participants did not undertake training, allowed us to observe stabilization of improvement in 6mwd at the level of 57 m. this result is highly satisfactory since it indicates a statistically significant long-term improvement in patients’ health and exceeds the mcid for the population of patients with pah [30].regular exercise training over a long period of intervention led to a statistically significant increase in the resting level of sao2..regrettably, the effect did not persist in the subsequent six months of observation.in our study, values of handgrip strength in the pah group did not differ significantly from those in the control group at the time of inclusion in the study.the training program developed for the purpose of this study did not include exercises with significant resistance [31] due to insufficient evidence regarding the safety of this form of exercise, and a lack of direct medical supervision during home training.despite the exclusion of typical resistance training from the rehabilitation program, a statistically significant improvement in skeletal muscle strength was achieved, although the effect was not permanent.the rehabilitation program developed for the purpose of the present study led to an improvement in pimax and pemax after the intervention period, and an increase in pimax and pemax after the observation period compared to baseline values.to date, no reports examining the effect of physical training on body composition in patients with pah have been published in the available literature.it was found that despite the passage of time and a tendency towards increasing body weight and body mass index (bmi), patients maintained the percentage of body fat (pbf) at a constant level.during the six-month observation phase, which followed the compulsory intervention period, patients almost completely stopped following the recommended training protocol.the first is the small sample size caused by the low incidence of pah in the general population (estimated prevalence and annual incidence of pah in poland – 30.8/mln and 5.2/mln, respectively [36]) and exclusion of patients unable to follow the recommended training program.in multiple studies evaluating the effectiveness of rehabilitation in patients with pulmonary hypertension, the number of patients in the intervention group ranged from 12 to 30.the second limitation concerns the model of monitoring adherence to therapeutic recommendations which was based on self-reports of patients supported and supervised by a carer/family member, and weekly telephone verification by the research team.in summary, the present study provides key data on the feasibility of the developed home-based exercise training program and offers hope for physiotherapeutic care for patients with pah who do not have access to other forms of rehabilitation.the results of our study prove that the home-based exercise program developed by us is safe, effective and acceptable to patients with pah, which encourages its implementation in centers which do not provide inpatient or outpatient rehabilitation.conclusions home-based rehabilitation program is an entirely safe form of rehabilitation for stable patients with pah who can be supervised by a family member/caregiver.
number of words= 1553
[{'rouge-1': {'f': 0.3490540128829854, 'p': 0.8457009345794393,'r': 0.2199096929560506}, 'rouge-2': {'f': 0.21668862359371616, 'p': 0.448125,'r': 0.14289156626506025}, 'rouge-l': {'f': 0.37222805949987736, 'p': 0.7236312849162012,'r': 0.25055555555555553}}]
-----------------------------------------------------------------------------------------------------------------------------------
p5:
Extractive Summary:
for planking, both elbows were placed on the vibration platform, whereas the back, hip and both legs stay straight off the platform.for the side stay, the participants lay on their side on the platform, with a single elbow placed on the vibration platform.for v crunching, the patients rolled up their abdomen, pushed their shoulders towards the pelvis, and situated their upper hip on the vibration platform.the participants were randomly subjected to 16 testing conditions.before data collection, all the participants must be familiarized with all the target exercises.participants were instructed to perform each exercise under three vibration frequencies and no vibration.in total, each exercise was performed for 130 s (each exercise condition lasted for 10 s, with a 30 s break between two frequencies); the patients rested for 5 min between two exercises.during the exercises, the experimenter recorded the lumbar–abdominal muscle semg signals of the participants.each participant practiced the sequence of the vibration frequency and exercises randomly.emg data were collected by myoresearch xp master software version 1.07.17(noraxon inc., scottsdale, usa) with sampling frequency of 1500 hz.after we use medical alcohol and razor to reduce skin impedance.the electrodes (silver chloride, shangqiankang medical equipment co. ltd) were put in place referring to the updated introduction for semg [25].we choose the lumbar and abdominal muscles on the side of severe pain were multifidus (mf), erector spinae (es), rectus abdominis (ra) and abdominis oblique externus (aoe) respectively.the placement of the electrode is as follows: (1) mf: 2–3 cm from the midline at the level of l5, putted on and aligned with a line from caudal tip posterior spina iliaca superior to the interspace between l1 and l2; (2) es: an electrode was placed 2 cm apart from the midpoint of the bilateral iliac ridge connection; (3) ra: 2–3 cm lateral from the midline on the second segment of the muscle; (4) aoe: at half of the line from the anterior spina iliaca superior and the tip of the 11th rib [26].data analysis all original data were performed by myoresearch xp master software version 1.07.17 (noraxon inc., scottsdale, usa).first, the original emg signals were processed with a 10 and 500 hz noise by utilizing mat lab [9].for the four exercises, the middle 5 s of the data was used to calculate the emg root mean square (emgrms), which represents the magnitude of muscle activity.a higher rms value indicates more muscle activity.for the standardization of semg data, the value of rms under wbv was compared with that under no-wbv, which was considered as the degree of muscle activity during wbv training [2].statistical analysis statistical analyses were conducted with spss 17.0 and microsoft excel 2007.two-way anova with repeated measures was used to explore the effect of different wbv conditions for each muscle.if the sphericity assumption (mauchly’s test) was violated, we would use the greenhouse– geisser epsilon adjustment.the shapiro-wilk test was used to assess the normality of the descriptive data.statistical analyses were conducted as follows: (1) the main effect of frequency was determined by any conspicuous difference in muscle activation observed amidst the three different frequencies measured; (2) the main effect of exercise was determined by any conspicuous difference in muscle activation observed amidst the 4 different exercises measured; (3) and the frequencies × interaction effect: whether any the interaction effect of wbv frequencies and exercise forms.f-ratio (f) represents the each of main effects, and then it is relative to a critical f-value to evaluate its significance.we determined the muscle activation that was deeply influenced by wbv and used post-hoc analysis (paired t-test) with bonferroni adjustment for evaluation.the statistical significance level was set at p<0.05.results demographics twenty-one patients with clbp attended in the study (11 men, mean age, 22.4 ± 2.62 years).all the patients were recorded demographics by the same physical therapist (table 1), and all the statistics expresses by mean and standard deviation (sd).table 2 showed the muscle activities during different exercises with no-wbv/wbv training.muscle activity of multifidus the value of rms of mf increased compared with that under the same exercise without wbv (total average, 208.7%).the significant differences were detected at wbv frequency (p = 0.002,f = 10.736), exercise (p < 0.001, f = 10.799) and the exercise * frequency interaction (p = 0.044,f = 3.328).it showed that the effect of wbv frequency on the value of rms of mf was related to exercise.then, the rms of mf was highest when the wbv condition was in plank exercise with 15 hz (fig. 2).muscle activity of erector spinae the value of rms of es increased compared with that under the same exercises without wbv (the total average, 190%).the significant differences were detected at wbv frequency (p < 0.001, f = 12.958), exercise (p < 0.001, f = 5.967).the interaction effect of exercise and frequency (p = 0.225, f = 1.388) on es was no significant, indicating that the effect of wbv on es was not related these factors.then, using paired comparisons between exercises, the result found that there were significant among single bridge, plank and the v crunch.the highest muscle activation was induced by plank exercise.in the same way, 15 hz was the best frequency for muscle activation (fig. 3).muscle activity of rectus abdominis the value of rms of ra increased compared with that under the same exercises without wbv (the total average, 247.1%).the significant difference was detected at wbv frequency (f = 6.555, p = 0.018).the effect of exercise (f = 0.513, p = 0.590) and the exercise * frequency interaction (f = 0.525, p = 0.572) were no significant, expressing that effect of wbv frequencies on ra was not relate with exercises.then, we found 15 hz can induce highest degree of muscle activation by paired comparisons (fig. 4).among the exercises, there was significance between v crunch and plank, v crunch can induce highest degree of muscle activation.muscle activity of abdominal oblique externus the value of rms of aoe increased compared with that under the same exercises without wbv (the total average, 206.8%).the significant difference was detected at wbv frequency (f = 15.294, p < 0.001).the effect of exercise (f = 2.010, p = 0.152) and the exercise * frequency interaction (f = 0.975, p = 0.380) were no significant, it showed that effect of wbv frequencies on ra was not relate with exercises.then, we found 15 hz can induce highest degree of muscle activation by paired comparisons (fig. 5).among the exercises, there was significance between single bridge and plank.single bridge can induce highest degree of muscle activation.
number of words= 1071
[{'rouge-1': {'f': 0.4230200732311992, 'p': 0.8060703812316716,'r': 0.28675302245250434}, 'rouge-2': {'f': 0.25037797831308983, 'p': 0.43176470588235294,'r': 0.1763094209161625}, 'rouge-l': {'f': 0.39868499164117316, 'p': 0.6570967741935483,'r': 0.2861520190023753}}]
-----------------------------------------------------------------------------------------------------------------------------------
p6:
Extractive Summary:
during the intervention period, all enrolled players were instructed to avoid complementary res for their lower body, while no restrictions were given regarding res for their upper body.four players withdrew from the study prior to study completion due to illness and injuries not related to the study interventions, and seven players did not show up for posttests.the players started the test on their own initiative, and without verbal encouragement, by breaking the laser beam at the starting timing gate and sprinted to the finishing line as fast as they could.countermovement jump following ≥3 min rest from the sprint test, the players performed the cmj test on the portable force platform (hurlab fp4, hur labs oy, kokkola, finland) following the body mass measurement.a mechanical goniometer was held to the lateral part of their knee joint by an instructor to ensure that the players reached 90° of knee flexion before they were given a verbal “go” and they could start the concentric phase of the lift.all intervention sessions were performed in the same laboratory and supervised by the same instructor.at the end of the 6-week exercise interventions, the participants performed post-tests in the same order as the pre-tests.flywheel squat group the players allocated to the fw group was equipped with a vest on their upper body connected with a band to the fw device (#215 yoyo squat unlimited pro, nhance, yoyo technology, stockholm, sweden).in the fw device, different sized spinning inertia fws can be connected to the pivoting shaft (size #0.5: 0.0125 kg·m− 2, #1: 0.025 kg·m− 2, 2#: 0.05 kg·m− 2, 4#: 0.1 kg·m− 2).the first two sessions (week 1) were familiarization sessions, which consisted of three sets with six repetitions.thereafter, from week 2, the players performed three sets with six repetitions with mivcs followed by 3 × 5, 4 × 5 and 4 × 4 repetitions in week 3, 4 and 5–6, respectively.recovery time between sets was set to ≥3 min.the players started in a partial squat position (~ 90° knee angle) and performed first a standardized warm-up set with six repetitions using the #1 inertia fw (0.025 kg·m− 2).when the fw continued to rewind again, this immediately forced the player to bend their knees and begin the eccentric contraction phase of the next repetition.during the sets and sessions, the load (watt) was monitored using the manufacturer’s application (bluebrain, kuopio, finland) on a portable tablet (samsung galaxy s4, samsung electronics, daegu, south korea) connected to the fw device through bluetooth.the starting inertia at week 2 was set to #1 (0.025 kg·m− 2).the first two sessions (week 1) consisted of three sets with eight repetitions at ~ 70% of 1rm.we assumed a 25% dropout and thus aimed to recruit at least 45 participants (15 in each group); following dropouts (22.5%), we ended up with 13 (fw), 13 (bfw) and 12 (control) in our three groups for the final analyses.data are shown as mean ± sd unless otherwise is stated.results the pre- and post-test results are presented in table 3.there were differences in changes in the 10-m sprint test between the groups (between subjects effect: p < 0.001), where the fw and the bfw group equally decreased their 10-m sprinting time from pre- to post-test by 2% (between groups: p = 1.00, cohen’s d: 0.00, pre- to post-test: fw group: p < 0.001, cohen’s d: − 0.97; bfw group: p = 0.005, cohen’s d: − 0.96), while the control group did not decrease their sprinting time (p = 0.39, cohen’s d: 0.26; difference between fw and bfw vs control: both p < 0.001, both cohen’s d: − 1.39) (table 3).the individual change in 10-m sprint time from pre- to post-test and the association with 1rm partial squat change is illustrated in fig. 3.two out of the 13 in the fw group did not experience a game changing relevant change in 10-m sprint performance (≥0.02 s decrease in 10-m sprint time; range fw group: 0.02 to − 0.08 s, mean increase: − 0.03 ± 0.01 s).there were differences in changes in the cmj test between the groups (between subjects effect: p < 0.001), where the fw and the bfw group equally increased their jump height in the cmj test from pre- to post-test by 9 and 8%, respectively (between groups: p = 1.00 cohen’s d: − 0.16; pre-to post-test: fw: p < 0.001, cohen’s d: 1.70; bfw: p < 0.001, cohen’s d: 1.54), while the control group did not increase their jump height (p = 0.75, cohen’s d: 0.09; difference between fw and bfw vs control: both p < 0.001, cohen’s d: fw vs control: 2.15, bfw vs control: 1.94) (table 3).the individual cmj change from pre-to post-test and the association with 1rm partial squat change is illustrated in fig. 4.in the bfw group, 11 out of the 13 increased their jump height (range: − 0.43 to 6.10 cm, mean increase: 2.78 ± 1.80), while seven out of the 12 players in the control group experienced an increased jump height from pre- to post-test (range: − 1.64-1.02 cm, mean increase: 0.07 ± 0.72 cm) (fig. 4).there were differences in changes in the 1rm partial squat test between the groups (between subject effect: p < 0.001), where the bfw group increased their 1rm squat by 46%, which is more than the fw group’s increase of 17% (difference between groups: p < 0.001, cohen’s d: 3.43, pre- to post-test: fw: p = 0.001, cohen’s d: 3.13, bfw: p < 0.001, cohen’s d: 3.17), and the bfw and the fw group increased their 1rm squat more than the control group (difference between fw and bfw vs control: both p < 0.001, cohen´s d: fw vs control: 2.71, bfw vs control: 4.93, pre-to post-test control group: p = 0.10, cohen’s d: 0.51).we observed a negative linear association between the change in maximal partial squat strength and the change in sprint time (1rm: r = 0.39, r2 = 0.15, p = 0.02) (fig. 3).we observed a positive linear association between maximal partial squat strength and jump height (r = 0.52, r2 = 0.27, p = 0.001) (fig. 4).discussion in this randomized controlled trial of recreationally active football players, fw and bfw hl squats equally improved 10-m sprinting time and cmj height while bfw hl squats was superior to fw squats in improving maximal partial squat strength.finally, we observed linear associations between changes in maximal partial squat strength and changes in 10-m sprinting time and cmj, respectively.although not always consistent [39], sprint improvements following fw squats is reported previously [40, 41], while improvements in jump height following fw re seem to be a consistent observation [39–41].these similar improvements between the bfw and fw groups are likely explained by neuromuscular adaptations induced by mivcs [12].for example, using novel high density surface electromyography recordings, a recent study showed an increased motor unit discharge rate accompanied by a decreased motor unit recruitment threshold following 4 weeks of isometric mivcs [42].moreover, it seems that peak rate of force development is associated with peak motor unit discharge rate, which also seem to be generated prior to maximal force development [16], which thus seem to explain the underlying neural mechanisms for improvements of high velocity movements following re [12].however, it is reported that neural adaptations preliminary occurs within the first 1–2 weeks of re [25, 43].thus, although the strength of the associations between change in sprint time or jump height and change in maximal squat strength were unchanged when including body mass change as independent variable, we cannot rule out whether our 6 week long intervention induced morphological changes (e.g. increase in pennation angle, fascicle length and cross-sectional area), which normally occur as a result of longer exercise programs.for example, a previous study assessing the effect of fw re revealed changes in muscle fascicle length and pennation angle, which was paralleled with hypertrophy gains [44].the bfw group experienced a more than two-fold larger increase in 1rm squat (46%) than the fw group (17%).a meta-analysis reported that fw re is not superior to traditional re for strength improvements [45], which corroborate our findings.nevertheless, the difference in 1rm squat strength between the bfw and the fw group in our study is likely an effect of test specificity where the exercise performed by the bfw group was isotonic to the test; this is shown previously for the squat exercise [46].consequently, we urge for cautious interpretation when comparing 1rm gains between the bfw and fw group.a previous meta-analysis comparing concentric and eccentric re reported superior gains in maximal strength following eccentric re [24].however, their stratified analysis of exercise intensity revealed no differences between the two exercise modalities [24].in fact, in studies comparing solely concentric low intensity (75% of 1rm) contractions with concentric and subsequent eccentric overload contractions (> 100% of 1rm), superior 1rm gains are reported from subsequent eccentric overload [47, 48].while studies comparing solely concentric higher intensity (maximal 6- and 10rm and > 85% of 1rm) with subsequent eccentric overload reported similar gains in 1rm [49, 50].this indicate that high external loads (> 85% of 1rm) should be applied to easily recruit the higher threshold motor units [14], which is responsible for the highest force productions [13].strengths to our knowledge, this is the first randomized controlled trial comparing fw re to hl re practices for maintaining [21] and improving [22, 23] sprint and jump height performance in football.due to the comparison in our study, one can assess the applicability of fw re in football.limitations some limitations need to be addressed.first, football involves multiple changes of direction at high velocities [51].as changes of direction involves decelerations and subsequently accelerations in a different direction, the ability to utilize the elastic energy stored in tissues from deceleration during eccentric contractions into a subsequent concentric acceleration phase can be decisive in football [51].furthermore, by performing 4 × 4 repetitions and increasing load when reaching five repetitions in the bfw, without any mid-test 1rm to adjust relative load, there could have been a possibility of some players in the bfw group exercising at < 85% of their actual 1rm as their actual 1rm increases during the intervention.however, this protocol is proved highly effective in improving maximal strength [21, 22, 34] and moreover, the increase from week to week was high in this group (fig. 2), ultimately leading to a 46% increase in 1rm, which is towards the highest reported increases in 1rm in football players [8].
number of words= 1723
[{'rouge-1': {'f': 0.31817632098602594, 'p': 0.8224115755627011,'r': 0.1972430668841762}, 'rouge-2': {'f': 0.19791043806140277, 'p': 0.42161290322580647,'r': 0.12930359085963003}, 'rouge-l': {'f': 0.35875584985046366, 'p': 0.7074269005847953,'r': 0.2403125}}]
-----------------------------------------------------------------------------------------------------------------------------------
p7:
Extractive Summary:
background childhood obesity is one of the major challenges in developed and developing countries where the prevalence of obesity amongst secondary school children has increased in the last few decades [1, 2].childhood obesity is a major concern for policy makers because overweight and obese children are likely to remain obese into adulthood [3].they are also more likely to develop noncommunicable diseases at a younger age and have a shorter life expectancy.previous literature showed evidence of a steep decline in physical activity among boys after primary school years, and low levels of activity among girls throughout primary and high school years, with adolescent years experiencing the largest declines [5, 6].given that adolescence is a crucial phase to develop and establish healthy lifestyle behaviours, it is important to explore potential interventions that focuses on increasing physical activity during adolescent years [7].a strategy that is particularly promising among adolescents is social norm messages [8].this is based on the notion that individuals use peer norms as a standard to compare their own behaviours.adolescents care about how they perform against their peers or whether their behaviour is approved by others [9], as childhood (including adolescence) is a formative period when friends are primary points of reference in deciding which behaviours and values are desirable and which are not [10].in fact, in studies conducted with adolescents, peer influence was found to be a significant correlate of physical activity [11].according to social cognitive theory, perceptions of others’ behaviour lead to behaviour change in an individual [12].this has been used to change several unhealthy behaviours, such as alcohol and drug use, gambling and recycling [13, 14].disclosing descriptive norm information with personal identification may overcome this problem.this could be explained by their desire to fulfil others’ expectations and their concern about how adherence or non-adherence to social norms may affect their reputation.as such, individuals who know that their information will be shared with others may be more motivated to achieve a goal visible to others, regardless of their state of activity.adolescents aged 13 to 16 were randomized into one of two arms, and each arm consisted of 13 groups of 12 participants.we hypothesized that the average number of steps taken by adolescents will be higher when descriptive norm message is onymous compared to when it is anonymous.we also investigated the trajectories of physical activity.we hypothesized that, among those in onymous arm, girls and those who know other participants in their group will be more likely to be in a trajectory with increasing step counts because perceived peer pressure tends to be stronger among females than among males [22, 23] and among those who know each other compared to those who do not [24].methods recruitment and eligibility between december 2016 to december 2017, 342 participants were recruited on a rolling basis.recruitment was achieved via a combination of different recruitment strategies such as newspaper advertisement and wordof- mouth.individuals who were interested were asked to contact the study coordinator for their eligibility.to be eligible for the study, participants must be: (1) singaporean or singapore permanent resident; (2) between 13 and 16 years old; (3) willing to wear a pedometer for 16 weeks; and (4) english-speaking, which is the dominant language spoken by this age group in singapore [25–28].a doctor’s approval was required if the participant answered “yes” to any of the items in par-q.informed consent was obtained according to the national university of singapore-institutional review board (nus-irb) protocol, both from the participant and one of the parents or guardians.the trial lasted 16 weeks and participants were compensated for sgd 50 (~usd 36.23) upon completion of end-of-study survey.of the 342 participants who registered for the study, 13 participants did not complete the baseline assessment and were excluded from the study.a total of 311 participants who completed the baseline assessment (31 dropped out) were randomized to either anonymous (n = 155) or onymous (n = 156) arms.the participants were informed of their allocated arms after they completed the baseline assessment.a batch included one anonymous arm and one onymous arm, consisting of 12 participants in each arm.this method resulted in 13 batches in total.we chose 12 participants since we found this number to be large enough to create competition among group members, but small enough that participants can start the trial before they lose interest in the study.figure 1 shows the recruitment and randomization process.participants were ranked according to the total step count accumulated in the past week within their group.to discourage lower levels of physical activity, those with a step count of zero were not included in the list for both arms.paediatric quality of life (pedsql) scale, asian adolescent depression scale (aads), social support and exercise scale, physical activity self- efficacy (pases) scale, and physical activity enjoyment scale (paces-8) were administered in both surveys.the end-of-study survey also included questions about participants’ experience with the study and whether they knew participants in their group and in the study.the survey instruments developed for this study are provided as additional file 1.measures steps (primary outcome) steps were measured by a fitbit flex™ wireless pedometer.fitbit devices have been validated in measuring step counts among healthy individuals [29].average weekly number of steps was used for the analysis.pediatric quality of life (pedsql) inventory score quality of life (qol) was assessed using pedsql™ 4.0 generic core scales, a 23-item scale that was developed to evaluate qol in teenagers, and has been widely used in other pediatric qol studies [30].higher score indicates better qol [31].license was obtained to use this scale for this study.the total score is the sum of the 20 items.possible score for this instrument ranges between 20 to 100.social support and exercise (sse) survey exercise-related support from family and friends was estimated using the social support and exercise survey [34].this 13-item instrument evaluates the behaviour and attitude of family and friends toward their participation in exercise in the past 6 months with a 5-point likert scale.eleven of the 13 items measure supportive behaviour and attitude toward exercise, while two items measure negative social support associated with exercise.reverse scoring was performed for the two items, measuring negative social support.possible score for this scale ranges between 26 to 130.descriptive norms might have been more effective in persuading subjects to adopt target behaviour if the normative reference group was a more relevant social group such as close friends, or peers that they interact regularly with such as classmates.however, the differences were statistically significant at the 10% level only for the physical-activity self-efficacy.to our knowledge, this is the first study that measured the effect of descriptive norms objectively by using step count directly extracted from pedometers.
number of words= 1093
[{'rouge-1': {'f': 0.3678691754528278, 'p': 0.8066412213740457,'r': 0.23826503923278117}, 'rouge-2': {'f': 0.19643670440890573, 'p': 0.35735632183908045,'r': 0.13544502617801046}, 'rouge-l': {'f': 0.3512347168701048, 'p': 0.6368789808917197,'r': 0.24248062015503877}}]
-----------------------------------------------------------------------------------------------------------------------------------
p8:
Extractive Summary:
the primary goal of the ppe as described in the 4th edition ppe monograph, which is now an older version but was used at the time of this study, is to (1) screen for conditions that may be life-threatening or disabling, and (2) screen for conditions that may predispose individuals to injury or illness [5].currently, there is no standardization for healthcare providers on how the ppe is completed, and there are questions surrounding the effectiveness of the ppe in meeting its objectives to properly screen athletes.effective screening tests must satisfy 2 requirements as described by the us preventive services task force: (1) the test must be able to detect abnormalities earlier than without screening and (2) the screening must be accurate [8].due to the need to understand more completely how to better screen athletes for msk injury we designed a survey to gain information from physicians performing ppe’s.our aims were to understand current physician practices in regard to the msk screening exam, if physicians who perform ppes feel the current goals of the ppe are being met, and if there may be opportunities for education in teaching of the msk screening examination.exclusion criteria included providers who are not currently practicing, and individuals that do not perform the ppe as part of their practice.the study utilized a cross-sectional survey-based redcap instrument for data collection.3,000 members of the aafp were mailed postcards that contained a link and scannable qr code for participants to access the survey and 3,871 members of the amssm were sent an email that was generated through the organization on two occasions, one month apart, containing a link to the survey.the majority of participants, 72 %, were family medicine physicians with the remaining 28 % being composed of orthopedic, internal medicine, emergency medicine, pediatric, and physical medicine and rehabilitation physicians.participants practiced across 46 states collectively.twenty-six percent of participants reported that they do not perform a physical exam at all as part of their msk screening examination, while 86 % of participants agreed that the msk exam should be performed.educationally, 51 % received training for the msk ppe in residency, and 62 % had training in fellowship.the majority of respondents (82 %) were familiar with the 4th edition ppe monograph and 80 % either moderately or strongly agreed that they use this as a guideline for their msk screening exam.fifty-one percent of respondents received training for the msk ppe in residency and 62 % in fellowship programs, which highlights the lack of standardization in curriculums.this calls attention to the need for continued standardization of the msk screening exam, as well as further research to validate objective screening exams for the prevention of msk injury.a recent study by teyhen et al. was able to show in a military population that the sum of a number of risk factors was able to produce a highly sensitive model for identifying those at risk for msk injury [11].identifying the high-risk population would provide an opportunity to direct limited prevention resources to the most at-risk individuals with the ultimate goal to reduce overall injury risk.our survey was the first that we are aware of that looked to gain provider insight to the msk exam.it also highlights physician concerns that the goals of the ppe are not being met with the msk screening.future steps include creation of a validated msk screening model, similar to other primary prevention screening models (cad, colon cancer, etc.), that accomplishes the goals of the msk portion of the ppe, which can then be implemented to be a standard part of the ppe ex
number of words= 592
[{'rouge-1': {'f': 0.5540046698918702, 'p': 0.7856862745098039,'r': 0.427843137254902}, 'rouge-2': {'f': 0.32735279012278046, 'p': 0.44704918032786883,'r': 0.2582160392798691}, 'rouge-l': {'f': 0.45579923681846435, 'p': 0.610880503144654,'r': 0.36351535836177473}}]
-----------------------------------------------------------------------------------------------------------------------------------
p9:
Extractive Summary:
contracture usually occurs in people with joint problems or stroke patients along with the elderly or immobile patients [6, 7].various therapeutic measures are taken to prevent and treat ankle contracture, based on the situation of every person and the severity of the contracture [10, 11].participants the inclusion criteria included hospitalization in the icu ward for more than 1 week, age between 18 and 60 years, patients who were not able to move their legs voluntarily (unconscious patients and those who were not able to move their lower limbs due to paralysis), agreement to sign the written consent form by the patient or his or her guardian (in case of unconsciousness), lack of disease or musculoskeletal disorder according to a physical examination, lack of damage inside or around the ankle, no history of treatment by electrical stimulation in the ankle zone, no skin disease hindering placement of electrodes or causing sensitivity to electrical stimulation, absence of electrical implant devices (such as pacemaker), and lack of sever spasticity that prevented joint movement.a therapist who had been trained by an experienced physiotherapist applied the ankle stretches for all patients.among the tens mechanism is the activation of nerve impulse in a large number of alpha afferent nerve roots, resulting in the excitation of inhibitory neurons of the dorsal horn or the release of endorphin or both [14].further, tens enhances blood circulation close to the electrodes, which indirectly contributes to regeneration, reduction of spasm and contracture, and relaxation of muscles [14].the tens mechanism in mitigating spasticity and contracture of ankle can be due to the excitation of afferent fibers in the peroneal nerve [15].increased blood circulation close to the electrodes is also effective in reducing ankle contracture.as there is no need for active participation by the patient in tens application, it can be used by people who have not been able to perform physical activity for a long time.the therapist who performed the interventions were not informed about the participants groups.in the first session, the interventions (ankle stretching or tens stimulation) were explained for patients (or their relatives in case of unawareness).data analysis data analyses were conducted by spss 21.0 (spss inc., chicago, il) and p values of < 0.05 were set at level of significance.frequency (percent) and mean (standard deviation) were used to summarize the accumulated data.the comparisons of background variables including demographic variables and clinical parameters were investigated between the experimental and control groups using the t-test and the chi-squared test.normality was confirmed for all ankle range of motion parameters by the kolmogorov–smirnov one-sample test.the within-group differences in time trend for each group was assessed by repeated measures analysis of variance.for this test, assumption of the sphericity (consistency of correlations) was not confirmed by mauchly’s test.therefore, p values were stated based on the greenhouse–geisser test in both groups.additionally, the basic measurements of the ankle range of motion parameters were adjusted as covariate and differences between groups from baseline to weeks 1 and 2 were assessed by analysis of covariance (ancova).results in this trial, all the patients remained in the study and there was no loss to follow up on during the study (fig. 3).there was no statistical difference between groups regarding drug use.mean values of dorsiflexion and plantar flexion during the study are shown in figs. 4 and 5.in both groups, the increase in the mean values of ankle range of motion parameters was significant over time.based on the sidak post-hoc test, the differences between the first and the second measurements, between the second and the third measurements, as well as between the first and the third measurements were significant (means ranged over 44– 48 for plantar flexion and means ranged over 5–11 for dorsiflexion, p < 0.001 for all of time points; table 2).after making adjustments for the baseline measurement, the differences between the experimental group and the control group regarding change in ankle plantar and dorsiflexion from the first measurement (before intervention) and the last measurement (after intervention) remained significant (mean between-group differences ranged over 2.67– 3.57, (95% ci = 2.04 to 4.01), p < 0.001).applying tens at the foot and ankle has the potential to be a highly beneficial intervention in deficits in these abilities [23].it is hypothesized that using tens in the foot and ankle area would improve the balance and postural control [23].as tens improves strength, joint position sense and balance control, it is reasonable that it has effects on joint mobility.however, there are many related studies on other joints.therefore, the novelty of our study is investigating the effect of tens on ankle stiffness in immobility conditions.it has been shown in many studies that adding tens to an intervention of joint movement (passive or active) had an impact on joint mobility improvement.in the three days after the surgery, he used tens along with a continuous passive motion (cpm) device, but he did not observe any significant difference in adding tens to joint passive motion for enhancing the range of motion.most of related studies were on knee joint as the involvement of this joint is more than others.in similar papers about other joints, as mentioned previously, different controversial results were observed.
number of words= 852
[{'rouge-1': {'f': 0.4392668472884981, 'p': 0.7780536912751679,'r': 0.3060178970917226}, 'rouge-2': {'f': 0.2565856268598043, 'p': 0.4168013468013468,'r': 0.18534154535274355}, 'rouge-l': {'f': 0.3969225546860503, 'p': 0.6149101796407186,'r': 0.29303921568627456}}]
-----------------------------------------------------------------------------------------------------------------------------------
p10:
Extractive Summary:
the model-based process includes two consecutive steps: a kinematic identification based on procedure of solidification [34], combined with inverse kinematics and an inverse dynamics process that provides the elbow joint net torque (for more details, see [17]).as a first step to test the accuracy of the model, the aim of the present study is (1) to assess the maximal elbow joint torque variability during cyclic elbow flexion extension movements and (2) to assess participant test-retest repeatability in healthy young adults.methods participants twelve healthy young adults (age = 23 ± 2; male n = 6) were included in the present study.exclusion criteria were known musculoskeletal or orthopaedic pathology, on the basis of a questionnaire in participants.the study was approved by the research ethics board of ste-justine hospital, montreal, canada (ethics case #3362).a written informed consent was obtained from participants.the research was in compliance with the helsinki declaration.procedure experimental set-up the experiments were conducted on cyclic elbow flexion-extension movement with the upper arm maintained vertical.as illustrated in fig. 1, an experimental chair was designed to enable standardized motion of elbow flexion-extension in the sagittal plane.the person depicted in fig. 1c gave a special consent to publish this one.particularly, our incentive was to minimize the elbow joint motion during the task, but without mechanically blocking it, to highlight the behaviour of only one joint, i.e. the elbow.consequently, right elbow optokinetic sensors were inserted in specific holes created on the side of the chair rest (fig. 1a).further, to limit the range of the flexion-extension motion (approximately 50°), 'sensitive' stops were placed to keep the movement between 70 and 120 degrees of flexion (fig. 1a).this arc (70–120) was chosen because it corresponds to range of movement involve in many functional tasks [38].the chair was adapted in height and depth in order to seat the participant with their hips and knees flexed at 90 degrees, and the right arm placed vertically downward.the participants were equipped with optokinetic sensors, placed on the following anatomical landmarks: the acromion, the middle of the arm (technical marker), the lateral epicondyle, the middle of the forearm (technical marker), the radial styloid, and both extremities of the dumbbells.this placement was set to enable the three-dimensional kinematic reconstruction of the upper limb and the dumbbell.the displacement of the markers was filmed by six infrared cameras (elite-bts, milano, italy) cadenced at 100 hz.participant instructions during experimentation, the participant sat on the chair.the participants were asked to perform 10 cycles of flexion-extension, following the rhythm of a given metronome, with and without dumbbells.participants had to keep the shoulder and elbow joint as motionless as possible and the dumbbell axis horizontal.participants were involved a few minutes with the dumbbells, before beginning the experiments.the participants had to perform ten elbow flexionextension movements with five different masses: 0, 1, 2, 3 and 4 kg, and at three motion frequencies, 0.5 hz (i.e. a cycle in 2 seconds), 0.33 hz (1 cycle in 3 seconds) and 0.25 hz (1 cycle in 4 seconds).the order of the masses and frequencies was drawn randomly by the operator.each male participant performed the whole experimental protocol twice in order to assess test and retest reproducibility of the joint torques.the retests were performed approximately 20 min after the tests, without removing the kinematic sensor.joint torque quantification process using the measurements of kinematic sensors, a 3d multibody model of the human body [17] provides the elbow joint torques via these three consecutive steps: 1. the full model joint kinematics: the system is modeled as a constrained multibody system, using kinematic loops.2. the joint kinematic identification: the joint coordinates q, velocities q and accelerations q are numerically determined by an optimization process that estimates the joint coordinates of the multibody model that best fit the experimental joint positions.3. the inverse dynamics: using recursive newton-euler formalism, a 3d multibody model [17] provides the vector qinv of joint forces and torques during movement as follows: qinv ¼ f q; :q ð ; q€; fext ;mext ; gþ ð1þ where f is a function of the kinematics q, q̇, q and represents the inverse dynamical model of the human body, on the basis of the external forces fext and torques mext applied to the system, and also gravity g. the inertia parameters of the body segments have been defined using the table from de leva [39].these equations were symbolically generated by the robotran software [40], ucl, which allows us to straightforwardly interface these equations with any numerical process, such as the optimization process presented above and the time simulation of the trials.statistical analysis data was reported as mean (standard deviation) (sd).normality of the distributions was determined using the kolmogorov–smirnov test.for each frequency (0.5, 0.3, and 0.25 hz) and mass (0, 1, 2, 3, 4 kg), the peak torque variability within each trial was assessed by computing the coefficient of variation (%cv).the aim of this intra-test variability analysis was to enable to average the peak torques of each trial for the repeatability analysis.
number of words= 823
[{'rouge-1': {'f': 0.3407432090548974, 'p': 0.7635483870967743,'r': 0.21930555555555556}, 'rouge-2': {'f': 0.19203600455480857, 'p': 0.3564864864864865,'r': 0.13141367323290848}, 'rouge-l': {'f': 0.31997376216673723, 'p': 0.5994117647058823,'r': 0.21823529411764706}}]
-----------------------------------------------------------------------------------------------------------------------------------
p41:
Extractive Summary:
background vascular leiomyosarcoma (lms) is a kind of malignant mesenchymal tumor that originates from the vascular wall’s smooth muscle layer.sixty percent of cases occur in the great retroperitoneal veins, including the inferior vena cava (ivc), renal vein (rv), and adrenal vein (av); these cases are called retroperitoneal vascular leiomyosarcoma (rvlms).rvlms is characterized by smooth muscle cells extending beyond the blood vessel and spreading along the wall.te tumor progresses slowly and tends to invade surrounding tissues and metastasize within the blood circulation, leading to a worse prognosis [1].women are more susceptible to these tumors (male: female=1: 4), especially those aged 40–70 [2, 3].because of unnoticeable symptoms in the early stage, patients usually do not visit the hospital until the presence of upper abdominal or lower back pain due to the compression of other organs by the enlarged tumor.symptoms of venous obstruction, such as lower limb edema and sensory abnormalities, may also occur.dyspnoea or budd-chiari syndrome can also occur in some severe cases due to occlusion of the trachea or hepatic vessels [4].moreover, confusing image presentations and complicated retroperitoneal structures contribute to the low accuracy of preoperative diagnosis.treatment decisions in clinical practice are challenging.rvlms is a rare disease, accounting for less than 1/100000 malignant tumors, and no more than 500 cases have been reported in previous literature.although there are few guidelines, surgical removal to obtain a negative margin has been indicated signifcantly improve tumorspecifc survival [5].artifcial blood vessel replacement or autologous liver transplantation may be considered during surgery, depending on the degree of tumor invasion.since 2018, there have been four cases of rvlms from diferent blood vessels treated in our hospital.in this article, we will review these cases and related publications to develop a better understanding of rvlms, try to establish a clinical decision map, and provide suggestions for diagnosis and treatment in future work.methods patient information we continuously collected all patients in our hospital who underwent surgical treatment with “retroperitoneal mass” and were confrmed to be rvlms by pathological examination from august 2018 to february 2020.we retrospectively collected all rvlms cases in our hospital from august 2018 to february 2020.te main symptoms, past history, preoperative american society of anesthesiologists (asa) grade and tumor size and side are presented in table  1.preoperative computed tomography urography (ctu) and renal and adrenal hormone determination, including blood catecholamines, 24  h 3-methyl-4-hydroxymandelic acid (vma), adrenocorticotropic hormone (acth), plasma cortisol rhythm and renin-angiotensin ii-aldosterone system (raas) function in standing and recumbent positions, were conducted.te preoperative contrastenhanced computed tomography (cect) images are shown in fig.  1.a single round low-density mass was found in three patients: two were located in the right adrenal gland area (cases 1 and 3), and the other was located in the right renal hilum area (case 2).in case 4, the patient was found to have a sizeable lobular mass in front of both the right kidney and adrenal gland.in two cases, the tumor expanded into the ivc and formed a tumor thrombus; in case 4, the tumor extended to the lower edge of the second hilar region.all patients whom paragangliomas cannot be excluded were given phenoxybenzamine two weeks before the operation to prevent possible blood pressure fuctuations tree patients underwent retroperitoneal laparoscopic tumor resection; in case 2, the patient was converted to open surgery.one patient underwent open surgery.postoperative follow-up was conducted by outpatient revisits and phone calls.multidisciplinary case discussion te diagnosis and treatment plan of soft tissue mass needs to be decided by a multidisciplinary team (mdt) meeting, which includes doctors of general surgery, urology, radiology, oncology, and pathology.in this study, the tumor has a clear boundary, seems to have a capsule, and no distant metastasis was detected.mdt discussed and decided to choose surgery as the frst choice.operation procedure te intestinal preparation was performed one day before the operation.a total of 800–1200  ml of blood was prepared.laparoscopic surgery tree patients underwent retroperitoneal laparoscopic surgery.te ports were set at the following points to introduce the camera and tools (fig.  2): the intersection of the twelfth rib and the edge of the psoas major, the costal margin along the anterior axillary line, and the iliac crest along the midaxillary line.for example, the italian sarcoma collaborative group adopted the ei regimen for primary large adult soft tissue sarcoma, and showed that the chemotherapy group obtained nearly 30 months of dfs and os prolongation compared with the non-chemotherapy group [33].however, smac meta-analysis, which included four large randomized controlled trials involving more than 1900 patients, showed no diference in os between patients receiving postoperative monotherapy and those without chemotherapy [28, 34].terefore, the specifc population should be selected for adjuvant chemotherapy, and the increase of dose and combined medication should be considered.te adjuvant chemotherapy is recommended with the following conditions: (1) high grade and a large volume of tumor; (2) te operation did not reach the safe surgical boundary; (3) recurrent tumor operations.at present, doxorubicin combined with ifosfamide is still an ideal chemotherapy regimen.a clinical trial done by te european organization for research and treatment of cancer (eortc) (no.62012, nct00061984) showed that the response rate (26 vs 14%), progressionfree survival (7.4 vs 4.6  months) and overall survival (14.3 vs 12.8 months) of combined medication were better than those of doxorubicin alone [35, 36].however, there is no recognized second-line regimens, and diferent drugs should be selected according to the histological type.te application of adjuvant radiotherapy has not reached a consensus, most clinical studies show optimistic prospects, but waiting for more rigorous clinical trials to prove its efectiveness [37].in this study, the follow-up imaging examination of case 1 showed local recurrence.te recurrent tumor bed was located in the operation area, and the patient refused to receive systemic chemotherapy, so local radiotherapy was recommended.no progression was found in the following 2 months.unfortunately, the patient was lost to follow-up and could not be further evaluated.postoperative adjuvant targeted therapy is also worth exploring in the future.in recent years, a number of clinical trials of targeted medications for lms are ongoing and showed optimistic results.in 2016, fda accelerated the approval of olaratumab (pdgfrα antibody) is on the market, becoming the frst frst-line drug for soft tissue sarcoma [38, 39].in terms of limitations, the nature of this case series study in terms of the lack of a control group made the results only a reference for clinical practice rather than a guideline.due to the low prevalence of rvlms, there remains no consensus on the standards for treatment of the disease.case-controlled or cohort studies should be conducted to determine the potential cause and prognostic factors in future work.conclusion and perspective tis study summarizes the diagnosis and treatment experience of rvlms in our center based on data from four patients.rvlms is complicated to diagnose preoperatively due to uncharacteristic manifestations and confusing imaging results.most of the patients were diagnosed by intraoperative and postoperative pathology.completed preoperative evaluation and mdt collaboration are essential, as well as careful designation of the surgical plan to remove the mass and rebuild the blood vessels.te innovative use of the satinsky clamp in our center to block blood vessels reduced the difculty of surgery, but we could not statistically compare this method with the traditional blocking method or with veno-venous shunting because of the small number of cases.
number of words= 1191
[{'rouge-1': {'f': 0.3439454404471953, 'p': 0.7126229508196722,'r': 0.22667466027178257}, 'rouge-2': {'f': 0.16650653970631502, 'p': 0.2739473684210526,'r': 0.11960000000000001}, 'rouge-l': {'f': 0.30882279674494256, 'p': 0.537005076142132,'r': 0.21673046251993622}}]
-----------------------------------------------------------------------------------------------------------------------------------
p12:
Extractive Summary:
as these surgeries are being performed on younger and broader cohorts, it is pertinent to elucidate these outcomes.this observational study evaluates baseline, and medium-term serum corrected calcium, vitamin d and parathyroid hormone levels following roux-en-y gastric bypass (rybp) and sleeve gastrectomy (sg).method a retrospective analysis was conducted on prospectively collected data of 370 participants from the ages 17 to 71 who underwent rybp or sg between january 2015 and december 2016 inclusive, with at least 2 years followup.all patients were assessed and prepared for surgery by a single multidisciplinary team consisting of a surgeon, endocrinologist, dietician, psychologist, and clinical nurse educator within a single-centre group practice in melbourne, australia.preparation included education around the importance of nutritional supplementation guided by the specialist dietitian.patients underwent either rybp or sg depending on appropriate indications performed by two surgeons with a standardised technique.the same single-centre team delivered postoperative care and follow-up.further appointments were offered as required by individual circumstance as determined by the treating professional.collected data includes demographic parameters, body mass index (bmi) and laboratory values (serum 25-oh vitamin d, corrected serum calcium levels, estimated glomerular filtration rate (egfr), haemoglobin a1c (hba1c) and parathyroid hormone (pth) levels.our laboratory’s normal reference ranges are: 25-oh vitamin d > 50 nmol/l, corrected serum calcium 2.15– 2.65 mmol/l, pth 1.5–7 pmol/l.follow-up rates were estimated as per adherence to vitamin d laboratory tests counts.compliance to supplementation was recorded as ‘poor’ and ‘full’ following discussions with the patient by the specialist dietician or treating surgeon.during the follow-up period, participants underwent a standard calcium and vitamin d supplementation protocol as described here.all bariatric patients were recommended 3000–5000 iu of vitamin d per day and 600–900 mg of calcium citrate per day.further 600– 900 mg of calcium through dietary intake was encouraged.throughout the follow-up period, if vitamin d levels remained over 75 nmol/l, patients were advised to continue 1000 iu of vitamin d per day.for vitamin d levels between 50 and 70 nmol/l, patients were advised a further 2000–3000 iu per day.for vitamin d levels between 30 and 50 nmol/l, patients were advised a further 3000–5000 iu per day.if vitamin d levels dropped below 30 nmol/l, patients were given 5000 iu per day oral supplementation or a 600,000 iu intramuscular injection.patients who were deficient before surgery were given appropriate supplementation.pregnant patients were excluded due to inconsistent weights and physiological fluctuations.primary and revision bariatric surgeries were collected and noted separately to account for heterogeneous medical and nutrition status.however, statistical analyses were simplified into either rybp or sg.operative technique sleeve gastrectomy was performed over a 36 fr bougie and included antral resection after complete mobilisation of the stomach from gastro-oesophageal junction to the pylorus.any hiatal hernia evident was repaired.rygb was performed, creating a gastric pouch of approximately 6 cm × 2 cm, incorporating complete exclusion of the fundus.the biliopancreatic limb was routinely 100 cm, and the alimentary limb 90–100 cm.the anastomotic technique was standardised between the two surgeons, with the gastroenterostomy being 2/0 vicryl handsewn over a 36 fr bougie and the entero-enterostomy being a side-to-side stapled anastomosis completed with 2/0 vicryl handsewn closure of enterotomies.statistical analysis statistical analyses were performed with spss 26 (spss inc chicago).a value of p < 0.05 was considered statistically significant.student’s t-tests, with equal variances assumed, are used to compare the difference of normally distributed data (calcium levels) between the two surgical procedures.compliance follow-up rates were 71%, 61% and 47%, at 6, 12 and 24 months, respectively.of the 174 participants reviewed at 24 months, compliance to vitamin and mineral supplementation was recorded in 152 cases.discussion this study comprises one of the largest cohorts to date, evaluating the impact of commonly performed bariatric surgeries on bone metabolism over a medium-term follow- up period of 24 months.following both surgeries, mean serum calcium levels significantly rise compared to baseline measurements by 24 months.this transient rise in vitamin d levels at 6 months is reflected in several other studies [6, 7].this is one of the few studies on this matter with follow-up data over 12 months and well into 72 months.following a transient fall in pth levels at 6 months after rybp, these levels rise back to preoperative levels by 24 months.studies with stable or lower pth post- rybp often has low follow-up periods (< 12 months) or more robust supplementation.despite a similar fall in pth levels following sg at 6 months, these levels continue to remain below preoperative concentrations.we found that those with high pth levels (greater than 7 pmol/l) preoperatively, significantly improved to normal levels by 24 months following sg but not rybp.these limitations may overestimate the strength of the results as it may be the most compliant patients who attend follow-up meetings at 24 months.higher parathyroid hormone levels following roux-en-y gastric bypass may be of significance to poor long-term bone health and the need for a more robust micronutrient supplementation protoc
number of words= 801
[{'rouge-1': {'f': 0.3705520455398474, 'p': 0.7011475409836065,'r': 0.25181818181818183}, 'rouge-2': {'f': 0.18358339806902674, 'p': 0.2922222222222222,'r': 0.13382978723404254}, 'rouge-l': {'f': 0.35928763269811864, 'p': 0.5863398692810458,'r': 0.2589952153110048}}]
-----------------------------------------------------------------------------------------------------------------------------------
p13:
Extractive Summary:
the remaining 483 respondents comprised the study cohort (table 1).among these respondents, 76% regularly perform incisional hernia repairs, and 37% of those considered themselves experts.in contrast only 1.5% and 0.2% of surgeons who repair incisional hernias infrequently or only when on call consider themselves experts, respectively.given the clinical scenario of a patient with an incisional hernia following a right hemicolectomy with a 10 × 6 cm midline fascial defect, the majority of surgeons indicated that they would perform an open repair (74%), followed by laparoscopic (18.4%) and hybrid (7.6%) approaches.almost all surgeons reported that they would use a permanent mesh to repair the hernia (table 2) and 93% would place anchoring sutures.seventy-three percent of surgeons reported that they would perform a primary fascial closure and just under half would perform a component separation (table 3).surgeons performing open procedures were more likely to use primary fascial closure compared to those who would perform laparoscopic operations (83% versus 23%, p < 0.05).of those who perform a component separation 57% indicated that they would perform a posterior component separation.the self-reported experts were more likely to report using primary fascial closure (83% versus 67%, p < 0.05) and component separation (58% versus 39%, p < 0.05) compared to the non-experts.the most common “overall surgical approach” was an open repair with mesh and primary fascial closure aided by a component separation.this was described by 37% of all respondents and 40% of the self-reported experts (table 4).when only considering mesh location, primary fascial closure and component separation twelve different operations were described to repair the hernia in the clinical scenario (table 5).the most common operation was an open repair, with an intraperitoneal mesh, with primary fascial closure and a component separation.discussion previously, very little has been known about the management of incisional hernias in canada.in this study of self-reported practice patterns almost all surgeons reported that they would use permanent mesh.this is supported by the literature and is in keeping with current guidelines [1].it was the most consistent practice among respondents.the use of laparoscopic surgery reported by surgeons in this study was considerably lower than that reported in previous clinical studies [4, 5].this likely reflects the size of the defect described in our clinical scenario and the technical challenges associated with closing the fascia laparoscopically [6].there was considerable variation among canadian surgeons regarding mesh type, mesh location, component separation techniques and the overall surgical approach they would use to repair an incisional hernia.the variation in surgical practice observed in this study and in previous retrospective studies from europe and the united states [4, 5, 7, 8] is likely due, in part, to a lack of high-quality evidence to guide care.despite the fact that incisional hernia repair is a common procedure and is associated with high recurrence rates, very few randomized controlled trials (rcts) have been performed to evaluate surgical techniques [9].most of the literature consists of observational studies and case series.studies frequently combine heterogeneous patient populations with incisional, primary ventral and umbilical hernias [7].within studies focused exclusively on incisional hernias, there is often substantial variation regarding hernia size, hernia location and co-morbidities among patients [10, 11].a recent systematic review of study quality among rcts for incisional hernia repair highlighted several limitations.this included inconsistent techniques to determine hernia recurrence among studies (clinical exam and/or imaging) and variability in length of follow-up after surgery ranging from 1 to 64 months [9].another important limitation associated with the hernia literature is the lack of standardized definitions.this is of particular concern for the anatomic planes use for placement of mesh.the terms “onlay”, “overlay”, “inlay”, “underlay” and “sublay” have been used interchangeably to describe various repairs [12].this has made hernia research, evaluation of outcomes and comparison of studies difficult.to address this issue parker et al. have proposed a standardized classification of abdominal wall planes developed by a consensus panel of abdominal wall reconstruction experts [13].they defined eleven potential locations for mesh placement, some of which have not commonly been described.our survey was developed and sent out prior to this publication.the number of anatomic planes and definitions we used differed from those proposed by parker et al. but were in keeping with what has been described in the literature.
number of words= 696
[{'rouge-1': {'f': 0.4734966869147305, 'p': 0.8432793522267206,'r': 0.3291587516960651}, 'rouge-2': {'f': 0.28132165147729093, 'p': 0.4643089430894309,'r': 0.20179347826086957}, 'rouge-l': {'f': 0.42277935096628705, 'p': 0.663984962406015,'r': 0.3101215805471125}}]
-----------------------------------------------------------------------------------------------------------------------------------
p14:
Extractive Summary:
background operative management of traumatic hollow viscus injuries has been a subject of much debate, and colon injury especially remains a feared entity [1, 2].simple suture, with interrupted or running stitches of absorbable thread, was preferred in case of small wounds with clean edges.resection and anastomosis were used if the bowel was ischemic or containing several wounds on a small segment.anastomosis without resection was used to treat transfixing perforations without ischemia [21].if preservation of bowel continuity was considered unadvisable, a stoma was made, either as a loop stoma or as a resection and double stoma.if the patient survived, a second procedure was performed 24 to 48 h later for definitive treatment of the bowel injury.specific morbidity included intra-abdominal abscess, anastomotic or suture leak, wound abscess, and renewed intra-abdominal bleeding.morbidity was defined as complications occurring during the hospital stay or in the month following surgery.the severity of complications was evaluated with the clavien–dindo classification.mortality was defined as in-hospital death.chi-squared tests, fisher’s exact tests and student’s t tests were used for univariate analyses.factors identified with an alpha level less than or equal to 0.05 were tested using logistic regression.results are presented as means ± sd, as medians and iqrs, or percentages.a p value less than or equal to 0.05 was considered statistically significant.statistical analyses were conducted using spss (ibm corp, spss statistics for windows, version 23.0, armonk, ny, usa).(table 1).there was no significant difference between the choice of treatment for small bowel injuries and that for colon injuries (p = 0.318).there was no difference between these two groups in terms of preservation of bowel continuity (p = 0.348), morbidity (p = 0.401) or mortality (p = 0.445).these complications and their management are detailed in table 3.concerning stoma creation, there were 15 ileostomies and 9 colostomies.five were successful, one patient had emergent laparotomy for anastomosis leakage and double-barrel ileostomy was created anew.he had uneventful ileostomy reversal 2 months later.the first patient presented with bowel ischemia following mesenteric disinsertion; he had small bowel resection and ileoileal anastomosis.he suffered anastomosis leakage and peritonitis on pod 8, and was treated by emergent laparotomy and double-barrel ileostomy.the small bowel anastomosis was ischemic; it was resected and made into a double-barrel ileostomy.discussion bowel injuries are most common in abdominal trauma, and most general surgeons will be confronted to the problem of repairing these injuries [22].stoma creation has become less and less popular when dealing with traumatic bowel injuries.their conclusion was that primary repair was at least as safe as colostomy, in the absence of major risk factors such as arterial hypotension, delayed operation, multiple associated injuries, and destructive colon injuries requiring resection.their study was followed by four other trials, all in favor of primary repair or anastomosis [7–10].ultimately, sharpe et al. reported an 80.4% rate of primary repair or anastomosis, with a 2.5% rate of postoperative fistulas [15].in our experience, fistula rate was a low 2.2%, with no leakage after colon injury repair.however, in our study, a major risk factor for severe morbidity was the creation of a stoma; this was tested in a multivariate analysis against other factors such as trauma severity, shock on admission and polytransfusion.high-risk patients were defined as patients presenting with a delay longer than 12 h, hemodynamic shock, associated injuries, contamination, transfusion of over 6 rbc units, or left-sided colon injuries; for these patients, anastomosis or primary repair is conditionally recommended.conclusion bowel continuity should be preserved as often as possible when managing intestinal trauma, regardless of the site of injury.
number of words= 577
[{'rouge-1': {'f': 0.4419963159233977, 'p': 0.7580733944954128,'r': 0.31193548387096776}, 'rouge-2': {'f': 0.19338697101169386, 'p': 0.2865898617511521,'r': 0.14592891760904686}, 'rouge-l': {'f': 0.38279605761607877, 'p': 0.6054330708661417,'r': 0.27987654320987654}}]
-----------------------------------------------------------------------------------------------------------------------------------
p15:
Extractive Summary:
therefore, we investigated the clinical features, treatments, and long-term follow-up outcomes of lumbar hernias based on data obtained from 28 consecutive patients at our institution in the present study.written informed consent were obtained from each patient in this cohort.this study was approved by the institutional review board of west china hospital and was carried out in accordance with the declaration of helsinki.anaesthetic and surgical procedure general or local infiltration anesthesia was used for tension- free lumbar hernia mesh repair in this study.no sedation or analgesia was preoperatively used as premedication for those who under local infiltration anaesthesia.the local anesthetics solutions were comprised of 20 ml of 2% lidocaine, 10 ml of 1% ropivacaine and 2 ml of 0.1% epinephrine, and adding normal saline to the total amount of 160 ml.in general, 40–50 ml were injected for unilateral lumbar hernia.stepwise subcutaneous dissection and blunt dissociation of muscles (some overlying stretched muscle fibers were resected if necessary to expose the defect) were used to expose the hernia sac.and then, the hernia sac was dissected from its surroundings and reduced.a pre-peritoneal plane was created with blunt swab dissection.in the present study, mesh repairs were made using the ultrapro™ plug (upp, ethicon, norderstedt, germany), ultrapro™ hernia system (uhs, ethicon, norderstedt, germany), and proceed™ surgical mesh (proceed, ethicon, somerville, usa) according to the size and location of abdominal wall defect.after reducing the sac (especially for those with small hernia defect), the anchor of the upp was then placed through the defect into the preperitoneal space without any suturing, as it would unfold automatically due to its elasticity.the rim was then sutured onto the margins of the defect with 3–0 absorbable suture (fig. 1).for the relatively large hernia ring, after the hernia sac was fully reduced and the preperitoneal space was separated, the bottom mesh of uhs device was inserted through the defect; during the placement of mesh, it is essential to ensure that the bottom mesh was extended 2–3 cm or more from the defect edge, and the upper mesh was then sutured with the defect surface.if hernia sac was huge, it was excised intraoperatively.the mesh with the appropriate size would be placed according to the defect of the abdominal wall (mesh edge beyond defect range at least 5 cm), and the mesh was flattened and fixed properly.and then, the wound was closed.the drainage tube was not placed routinely unless the wound was large.calculations statistical analysis was performed with the statistical package for the social science (spss) version 21.0 for windows (spss inc, chicago, il, usa).continuous variables were expressed as mean ± standard deviation or median (range).measurement data was analyzed by variance analysis.categorical were described as frequencies and percentage, and compared with chi-square or fisher’s exact test.all p values were two-sided, with p < 0.05 indicated statistically significant.results patient and clinical characteristics until august 2020, a consecutive series of 28 patients with lumbar hernia in our institution were retrospectively collected, including 13 males (46.4%) and 15 females (53.6%), with the male-to-female ration of 0.87 (table 1).only 1 5-year-old patient had congenital lumbar hernia (unilateral), while the remaining patients (27 cases) had acquired lumbar hernia; of the 27 patients, 20 (71.4%) cases were primary, while a total of 7 (25.0) patients were secondary.there were 11 (39.3%), 15 (53.6%) and 2 (7.1%) cases had right, left and bilateral lumbar hernia for the entire cohort, respectively.in other words, there were 30 lumbar hernias in this study.totally, 25 cases underwent extraperitoneal repair.no patient in the group b required conversion to general or spinal anaesthesia.there were no significant between-group differences in sex, age, bmi, and side of lumbar hernia (p > 0.05).though a trend for smaller size of abdominal wall defect and shorter operation time were observed in the group b when compared with group a, the differences were not significant (p > 0.05).no postoperative bleeding and infection occurred.of note, patients in the group b had a shorter hospital stay than that of group a (3.5 ± 1.3 days vs. 7.1 ± 3.2 days, p = 0.001), as well as total hospitalization expenses between the two groups (2988.6 ± 1268.8 $ vs. 1299.0 ± 229.3 $, p < 0.001).with a median follow-up duration of 45.9 months (range 1–113 months), only 1 (3.3%) lumbar hernias recurred for the entire cohort.in addition, there was no significant difference with respect to chronic wound pain and foreign body sensation between the two groups during the follow-up period (table 2).discussion lumbar hernias can be classified based on location and etiology [1].according to the anatomical location of the defect, lumbar hernias were divided into grynfeltt hernia (the superior triangle) and petit hernia (the inferior triangle).however, blunt abdominal trauma may also create lumbar hernia, which was classified as the “diffuse” type and was not be confined to these two triangles [12, 13].the superior lumbar triangle is an inverted triangle whose base is formed by the 12th rib and the serratus posterior inferior muscle, while the inferior lumbar triangle is an upright triangle whose base is formed by the iliac crests.
number of words= 833
[{'rouge-1': {'f': 0.451948297063113, 'p': 0.8182758620689656,'r': 0.3121875}, 'rouge-2': {'f': 0.2873157808243457, 'p': 0.4852249134948097,'r': 0.2040782122905028}, 'rouge-l': {'f': 0.4422292772027485, 'p': 0.71,'r': 0.32112107623318387}}]
-----------------------------------------------------------------------------------------------------------------------------------
p16:
Extractive Summary:
however, in rectal cancer, a laparoscopic approach is quite different and more difficult than that in colon cancer.however, laparoscopic rectal surgery has been associated with limited dexterity with nonarticulating unstable instruments, unnatural hand–eye coordination, and flat 2-dimensional (2d) vision [9].several previous studies documented that robotic surgery is equivalent to laparoscopic surgery with respect to perioperative and oncologic outcomes [11–13].to date, few reports with data have evaluated the short-term and long-term outcomes of robotic surgery compared with laparoscopic surgery for rectal cancer.our centre is one of the earliest hospitals to introduce the da vinci ® surgical system (intuitive surgical, sunnyvale, ca, usa) in china.here, we evaluate the oncologic outcomes of rectal cancer by robotic surgery or conventional laparoscopic surgery including the primary endpoint of survival outcomes and the secondary endpoints of the general conditions of the operation, postoperative complications and pathological characteristics.all patients included in this study met the following criteria: (1) the disease was histologically defined rectal adenocarcinoma; (2) all the patients underwent tme; (3) tumour size was measurable, and pathological evaluation records of pelvic lymph nodes were complete; (4) the patient had no history of malignancy in other organs; and (5) the clinicopathological and follow-up data of the patients were complete.the exclusion criteria were as follows: (1) age > 80 and < 18 y; (2) other malignant tumours; (3) tnm stage at 0, iv; (4) multivisceral resection; (5) palliative resection; (6) restaging surgery; (7) abdominal and pelvic exploration only; and (8) incomplete patient information.before surgery, all patients were informed of the detailed characteristics of both robotic and laparoscopic surgical procedures.the study protocol followed the ethical guidelines of the 1975 declaration of helsinki, revised in 2000.all related procedures were performed with the approval of the internal review and the ethics boards of the first affiliated hospital, nanchang university.the intraoperative and perioperative conditions (e.g., operation time, intraoperative bleeding, and complications), postoperative complications (e.g., anastomotic leakage, bleeding, wound problems, urinary retention, and the development of an ileus) and survival time were also collected.before march 2016, patients were chosen to undergo laparoscopic surgery.survival curves were obtained by the kaplan–meier method, and the os and dfs rates were compared by the log-rank test.only factors with a p < 0.05 in the univariate analysis could be included in the multivariate analysis using a stepwise method, and variables with a p < 0.05 and hazard ratio (hr) > 20% were kept in the final model.a two-tailed p < 0.05 was considered significant.all statistical analyses were performed using the spss 22 software package (spss, chicago, illinois, usa).a similar sex distribution was observed in the robot and laparoscopy groups, and most patients were men.age, bmi, preoperative serum cea, asa class and previous abdominal surgery were not significantly different between the two groups.tumour location from the anal verge was significantly shorter in the robot group than in the laparoscopy group (5.9 ± 2.6 cm vs. 8.5 ± 3.6 cm, p < 0.001).survival analyses and prognostic factors the os rates at years 1, 3 and 5 in the robot and laparoscopy groups were 96.6%, 88.7%, and 87.7% vs. 96.7%, 88.1%, and 78.4%, respectively (p = 0.925, fig. 2a).in the univariate analysis, the factors associated with 5-year os and dfs were age, cea, tumour location, tnm stage, differentiation grade and lymphovascular invasion (table 2).the operation time was significantly decreased in the robot group compared with the laparoscopy group (163.5 ± 40.9 vs. 190.5 ± 51.9 min, p < 0.001).postoperative complications occurred in 50 of the 314 patients in the robot group, which was significantly lower than that in the laparoscopy group, with 71 of the 220 patients (15.9% vs. 32.3%; p < 0.001).no significant differences were observed between the two groups with respect to the occurrence of short- and long-term complications, including anastomotic leakage, anastomotic bleeding, wound problems, ileus, intraabdominal abscesses, anaemia, ascites, adhesions, incisional hernias, anastomotic strictures and rectovaginal/ rectovesical fistulas (p > 0.05, respectively).however, the occurrence of short- and long-term urinary retention in the robot group was significantly lower than that in the laparoscopy group (1.9% vs. 7.3% and 0.6% vs. 4.1% p < 0.05, respectively).there was no significant difference with respect to lymph nodes retrieved between the two groups [13 (7) vs. 13 (6.3), p = 0.389].discussion in this retrospective comparative cohort study, we found no beneficial survival effect of robotic surgery on patients with rectal cancer compared to those receiving laparoscopic surgery.importantly, the time to 1st gas passing and 1st soft diet and the length of hospital stay were significantly shorter in the robot group, indicating that robotic surgery might enhance recovery after surgery.the postoperative pathological parameters that can measure the quality of rectal surgery are crm positivity and the number of harvested lymph nodes of the resected specimen; both of which were not significantly different between the robotic and laparoscopic surgical approaches.however, in the laparoscopy group, there were no cases with positive crm, and local recurrence occurred in 12 cases (12/224) with negative crm.
number of words= 818
[{'rouge-1': {'f': 0.5239183005514241, 'p': 0.8005699481865285,'r': 0.38936579841449603}, 'rouge-2': {'f': 0.3249336478626121, 'p': 0.4751948051948052,'r': 0.24687074829931974}, 'rouge-l': {'f': 0.4950673989642503, 'p': 0.6955924170616115,'r': 0.3842857142857143}}]
-----------------------------------------------------------------------------------------------------------------------------------
p17:
Extractive Summary:
background pancreatic leakage (pl) is a complication associated with various pathophysiological conditions, such as pancreatitis, surgery, and trauma, including iatrogenic events from invasive medical procedures such as endoscopic retrograde cholangiopancreatography (ercp) [1–5].if pl persists, autodigestion of peripancreatic tissues occurs, which can cause ruptured aneurysms and/or multiple organ failure [1, 2, 6–8].one cause of severe pl is disconnected pancreatic duct syndrome (dpds).dpds is characterized by blockage of the main pancreatic duct (mpd) with no access to the upstream pancreatic duct, concurrent with a persistent nonhealing pancreatic fistula or pancreatic fluid collection [3–5, 9, 10].in cases of severe dpds, patients may develop systemic inflammatory response syndrome (sirs) with nonlocalized pl, and two therapeutic strategies are often reluctantly attempted.the american gastroenterological association recommends distal pancreatectomy in the first 30–60 days for dpds, but this approach has high morbidity [14].the european society of gastrointestinal endoscopy guidelines recommend long-term placement of transluminal plastic stents after transluminal drainage of walled-off necrosis in dpds patients [15].if the endoscopy approach fails, surgery including distal pancreatectomy or r-y drainage is recommended.the italian and japanese guidelines do not specifically mention dpds, but they state that intervention (including radiological, endoscopic, or surgical) should be performed when necrotizing pancreatitis leads to clinical deterioration or ongoing organ failure [16, 17].as yamada et al. reported, endoscopic ultrasonography- guided drainage is one option for dpds [18].it is less invasive than two-stage surgery.however, the success rate has been reported as ranging from 38 to 73% [3, 19].the difficulty of this procedure depends on the anatomical condition, which is impossible to predict in advance.moreover, this method is not applicable in all cases.in our method, we were able to anatomically design the internal drainage route prior to the procedure.this approach is relatively easy and feasible.an experienced surgeon may be concerned that the fistula generated in the jejunum via our “intentional internal drainage tube method” may worsen pl due to the reverse overflow of bacteria-rich intestinal juice, thereby activating the pancreatic juice.this drawback has been overcome as follows.first, the jejunal fistula was designed at the r-y limb, where bile and food do not pass through.second, the created fistula only contained the tube held by a tobacco suture.third, immediately after surgery, the intentional internal drainage tube was connected to a continuous suction device.this suction device mainly served to direct pl into this intentional internal drainage tube rather than into the other external drainage tubes.these maneuvers aimed to prevent the reverse flow of contaminated intestinal juice into the peritoneal cavity.we applied this intentional internal drainage tube method during the third operation on day 80.however, after reviewing the case history, we noted that it could also have been performed in the second operation on pod 9 to shorten the hospital stay.due to the simplicity and minimally invasive nature of this method, we believe this new method may represent an alternative approach for treating various types of nonlocalized persistent pl and may also be used prophylactically for central pancreatectomy in which there are two dangerous causative margins for severe
number of words= 496
[{'rouge-1': {'f': 0.5122303538523001, 'p': 0.7177272727272728,'r': 0.39821497120921306}, 'rouge-2': {'f': 0.24408491988636682, 'p': 0.3209505703422053,'r': 0.19692307692307692}, 'rouge-l': {'f': 0.41754474620788196, 'p': 0.5764102564102564,'r': 0.32732899022801304}}]
-----------------------------------------------------------------------------------------------------------------------------------
p18:
Extractive Summary:
at the time of tracheal intubation, there was no vomitus causing asphyxiation; however, there was massive foamy sputum intratracheally.laboratory test results showed a decreased total white blood cell count (4400 cells/μl; neutrophils, 62.2%) and significantly increased level of c-reactive protein (19.27 mg/dl).the serum blood urea nitrogen (58.9 mg/dl) and creatinine (2.48 mg/dl) were also increased, indicating acute renal dysfunction.his serum troponin level was negative but d-dimer (10.9 μg/ ml) was increased.ct imaging during autopsy (fig. 3) did not show any significant change, such as bowel perforation in abdomen, from the time of admission.cardiomegaly was not significant; however, a large quantity of intratracheal fluid and upper and dorsal infiltration of the lungs were detected.based on the clinical course and these examination results, apart from postoperative constipation, acpo was considered the fundamental disease and developed fulminant obstructive colitis leading to critical sepsis rapidly.discussion and conclusion acpo, considered synonymous with ogilvie’s syndrome, is a rare clinical state characterized by acute colonic dilatation in the absence of any mechanical obstruction [4].this peculiar condition is primarily observed in hospitalized patients with severe illnesses such as cardiac or neurologic events, trauma, and infection, and in some who have recently undergone surgery [1].common symptoms are acute massive abdominal distention and pain; however, nausea, vomiting, and constipation are not consistently present [5].ironically, the signs of systemic toxicity do not appear until catastrophic complications have occurred.if left untreated, progressive dilation of the colon can result in colonic mural ischemia or perforation in up to 15% of cases, and the mortality rate is estimated to be 40% [1].it is believed that functional disturbance of colonic motility leads to the development of acpo, but the pathophysiology is not entirely clear.the most common theory used to explain this condition is an autonomic imbalance between the sympathetic and parasympathetic innervation of the colon, resulting in a relative excess of sympathetic over parasympathetic tone and leading to dilation of the colon [1].for patients with severe underlying conditions or those who have undergone recent surgery, the sympathetic reaction might be relatively increased.numerous medications such as anticholinergics, opiates, calcium channel blockers, and psychotropic drugs have also been associated with acpo [1].patients with basic chronic illnesses might be using multiple medications that affect bowel movement; therefore, they are likely to develop acpo.our patient did not have a history of constipation, mental illness, or the use of any medications that could influence bowel movement during the preoperative period; therefore, his background likely did not cause acpo.remifentanil and fentanyl are the only perioperative medications that could be considered potential causes of acpo; however, their effects would have already worn off at the time of re-admission.although intraoperative injuries of the intestine or autonomic nerves during surgery were not identified retrospectively, some perioperative complications must be excluded.according to a review of 15,176 tapp cases, the incidence rate of postoperative bowel obstruction was reported to be 0.06% [6].our patient also developed dilatation of bladder; therefore, it seemed that a wide range of organ hypomyotonia had occurred in the pelvis.the functions of the pelvic organs are innervated by the autonomic plexus, including the superior hypogastric plexus comprising sympathetic nerves, the inferior hypogastric plexus comprising mixed sympathetic and parasympathetic nerves, and the pelvic splanchnic nerves as parasympathetic nerves [10].
number of words= 535
[{'rouge-1': {'f': 0.3813361904469467, 'p': 0.6255555555555556,'r': 0.2742628774422735}, 'rouge-2': {'f': 0.18401276165570804, 'p': 0.2641747572815534,'r': 0.1411743772241993}, 'rouge-l': {'f': 0.3450763908907466, 'p': 0.49857142857142855,'r': 0.26384615384615384}}]
-----------------------------------------------------------------------------------------------------------------------------------
p19:
Extractive Summary:
all patients were divided into high-difficulty group (n = 140, 48.1%) and low-difficulty group (n = 151, 51.9%).among the 291 patients, 180 (61.8%) were male while 111 (38.2%) were female, with a mean age of 52.3 (range 18–76).the mean surgical time was 120.3 min (60–250 min) and mean blood loss was 33.1 ml (5–400 ml).in the low-difficulty group, one patient had cerebrospinal fluid leakage and two patients had dysphagia.whereas in the highdifficulty group, the main complications reported were cerebrospinal fluid leakage in three patients, epidural hematoma in one patient and dysphagia in four patients.establishment of a nomogram for predicting surgical difficulty comparisons between the low-difficulty and high difficulty groups showed that age (p = 0.017), sex (p = 0.006), number of operation levels (p < 0.001), high signal intensity (p < 0.001), opll (p < 0.001) and spinal canal occupational ratio (p < 0.001) significantly correlated with surgical difficulty.the roc curves further showed that the best thresholds for age and spinal canal occupational ratio were 55 (sensitivity: 53.2%; specificity: 68.9%) and 0.45 (sensitivity: 74.2%; specificity: 70.5%).multivariate logistic regression analysis revealed that number of operation levels (or = 5.224, 95%ci = 2.125–12.843, p < 0.001), high signal intensity (or = 4.994, 95%ci = 1.636–15.245, p = 0.005), opll (or = 6.358, 95%ci = 1.932–20.931, p = 0.002) and the spinal canal occupational ratio > 0.45 (or = 3.988, 95%ci = 1.343–11.840, p = 0.013) were independent risk factors for surgical difficulty of anterior cervical surgery (table 2).based on these results, number of operation levels, high signal intensity of spinal cord, opll and spinal canal occupational ratio can be used to establish a nomogram for use in predicting surgical difficulty (fig. 3).evaluation of prediction nomogram the roc curve for the nomogram model showed that the c-index was 0.906, which was an indication of a robust discriminative ability (fig. 4a).the calibration curve demonstrated that the probability of high-difficulty predicted by the nomogram model agreed well with actual practice (fig. 4b).discussion as the population ages, the incidence of degenerative cervical disease is continuously increasing, which is the foremost cause of neurological deficit among adults worldwide [6, 7].there is a paradigm shift in the treatment of degenerative cervical disease from posterior to anterior decompression, due to the direct removal of spinal cord compressions, such as disc herniation, osteophytes, and opll [8–10].however, the anterior approach is associated with potential risks that can cause life-threatening complications in vulnerable organs anterior to the vertebral body, such as trachea, esophagus and blood vessels [3].therefore, cervical spine surgery is associated with higher surgical difficulty than lumbar surgery hence the need to evaluate surgical difficulty and the associated risk factors for optimal performance in acss.from the perspective of developing surgical protocols, assessment of surgical procedure and surgical skill training, a classification system determining the difficulty level is extremely important.moreover, adequate physician– patient communication is necessary for patients with high surgical difficulty.researchers have established several scoring systems for assessing the surgical difficulty.however, a preoperative scoring system evaluating the surgical difficulty of acss has not been reported.therefore, due to lack of a gold standard to estimate for surgical difficulty, surrogate indicators are often used to indirectly reflect the difficulty and these include operative time, intraoperative blood loss and incidence of complication [11–14].high-difficulty acss has been correlated with longer operative time and more intraoperative bleeding.in this study, a number of parameters such as age, sex, bmi, number of operation levels, high signal intensity of spinal cord on t2-weighted images, opll, sagittal cervical circumference, coronal cervical circumference, cervical length, spinal canal occupational ratio, and coagulation function index were evaluated to identify risk factors for high-difficulty acss.previous studies confirm that the elderly, obese patients and male gender are associated with longer operative time [11, 12, 15].however, this study reported that there is no significant difference in terms of age, sex, and bmi between low-difficulty and high-difficulty groups of acss.coagulation function index and platelet count are associated with intraoperative hemorrhage, however, no statistical significance in these factors was reported in our study.to our knowledge, thickness of neck might influence the operative time due to the difficulty of surgical exposure.although surgical time tend to be longer in patients with thick neck, this trend did not reach statistical significance in our study.
number of words= 703
[{'rouge-1': {'f': 0.6049819420110468, 'p': 0.8802981029810297,'r': 0.4608496732026144}, 'rouge-2': {'f': 0.4430587191243899, 'p': 0.6325000000000001,'r': 0.3409424083769634}, 'rouge-l': {'f': 0.5531880448318804, 'p': 0.7366666666666666,'r': 0.4428813559322034}}]
-----------------------------------------------------------------------------------------------------------------------------------
p20:
Extractive Summary:
background hiatal hernia is a common condition often associated with symptomatic gastroesophageal reflux disease (gerd) [1].laparoscopic hiatal hernia repair (lhhr) is now considered to be the gold standard for the management of hiatal hernias, and is associated with a reduced rate of perioperative morbidity and shorter hospital stay compared with the open approach [1, 2].the standard steps include the excision of the sac, a thorough oesophageal mobilisation, primary closure of the hiatus, and a fundoplication [3, 4].oelschlager et al. reported that the recurrence rate after pure suture repair without mesh reinforcement is as high as 59% at 5-year follow-up [4].primary repair of the paraesophageal hiatal hernia is a significant risk factor for recurrence, especially when suturing the pillars of the diaphragm together under tension for the giant hiatal hernia.two randomised trials have demonstrated that a significant reduction in recurrence rates can be achieved by using synthetic mesh for large hiatal hernia repair [5, 6].however, a few synthetic mesh-related complications, such as mesh erosion, stricture and dysphagia, are reported [7, 8].in order to avoid those complications, the surgical technique for reliable mesh fixation has been improved, and the mesh specially designed for hiatal hernia has also been improved.on the other hand, recent studies have reported favourable results from using biological mesh for hiatal hernia repair [9, 10].due to the concern about potential complications related to synthetic mesh, biomaterial was adopted in the repair.however, the long-term results, especially for recurrence, still needs to be investigated further.in our study, a retrospective study was conducted into laparoscopic hiatal hernia repair at our centre to review our experience with biological mesh for crural reinforcement and on mesh fixation with medical glue and suture.methods a retrospective study was conducted for a consecutive series of patients undergoing laparoscopic hiatal herniorrhaphy for symptomatic hiatal hernia between january 2018 and january 2019.preoperative evaluation routinely included endoscopy, ct scan, upper gastrointestinal (ugi) series and oesophageal manometry test, and 24-h ph monitoring.surgical technique one dose of antibiotics was administered at induction.five laparoscopic ports were routinely used.most surgeons currently use suture and some use tacks or staples for fixation.both of these methods have their problems and do not allow strong, uniform, and immediate fixation of the mesh to the crural fibres.having used the ncba glue for mesh fixation in laparoscopic inguinal hernia repair since 2009 at our centre, we noticed that the fixation of the mesh was strong, immediate, and uniform [27].on the other hand, the application of tacks at hiatus has been reported to cause serious complications such as cardiac tamponade and mortality [29].as a result, tacker fixation has been strongly advised against by several surgeons [30].in addition, the sages guidelines for management of hiatal hernia state that care should be taken about the mesh fixation technique.in particular, tacks can breach the aorta or pericardium when applied low on the left crus or anteriorly near the apex of the crura [30].in comparison, medical glue fixation is safe without the risk of penetrating important organs.furthermore, some studies reported that fixation by glue combined with suture is as strong as tacker [28].in conclusion, biological mesh reinforcement of crural closure was safe and effective for repairing large hiatal hernias.medical glue combined with suture can provide solid and secure fixation and can reduce the serious complications caused by fixation.the limitation of this study includes the small number of patients and relatively short follow up period.ideally, a multiple center clinical study with control group could be carried out for further research.conclusions biological mesh fixed with suture and medical glue was safe and effective for repairing large hiatal hernias.of course, a longer follow-up is still needed for determining long-term outcom
number of words= 603
[{'rouge-1': {'f': 0.4239131577224494, 'p': 0.768019801980198,'r': 0.2927488151658768}, 'rouge-2': {'f': 0.26221473745279916, 'p': 0.4381592039800995,'r': 0.18708860759493673}, 'rouge-l': {'f': 0.4049599755830247, 'p': 0.6648275862068966,'r': 0.29115384615384615}}]
-----------------------------------------------------------------------------------------------------------------------------------
p21:
Extractive Summary:
background solitary fibrous tumors (sfts) are uncommon mesenchymal neoplasms that occur most frequently in the pleura and occasionally in the orbit [1].sfts are generally considered benign; however, recurrent and malignant variation have been reported.several studies have reported the highly vascular features of sfts, which is especially prevalent in recurrent sfts.preoperative imaging, including magnetic resonance imaging (mri) and computed tomography angiography (cta), are complex assessment tools that may predict hypervascularity [2, 3].traditional resection via orbitotomy or craniectomy would carry a risk of uncontrolled bleeding and residual tumors.embolization is an alternative approach to minimize intraoperative hemorrhage and improve the gross total removal (gtr) rate.there are few reports of transarterial embolization of orbital sfts; only 4 have been published in english [1, 4–6].in this study, we successfully resected a large, recurrent orbital sft with multiple feeder vessels after transarterial embolization.this is the first chinese case of a successful embolization of 2 main feeder vessels, which also demonstrated a different operative approach to achieve gtr in an orbital sfts without performing an osteotomy.case presentation a 53-year-old chinese woman presented to the hospital with an 8-year history of progressive proptosis of the right orbit.it is associated with progressive reduced of vision, visual field deficits and excessive tearing.she denied diplopia, eye pain or headaches.nine years prior to this presentation, she had undergone surgical resection of a right orbital tumor via anterior orbitotomy, and histopathology confirmed that it was an sft.nine months post operative, patient began to develop symptoms that reflected a slow increase of tumor size.because the tumor was closely adherent to the optic nerve, it was thought that resection might injure the nerve, potentially damaging the patient’s sight.considering her normal best corrected visual acuity, patient was advised to attend regular follow-ups.however 2 years later the patient was no longer heard from until 2018.on examination, there was a large non-pulsetile, noncompressible mass in the right orbit cause the proptosed eye displaced infero-temporal.however, recurrent sfts might be malignant and invasive, and resecting them under limited visualization and in a piecemeal manner may cause uncontrolled bleeding and leave residual tumor tissue.considering the patient’s poor vision and her willingness, we obtained the patient’s written consent to perform an ophthalmectomy to optimize exposure of the lesion with few complications.in this case, embolizing the tumor’s main blood supply caused an immediate cascade of reactions, including intense eye pain, loss of vision, conjunctival edema, and a decrease in heart rate.the eye pain may have been the result of acute ischemia or vasospasm of the tumor, or the cytotoxicity of dimethyl sulfoxide adjuvant used in onyx.conjunctival edema secondary to circulatory disturbance and inflammatory reaction likely raised intraorbital pressure in the eyeball, which further aggravated the pain [13].however, the associations between these phenomena remain to be investigated.loss of light perception could possibly have been the result of embolization of the neoplasm’s main blood supply and arteria ophthalmica.the decrease in the patient’s heart rate and its immediate return to normal levels after resection may be an example of the ‘oculo-cardiac reflex’.heart rate variability before, during, and after the resection requires further investigation.in conclusion, orbital sfts can be hypervascular, especially in recurrent and invasive cases.preoperative cta can be performed to assess the blood supply to the lesion.embolism can be an effective technique to minimize intraoperative hemorrhage.the size and location of the tumor, patient visual function, as well as patients’ intention should be considered when deciding the surgical approach.the surgical techniques described in this study could be a feasible approach for complete resection of orbital sf
number of words= 579
[{'rouge-1': {'f': 0.4232754513051911, 'p': 0.751592039800995,'r': 0.2945901639344263}, 'rouge-2': {'f': 0.203647363872083, 'p': 0.315,'r': 0.15045977011494255}, 'rouge-l': {'f': 0.37777786644853667, 'p': 0.6191803278688526,'r': 0.27180722891566267}}]
-----------------------------------------------------------------------------------------------------------------------------------
p22:
Extractive Summary:
all wounds are documented in the electronic patient chart with the respective measures and the applied dressings.we administer cefuroxime and metronidazole as single shot antibiotics preoperatively or cefuroxime only in cases of upper gi surgery for ssi prevention.surgical technique in our department, all median laparotomies are closed with 2 looped number 1 pds sutures (ethicon, johnson&johnson, norderstedt, germany) in a mass suturing technique.oblique incisions are closed with two separate running pds sutures for anterior and posterior rectus sheath.ba is diagnosed clinically during wound inspection on the surgical ward or occasionally during revision laparotomy.generally, a thorough exploration of the abdominal cavity is performed during revision surgery to check for intra-abdominal pathologies or other underlying causes for ba.depending on the fascial quality, the fascial defect will be closed either with continuous looped number 1 pds or interrupted number 2 vicryl sutures (ethicon, johnson&johnson, norderstedt, germany) according to the surgeon’s discretion.if the fascial quality seems insufficient for primary fascial closure or would be closed with tension, a vicryl mesh can be placed in ipom position before the fascia is closed.if closure of the fascial defect is impossible due to extensive fascial necrosis or intestinal edema, a vicryl mesh will be placed in the inlay position to prevent evisceration and, subsequently, a v.a.c.dressing (kci, acelity, wiesbaden, germany) will be installed to create a laparostomy.clinical parameters of all patients, the following patient inherent and procedure specific parameters were assembled in our database: primary procedure, main diagnosis, comorbidities and data of their postoperative course, including general complications and length of stay (los).additionally, we recorded data about the operative setting, such as emergent or elective procedure, type of laparotomy, surgical approach (laparoscopic or open), duration of primary surgery (dos), wound contamination class according to cdc and postoperative icu stay.albumin levels at the time of ssi development were recorded if available.we collected further information for the patients who developed a ba: condition of abdominal fascia as reason for ba, total number of revisions, type of ba closure, implantation of mesh and its respective position within the abdominal wall (intraperitoneal onlay mesh = ipom, fascial bridging = inlay), recurrence of ba, postoperative enteroatmospheric fistula and if skin closure was possible during the course of treatment.the data on reason for ba and technical features of ba closure as well as the presence of intraabdominal abscesses were taken from the operation report of the ba closure procedure.the reason for ba is classified as technical with unimpaired fascial quality (i.e. breaking of suture material), superficial infection (if a subcutaneous wound infection existed with simultaneous fascial dehiscence) or fascial necrosis (if fascia mainly was necrotic with a vital non-infected subcuticular layer).statistical analysis we calculated frequencies of all dichotomous variables.for continuous variables we tested for normal distribution and calculated means with standard deviation (± sd) or median with interquartile range (iqr).in the univariate analysis, the unpaired t-test or mann– whitney-u-test was applied when testing for variance of continuous variables as well as the chi2- test or exact fisher test for dichotomous or categorical variables.significance levels were set at p = 0.05.we used the binary logistic regression model with backward stepwise selection to test for specific risk factors of ba.we included all factors from the univariate analysis that had reached a p-value of ≤ 0.1 and considered results statistically significant in the multivariate analysis if the p-value was ≤ 0.05.all risk factors are expressed as odds ratio (or) with their respective 95% confidence interval (95% ci).for data acquisition, we used excel 2016 (microsoft, münchen, germany) and spss 25 for data analysis (ibm statistics, ehningen, germany).the study is reported adhering to the stroccss 2019 statement on reporting of cohort studies in surgery [24].results patients’ characteristics in total, 504 patients with abdominal surgical site infections were enrolled in the study between 2015 and 2018.there were 207 (41.1%) females and 297 (58.9%) males.of those, a total of 111 patients had ba, of which 69.4% were male, compared to 56% males in the superficial surgical site infections group (sssi, p = 0.011).the demographic and procedure specific data of the whole cohort and the respective subgroups (ba group and sssi group) are shown in tables 1 and 2, whereas table 3 displays the relevant postoperative complications of those groups.all in all, patients with abdominal fascial dehiscence stayed approximately 6.5 days longer in hospital compared to the sssi group (32.50 days (iqr 18) and 26.00 days (iqr 22); p < 0.001).univariate analysis of risk factors for burst abdomen (ba) there was no difference between ba group and sssi group with regard to wound contamination class (p = 0.713), duration of surgery (0.745), and surgical approach (p = 0.524).but most of the ssi occurred after open surgery (> 90%).sex, intestinal resections, emergency setting and liver cirrhosis reached the significance level in the univariate analysis und have been entered into multivariate analysis (tables 1 and 2).chronic inflammatory disease as risk factor did not reach significance levels (p = 0.065) but will be considered for further analysis according to our criteria mentioned above.patients with burst abdomen suffered more often from acute renal failure (p = 0.01), delirium (p = 0.01) and had higher mortality rates (9.9% vs 3.3%; p = 0.004).additionally, the plasma albumin levels at time of ssi were lower in the ba group compared to the sssi group (p = 0.002).multivariate analysis of risk factors for ba the multivariate analysis revealed the following parameters as significant risk factors for ba: intestinal resection (or 4.006; 2.456–6.535, p < 0.001), liver cirrhosis (or 2.568; 1.355–4.866, p = 0.004) and emergency surgery (or 1.658; 1.050–2.617, p = 0.03).sex turned out to be an important risk factor as well but failed to reach significance levels (p = 0.08).adding the postoperative complications to the regression model (acute renal failure, delirium and bleeding) eliminated emergency surgery from the regression model and included delirium (or 5.058; 1.349–18.965, p = 0.016).furthermore, in this newly calculated regression model, the odds ratios of the previously considered risk factors for the development of an abdominal fascial dehiscence also increased, especially for intestinal resection (or 172.510; 22.195–1340.796, p < 0.001) and to a lesser extent for liver cirrhosis (or 4.788; 2.034–11.269, p < 0.001).albumin levels at time of ssi were a significant risk factor (or 0.911; 0.861–0.986, p = 0.001).as this parameter was only available in 270 patients, we disregarded it in the final analysis, although it hints at low albumin levels as risk factor for ba.characteristics of patients with burst abdomen (ba group) of all burst abdomen patients (n = 111), 110 (99.1%) had an operative revision.one patient (0.9%) was treated conservatively, which was due to a palliative situation and a limited fascial dehiscence.abdominal fascial dehiscence was mostly caused by superficial infections (40.7%), fascial necrosis (20.4%) or technical issues (13.9%).an intrabdominal abscess was present in 12.7% of the revision laparotomies.technical details of ba closure can be found in table 4.during ba repair, complete fascial closure was achieved in 69.4%, in the rest of the cases no fascial closure was possible and subsequently an absorbable mesh was implanted in inlay position for fascial bridging.no significant tendency could be found as preferred method of fascial closure (type of suture with or without ipom mesh, p = 0.381).two patients developed enteroatmospheric fistula, one after ipom mesh implantation, the other after complete fascial closure.univariate analysis of risk factors of burst abdomen recurrence (bar) for the evaluation of burst abdomen recurrence (bar), two additional patients had to be excluded from the analysis as they were reoperated on for other reasons than recurrent abdominal fascial dehiscence.the patient comorbidities and post-operative complications for the comparison of ba and bar groups can be found in tables 4 and 5.
number of words= 1268
[{'rouge-1': {'f': 0.3571574570838258, 'p': 0.8148979591836736,'r': 0.22869565217391305}, 'rouge-2': {'f': 0.21293489605905078, 'p': 0.41470989761092153,'r': 0.1432414793328499}, 'rouge-l': {'f': 0.3828579103396007, 'p': 0.7208875739644971,'r': 0.2606412478336222}}]
-----------------------------------------------------------------------------------------------------------------------------------
p23:
Extractive Summary:
background congenital scoliosis is caused by spine abnormalities that lead to imbalanced growth.a frequent cause is a single hemivertebra as a unilateral failure of formation.because of local deformity and asymmetric loads, unaffected neighboring vertebrae subsequently show asymmetric growth.this congenital anomaly, apart from causing adverse effects on cardiopulmonary function, also causes psychological distress [1].nonsurgical treatment for hemivertebra is seldom successful; 75% of the curves are progressive and only 5–10% can be treated with bracing [2].early surgical intervention can mitigate deformity progression and psychological stress [3].surgical treatment modalities include in situ posterior spinal fusion, combined anterior and posterior in situ spinal fusion, hemivertebra resection [4], and guided growth procedures such as growing rods [5], vertical expandable prosthetic titanium rib (veptr, synthes spine) [6], and shilla [7] procedures.posterior thoracolumbar hemivertebra resection and short-segment fusion provides satisfactory deformity correction and rigid stabilization, with markedly less surgical time and fewer complications than other procedures [8].the first long-term follow-up of postsurgical outcomes for this treatment was reported by chang et al. in 2015 [9], for 18 congenital scoliosis patients with a mean follow-up of 11.4 years; however, the hemivertebrae location was not reported.overall, long-term follow-up data are limited for this surgery.thoracolumbar region was the transition area between fixed thoracic segment and relatively motional lumbar segment, where facet joints change from coronal to sagittal plane and the spine alignment change from lordotic to kyphotic form.therefore, this study aimed to determine surgical outcomes of posterior thoracolumbar hemivertebrae resection and short-segment fusion with pedicle screw fixation for treatment of congenital scoliosis with a minimum of 5-year follow-up.we believe that our study of the clinical efficacy of thoracolumbar hemivertebra resection and short segment fixation is of guiding significance for clinical treatment.methods this retrospective study analyzed 27 consecutive patients with congenital scoliosis secondary to thoracolumbar hemivertebra who underwent posterior thoracolumbar single hemivertebra resection; transpedicular, shortsegment fixation; and fusion from january 2007 to january 2015 with over 5-year follow-up.the corresponding author performed all surgeries.all patients were advised regarding possible surgical outcomes and signed written informed consent before surgery.the institutional review board of the authors’ hospital approved this study.inclusion criteria were (1) congenital spinal deformity caused by a single hemivertebra requiring surgical treatment (curve magnitude: > 25° and/or with fast progression, defined as documented progression of the curve of > 5° in 6-month follow-up and/or failure of conservative treatment); (2) hemivertebra located in thoracolumbar (t–l) region (t11–l2); (3) short-segment fixation and fusion (≤ 6 levels); and (4) > 5 years of radiographic follow-up after initial surgery.exclusion criteria were (1) anterior approach, (2) operative approach using hooks as anchors, (3) > 1 hemivertebra or in combination with other congenital spinal deformities, (4) congenital kyphosis without scoliosis, and (5) previous spinal surgery.standard standing long-cassette anterior–posterior and lateral radiographs were performed preoperatively, postoperatively, at 2-year follow-up, and at final follow-up (minimum of 5 years postoperatively) (fig. 1).operative reports and medical records were reviewed for operative and demographic data and any complications.radiographic assessment whole-spine radiographs were reviewed by an independent observer to measure deformity.curves measured in the coronal plane (figs. 2 and 3) included segmental scoliosis, total main scoliosis, compensatory cranial curve, compensatory caudal curve, trunk shift, and shoulder balance.segmental scoliosis was measured from the upper vertebral endplate above the hemivertebra to the lower endplate below the hemivertebra.total main scoliosis was the maximum scoliosis angle between the two most tilted vertebrae.compensatory cranial curve and the compensatory caudal curve were also measured.trunk shift was measured as the perpendicular distance from the sacrum center to the plumb line drawn from the midpoint of the seventh cervical vertebra (c7).shoulder balance was evaluated using radiographic shoulder height, defined as the difference in soft tissue shadows directly superior to acromioclavicular joints on both sides, designated positive (+) when the left shoulder was higher and negative (−) when the right shoulder was higher.segmental kyphosis (sk) and sagittal balance were measured in the sagittal plane (fig. 4).sk was measured from the upper endplate above the hemivertebra to the lower endplate below the hemivertebra.sagittal balance was measured as the distance from the c7 plumb line to a perpendicular line drawn from the posterosuperior corner of the sacrum, designated as positive (+) when the plumb line was anterior and negative (−) when posterior to the posterosuperior corner of the sacrum [10].in 2015, chang et al. first reported the adding-on phenomenon after hemivertebra resection, in which 11.1% of patients (two of 18) showed distal adding-on phenomenon [9].subsequently, in 2016, they reported 11.1% of patients (five of 45) showed distal adding-on phenomenon after hemivertebra resection [28].in the present study, the incidence of adding-on was 14.8% (four of 27 patients), which was slightly higher than reported in these previous studies.the cause of the distal adding-on phenomenon is unclear.it may be a compensatory change for maintaining a well-balanced spine such as a balanced shoulder level, truncal shift, or listing.for patients with thoracolumbar hemivertebra after hemivertebra resection and short-segment fusion, the high mobility of the lumbar region makes this compensatory change more likely to occur.the goal in the treatment of congenital scoliosis is to achieve a straight spine with a physiologic sagittal profile with as short a fusion segment as possible.delayed treatment of an advanced deformity in older children or adults, however, must include the secondary structural curves and therefore requires long fusion segments.furthermore, correction of these rigid curves is more difficult and associated with a higher risk of neurologic compromise.progression of the curve is most rapid during the adolescent growth spurt, and progression stops only at skeletal maturity.spontaneous neurologic deterioration due to compression of the spinal cord may occur in the case of congenital kyphoscoliosis, which generally manifests during the adolescent growth spurt at a mean age of 13.7 years [29].early and complete correction of the local deformity mitigates the development of secondary changes.delayed treatment of an advanced deformity always requires an extensive and complex correction procedure.to date, however, there is no agreement regarding the appropriate surgical window, despite the fact that most clinicians suggest surgery as early as possible after 1 year of age.however, patients with congenital scoliosis who visited our spine center for the first time are often over 7 years old.for these patients, after the diagnosis of scoliosis, we performed the surgical treatment [30].although they are older, they were satisfied with the clinical efficacy.in this study, eight patients were younger (age < 7 years) and 19 patients were older (age > 7 years).this study has several limitations.follow-up did not include quality of life.further trials are needed to analyze a correlation between radiological correction and clinical outcome (scoliosis research society scores-24, oswestry disability index).all radiologic measurements were performed by one independent radiologist; although this design may introduce single observation error, the results were consistent with values in the literature.this study reviewed only postoperative complications, and the small number of study patients may overestimate the rate of complications.
number of words= 1129
[{'rouge-1': {'f': 0.36910693751905843, 'p': 0.7163414634146341,'r': 0.24860151642796968}, 'rouge-2': {'f': 0.19684718228001688, 'p': 0.32688073394495415,'r': 0.14082630691399664}, 'rouge-l': {'f': 0.3406875585705375, 'p': 0.6143037974683545,'r': 0.23570327552986514}}]
-----------------------------------------------------------------------------------------------------------------------------------
p24:
Extractive Summary:
between two endoprostheses, the modular endoprosthesis is regarded as the prior solution to provide good initial stability and relatively rapid restoration of function by some surgeons since the early 1970s [2, 8–11].recently, with advances in imaging and systemic therapies, patients survive longer from malignancies after hemipelvic limb-salvage surgery [12–14].as a result, mechanical complications, caused by poor osseointegration, persistent micromotion, and defective bodyweight transmission, have become a major problem and require a proper solution [10–12].the treatments for aseptic loosening and fracture are seldom described.so far, reported therapeutic options include a conservative method, partial retrieval of the endoprosthesis, total retrieval of the endoprosthesis followed by flail hip, and total retrieval of endoprosthesis followed by revision [2, 10, 11, 15].the conservative method is a mainstream treatment but failed endoprosthesis can gradually destruct host bone during daily activities [15].therefore, surgical intervention is ultimately inevitable for some patients.partial retrieval of the endoprosthesis is suitable for endoprosthesis fracture without instability, such as retrieving fractured parts for pubic plate fracture [2, 10, 11].loosening with instability should consider the total retrieval of the endoprosthesis, and the residual bone defect requires appropriate reconstruction [10, 11, 16].indeed, the flail hip can restore partial lower-limb function, but limb length discrepancy and extended immobilization duration are unacceptable for some patients.theoretically, revision with a proper endoprosthesis is another potential method by offering immediate support to restore lower-limb function.as two standard options in primary hemipelvic reconstruction, the modular hemipelvic endoprosthesis and the 3d-printed custom-made hemipelvic endoprosthesis vary in the compatibility to host bone.the modular endoprosthesis is pre-manufactured with a component design, allowing intraoperative adjustment according to diversified bone defects; whereas the matching degree is usually sacrificed when connecting modular hemipelvic endoprosthesis with uniform appearance to host pelvic bone with an irregular shape [2, 11, 12].in contrast, a 3d-printed custom-made hemipelvic endoprosthesis is individually designed based on a patient’s data [17].the compatibility can be ensured with advanced design software and precise manufacturing techniques.additionally, a 3d-printed custom-made hemipelvic endoprosthesis can be fabricated as one component to reduce the junction part’s possibly mechanical failure.therefore, the 3d-printed custom-made hemipelvic endoprosthesis is considered more proper for complex hemipelvic revision.as far as we know, the application of 3d-printed custom-made hemipelvic endoprosthesis in revision is not sufficiently represented in the literature.such workflow is highly demanding due to the destruction and migration of host bone, deformation of the medullary cavity, scar tissue generation, and potential failure during device retrieval.we recently designed a series of 3d-printed custommade hemipelvic endoprosthesis for patients with aseptic loosening and fracture of modular hemipelvic endoprosthesis, and a satisfactory clinical outcome was observed.the aims of this pilot study were (1) to assess the efficacy of 3d-printed custom-made hemipelvic endoprosthesis in restoring natural location of acetabulum for normal bodyweight transmission; (2) to evaluate the short-term function of the revision with this endoprosthesis and (3) to identify short-term complications associated with the use of this endoprosthesis.methods patients between february 2017 and december 2017, seven patients with aseptic loosening and fracture of modular hemipelvic endoprosthesis (chunli co., ltd., tongzhou, beijing, china) received total retrieval of modular hemipelvic endoprosthesis and revision with 3d-printed custom-made hemipelvic endoprosthesis.the indications for revision with 3d-printed custom-made endoprosthesis were (1) aseptic loosening with instability; (2) aseptic loosening with severe pain; (3) migrated endoprosthesis endangering essential structures around the endoprosthesis; (4) willing to take the potential risks of the 3d-printed custom-made endoprosthesis.there were four males and three females with a median age of 49 years (range 25–60).totally, five patients had type i + ii resection, and two patients had type i + ii + iii resection, and all patients received en bloc resection and reconstruction with modular hemipelvic endoprosthesis and construct femoral head autografting as their primary treatment [1].after primary surgery, all patients underwent plain radiograph (pr) of the pelvis before discharge and monthly in the first three months and trimonthly after that (fig. 1a, b).3d-computerized tomography (3d-ct) of the pelvis was performed before discharge and yearly after the operation (table 1).initial diagnoses were solitary plasmacytoma in three patients, chondrosarcoma in one, solitary metastatic squamous carcinoma in one, alveolar soft tissue sarcoma in one, and osteosarcoma in one.patients with solitary plasmacytoma received chemotherapy (melphalan, prednisone) for five circles after primary surgery.pulmonary metastasis occurred in patient 4 with alveolar soft tissue sarcoma before the primary surgery and received targeted therapy regularly after primary surgery with apatinib (500 mg, q.d.).the patient with osteosarcoma received chemotherapy (doxorubicin, cisplatin) for nine circles after primary surgery.the median follow-up duration between primary and revision surgery was 36 months (range 16–73), and the median follow-up duration from revision to latest follow- up was 29 months (range 24–34).six patients were no evidence of disease, and one patient was alive with disease.no patient was lost to follow-up (table 1).this study was approved by the ethical committee of our institution.written informed consent to participate in this study was obtained from all the patients.endoprosthesis design and fabrication before endoprosthesis design, pr and 3d-ct with metal artifact reduction technique of the pelvis were performed (fig. 1c, d).the ct data were used to build virtual 3d models in the mimics v20.0 software (materialise corp., leuven, belgium) after assessing and reducing the remaining artifact.consequently, the pain relief during follow-up, the good walking ability, the stable endoprosthesis without migration, and the radiographic images demonstrated ideal osseointegration.as to preventing endoprosthesis fracture, (1) the reconstruction of the intact pelvic ring improved mechanic distribution around the endoprosthesis; (2) the endoprosthesis strength was reinforced by continuous solid structure inside the endoprosthesis.finally, a porous structure can disrupt the sciatic nerve; therefore, we polished the sciatic foramen of the endoprosthesis.this study also had limitations.firstly, our follow-up is short; unknown drawbacks might occur in long-term follow-up.secondly, the retrospective, non-comparative design and the small number of patients limited the power of this series.hence, a more extensive multi-institutional study is needed to compare this approach with other solutions, such as flail hip, hip transposition, and updated modular hemipelvic endoprosthesis.the revision surgery and appropriate rehabilitation program improved patients’ function to a median msts score of 22 and pain-free ambulation.the incidence of the complications was low via this individualized workfl
number of words= 1005
[{'rouge-1': {'f': 0.36106769925299087, 'p': 0.7728112449799196,'r': 0.2355629139072848}, 'rouge-2': {'f': 0.27582329317269083, 'p': 0.5538709677419356,'r': 0.18363636363636365}, 'rouge-l': {'f': 0.34540651551553125, 'p': 0.6515602836879433,'r': 0.23498993963782697}}]
-----------------------------------------------------------------------------------------------------------------------------------
p25:
Extractive Summary:
background baclofen has been clinically available for over 60 years [1].recent experiments reported by ourselves [5] which assessed the effects of commonly employed adjuvant agents on peripheral nerve stimulation-related neuromodulation identified that baclofen may have an analgesic effect on bladder nociception.it is known to produce analgesia by itself as measured in rats and mice in the hot plate, tail flick and acetic acid-induced writhing [7–21].in humans, the spinal administration of baclofen has also demonstrated short-term analgesic effects on spinal cord injury-related pain and post-stroke pain [24] and provides analgesia following surgery, in most cases by augmenting opioid effects [25–27].despite these encouraging basic science and clinical reports, baclofen has not been used frequently for non-neuropathic pain in humans.in rat models, baclofen dose-dependently reduced heroin- seeking behavior [28, 29], reversed behavioral sensitization to morphine [30], prevented reinstatement of heroin-seeking behaviors [28] and enhanced extinction of opiate and methamphetamine-induced conditioned place preference [31, 32].the following studies were therefore performed to test the potential utility of baclofen as an analgesic agent in the treatment of bladder pain and more generally, to enhance the database for alternatives to the use of opioids in the treatment of chronic pain.experimental procedures overview in the present experiments, we examined the antinociceptive effects of both systemic and spinal administration of baclofen.female rats were exclusively employed for practical reasons (difficult to cannulate male rats’ urethras) but can be justified by the observation that vmrs and cardiovascular responses to urinary bladder distension in female rats are more reliable and robust than in male rats [33] and chronic bladder pains have a high female prevalence in humans [34].intravesical zymosan treatment has been demonstrated to produce a robust bladder inflammation and hypersensitivity to ubd [36].neonatal bladder inflammation (nbi) some groups of female rat pups were given three daily neonatal treatments of zymosan beginning on postnatal day 14 (p14–16).in one group, a 1% zymosan (0.1 ml) solution was instilled intravesically via a 24 gauge angiocatheter placed through the urethra and allowed to dwell for 30 min.the bladder then was drained, the catheter removed, and the rat permitted to recover.the control group only received anesthesia and no catheterization or zymosan treatment.all rats were kept on a warmed heating pad during treatments and received ampicillin (0.05 mg in 0.05 ml s.c.) at the end of each treatment before being returned to their dams.intrathecal catheters in experiments in which selective spinal action of drugs was studied a 7.8 cm catheter made of pe10 tubing was inserted via an incision in the atlanto-occipital membrane following surgical exposure and threaded down through the subarachnoid space to the lumbosacral region under deep isoflurane/oxygen anesthesia.in most cases, the catheter was used immediately in non-survival experiments (as per sections “protocol for spinal baclofen cumulative dosing experiments” and “protocol for assessing effects on hemodynamic responses to ubd”), but in 13 rats the catheter was attached to an alzet 2001 osmotic minipump (durect corp, cupertino, ca, usa) allowing for a 7-day infusion of either normal saline or a baclofen solution as described in section “dorsal horn spinal neuronal responses to ubd”.protocol for systemic baclofen cumulative dosing experiments repeated 60 mmhg ubds were administered with a 3 min inter-trial interval until stable vmrs were established.graded, constant-pressure air distensions of the urinary bladder (20 s duration; 3 min inter-trial interval) of ascending pressures at intervals of 10, 20, 30, 40, 50 and 60 mmhg were then administered to quantify the graded stimulus–response.protocol for assessing effects on hemodynamic responses to ubd in these rats, a single 40 ng dose of it baclofen (10 μl injection; 10 μl normal saline flush) or equal volume of it normal saline was administered and responses to repeated ubds (60 mm hg, 20 s, 3 min intervals) measures, allowing for assessment of the time course of the baclofen effect.the it saline-treated rats served as a control for repeated measures.fifteen minutes after the injection repeat graded ubd stimulus–response measures were obtained.the other half of the rats received a normal saline infusion at the same rate.the day before testing, all rats were re-anesthetized with isoflurane, adult bladder inflammation induced (section “adult bladder inflammation (abi)”) and their it catheters externalized and minipumps removed.to quantify neuronal responses, units were displayed oscillographically for continuous monitoring, discriminated conventionally from background, converted into uniform pulses and counted and saved by computer.because responses of different neurons to the same distending stimulus naturally vary in maximal response and total number of unit discharges, each unit’s response was normalized to that produced by the 60 mm hg response for purposes of within- and between-group comparisons.all of the rats studied in this protocol had also experienced neonatal bladder inflammation and adult bladder inflammation as per sections “adult bladder inflammation (abi)” and “reflex responses to urinary bladder distension (ubd)”.area-under-the-curve (auc) statistics were generated as measures of global responses to graded ubd and reported as a percentage of mean pre-drug measures.paired t-tests of pre/post measures were used for comparisons when appropriate.results ip baclofen produced dose‑dependent inhibition of vmrs to ubd the upper portion of table 1 presents mean area-underthe- curve (auc) measures normalized as a percentage of the pre-drug measures.figure 2a shows the analgesic effect of 40 ng of it baclofen on pressor responses evoked by a 60 mmhg ubd stimulus presented every 3 min when compared with it saline-treated controls.there was no significant quantitative or qualitative difference in the effects of the different drugs on different rat bladder inflammation pretreatment groups (i.e., nbi plus abi group vs. no nbi plus abi group vs. no nbi-no abi group).it baclofen still effective after chronic infusion baclofen or saline were infused continuously for 7 days via implanted osmotic minipumps to evaluate whether tolerance to the effects of baclofen would be apparent.specifically, the baclofen-treated group consisted of eight class 2 and two class 3 neurons and the saline-treated group consisted of seven class 2 and three class 3 neurons.using this classification scheme, both the baclofen-treated group and the saline-treated group consisted of five type i and five type ii neurons.baclofen had similar inhibitory effects on both the type i and type ii neuronal subgroups with no statistically significant differences noted.none of the rats studied had experienced bladder inflammation.discussion the most important finding of the present study was that the non-opioid agent, baclofen, inhibited responses to ubd in a variety of models of bladder nociception.these actions included spinal sites of action and the activation of gabab receptors.given its long history of clinical use and low toxicity, a trial in humans for the treatment of bladder pain would seem an appropriate next step [41].it is thought that this model may be particularly relevant to the disorder interstitial cystitis/bladder pain syndrome [34] in that it is associated with multiple features of ic/bps including the presence of increased micturition rates, a functionally small capacity hypersensitive bladder, altered bladder neurochemistry, the presence of vascular fragility of submuscosal tissues following prolonged hydrodistension, the presence of increased pelvic floor muscular tone, increased responsiveness to acute stress and increased responsiveness to intravesical cold and potassium-containing fluids [37, 42].indeed, targeting pelvic floor hypertonicity associated with chronic pelvic pain syndromes has been the strategy of mainstay therapies such as myofascial physical therapy [46–49] and new clinical research involving local injections of botulinum toxin with and without physical therapy [50–52].inhibitory effects of baclofen, administered either systemically or spinally, on urodynamic measures was the consistent observation.given that all dorsal horn neurons excited by ubd were robustly inhibited, this result should not be surprising as these neurons are likely important in both nociceptive and non-nociceptive bladder sensory functions.as noted previously, baclofen has been demonstrated to have significant benefits in association with reduction of addictive behaviors.the presumed mechanisms for its actions on addictive behaviors is an interaction with dopaminergic neurons of the ventral tegmental area [77] with a subsequent reduction in dopamine release in the nucleus accumbens.as such, it is notable that fadda et al [78] observed that baclofen blocked morphine-induced dopamine release at the nucleus accumbens.studies in opioid-tolerant rats could also assess whether there could be particular clinical benefits in subjects already treated with opioids prior to a trial of baclofen.in determining the potential analgesic benefit of baclofen for human use, it is important to consider the most efficacious route of administration that produces strong analgesia with the fewest side effects or toxicities.for example, odd reactions to systemic baclofen include things such as the induction of hiccup-like respirations [85] or diabetes insipidus [86].in rats, intraventricular baclofen impaired memory [87].this suggests that the most beneficial route of administration in humans may be spinal delivery because it is primarily restricted to the spinal segment to which it is administered and could potentially avoid issues related to cns effects and toxicity due to high dosing.that said, a limitation of the present study is that the main endpoints in most of the study were vmrs, a motor reflex.as a consequence, it must be considered whether baclofen, administered spinally, may have also been acting on motoneurons thereby increasing its potency in that assay.spinal administration would also likely miss some of the potential behavioral benefits (e.g. addiction prevention) associated with systemic baclofen use.the precise mechanisms of baclofen’s antinociceptive actions are not known.functional magnetic resonance imaging using continuous arterial spin labelling of the brain in humans before and after 21 days of systemic baclofen treatment (20 p.o.conclusions in summary, the compound baclofen, a drug currently approved for clinical use, which can be administered systemically or spinally, was demonstrated to have antinociceptive effects in animal models of bladder pain.
number of words= 1570
[{'rouge-1': {'f': 0.2785754073470166, 'p': 0.8211737089201878,'r': 0.16773976786805134}, 'rouge-2': {'f': 0.1638405062630585, 'p': 0.3530188679245283,'r': 0.10667481662591688}, 'rouge-l': {'f': 0.26610025182875646, 'p': 0.6459999999999999,'r': 0.1675609756097561}}]
-----------------------------------------------------------------------------------------------------------------------------------
p26:
Extractive Summary:
we then enclosed and passed referral letters if the results were judged as positive.this study was approved by the ethical review board of kochi medical school (ethical approval no. 16–12).written informed consent was obtained regarding pc screening using the psa spot test.all methods were carried out in accordance with relevant guidelines and regulations.statistical analysis all statistical analyses were performed using jmp® software (sas institute inc., cary, nc, usa).a p value < 0.05 was considered to indicate a statistically significant difference.the sensitivity, specificity, positive predictive value (ppv), negative predictive value (npv), and accuracy of the psa spot test were calculated.a box-and-whisker plot was used to show the correlation between the psa quantitative value and intensity of the color reaction band on the psa spot test.pearson’s correlation test was used to correlate the two variables described above.results in the validation series using serum samples from 54 patients suspected of having or followed up for pc, the sensitivity and specificity of the psa spot test were 9.1 and 96.8%, respectively, at a reading time of 15 min and 90.9 and 93.5%, respectively, at 30 min.of 1429 participants, 1223 (85.6%) had a psa value of less than 4 ng/ml (median psa, 1.270 ng/ml; range, 0.001–3.972 ng/ml), and 206 (14.4%) presented a psa value higher than 4 ng/ml (median psa, 6.304 ng/ml; range, 4.018–237.518 ng/ml).a total of 164 (11.5%) participants had a psa value between 4 and 10 ng/ml.the results of the test were affected by variations in the reading time.the sensitivity was very low at a reading time of 15 min (41.7%) but increased at a reading time of 30 min (79.9%), whereas the specificity of the test was similar (98.7% at 15 min, 93.0% at 30 min).thus, we chose 30 min as the optimal reading time for further analyses.the sensitivity, specificity, ppv, npv, and accuracy of the test were 79.9, 93.0, 65.4, 96.6, and 91.2%, respectively.among the 1151 participants with a psa value < 4 ng/ml, 1071 were correctly interpreted as negative using the psa spot test, whereas 80 were interpreted as positive, resulting in a test specificity of 93.0%.of the 80 false-positive results, 39 (48.8%) were in the psa range of 3–4 ng/ml (table 1).the specificity of the test in the psa range of 3–4 ng/ml was 63.9% (69/108).of 151 true positive results, 112 exhibited a psa value between 4 and 10 ng/ml; 34/112 (30.4%) were interpreted as weakly positive by the psa spot test, whereas 78/112 (69.6%) were judged as strongly positive (table 1).a total of 40 participants presented a psa value > 10 ng/ ml; 1/40 (2.5%) was judged as weakly positive, whereas 38/40 (95.0%) were judged as strongly positive.this has resulted in a spike in pc incidence rates, as previously undetectable cases of pc were unmasked.although mass screening for pc remains one of the most controversial issues in oncology, two large, high-quality rcts were carried out to evaluate psa screening.
number of words= 481
[{'rouge-1': {'f': 0.44075622968048644, 'p': 0.712512077294686,'r': 0.319063670411985}, 'rouge-2': {'f': 0.2425590889024801, 'p': 0.3612621359223301,'r': 0.18257035647279551}, 'rouge-l': {'f': 0.38178512372613443, 'p': 0.5345669291338583,'r': 0.29692307692307696}}]
-----------------------------------------------------------------------------------------------------------------------------------
p27:
Extractive Summary:
utuc describes a cancer originating in the renal pelvis, renal calices, and the ureter.the incidence of utuc has been estimated at 1–2 cases per 100,000 [8], although this varies between age, geographical region, occupation, and other factors.three databases were searched for this systematic review: medline, embase, and the web of science core collection.no limitations on dates were applied, and the final search was done on 4th february 2021.observational studies (cross-sectional and cohort) describing the incidence and prevalence of de-novo utuc in adults (≥ 18 years of age) were included.studies that reported both lower and upper urinary tract carcinomas were only included if data for utuc was separately reported.studies were excluded if no full text was available, or if the abstracts were in any other language than english.the reference lists of the papers selected were manually searched for further relevant studies to include in the data extraction.the following data were extracted: study title, study author(s), year of publication, journal, country, aim of study, population, setting, study design, study duration, outcomes measured, diagnostic criteria used, ethical approval, methods of data analysis, incidence (and 95% confidence interval), and prevalence (and 95% confidence interval).the data from the selected studies was found to be heterogeneous and hence a meta-analysis could not be conducted.results the results of the database searches, title and abstract screening, and full-text screening are outlined in fig. 1.after title, abstract, and full-text screening, 117 titles were excluded for a number of reasons, and 59 papers were included for this review (table 1).the studies selected included data ranging from the years 1943 to 2018.europe had the greatest number of studies (n = 26, 44%), followed by asia (n = 15, 25%), north america (n = 14, 24%), australia (three studies), and one worldwide study.of the studies from europe, 14 (54%) were conducted in the nordic countries of denmark, finland, iceland, norway, or sweden.of the studies from asia, seven (47%) were conducted in taiwan.the studies selected for this review included a wide variety of cancer registries and patient populations.eleven studies (19%) used data from the seer (surveillance, epidemiology, and end results) registry in the usa.in total, eight papers required data extraction using webplotdigitizer (additional file 2).the measures used to report incidence varied widely between different studies.other measures used in other studies include incidence density per 10,000 person-years and standardised incidence ratios.overall, there appeared to be a higher incidence of utuc with increasing age.this was seen in both males and females.incidence by sexthe calculation of age-standardised rates (asrs) varied according to what population was used in standardisation.in the 13 studies that reported on incidence of utuc according to sex [14, 19–24, 29–34], the asr values were adjusted based on different populations, including a european population [14], a standard world population [21], the world health organisation (who) standard population [32], the 1995 south korean population [22], and population data from the australian bureau of statistics [34].the comparative incidence between males and females was not clear from the studies.the study reported sir values for, among others, black americans, white americans, jewish israelis, and chinese singaporeans.the highest sir for renal pelvis cancers in this study was seen in white americans (0.79 per 100,000 person-years for males).the opposite was seen in ureter cancers, with white americans having a higher sir than black americans (1.49 [1.18–1.86] vs. 1.32 [0.16–4.78], respectively).the study by mathew et al. [37] covered 10 regions for both males and females, and a further five regions for males.this study used data from the cancer incidence in five continents resource and found that the region with the highest sir of renal pelvis cancers between 1973 and 1992 was the bas-rhin region in eastern france, with an sir of 15.5 per 100,000 person-years in males.differences between regions within countries were also studied.for three of these studies, more accurate numbers were obtained by contacting the study authors.incidence by lynch syndrome our search of the literature found only one epidemiological study on the incidence of utuc in patients with lynch syndrome.patients were categorised according to familial risk groups, as well as the type of malignancy present in the parent.the overall sir of utuc in all patients was 1.2 (95% ci 0.9–1.5).the highest incidence of utuc was seen in patients whose parent and sibling had an hnpcc-associated cancer, one of whom was under 50 years old when diagnosed—the sir in this group was 29.6 (8.1–75.9).mok et al. [12] reported a higher incidence of ureter cancer in patients with a low estimated glomerular filtration rate (egfr) of < 45 (13.7 per 10,000 personyears) compared to patients with a higher egfr of ≥ 90 (0.3).in total, the estimated complete prevalence for epithelial tumours of the renal pelvis and ureter was 101.0 (se 1.5) per million persons.this was nearly double that of epithelial tumours of the penis, which was the next most prevalent cancer at 55.4 (se 1.1) per million persons.the trend in incidence seen in other factors—sex, race, geographical region, and calendar time—was less clear.exposure to other chemicals, such as the azo dyes used in the printing and dyeing industry, have also been linked to increased bladder cancer incidence [57].incidence by other population characteristics a number of studies included in this review reported an increased utuc incidence in different populations and patient groups.however, estimated complete prevalence was only available for the index year 2003, and data was only used from 22 registries in 12 countries as representative of the eu27 countries.discussion data from the 59 papers included in this review highlight a number of trends in the epidemiology of utuc.the association between occupational exposure and other urological cancers has been studied, with a high incidence of bladder cancer in seamen reported in a study on the nordic occupational cancer cohort [56].this could explain the increased utuc incidence observed in printers in the study reported by michalek et al. [27].the studies that reported values on race generally found a mixed picture in the incidence of utuc between black and white americans.racial disparities between black and white americans have been studied and have been found to exist in other urological cancers [58, 59].the seer (surveillance, epidemiology, and end results) program is a database that collects data on cancer incidence in the usa and is a common tool used in epidemiological studies of cancer [61].however, many excluded studies from this review considered the seer category of ‘kidney and renal pelvis’ cancers as cancers of the renal pelvis.similarly, many of the studies excluded from this review cite the comprehensive study by siegel and colleagues [63], but this paper has the same limitation as above in that renal pelvis cancers are grouped together with kidney cancers.conclusion although utuc is a relatively rare urological cancer, it is still an important public health concern in many areas and patient populations around the world.the trend of utuc incidence for sex, race, and calendar time is less clear due to a wide variety of measures used to report incidence.the results of this review provide epidemiologists, public health specialists, and clinicians a better understanding of the epidemiology of utuc in order to guide diagnosis and preventi
number of words= 1172
[{'rouge-1': {'f': 0.3916955585188225, 'p': 0.8400000000000001,'r': 0.2553932584269663}, 'rouge-2': {'f': 0.2512768888996546, 'p': 0.48471571906354516,'r': 0.16959839357429718}, 'rouge-l': {'f': 0.4117900004848144, 'p': 0.7347058823529411,'r': 0.2860611854684513}}]
-----------------------------------------------------------------------------------------------------------------------------------
p28:
Extractive Summary:
until recently, the definition has been revised as ‘waking up to pass urine during the main sleep period’ [2].they reported that prevalence of nocturia among younger populations (20–40 years old) with one or more voids per night varies from 11–44% whilst those with two or more voids per night has been reported to range between 2–18% [3].increase in the number of voids per night has serious consequences on sleep, symptoms, morbidity from fatigue and falls and health-related quality of life [5–7].a cross-sectional study had been carried out in institute of urology and nephrology (iun), kuala lumpur, malaysia in 2007 and reported that the prevalence of nocturia was 29% among patients presented with lower urinary tract symptoms (luts) [10].convenience sampling was applied to obtain responses from any malaysian adult aged ≥ 18 years old.the respondents were approached based on the age population percentage reported by department of statistics malaysia (https:// www.a complete response was considered when the respondent answered all required questions until the last page of the survey, whereas for the questionnaire that was not done until the last page of survey was categorised as ‘incomplete response’.questionnaire the questionnaire consisted of three sections was developed for this study (additional file 1).those who had to wake up from sleep at night to urinate at least once were required to answer all items, while those without night urination were exempted from some questions pertaining to night urination and sleeping issues and treatment-seeking behaviour.three languages of questionnaire were pre-tested with a pilot study (n = 40, respectively) to confirm on the simplicity of language used and to assess the comprehension of the questions.feedbacks were taken into consideration to incorporate into the final questionnaire.a second pilot study was conducted after the changes made to the questions, followed by a retest of the pilot study to assess internal consistency.data collection previous to the data collection, the researchers or the trained research assistants used a pre-trained structured protocol to introduce the instrument to the participants based on the inclusion criteria.in order to increase the response rate and outreach nationwide, data was collected by mixed mode, i.e. face-to-face and online modes.face-to-face data collection was important to address the low level of technology literacy and/or limited access to the internet among certain segments of the population i.e. the elderly as well as the suburban and rural folks.it took place in public areas i.e. shopping malls, parks, restaurants, cafeteria and markets in rural and urban areas.the approached subjects spent approximately 5–10 min on the printed self-administered questionnaires.for the online mode, a third-party online platform was utilised to collect the responses.an information sheet about this research study along with a written consent form were attached to each copy of the questionnaire.all subjects signed the written informed consent form prior to answering the questionnaire.data analysis continuous variables were presented as mean and standard deviation; whilst categorical variables were presented as frequency and percentage.all data collected were tabulated and analysed using the statistical package for social sciences software, version 24.0.a total of 60.7% out of 4616 respondents had sought or would consider seeking medical attention for nocturia from urologist.there were 39.6% and 21.3% who opted for general practitioner and gynecologist, respectively.this was followed by 17.7% and 17.2% who had chosen pharmacist and nephrologist, respectively.discusssion this study presented the first nation-wide report on prevalence of nocturia (at least one void at night) in a multiracial population of malaysia.the overall prevalence of nocturia among malaysian adults was high (57%), which was comparable with those reported from studies in asian population, yet relatively higher compared to western populations.liew and colleagues reported that the prevalence of nocturia among singaporean adults was 55% [13].meanwhile, the reported prevalence were 46% in the dutch population [14], 31% in the us population [5] and 28.4% in the turkish population [15].in the present study, there was a significant higher prevalence among women.in addition, the prevalence of nocturia was found to increase with age, which corroborated the findings of other studies [4, 5, 18].malay respondents had a significant higher prevalence compared to other races as revealed by the univariate analysis, yet this prevalence was not significant in multivariate analysis.chinese and indian respondents on the other hand, had significantly lower prevalence compared to malay in both univariate and multivariate analyses.however, liew and colleagues reported that there was no significant difference between malay, chinese and indian respondents in singapore [13].a prolonged hyperglycemic state in diabetes mellitus patients on the other hand, may cause an alteration in urinary bladder activity, predisposing to diabetic cystopathy [27].additionally, overactive bladder was also found to be one of the risk factors in this study.this could be due to the comorbidities determined by self-reporting of respondents in the present study, which might have led to underestimation of the prevalence of these comorbidities in malaysia.interestingly, the prevalence of nocturia was about one third of respondent despite any drink consumption.this study also revealed that nocturia was regarded as moderately bothersome for malaysians, particularly those with two or more nocturnal voids.furthermore, more than one-third of respondents in this study claimed to have disturbed sleep about half the time or more after nocturnal awakening.in addition, a vast majority of those who had not sought treatment in the present study perceived that nocturia was not a significant medical issue to seek treatment.as such, addressing the misconceptions about nocturia can help to increase the awareness, enhance treatment-seeking behaviour and also improve the overall quality of life among people with nocturia.interestingly, this study revealed that most women did not seek treatment for nocturia as compared to men.this is in contrast with the finding from a study conducted in the primary care setting in malaysia, which revealed the higher tendency for women to seek treatment when compared to men [44].in the present study, convenience sampling was conducted instead of stratified sampling to define the population in malaysia.despite this limitation, the pool of subjects sampled were closely reflecting the real population in malaysia.conclusions the prevalence of nocturia among malaysian adults is high and strongly influenced by age, sex, race and comorbidities.there is a significant correlation between frequency of nocturnal voids with sleep disturbance and degree of bother.the findings from this study may serve as impetus to better address this issue via educational interventi
number of words= 1034
[{'rouge-1': {'f': 0.39598717782584886, 'p': 0.7002521008403362,'r': 0.27604395604395604}, 'rouge-2': {'f': 0.21992634663310603, 'p': 0.34808988764044946,'r': 0.16074243813015582}, 'rouge-l': {'f': 0.3844187216909915, 'p': 0.5821951219512196,'r': 0.2869421487603306}}]
-----------------------------------------------------------------------------------------------------------------------------------
p29:
Extractive Summary:
background renal cell carcinoma (rcc) is a malignancy that arises in the nephron tubules and has very heterogeneous histologic and clinical manifestations, accounting for approximately 90% of all cases of kidney cancer and 2.4% of all adult tumors [1].the incidence of rcc continues to increase [2].as of 216, there are more than 15 rcc subtypes classified by the world health organization [2], based on histologic and molecular criteria.clear cell rcc (ccrcc) is the most common subtype, accounting for 75% of rcc cases, followed by papillary rcc (prcc) and chromophobe rcc (chrrcc) [3].the mit family of translocation carcinomas (trcc) and succinate dehydrogenase deficiency rcc (sdhd rcc) are rare subtypes, which are diagnosed based on molecular alterations.recent advances in next-generation sequencing (ngs) technology have revealed numerous genetic alterations that are important in rcc pathogenesis and prognosis [4–6].for instance, the driver mutation of each subtype, such as von hippel-lindau (vhl), pbrm1, and brca-1 associated tumor protein 1 (bap1) in ccrcc; met and fumarate hydratase (fh) in prcc; and phosphatase and tensin homolog (pten) or tp53 in chrrcc; as well as the mutations related to prognosis, such as bap1, pbrm1, or set domain-containing 2 (setd2) in ccrcc and cyclin-dependent kinase inhibitor 2a (cdkn2a) in prcc have been identified in numerous ngs studies [7].in the era of precision medicine, determining the molecular alteration in each rcc by ngs analysis is essential for diagnosis and treatment planning.however, asian data have been limited in previously conducted large-scale ngs studies [8] and the histologic subtype and clinical behavior of rcc can differ among races [9].thus, korean ngs data are essential for precision medicine.we developed a pan-cancer panel to screen for important genetic alterations in various solid tumors, including major urological cancers, such as prostate, kidney, and bladder cancers.we also analyzed the clinical and genetic factors contributing to metastasis in ccrcc to determine the clinical utility of the pan-cancer panel assay for high-risk ccrcc.methods patient selection all patients were selected from the seoul national university prospectively enrolled registry for renal cell carcinoma – nephrectomy (super-rcc-nx).this is a prospective, multidisciplinary, and biobank lined cohort that was established in march 2016 [10].this prospective cohort collects patients’ preoperative information, pathologic reports, surgical procedure details, and information on postoperative complications functional outcomes, and oncological outcomes.we selected 31 advanced rcc patients who were pathologically t3–4 or n1 or m1.the patients had undergone radical nephrectomy surgery from march 2016 to june 2016.the tumor and normal tissues that were collected in the operating room or frozen biopsy room were immediately stored in a − 195 °c liquid nitrogen tank at the snuh cancer tissue bank.cancer panel the first-panel version 3 and 3.1 snuh cancer panel was used for this analysis.the panel was developed by snuh.it includes all exons of 183 genes, specific introns of 23 fusion genes, the telomerase reverse transcriptase promoter region, 8 microsatellite instability (msi) markers, and 45 drug target lesions.the total length captured was approximately 1.949 mbp.the first-panel was designed to screen for the important genetic alterations in major urological malignancies, including prostate, bladder, and kidney cancers [11].we selected rcc-related mutations by reviewing landmark studies, and finally selected 25 renal cell carcinomarelated genes, including vhl, pbrm1, setd2, and met mutations, in the first-panel version 3.x.dna extraction from fresh frozen tissue fresh frozen tumor tissues were homogenized and lysed with proteinase k. total dna was isolated from each target using the maxwell® 16 csc dna blood kit (promega corp., madison, wi, usa).extracted dna was quantitated using a quantus fluorometer (promega corp.) and tapestation4200 (agilent technologies, santa clara, ca, usa).capture library preparation and sequencing the quality of functional genomic dna was assessed using the 2200 tapestation system (agilent technologies) before preparation of the library.the input dna (200 ng ~ 1 μg) was sheared using an s220 focusedultrasonicator (covaris, inc., woburn, ma, usa).paired-end libraries were prepared with the sureselectxt target enrichment system kit (agilent technologies) for the illumina paired-end sequencing library protocol using snuh first cancer panel v3.0 and v.3.1, according to the manufacturer’s instructions.the quality of the dna library was evaluated using a bioanalyzer 2100 and dna 1000 chips (agilent technologies).the final libraries were sequenced on the illumina hiseq 2500 platform (2× 100 bp and 1000× coverage).next-generation sequencing targeted ngs was performed using the illumina hiseq 2500 platform.sequencing data were transformed as fastq files and quality control (qc) by fastqc and trimmomatic (0.33).binary alignment/map (bam) formation was performed after alignment based on the reference genome (grch 37) by bwa (0.7.12) and picard (1.134).qc of bam files was performed using samtools (v1.2) and gatk (v3.3).single nucleotide polymorphism (snp) discovery was performed using mutect (1.1.7) and samtools (v1.2).indel and copy number variation (cnv) discovery were performed by indelgeontyper (0.36.3336) and conifer (0.2.2), respectively.the fusion search was conducted using delly (0.7.2).all data were converted to vcf format and annotated using annovar.variant prevalence comparison of snuh pan-cancer data and tcga prostate cancer data three rcc databases were downloaded from the cancer genome atlas (tcga; tcga-kirc, tcga-kirp, tcga-kich) variant maf files, belonging to the nci gdc data portal.in addition, because we used targeted sequencing to detect alterations in rcc, we did not determine the whole mutational profile of the rcc patients who were screened.despite these limitations, we successfully conducted a pan-cancer panel analysis with good sequencing depth (> 400).the data shed light on the feasibility of using the pan-cancer panel for the diagnosis of rcc, and the possibility of deriving meaningful prognostic information from the mutational profiles.conclusion the pan-cancer panel comprised of rcc-related genes is a feasible and promising tool to evaluate genetic alterations in advanced rcc.
number of words= 917
[{'rouge-1': {'f': 0.5002376111792957, 'p': 0.7697635933806146,'r': 0.370507614213198}, 'rouge-2': {'f': 0.3623325960575354, 'p': 0.541563981042654,'r': 0.27223577235772356}, 'rouge-l': {'f': 0.5016731837826713, 'p': 0.7366666666666666,'r': 0.3803448275862069}}]
-----------------------------------------------------------------------------------------------------------------------------------
p30:
Extractive Summary:
background bladder cancer is the tenth most common cancer worldwide, accounting for 3% of all cancers.moreover, cancer is another associating factor for morbidity and mortality.evidence has shown that dm may be associated with higher incidence and poor prognosis of bladder cancer [4–9].furthermore, poor glycemic control results in increase of oxidative stress, upregulation of series of cell molecules, and inflammation process, which are thought to have negative effect on cancer prognosis.however, evidence on the effect of glycemic control on the outcomes of bladder cancer is limited.besides, most studies use single hba1c data for evaluation, which may not be representative of longterm glycemic control during the follow-up period.therefore, in the current study, we aimed to investigate the effect of dm and glycemic control on the outcomes of nmibc by using mean hba1c data.we obtained hba1c data from patients who received treatments of dm in our hospital and calculated mean hba1c levels by averaging the sum of hba1c data from the time of diagnosis of bladder cancer to the time of the first recurrence.recurrencefree survival (rfs) was defined as the period from the date of the initial transurethral resection of bladder tumor (turbt) to the date of the operation in which the first cancer recurrence was found.progression-free survival (pfs) was defined as the period from the date of the initial turbt to the date of the operation or image study in which the first cancer progression was found.the patients were subsequently diagnosed with nmibc in our hospital from january 2012 to december 2017.patients who had newly diagnosed nmibc with a follow-up period of more than 2 years were included.treatment and follow-up protocols in our center, resident doctors must receive a 5-year training program for urological specialist.in the first and the second years, junior resident doctors should be trained as assistants and taught by seniors.they should learn the procedure skills by watching and simulation.for pursuing quality and safety of surgeries, all procedures must be supervised by attending doctors in charge.intravesical therapy would be given if no contraindication existed, and the first dose would be given within 24 h after the operation.the follow-up protocol in our center strictly met the current urological association (aua) and european association of urology (eau) guidelines for bladder cancer.study design we retrospectively analyzed data from a prospective database.patient profiles and disease characteristics, including age at the time of diagnosis, sex, body mass index (bmi), history of smoking, comorbidities, cancer stage, histology grade of urothelial carcinoma, tumor number, tumor size, intravesical therapy, date of diagnosis, date of recurrence, and date of progression were collected.subgroup analysis for the dm group, which was further divided into the proper glycemic control and poor glycemic control groups, was performed to investigate prognostic factors in the dm group.outcomes were recurrence, progression, rfs, and prs.statistical analysis data were analyzed using spss version 22 (spss inc., chicago, il, usa).categorical variables were analyzed using chi-squared test; rfs and pfs were analyzed using kaplan-meier analysis.results patient selection and baseline characteristics we screened 845 patients who had a diagnosis of bladder cancer.the median age at diagnosis of bladder cancer was 67 years.prognosis analysis dm was not significantly associated with higher rates of recurrence (odds ratio (or) = 1.52, 95% confidence interval (ci) 0.86–2.69, p = 0.15) and progression (or = 0.73, 95% ci 0.20–2.60, p = 0.62) (supplementary table s1).kaplan-meier analysis of rfs and pfs revealed no significant difference between the dm and non-dm groups (fig. 1a and supplementary fig. s2a).univariate cox proportional hazards regression showed that male sex (hazard ratio (hr) = 1.94, 95% ci 1.14–3.30, p = 0.014), t1 stage (hr = 2.14, 95% ci 1.47–3.12, p < 0.001), cis (hr = 1.58, 95% ci 1.03–2.41, p = 0.036), high grade (hr = 1.72, 95% ci 1.14–2.58, p = 0.010), tumor number ≥ 3 (hr = 2.49, 95% ci 1.58–3.94, p < 0.001), and tumor size ≥3 (hr = 1.94, 95% ci 1.12– 3.36, p = 0.018) were associated with higher recurrence (table 1).multivariate cox proportional hazards regression showed that t1 stage (hr = 2.05, 95% ci 1.06–3.97, p = 0.034), tumor number ≥ 3 (hr = 3.46, 95% ci 1.90– 6.33, p < 0.001), and tumor size (hr = 1.90, 95% ci 1.05–3.42, p = 0.033) were independent risk factors for recurrence (table 1).several studies have discussed the effect of dm on bladder cancer.metformin has been discovered to suppress tumor by activating amp-activated protein kinase and liver kinase b1 and downregulating mammalian target of rapamycin and insulin-like growth factor-1 [16].studies have also reported that overexpression of igfs and their binding proteins is associated with poor prognosis in bladder cancer [29, 30].third, we performed kaplan-meier analysis for patients without dm, patients with dm and proper glycemic control, and patients with dm and poor glycemic control.
number of words= 786
[{'rouge-1': {'f': 0.4214808876773474, 'p': 0.8159677419354838,'r': 0.2841203703703704}, 'rouge-2': {'f': 0.25613300955549057, 'p': 0.4505668016194332,'r': 0.17892236384704518}, 'rouge-l': {'f': 0.38744581693251035, 'p': 0.6890476190476191,'r': 0.26948849104859335}}]
-----------------------------------------------------------------------------------------------------------------------------------
p31:
Extractive Summary:
self-reported data on frequency of performing pelvic floor (i.e., kegel) exercises after rp was ascertained on the follow-up questionnaires.we also used the lower bounds of sub-scale-specific ranges to classify participants in sensitivity analyses.multinomial logistic regression was used to explore bph- and ed-related factors associated with short- and long-term improvement beyond baseline and maintenance in urinary and sexual outcomes (in sub-scales with at least 10 men for stable estimation).the majority of included participants (mean age = 60.7 years) were caucasian (91.9%), had completed at least some college education (83.6%), earned ≥ $75,000 per year (61.4%), were married or living with a partner (82.5%), and had never smoked (62.5%, table 1).among participants with data on the charlson comorbidity index (n = 167), 103 (61.7% of 103 and 26.2% of 394) had at least one comorbidity.considering their prostate cancer-specific characteristics, most participants had clinical stage t1 (78.5%) and pathologic stage t3 (88.9%) disease, with a pre-surgical psa concentration between 4 and 10 ng/ml.five weeks after surgery, mean levels of each of these outcomes were markedly decreased.improvement was greatest for voiding dysfunction-related bother (22.1% of men at 5 weeks; 50.9% at 12 months), followed by sexual bother (7.3% at 5 weeks; 10.6% at 12 months), and urinary function (9.8% at 12 months, tables 2 and 3).in contrast, each of the individual measures of bph, as well as the bph composite measure, were associated with improved voiding dysfunction-related bother at both 5 weeks and 12 months post-surgery (5 weeks: odds ratio [or] = 3.9, 95% confidence interval [ci]: 2.1–7.2; 12 months: or = 3.3, 95% ci: 2.0–5.7 for the bph composite measure).with respect to sexual outcomes, no significant associations were observed for bph- or ed-related factors with improved or maintained sexual function.positive associations were also observed for several of the individual measures of bph, including α-blocker use (or = 4.4, 95% ci: 1.1–18.0) with improvement in sexual function at 12 months post-surgery, and bph medication use (or = 5.4, 95% ci: 2.2–13.0) and α-blocker use (or = 5.6, 95% ci: 2.2–14.6) with improvement in sexual bother at 5 weeks.this is likely due to confounding by indication, whereby men who experienced large declines in sexual function may have been more likely to use ed medications or devices after surgery.to our knowledge, improvement in outcomes besides voiding dysfunction has not previously been documented.our finding of clinically meaningful improvement in urinary continence among a small proportion of men treated for prostate cancer by rp was unexpected and differs from most previously published findings [10, 21, 22].one possible explanation for this finding may be surgical elimination of bladder outlet resistance by rp.studies of bladder outlet resistance have demonstrated that the bladder detrusor muscle undergoes structural and functional changes, with initial hypertrophy, then compensation, and then decompensation [23–25].another possible explanation is improved communication surrounding sexual function.prostate cancer treatment and its known sexual side effects may be a launching point for men to have an open conversation with their providers and partners about their sexual function.previous studies have demonstrated that spousal communication is a key factor in healthy sexual function recovery after prostate cancer treatment [26].for many men, prostate cancer treatment discussions may prompt discussions regarding sexual function that have not been addressed in the past.the mechanisms through which open communication may improve post-rp sexual outcomes might be similar to those in studies of premenopausal women with dyspareunia, which found that open communication between partners was critical for improving sexual function and distress [27].this hypothesis is supported by growing evidence that bph and ed may be caused by common biologic mechanisms [28].as such, improvements in bph-related symptoms from rp may also result in improvements in sexual outcomes.alternatively, certain bph-related medications are known to be associated with decreased sexual function [29].in support of both of these mechanisms, our data demonstrated a higher odds of improved sexual bother associated with the bph composite index.finally, it is also possible that rp may relieve sexual pain in men with pre-surgical chronic prostatitis/chronic pelvic pain syndrome, thereby contributing to improvements in sexual bother.although each of these explanations is speculative, we believe they warrant further study for their possible, eventual guidance for prostate cancer therapeutic decision-making and overall patient counseling.additionally, use of the epic-50 rather than the shorter epic-26 allowed us to gain a more comprehensive understanding of both urinary and sexual function and bother; and our unique statistical analysis allowed us to explore the full range of outcome trajectories rather than just mean outcome levels.although reasons for improvement in urinary function (continence) and sexual bother are unclear, improvement in voiding dysfunction-related bother likely relates to relief of bph symptoms by rp, as prostatectomy is a known, effective therapy for severe bph.
number of words= 775
[{'rouge-1': {'f': 0.3616445393136767, 'p': 0.6559375000000001,'r': 0.24964071856287426}, 'rouge-2': {'f': 0.18039092110256164, 'p': 0.27784313725490195,'r': 0.13354916067146283}, 'rouge-l': {'f': 0.3399562602515036, 'p': 0.5164285714285715,'r': 0.25337408312958437}}]
-----------------------------------------------------------------------------------------------------------------------------------
p32:
Extractive Summary:
introduction urinary tract infection (uti) is one of the main reasons for emergency medical consultation.one in three women over the age of 18 will experience a uti, and many of them will have repeated infections [1].cranberry products are the most promising natural health alternatives for the prevention of utis [3].cranberry has been shown to inhibit the adhesion of uropathogenic escherichia coli to uroepithelial cells [4].tis discrepancy is mainly explained by a lack of compliance, lack of statistical power and variable pac concentrations in the tested products.according to ex  -vivo clinical studies (dose–efect studies evaluating the optimal dose for urine anti-adhesion efect), the quantifcation of pacs requires standardized, reproducible methods and should be at least 36 mg/ day [5, 6].materials and methods te cranberry extract for prevention of recurrent uti trial (paccann) was a randomized, double blind, controlled, clinical trial performed at the institute of nutrition and functional foods (inaf).te protocol, consent form and all procedures were approved by the institutional ethics committee of laval university.written informed consent was obtained from all study participants.te control dose was standardized at 1% pacs which is comparable to the majority of cranberry products approved by health canada [8].cranberry pacs were manufactured by nutra canada (now part of diana food canada) and similar in size, smell and taste.randomization concealed randomization was generated using computer assisted randomization by blocks of 10.episodes were categorized as symptomatic uti with bacteriuria in the presence of≥ 103  cfu/ml of uropathogenic bacteria.sample size and statistical analysis we estimated that 35% of patients in the control group would present at least one uti during the 24-week follow-up period [3].missing data was excluded from analyses for post-hoc questions.interactions with the treatment groups were tested in multivariate regression model using a backward elimination method.symptomatic uti a total of 45 symptomatic utis were diagnosed in the high dose pac group compared to 59 in the low  dose group.uti-free median was 24.0 weeks in the high dose group compared to 16.6 weeks in the low dose group.te hazard ratio for the diference between the number of subjects who had experienced a frst symptomatic uti by the end of the 24-week period was 0.73 (95% ci 0.45–1.16; fig. 2).compliance, side efects and adherence to double‑blind procedures compliance based on number of returned capsules at 24 weeks was similar in both groups (92.9% in the high dose group vs 92.7% in the low  dose group, student t-test p=0.9).it is of particular interest that this dose was compared to another cranberry extract containing a low concentration of pacs, as often found on the canadian supplement market to prevent uti.our results indicate that the intake of 2×18.5  mg pacs daily was associated with a non-statistically signifcant 24% reduction in the risk of symptomatic uti compared to a daily dose of 2×1 mg pacs during a 24-week follow-up period, similar to the results of a recent meta-analysis [16].te present study is distinct given that we recruited according to a guideline-based defnition of recurrent uti [17].no signifcant efect was observed in women with higher past uti burden.in women experiencing higher level of uti (>5), certain factors that were not measured in our study could explain the mitigated response to pac intake.it is possible that in these women, uti recurrence may be the result of complex interactions between bacterial urovirulence and a particular host susceptibility (altered gut microbiota, less efcient adaptive immune response) [22–24].considering that pacs are poorly absorbed and are difcult to quantify in the urine, future studies should focus on the efect of these molecules on the gut microbiota, a natural reservoir for uropathogenic escherichia coli [22, 25].gut microbiota composition and function largely vary between individuals which might explain differences in individual’s susceptibility to uti.in order to provide mechanistic insight explaining diferent levels of recurrence, feces and urine samples are being analysed and results will be published in a future article.similarly, maki et  al. [18].te use of a control dose instead of placebo  may explain why we were unable to fnd a signifcant reduction in the recurrence of uti in our cohort with a high uti burden.urine culture contaminated by improper clean-catch urine technique were excluded from analyses in order to mitigate a risk of detection bias.we also experienced technical issues such as delays in delivery to the microbiology laboratory and improperly stored samples.in a subset of participating women with a history of less than 5 utis per year, the daily consumption of 2×18.5  mg pacs resulted in a signifcant reduction in the rate of symptomatic uti during the trial period compared to 2×1 mg pac.tese fndings need to be tested in women with moderate burden of recurrent uti who may beneft from a preventive treatment with a split dose of 37 mg/day of pacs from cranberry extract, with few associated side efects.further investigations are also needed to examine dose-dependent impacts of cranberry pacs for the prevention of recurrent uti and their efects on the microbio
number of words= 815
[{'rouge-1': {'f': 0.38690558513350876, 'p': 0.6679381443298968,'r': 0.27232558139534885}, 'rouge-2': {'f': 0.19664127887966154, 'p': 0.29758620689655174,'r': 0.14683352735739233}, 'rouge-l': {'f': 0.34971614883007285, 'p': 0.5766666666666667,'r': 0.2509523809523809}}]
-----------------------------------------------------------------------------------------------------------------------------------
p33:
Extractive Summary:
because of the unique biological inherent properties of the tumor, the complex anatomic features of the knee and problems associated with extensor reconstruction, tumors arising around the knee pose a unique set of challenges in the management of soft tissues and the restoration of limb length and joint kinetics.to guide the treatments selection and decision making, it may be crucial to clarify the prognostic factors that significantly predict the survival of prosthesis used for reconstructing the knee operated for tumors.until now few studies have analyzed the prognostic effect of patients’ and operationrelated characteristics.patients with reconstructed knee for medical conditions other than bone incomplete medical document were excluded.kaplan-meier survival curves were generated for all prognostic factors involved in multivariable analysis.hazard ratio (hr) and confidence interval (95%ci) were selected as the effect size for both univariate and multivariate cox model.c-index and calibration curve were used to evaluate the discrimination ability and accuracy of the nomogram respectively.finally, after fitting of a new cox model using the variables involved in the graphical nomogram, a webpage-based dynamic nomogram, which could provide predicted survival curve and survival rate at any time point as wanted, was established.all analyses have been performed with program of r language (foundation for statistical computing, vienna, austria).the univariable and multivariable cox models were fitted using the r package “survival”.the graphical nomogram and the online dynamic nomogram were generated with the packages “rms” and “dynnom”.the baseline characteristics is shown in the table 1.among these patients, there were 99 males (63.46%) and 53 (36.54%) females, and 24 male (51.06%) and 23 female in the training and validation samples, respectively.the mean age was 35.31 ± 17.54 and 31.09 ± 16.17 years in the training and validation samples at the time of surgery.the distal femur (n = 99, 63.46%) was more common than proximal tibia (n = 57, 36.54%) concerning the involved anatomical location.the distribution of the tumor histology is presented in fig. 1, showing that malignant primary tumor (osteosarcoma, 72; malignant fibrous histiotoma, 9; chondrosarcoma, 6; leiomyosarcoma, 3; synoviosarcoma, 2) was the predominated tumor type, followed by borderline tumor (giant cell tumor of bone, 53; fibrous histiotoma, 3) and malignant metastatic tumor.within 6 months post-operatively, a total of 26 (16.67%) and 7 (14.89%) patients were encountered with operation related complications, in the training and validation groups.the prosthesis survival at 12, 60, 120, and 180 months were 96.7, 85.8, 76.9 and 44.0%, and 95.7, 82.9, 67.5 and 45.0% in training and validation groups, respectively.results of univariate and multivariate cox analyses figure 3 shows the results of running log-rank test for the continuous predictors, presenting the optimal cut-off points to convert these factors into categorical variables.the results of univariate cox analysis are shown in table 2, and the kaplan-meier survival curves for the significant and marginally significant (p < 0.15) prognostic factors are presented in fig. 4.the forest plot in fig. 5 shows the effect sizes of multivariate analysis, presenting that anatomical location (hr = 0.42, 95%ci: 0.20 ~ 0.89, p = 0.024), length of prosthetic stem (hr = 0.44, 95%ci: 0.19 ~ 0.99, p = 0.048) and length of bone resection (hr = 11.08, 95%ci: 3.34 ~ 36.78, p < 0.001) remained to be statistically significant prognostic factors for prosthetic survival.establishing and validation of nomogram model basing on the significant prognostic factors screened by multivariate cox model, a graphical predicting nomogram model was developed, to calculate the predicted prosthetic survival probabilities at 1, 5, and 10 years post-operatively, as well as the median survival time (fig. 6).the c-indexes were 0.717 (95%ci: 0.497 ~ 0.937) and 0.726 (95%ci: 0.244 ~ 1.208) for the training and validation samples respectively, demonstrating that the newly established nomogram possesses favorable discrimination ability.the calibration curves at 12, 60 and 120 months for the two samples are displayed in fig. 7.generally, favorable consistencies between the predicted and actual survival were presented, indicating satisfactory accuracy of the novel predicting model.to facilitate the usage of our novel predicting model, a web link (https://hellonihao.shinyapps.io/pangcg_ dynnomapp/) was generated to automatically calculate the prosthetic survival online for each individual patient.the survival probability at any time point could be obtained by dragging the sliderbar “time”.case presentation figure 8 and fig. 9 show a case of patient who was operated for bone tumor locates at distal femur and reconstructed with a prosthesis that was more than 14.4 cm on length of stem, and more than 10.4 cm of bone was resected during tumorectomy.using the graphical nomogram, only approximate survival rates could be read on the plot by drawing a vertical line through the total point.while, through the webpage-based dynamic nomogram, the detailed prosthetic survival probabilities at 12, 60 and 120 months could be automatically calculated, which is shown to be 0.890, 0.305 and 0.068, respectively.discussion multimodality limb salvage therapy has replaced amputation as the preferred form of treatment for primary musculoskeletal tumors primarily owing to improvements in adjuvant and neo-adjuvant therapy, as well as advances in imaging and diagnostic modalities [2–4].after the resection of bone and soft tissue tumors, reconstruction of the resulting defects was challenging because of the unique anatomic features of the knee.limb-salvage surgery for malignant bone tumors must be successful in three different respects, that is, the cure of oncology, the durability of prostheses and the satisfactory of the function in the limbs.in the current study, we identified that the anatomical location of tumor, length of prosthetic stem, and length of bone resection during operation were significantly associated with the prosthetic survival.basing on these factors, we also generated a graphic nomogram and a web predicting tool, which could easily applied for predicting survival probabilities of the implant at several time points.until now there are only a few predicting models that have been developed to help estimate the prosthetic survival probability, using independent prognostic factors.in the studies of zhang et al. [19], they identified the significant factors associated with the incidence of aseptic loosening after tumor prosthetic replacement around the knee, demonstrating that tumor anatomical location (proximal tibia vs. distal femur), length of prosthetic stem (< 14 cm vs. > = 14 cm), and prosthetic motion mode (fixed hinge vs. rotation hinge) were independent predictors.after that, they established a graphical nomogram to help predict the risk of aseptic loosening at the time points of 5 and 10 years.this model, however, could only provide approximate risks of aseptic loosening at two points, and the applying of the model is not convenient as one need to calculate the sub-scores of the prognostic factors and the total score, and to gain the predicted survival by drawing vertical line.finally, it is difficult to collect large sample size in this topic due to the low frequency of tumor resection and endoprosthetic reconstruction for tumor around knee.
number of words= 1113
[{'rouge-1': {'f': 0.4462318979175913, 'p': 0.8206631299734748,'r': 0.30642439431913115}, 'rouge-2': {'f': 0.24605798353581002, 'p': 0.4077659574468085,'r': 0.17618729096989966}, 'rouge-l': {'f': 0.39685380386609537, 'p': 0.6334517766497463,'r': 0.28893491124260356}}]
-----------------------------------------------------------------------------------------------------------------------------------
p34:
Extractive Summary:
technological advances in furs design (such as improved active tip deflection and digital image transfer), laser lithotripsy techniques, advances in instruments as tipless baskets and ureteral access sheath (uas) have resulted in increasing the utilization of furs in treatment of renal and upper ureteric calculi.european association of urology (eau) guidelines recommended shock waves lithotripsy (swl) and endourological for treatment of renal stones < 2 cm [2].however, the effect of case volume per year on selection of certain indications, utilization of specific techniques and preference of instruments was reported in only one study that involved only european urologists [7].high-volume urs cases per year was proved to result in better outcomes (such as shorter operative time, better stone free rates, shorter hospital stay, less need for retreatment, fewer and less severe complications) when compared with lower case-volume [8].methods study design an email invitation was sent to endourological society members to answer an anonymous online survey in june 2017.however, highvolume endourologists used their experience to reach efficient outcomes [5, 8].the second important difference was the preference of semirigid urs for treatment of upper ureteric calculi.although high-volume endourologists used semirigid urs less frequently than low-volume group (68% vs 82%, p = 0.044).however, a considerable number of endourologists in each group preferred semirigid urs for treatment of upper ureteric stones.this may be attributed to financial causes in some countries, less experience in using furs by some urologists (especially in low-volume group) or choosing to start with semirigid urs and if stone fragments escaped to the kidney, furs can be used to retrieve them.the third difference in this survey was utilization of laser dusting of stones by more endourologists in lowvolume group (63% vs 45%, p = 0.031).high-volume endourologists prefer fragmentation and stones retrieval to achieve immediate stone free status in most patients, while low-volume surgeons prefer dusting and leaving gravels for spontaneous passage.dusting uses low energy, high-frequency and long pulse duration setting to fragment stones into fine powder and small fragments that are left for spontaneous passage [11].the stone free rates for fragmentation and basket retrieval were significantly better than dusting, but operative time was shorter for dusting [12, 13].advantages of uas include ease of multiple passages of the scope, rapid stone extraction, higher stone free rates, low intrarenal pressure during prolonged procedures [14, 15].it is obvious that about two thirds of endourologists of different levels of case load believe that the advantages of uas outweigh its disadvantages.a similar result was reported from a large european survey where 70.7% used uas [7].when asking about the use of a safety guidewire, 68% of group 1 and 60% of group 2 indicated that they used it in all cases.in the past, use of a safety guidewire was highly advised to allow smooth access to the pelvicalyceal system and facilitates ureteral stent placement in case of ureteral or collecting system injury [18].in the present study, it was observed that 13% of high-volume surgeons did not use a safety guidewire compared to 6% of the other group.these rates are more than the reported 1.9% in the previous survey by dauw et al. [6] and we think that it is expected to increase in future.conclusions the use of furs for treating upper tract calculi has expanded by high-volume endourologists to include large renal stones > 20 mm.
number of words= 547
[{'rouge-1': {'f': 0.46787083753277825, 'p': 0.6796654275092937,'r': 0.3567132867132867}, 'rouge-2': {'f': 0.21943163639890353, 'p': 0.2938805970149254,'r': 0.17507880910683013}, 'rouge-l': {'f': 0.4074976597487685, 'p': 0.5634210526315789,'r': 0.3191694352159469}}]
-----------------------------------------------------------------------------------------------------------------------------------
p35:
Extractive Summary:
intrafascial technique was a kind of refinements, characterized by developing a dissection plane medially/internally to the prostatic fascia, in order to maximally preserve peri-prostatic nerves and to enhance the post-surgical recovery of continence and potency.this technique is now applied worldwide in combination with different surgical approaches and procedures [5, 6].controversy about the intrafascial nerve-sparing technique has persisted since its introduction [7].most doubts were concentrated at its oncological safety, considering the necessity of removing all fascial coverings of the prostatic surface in resecting a tumor.it is generally recognized that the greater extent of structures spared, the higher is the risk of residual tumor; therefore, some surgeons would be concerned that intrafascial dissection would compromise its oncological safety and incur a risk of a higher rate of positive surgical margins (psms) [8].there were obvious variations regarding the oncological results reported by different surgeons, which may be influenced by factors such as surgeon characteristics, surgical procedures, patient inclusion criteria, and outcome assessment methods.the purpose of this study was to critically summarize existing clinical trials and provide a detailed and comprehensive assessment of the oncological findings of intrafascial prostatectomy to guide urologists in selecting the appropriate technique.methods inclusion criteria inclusion criteria were set according to picos (patients, intervention, comparison, outcomes, and study design) principle as presented in table 1.this review included trials designed as surgery series or controlled studies.included studies had at least one arm that is performed using intrafascial techniques, including veil technique and other techniques approaching fascial planes close to the prostatic capsule and internal to the prostatic fascia, regardless of the types of surgery including retropubic radical prostatectomy (rrp), laparoscopic radical prostatectomy (lrp) and robot-assisted laparoscopic radical prostatectomy (ralrp).study paralleling convetional interfascial technique was included in the comparative analysis as a control study.we excluded studies from the comparative assessment, including extra-fascial or wide-dissection or non-nerve-sparing prostatectomy, and for these studies we only extracted data from the intrafascial group.data sources and searches database searches were performed for published articles till june 2018 on pubmed.the following keywords were used across the “title” and “abstract” field including: (“intrafascial” or “veil” or “curtain dissection” or “incremental nerve sparing” or “high anterior release”) and “radical prostatectomy”.with regard to a comparative analysis of the psm rate, some studies had a selection bias between the intra- and inter-fascial groups.eight trials paralleling conventional interfascial technique as controlled groups were included for comparative meta-analysis.the total psm rate ranged from 2.2 to 35%, with a pooled rate of 14.2% (498/3351, 95% confidence interval [ci]: 11.0–17.3%).ci: 26.3–35.2%), 34% (17/50, 95%no obvious differences could be detected among the 3 surgery types of lrp, rrp, and ralrp.ci: 7.0–12.4%) in pt2 cancers (additional file 2: figure s1) and 44.0% (208/527, 95% ci: 34.9–53.2%) in pt3 cancer (additional file 3: figure s2).meta-regression analysis table 4 summarized the result of meta-regression evaluating the impact of confounding factors affecting psm rates.moreover, preservation techniques including d-fascia preservation, puboprostatic ligament sparing, selective ligation of dvc, had no significant influence on psm rate.figure 3 depicted the meta-regression plot to describe the effect of the confounding factors of patient age, pt2 cancer percentage, and ssos score on total psm rate.none of the 3 meta-analyses demonstrated a significant difference between the intra- and interfascial groups.discussion in this study we conducted a systematic review and pooled analysis of oncological outcomes following intrafascial nerve-sparing prostatectomy.the authors proposed an average psm rate of 15 and 10% could be expected for all-stage and pt2 cancers after ralp, respectively.in this present meta-analysis, we found that the pooled psm rate in the intrafascial group was, overall, 14.2% in all stages of prostate cancer and 9.7% in pt2 disease regardless of surgery types, which seemed to match the results of previous review.from this point of view, oncological outcomes of intrafascial technique seemed acceptable, or at least not worse than with conventional approach.however, a hasty conclusion of the oncological safety of this technique should be avoided, as there are additional issues worthy of critical appraisal.the mean percentage of pt2 cancer in the intrafascial studies included in our present review was 84.5% (range: 58.6–98.1%), which is significantly higher than that in the 17 surgical series included in novara’s review [31] (t-test of independent samples: mean difference = − 7.91569, t = − 2.618, p = .014); this meant, surgeons selected more patients with localized disease to perform intrafascial approach, but only comparable total psm rates were gained.moreover, according to the guidelines of the european association of urology, radical prostatectomy could be extended for indications such as patients with intermediate-risk, localized prostate cancer with clinical stage t2b–t2c, gleason score = 7, or psa 10–20 ng/ml [3].however, our review found that most surgeons applied more rigorous selection criteria against generally recommended indications for radical prostatectomy.for example, the vip team in 2005 reported a notably low psm rate with a robot-assisted surgical system wherein only 1 case of positive margin was detected from among 46 patients who underwent intrafascial ralrp [27].if intrafascial dissection is carried out among patients with a high risk of extracapsular invasion, the result may be unsatisfactory.however, as preoperative pathological staging could not be obtained, the risk of extracapsular invasion was predicted on the basis of the patient’s preoperative information.clinical stage, psa level, gleason score, and positive biopsy cores could be used as independent predictors of this risk and, therefore, we scored each factor and summed to ssos to quantitatively evaluate this risk preoperatively.we identified a significant correlation between the ssos and pathological pt2 percentage (pearson correlation = 0.749, p = .001).moreover, an obvious association could be identified from our meta-regression model by including ssos as a confounding factor, indicating that, for intrafascial prostatectomy, stringent case selection was associated with low risk of psm and quantitatively, each 1 point of ssos could decrease the total psm rate by 1.3% on average.with regard to subgroup analysis of balanced studies, pooled results revealed a higher psm rate, although statistically non-significant, with the intrafascial technique for pt2 and all-stage cancers.most authors described their dissection technique in the original manuscripts, but ignored pathological evaluation of the specimen regarding whether the utilized surgical technique was intra or interfacial nerve sparing.in the study of greco et al. comparing rrp with lrp, results demonstrated that both surgical types had similar psm rates [24].conclusions in summary, intrafascial technique resulted in a total psm rate of 14.2% for all-stage cancer and 9.7% in pt2 disease.preoperative ssos should be more than 7 points to obtain an acceptable postoperative psm rate of 15% on avera
number of words= 1072
[{'rouge-1': {'f': 0.40571701804219595, 'p': 0.8497202797202796,'r': 0.2664757709251101}, 'rouge-2': {'f': 0.2430161326466504, 'p': 0.4524561403508772,'r': 0.1661199294532628}, 'rouge-l': {'f': 0.4302045044910105, 'p': 0.7441573033707864,'r': 0.30255813953488375}}]
-----------------------------------------------------------------------------------------------------------------------------------
p36:
Extractive Summary:
pain during intercourse (dyspareunia) is also common, as well as the development of chronic pelvic pain (cpp) [1, 2].it is hypothesized that a specific immunological and inflammatory pathway is common to all of these conditions and endometriosis [3, 5].unfortunately, hormonal treatment can have intolerable side effects or become ineffective over time, while the effect of surgery is often short-lived [8].advances in the understanding of endometriosis have expanded the focus on less invasive and nonpharmacological treatments [8, 9].this mechanism makes it plausible for the anti-inflammatory effect of pa and exercise to impede the development of the disease and ameliorate the associated pain [12].however, these interventions have been studied mostly in terms of their ability to reduce the risk of developing endometriosis [12, 14, 15], and so little is known about the effect of pa and exercise on symptom improvement in women with endometriosis [12].pa was defined as “any bodily movement produced by skeletal muscles that requires energy expenditure” [21] and exercise was defined as “pa that is planned, structured, and repetitive for the purpose of conditioning the body” [21], consisting of cardiovascular conditioning, strength and resistance training, and flexibility.in the third step, the same authors independently assessed the methodological quality of the manuscripts that met the inclusion criteria, using quality assessment questionnaires appropriate for the design of each study as provided by the national heart lung and blood institute [23].we applied the method of “quality assessment of controlled intervention studies” for randomized controlled trials (rct), and the “quality assessment of before-after (prepost) studies with no control group” for intervention studies with no control group while adding relevant questions to determine exposure, risk, and confounding variables.” the quality of the included studies was rated as good, fair, or poor.results study selection this study identified 1879 citations (fig. 1).we identified four studies that described an intervention incorporating pa and/or exercise: two were rcts [27, 28] and two were pre-post studies with no control group [25, 26] (tables 1, 2).quality assessment, risk of bias, and exercise intervention assessment one study was rated as being of fair quality [27], while three were rated as poor quality [25, 26, 28].the detailed assessment including signaling questions are presented in tables 1 and 2.the rct of carpenter et al. [27] was judged as being of fair quality (table 1).though having a control group, the study was underpowered for determining whether exercise had an additional effect to danazol.however, since the study was designed to investigate if exercise could alleviate the side effects of danazol, it was not flawed per se.moreover, the sample was too small to allow comparisons of individual side effects, important secondary outcomes (pelvic pain, dysmenorrhea, and dyspareunia) were not reported, and the methods of randomization and outcome assessment were not reported.the intervention group had a higher level of education, a high percentage of homemakers, and a lower rate of employment, which is a confounder for quality-of-life assessments.the study of friggi sebe petrelluzzi et al. [25] was judged as being of fair quality (table 2).furthermore, a sample-size calculation was not reported, and there was no control group.the intervention consisted not only of pa and exercise, but also a range of modalities including behavioral cognitive therapy, which confounds the contribution of pa and exercise to symptom improvement.the study of awad et al. [26] was judged as being of poor quality (table 2).furthermore, no sample-size calculation was provided, and the inclusion and exclusion criteria appeared to be random from a clinical perspective.none of the articles provided a description of exercise progression [25–28], and only one included a description of individually tailored exercises [27].the level of exercise was only described for two studies [27, 28].two studies included women with surgically confirmed endometriosis [25, 27], while it was not specified how endometriosis was diagnosed by goncalves et al. [28].interventions the performed interventions are listed in table 1.primary and secondary outcome measures the primary and secondary outcomes for all studies are reported in table 3.the outcome reports were incomplete for all studies.notably, the control group also received physiotherapy following the intervention at their institution.effects of intervention on mental health aspects and well‑being the study of friggi sebe petrelluzzi et al. [25] measured stress levels using the perceived stress questionnaire (psq), salivary cortisol levels, and the 36-item short- form health survey (sf-36).we identified 4 interventional studies involving 129 women.however, their results have been inconclusive, mainly due to inclusion of observational studies of how pa and exercise may lower the risk of developing endometriosis [12, 14, 15].physiotherapy contains both active and passive modalities, but the optimal physiotherapy approach for endometriosis-associated symptoms is not clear [16].furthermore, exercise increases the production of leucocytes, cortisol, and adrenaline, all of which have potent acute anti‐inflammatory effects [33].one study showed improvements in daily pain scores [28], but no effect-size measures were provided, and so the strength of this association was uncertain.a recent systematic review and meta-analysis produced evidence that meditation itself is effective in improving the quality of life and pain in women with cpp [34], which was an inclusion criterion in the present study.no sample-size calculations were performed for those two studies, and so type ii errors might have been present.other reported effects were reduced stress levels by friggi sebe petrelluzzi et al. [25], and improvements in well-being and body image by goncalves et al. [28].both of these studies included women with cpp and applied a cognitive approach in addition to pa and exercise, which are both possible confounders for the effect of pa and exercise on endometriosis-associated symptoms [34].previous research has found that the pelvic floor muscle tension in higher in women suffering from endometriosis pain [35] than in controls without endometriosis.the high dropout rate found by goncalves et al. [28] indicates that it is pertinent to design exercise interventions that meet the needs of patients and fit their lifestyle.since endometriosis patients can show complex symptomatology, the cooperation of multiple disciplines such as physiotherapists and gynecologists could improve the quality of clinical research in this fie
number of words= 989
[{'rouge-1': {'f': 0.40007375584276084, 'p': 0.7377215189873418,'r': 0.27445736434108525}, 'rouge-2': {'f': 0.16713255928720322, 'p': 0.2509523809523809,'r': 0.12528612997090205}, 'rouge-l': {'f': 0.3762031491262751, 'p': 0.5752083333333333,'r': 0.27950323974082075}}]
-----------------------------------------------------------------------------------------------------------------------------------
p37:
Extractive Summary:
child marriage is a violation of human rights and children’s rights and exposes them to domestic violence, sexual abuse, rape, and deprives them of access to education [3].in accordance with the results of the population and housing census in 2016, approximately 14% of the population of iran are between the ages of 10–19 years old [3].in the field of social and cultural causes, we can mention the issues of tradition, custom, norms, and family and kinship relations in iran.consequently, in these societies, to preserve honor and dignity, they consent to marry girls at an early age and try to protect them through marriage at an early age [18].the destiny and future of a person directly depend on the proper confrontation with changes and developments in this period [19].these negative effects are not limited to the children and expand to the families and society as well.thus, the present research aimed at providing a proper chance for all adolescent girls to participate in this study.or in case of their consent, they were asked to provide an email or a phone number without their name and the link of the questionnaire was sent to them to fill in the house without any stress and confidentially.this questionnaire is available as additional file 1: appendix.the questionnaire of the viewpoint of adolescent girls regarding the consequences of child marriage included 12 questions and was measured on the basis of the 5-point likert scale (totally agree, agree, no opinion, disagree, totally disagree).two qualitative and quantitative methods were used to assess the content validity of questionnaires.in the qualitative method, the content validity of the questionnaire was assessed by 10 specialists in this field.these people included faculty members of midwifery and reproductive health of tabriz and tehran universities of medical sciences.after receiving their opinions, the required modifications were made.in the quantitative method, content validity was assessed using content validity ratio (cvr) and content validity index (cvi).the results indicated that the values of cvr and cvi amounted to 0.88 and 0.84, respectively.the normality of the data was determined by kurtosis and skeweness, all of which had a normal distribution.to report the status of knowledge and viewpoint of adolescent girls, and the causes and consequences of child marriage, the descriptive statistics including frequency (percentage), mean, and standard deviation (sd) were used.in multivariate analysis, a multivariate logistic regression model was used with adjustment of sociodemographic characteristics.then the variables that were related to the viewpoint with p < 0.2 were included in the multivariate logistic regression model along with the knowledge variable.the results of multivariate logistic regression were reported as a adjusted odds ratio (aor) with a 95% confidence interval (95% ci).p < 0.05 was considered statistically significant.more than two-thirds of the participants (63.3%) were students of public schools.among them merely parents of 10 participants (3.3%) were divorced.investigating the viewpoint of adolescent girls regarding the causes of child marriage revealed that the important issue leading to reduction of child marriage can be “intellectual, emotional, social, and economic maturity of girls plus their physical puberty (92.4% agree), increase in the girls’ education (79% agree) and increasing girls’ awareness regarding the consequences of early marriage in schools and media (69.6% agree).the results revealed that the mean knowledge of the adolescent student girls regarding child marriage amounted to 6.70 (ranging from 0 to 11).babiker et al. conducted a study in al jazeera.they found that the majority of the participants (77.4%) had perfect knowledge about the negative consequences of child marriage [24].vandana et al., in their research in india, manifested that knowledge of school girls regarding early marriage was 14.3 (ranging from 10 to 21) [2].it did not match the results of the present research.these differences might be due to regional and cultural differences.the results of a study in india revealed that more than half of school girls (52.5%) had a good attitude and 47.5% had a relatively good attitude regarding early marriage [2].sumalatha et al. conducted a study in which 73.3% of participants disagreed with early marriage [25].the results of the present research did not match the results of this study, which can be due to regional and cultural differences.however, the results of the survey regarding the values and attitudes of iranians included all provinces of the country, together with rural regions, nomadic tribes, and outskirts.investigating the viewpoint of adolescent girls regarding the causes of child marriage revealed that the important issue leading to reduction of child marriage can be intellectual, emotional, social, and economic maturity of girls plus their physical puberty, increase in the girls’ level of education and increasing girls’ awareness regarding the consequences of early marriage in schools and media.ahmed et al. carried out a study in egypt.in accordance with the rules and regulations of the international communities including unicef, child marriage includes any official, customary, or unofficial mirage, in which one or both of the spouses or sexual partners are under the age of 18-year-old [1].thus, the legal age of marriage in iran is different from other societies and the consequences caused by child marriage in iran are reported less than its actual measures.one of the limitations of the present research is being cross-sectional, due to which the relationship of knowledge and viewpoint in this research cannot be demonstrated as cause and effect relationship.the responses could be biased due to the nature of the questions of the research and the researcher tried to eliminate this limitation by ensuring the participant of confidentiality of their information and completing the questionnaires without mentioning their name and surname.besides, the present research was conducted in the city of tabriz, with azari ethnicity, therefore, its results cannot be generalized to other cities and ethnicities.conclusions the results of the present research indicated a good level of knowledge and negative attitude to child marriage among the girls.
number of words= 958
[{'rouge-1': {'f': 0.5479442368481846, 'p': 0.7753045186640473,'r': 0.4236945812807882}, 'rouge-2': {'f': 0.3219021686112577, 'p': 0.43811023622047246,'r': 0.2544181459566075}, 'rouge-l': {'f': 0.46286686350962586, 'p': 0.6206607929515418,'r': 0.36904306220095695}}]
-----------------------------------------------------------------------------------------------------------------------------------
p38:
Extractive Summary:
globally more than 25 million unsafe abortions women were reported in 2014, and most of them occur in developing countries [7].abortion was among the top leading causes of maternal mortality.according to the ethiopian demographic and health surveillance 2016 report, the maternal mortality ratio of ethiopia was 412 per 100,000 live births [11].report suggests that mch service utilization was determined by age, birth rank, educational level, monthly income, number of children, occupation, knowledge about sexually transmitted infections, educational status, residence, parity, partner communication, and the presence of tv/radio [13–19].in resource limited settings, evidence on the effect of stillbirth and abortion women on the next pregnancy was scarce, which affects the quality of maternal and child health services.this research work generates evidence and improves the quality and utilization of maternal and child health services.women out migrated from the catchment areas were excluded.baseline data were collected from the post-abortion women and stillbirth women in the gynecologic ward using the patient interviews.descriptive statistics were used to present the profile of study participants, mch service utilization and estimate pregnancy- related complications after stillbirth and abortion women.poisson regression was used to identify the determinants of mch service utilization after stillbirth and abortion women.results a total of 4117 women was included giving for the response rate of 96.6%, 64 women were out-migrated from the study areas and the medical records of 81 women were incomplete.the proportions of stillbirth and abortion women completed the survey was 95.5% and 96.9% respectively.from 3075 abortion women, 24.09% of study participants mention rape as a reason for abortion women and 58.08% of study participants not volunteer to mention the reason for abortion women.post traumatic stress syndrome (ptsd) was observed in 44.9% of study participants (table 2).during the consecutive 3 years, maternal and child health service (mch) was utilized by 42.1% of study participants.after adjusting for residence, marital status, stillbirth, abortion, family size, age, gravidity, the presence of tv/radio, knowledge on contraceptive, educational status, access to contraceptives; mch service utilization was associated with age, knowledge of contraceptives, educational status, the presence of tv/radio, marital status, and family size (table 3).gestational diabetes mellitus and pregnancy-induced hypertension were the common complications of stillbirth for the next pregnancy; pregnancy induced diabetes mellitus was observed 14.3% of pregnant mothers with a past history of stillbirth and pregnancyinduced hypertension were observed on 9.2% of mothers with a past history of stillbirth (table 4).most of the abortion women were single by marital status (91.2%) but only 4.4% of stillbirth women were single.this finding agrees with researcher results from nepal [21].this occurs to the reason that pregnancy before marriage is usually unintended and unsupported ending with an abortion [22].58.1% of abortion women do not want to report the reason for abortion, but, post-traumatic stress disorder was present in 36.8% of the post-abortion women and 68.9% of the stillbirth women.the low prevalence of ptsd among post-abortion women indicates the high proportion of unintended pregnancy in the group [24].hepatitis b was present on 6% of the abortion women and 3.2% of the stillbirth women.hiv was detected in 3% of abortion and 0.8% of stillbirth women.this finding was in line with the uganda research outputs [26].this is due to the reason that most abortion women were single with their marital status; they might have multiple sexual partners, which finally increase the risk of acquiring hiv.hepatitis c was diagnosed in 4.7% of abortion and 0.3% of stillbirth women.this indicates that the tendency for repeated abortion was higher.the ghanaian research article also reported poor knowledge of contraceptives among abortion women [29].mch service utilization was 4.3 folds higher among women with tertiary education [irr 4.29, 95% ci 3.72– 4.96], 3.14 folds higher among women in secondary education.
number of words= 611
[{'rouge-1': {'f': 0.5338855681476876, 'p': 0.7281920903954802,'r': 0.42143288084464553}, 'rouge-2': {'f': 0.3452026767725223, 'p': 0.4581019830028329,'r': 0.2769486404833837}, 'rouge-l': {'f': 0.4370282392786855, 'p': 0.5790909090909091,'r': 0.3509364548494983}}]
-----------------------------------------------------------------------------------------------------------------------------------
p39:
Extractive Summary:
background in 2018, cervical cancer (cc) caused 311,000 deaths worldwide, and 90% of these deaths occurred in lowand middle-income countries (lmics) [1].the who recently called for the elimination of cc as a highly preventable public health problem [2].however, this strategy showed mitigated success mainly because it requires substantial labor and because its performance can be highly variable [4, 5].burkina faso exemplifies such a situation.the intervention was based on the premise that primary hpv screening and subsequent management (triage and treatment) should be performed on the same day, the socalled “screen-and-treat” approach, to increase women’s chance of being fully screened and treated [9].as the leading cause of cancer mortality among women in the country [10, 11], the ministry of health considered cc a public health priority in 2011, resulting in many actions at the local and national levels.in burkina faso, the decentralized health system is divided into three levels.the peripheral level operates at the community and district level, providing basic preventive and curative care, it is the entry point in the health system.when necessary, patients can be referred from the primary level to the intermediate or central level, which consists, respectively, of regional hospitals and university or national hospitals [12].additionally, biomedical services are divided between public and private (including traditional health practitioners) sectors.currently, the national cc control strategy relies on via screening and cryotherapy delivered at primary and secondary healthcare facilities nationwide.in addition to via screening, colposcopy and more advanced treatment (leep, hysterectomy) are available in some private clinics or in the university hospitals of the two main cities (ouagadougou and bobo dioulasso).hpv testing was available either through self-performed or midwife-performed collection of vaginal specimens.women with via-positive lesions that fulfilled the international agency for research on cancer (iarc) criteria for cryotherapy [14] were supposed to be immediately treated with thermal ablation.otherwise, they were referred to an identified clinic for the appropriate treatment with the full cost covered by dotw.the intervention was implemented in 2019 in two urban primary healthcare centers in ouagadougou.their general characteristics are shown in table 1.these sites are believed to be similar to other urban primary health centers in terms of population, activity, staff and equipment.table 2 shows the theoretical framework of the paracao.component 2 – education of healthcare providers – the dotw program coordinator organized staff training.all midwives at participating sites received off-site three-block training.following a baseline of formative research and using consultations with main stakeholders and researchers (see additional file 1: table s4), we defined a theory of change [20] (toc, fig. 1) that depicts the various components of the implementation strategy, the assumed pathways through which these components could bring the targeted changes (mechanisms of change) and the underlying hypotheses that need to hold true for those changes to occur.sampling, data sources and collection table 3 outlines the data sources, participants, methods used and outcomes.the inclusion and exclusion criteria in the cohort were similar to those considered for screening eligibility.socioeconomic (ses) levels were calculated using a wealth index (see additional file 1: table s3) according to the asset method [21] and divided into terciles.participant cervical cancer literacy was assessed after cervical specimen collection using the cc literacy assessment tool (c-clat), a 16-item instrument that has been validated in various contexts [22–24].each item of the c-clat was scored as binary (0 = incorrect, 1 = correct), and the total score computed as the sum of individual items ranged from 0 to 16, with higher scores indicating higher literacy.facility routine health information system each participating facility has a health information system that collects routine data.we retrieved data derived from two registries: cc screening process (clinical data) and hpv testing (laboratory data) for the first implementation semester.qualitative data direct observations direct observations were performed at each participating facility.realized at various times of the day and of the week over 2 months, the observations covered 90 medical visits and 30 laboratory procedures.the same method was used to obtain a diverse sample of 20 women in terms of age, religion, ethnicity, and hpv status (n = 10 per facility).women were contacted 2 months after their involvement in the screening process and met outside the facilities.the interviewed women received transportation fee reimbursement as compensation for their time.semistructured in-depth interviews conducted in french with healthcare providers were performed during their working time and were recorded.dose indicators of dose were assessed at the individual level using data from the cohort study.quantitative data were analyzed using r software version 3.6.3, and qualitative data were analyzed using n-vivo software version 12.6.0.quantitative data analysis screening effectiveness and implementation data were described using counts and proportions for categorical data, means and standard deviations or medians and interquartile ranges (iqrs) for continuous data.implementation indicators that were considered nondiscriminant (i.e., variables with more than 95% or less than 5% frequency) were not included in the models.when collinearity among variables was detected, only one indicator was selected.each model was adjusted for the health center and demographic information (age, screening history, socioeconomic level, travel cost and literacy score).informed consent was obtained from all participants.the majority of them were aged between 25 and 35 years old (61.4%), lived in ouagadougou (97%) and had no previous screening history (65.3%), with no difference between study sites.in center b, the screening rate was lower but constantly increased from may to december.dose the average time to submit samples to a laboratory was 1.22 h (sd = 0.88) and was twice as high in center b than in center a (see additional file 1: table s6).the average time between specimen sampling and results disclosure was 2.7 days (sd = 4.01), with no statistically significant difference between centers.in the multivariate analysis (see additional file 1: table s7), screening completeness was positively associated with having performed a selfsampling (adjusted odds ratio (ora) = 4.18; 95% confidence interval (ci) [1.09–17.72]) and having a good understanding of the test results (ora = 10.62 95% ci [2.10–63.04]).the same factors were found when the analysis was restricted to women who did not benefit from a single visit (see additional file 1: table s7), alongside a higher odds of not completing the screening process when the time until result disclosure increased (ora = 0.16, 95% ci [0.02–0.89]).postresult satisfaction was negatively associated with the level of understanding of the results (ora = 0.06 95%ci [0.01–0.23]) (see additional file 1: table s8).postresult satisfaction was positively associated with the amount of money spent on travel (see additional file 1: table s9).effects of context on implementation outcomes a difference between the reach of the centers was observed at the beginning of the implementation, and these differences can be explained by a “launching effect”.indeed, the official start of the paracao was announced on tv and in newspapers, and despite the announcements indicating that two centers would be involved, the campaign was more focused on one of the two centers.in addition, despite having similar populations in their service areas, the two centers had different staffing and activity capacities (table 2), which could also explain the differences found in effectiveness and implementation outcomes.in one center, hpv test results could be given on the same day as the sampling, but if via was needed, the women had to return another day.in both centers, laboratory staff adapted the initial plan by setting closing times for sample collection that matched their own organizational schedules.the closing times were 10:00 am in one center and 11:00 am in the other, which allowed laboratory staff to perform other routine tests without affecting their working hourslikewise, the laboratory staff considered that they were trained for more than hpv testing: “it’s a plus for us, because you can test many other things than just hpv with this platform.this disappointment was higher among women who had heard of the screening procedure from peers: “my cowives told me that they explain the cancer causes and give advice.some participants believed that the healthcare providers should have decided instead of them, while others were happy with the opportunity offered.from interviews, it appeared that the women with the highest level of education and prior awareness of the screening novelty were more likely to choose selfsampling, as it respected their intimacy: “i appreciated it [the self-sampling] more, because with the old version and the speculum, it hurts.discussion this study provides insight into the implementation process of an hpv-based cervical cancer screening program (fig. 5), which is important for the future expansion of hpv-based screening of cc as envisioned by the who [26].we found that the healthcare providers of both facilities adhered relatively well to the various components of the screening implementation and accepted integrating hpv screening into their work schedules.as observed in other contexts, this type of adaptation to fit an organization could enhance sustainability [28, 29].through this analysis, we identified aspects of the implementation process critical to screening completeness and satisfaction.indeed, the literature shows that effective health communication requires both cultural and language adaptations [35, 36].in addition, self-sampling as an empowering tool [40] may have played a role in this high completion rate.dotw is an ngo known for its work with communities, which may explain why women were more prone to participate and return to the health center when contacted by dotw [44].moreover, the program was implemented in an urban area within a major city, where constraints associated with healthcare access—road conditions, field work, distance—are less important [38, 45], which could also explain this high completion rate [37].observations and interviews in healthcare centers revealed that wait times were related to health service organization and staffing.however, we tried to mitigate this effect through the observation process itself (establishing rapport, long-time observation) and later through a triangulation between all qualitative and quantitative data.this could have led to overestimation of extreme experiences (positive or negative).however, findings from interviews were convergent with findings from the cohort study, which suggests that this bias may have been reduced.we tried to overcome this issue through our cohort study sample size and maximum variation during interviews with women to reflect various screening situations.maintaining a single visit for the screen-and-treat approach may turn out to be more important in such settings, but more research is necessary to confirm this.conclusion despite some limitations, we believe that our results have important implications for future programs and healthcare providers in low-resource settings, especially in the context of expanding hpv-based screening strategies at the primary-care level.first, the single-visit approach should not be the ultimate goal for hpv-based cervical screening, and the multiple-visit approach is an acceptable option in the urban context as long as results are given within 48 h and adequate counseling is provided.baseline assessment is needed to adapt the intervention to workload and staffing constraints to reduce the wait times for testing and result disclosure as much as possible.
number of words= 1774
[{'rouge-1': {'f': 0.27856203453877176, 'p': 0.7853846153846153,'r': 0.1693059263214095}, 'rouge-2': {'f': 0.15141736290267765, 'p': 0.2977992277992278,'r': 0.10151709401709402}, 'rouge-l': {'f': 0.2879333799110005, 'p': 0.6549056603773584,'r': 0.1845320197044335}}]
-----------------------------------------------------------------------------------------------------------------------------------
p40:
Extractive Summary:
background a persistent research finding in finland and elsewhere has been variation in medical practices both between and within countries.some of the variation seems to be unwarranted, as these differences do not vanish when regional differences in need are taken into account, thus potentially reflecting inequity or inefficiency in the system [2].variation seems to exist especially if medical decision making involves discretion and the best treatment cannot be identified unambiguously [3].this is true for hysterectomy when performed for benign causes.while most studies investigating differences in use of hysterectomy have been longitudinal, studies on temporal variations within one country are rare.it has not been examined how variation has evolved since.the system is mainly financed by tax revenues and user-fees are generally low.the register covers all hospital discharges in all public and private hospitals in finland.persons whose permanent residence was outside finland and those in long-term care were excluded from the analyses.we defined hysterectomies using nomesco operational codes (lcc00, lcc01, lcc10, lcc11, lcc20, lcc96, lcc97, lcd00, lcd01, lcd04, lcd10, lcd11, lcd30, lcd31, lcd40, lcd96, lcd97, lef13, lef14).peripartum hysterectomies (mca30, mca33, mcw00) were not included in the study.age was classified into four age groups: 25–49 years, 50–59 years, 60–69 years and 70 or over.they are owned by federations of municipalities and responsible for organisation of public specialised care for the residents of their area.it is a relative measure that indicates whether the variation found is larger than could be expected by chance.to examine consistency in regional variation in use of hysterectomy from 2001 to 2018, the autocorrelations per region and per indication were computed according to the method described by westert et al. [2].the trend analysis was performed for biennial agestandardised hysterectomy rates with fixed effects for intercept and a polynomial function of time and random effects for intercept and slope varying across regions.in these models, the time trend of spatial variation is given by the correlation of the intercept and the slope in time at level 2.we interpreted a statistically significant negative correlation as decrease of regional variation with higher than average random intercepts linked with larger than average decrease in random slopes and vice versa.statistical analyses were performed using sas 9.4 (sas institute inc, cary, nc, us).results a total of 131,695 hysterectomies performed in finland 2001–2018 were included in the analyses.descriptive statistics of hysterectomies in 2001–2002 and 2017–2018 are summarized in table 1. in 2001–2002 the total number of hysterectomies performed was 21,305 and 11,812 in 2017–2018, which corresponds to a decline in agestandardised hysterectomy rates from 553/100,000 py in 2001–2002 to 289 in 2017–2018 (− 49%).in the beginning of the study period, leiomyoma was the most common indication followed by uterine prolapse and malignancy.hysterectomies performed due to leiomyoma decreased by 71 per cent during the study period, leaving uterine prolapse as the most common cause in 2017–2018.the rate of hysterectomies performed for leiomyoma decreased from 212/100,000 py in 2001–2002 to 61/100,000 py in 2017–2018, accounting for the largest part of decline in the total rate.compared to other indication groups, the decrease in rates was less pronounced in hysterectomies due to uterine prolapse (change − 27%) and malignancy (change − 14%).respectively, conservative and hysteroscopic treatment methods are increasingly being used for aub, leiomyoma and endometriosis.for other causes, the changes were relatively small (table 2).this suggests that most regions that have been above (or below) the national average on some time-point have stayed high (or low) in consequent years.the average autocorrelation varied from 0.31 (malign causes and endometriosis) to 0.50 (uterine leiomyoma).for all hysterectomies the correlation coefficient was − 0.84 and the correlation was smallest in genital prolapse (− 0.71).the overall decrease in rates could be attributed mainly to the decrease in hysterectomies performed for leiomyoma as it was the leading indication at the beginning of the study period.regional variation in indication specific rates has earlier been reported from germany [21], australia [22] and the netherlands [23].although variation in medical practices is widely known, changes in regional variation within one country have been a focus less frequently.a systematic review reported that more than 95% of discharges could be identified from the register and that the positive predictive value, i.e. the proportion of registerdetected cases that are confirmed to be true-positives according to external data varied between 75 and 99% for common diagnoses [31].a potential limitation is that the analysis takes into account the age structure of the different patient populations but lacks data on other patient characteristics likely to affect clinical decision-making, e.g. patients’ reproductive history, prior surgical procedures and other health related factors.also, age structures in operations differ by indication with younger patients in aub and endometriosis and we were not able to examine patients aged 25–39 years and 40–49 years separately as the numbers of cases did not enable it while we acknowledge that the reasons for hysterectomy can differ before perimenopausal period.as we did not have data on the incidence of ovarian cancer, cervical cancer, and uterine cancer we could not analyse them in more detail.more in-depth studies are needed by indication taking into account the seriousness of the disease, availability of other treatment options, patient preferences as well as comorbidi
number of words= 852
[{'rouge-1': {'f': 0.3728160772819767, 'p': 0.7772072072072072,'r': 0.24522321428571428}, 'rouge-2': {'f': 0.21616024158470287, 'p': 0.3912669683257919,'r': 0.1493296089385475}, 'rouge-l': {'f': 0.38630274417232224, 'p': 0.6699999999999999,'r': 0.2713888888888889}}]
-----------------------------------------------------------------------------------------------------------------------------------
p41:
Extractive Summary:
many women ceased ht abruptly, with some recommencing ht due to intolerable symptoms [10].ht is efective in vulvovaginal atrophy and may improve sexual function and other related symptoms such as joint and muscle pains, mood changes and sleep disturbances.however, prescriptions for ht have continued to tumble.prescriptions for ht declined steeply, with estimates that within 18 months of whi 2002, half of us women using systemic ht stopped treatment.teir analysis concerningly suggested that between 18,601 and 91,610 excess deaths occurred among hysterectomised women aged 50–59 years following the publication of the original whi.despite hormone therapy being efective in reducing all-cause mortality [22], many women seeking relief of menopausal symptoms exaggerate ht harms, overstate the perceived benefts of alternative therapies, or ignore their risks [23–25].existing models suggest decision-making is driven by both internal (e.g. individual characteristics, values, attitudes, beliefs and preferences) and external factors (e.g. healthcare provider context, facts and information) [19, 20] but these models have limitations.evidence was collated mainly from cross-sectional studies, with many conducted pre-whi 2002 [19, 20].women seek information to manage their health from a range of sources including social networks, social media, and the internet [9, 27].a consumer medicines call centre (mcc) operated by pharmacists provides an alternative, anonymous source for obtaining reliable and evidence-based medicines information [31].we sought to understand the information-seeking and decision-making behaviour of women accessing a mcc with menopause symptom management concerns, as evidence changed over time.methods data sources we longitudinally analysed routinely-collected health service data from two australian mccs operated by clinical pharmacists from mater health, brisbane: (1) queensland medication helpline (qmh), a state-wide service operating from july 1996 to september 2002, i.e. pre-whi and two months post-whi 2002; and (2) nps medicinewise, formerly the national prescribing service (nps) medicines line (ml), a nationwide service, september 2002–june 2010.together these datasets provide continuous, longitudinal data over 14  years; and can be divided into three time periods (fig. 1): • time 1 pre-whi 2002 (june 1996–9 july 2002), menopause-related calls were only 4.1% of all calls, most related to ht only.• time 2 whi 2002 publication and associated negative media (10 july–19 september 2002), menopauserelated calls increased to 16.6% of all calls.• time 3: post-whi (20 september 2002–30 june 2010), menopause-related calls decreased to 0.3% of queensland and 0.4% of calls from the rest of australia.te time ranges were identical for time 1 and time 3; but we used a deliberately narrower ‘window’ (10–26 july 2002, time 2a) for the qualitative analysis (fig.  1) to capture questions and concerns in the period immediately following whi 2002 and the media response.data collection call data were recorded manually on a scannable collection form and transformed into an electronic dataset.we fltered the testosterone calls by patient gender (‘female’ or ‘blank’) to enhance call retrieval and we excluded non-menopause data.call characteristics were compared among: (1) menopause-related and roc over time; (2) queensland and other states in the australian dataset; and (3) queensland callers of qmh and ml (additional fle  1).menopause-related calls surged in the two months after whi 2002 (july 2002, fig.  1); two in three calls (66.7%) were motivated by media reports.calls about menopause-related hm were few: only 8.4% before whi 2002 and 18.6% later on (fig. 1).tere were 846 total questions (some narratives had more than one question): 229 from before whi (time 1), 70 immediately after whi (time 2a), and 547 from september 2002 (time 3).many had abruptly ceased, or were seeking reassurance to cease, their ht (including women who had had a hysterectomy) because of breast cancer fears.“what should i do?similarly, calls about therapy options and regimen comparisons at times 1 and 3 were not evident at time 2a.information-seeking occurred in response to heightened uncertainty: worrying symptoms, therapy dissatisfaction, inadequate or conficting information, perceived risk, or change to treatment availability.any ambiguity stimulated decision reassessment.te more intense the uncertainty (e.g. negative media about ht), the more women relied on emotions to emphasise risk over beneft.negative media was the main driver of information-seeking for decision-making; calls increased 400% in the frst two weeks and remained high for the subsequent two months.while the efect on helpseeking was relatively short-lived, consumer concerns about safety persisted for years after whi 2002.in an invited review about media and the consumer, shapiro highlighted the pervasive impact of media on public perceptions of risk, citing vincent covello (center for risk communication, columbia university): “research has shown that strong beliefs about risk, once formed, change very slowly and are extraordinarily persistent in the face of contrary evidence.while this might simply refect lower ht use due to progressive product withdrawals after 2002, few studies assessing the relationship between product withdrawal and use could be retrieved [42–44].afterwards, women were more likely to seek confrmation that ht or hm were safe to use with their other medicines, suggesting increased interest in hm as a substitute for ht [24, 27].before whi 2002 some clinicians prescribed ht for women who were symptom free; while afterwards ht use was largely limited to patient requests for symptom relief [21].calls about hm primarily originated from areas with good access to service centres where a pharmacy, health store, or naturopath are more likely to be located.our fnding is consistent with a survey of hm use by 2,020 australian middle-aged women which found a higher prevalence of hm use for menopausal symptoms among women residing in metropolitan areas [46] perhaps refecting increased availability and income.te mccs were in australia, so our results might not refect the concerns of women in other places with diferent cultures and ethnicities.our study period ended in 2010, prior to the global consensus statement on menopausal hormone terapy in 2013 [47]—the ofcial reassurance made by various medical societies about the safe use of ht in specifc cohorts of women.we need to know more about women’s information needs for all available treatments, especially after 2013.tis is infuenced by heightened intolerance to risk, associated with a change in competing internal factors (woman’s specifc symptom concerns and tolerance to risk), and external factors (the impact of negative or conficting information, source trust and product availability (fig. 2).decision-making is an iterative process.negative or conficting source information, worrying symptoms, perceived treatment risk, and decreased treatment availability contribute to perceived risk predominance.
number of words= 1027
[{'rouge-1': {'f': 0.409017703949871, 'p': 0.7611314984709481,'r': 0.2796474953617811}, 'rouge-2': {'f': 0.17867967985532912, 'p': 0.275521472392638,'r': 0.13220984215413184}, 'rouge-l': {'f': 0.3580737365825675, 'p': 0.5984974093264248,'r': 0.2554545454545455}}]
-----------------------------------------------------------------------------------------------------------------------------------
p42:
Extractive Summary:
factors such as genetics, dietary habits, level of activity, and daily exercises cause differences in the natural age of menopausal in various communities.low self-efficacy, extensive cultural conflicts, and socioeconomic inequalities, belief and gender inequalities, knowledge of the menopausal process, and stressors are the most significant factors influencing the menopausal quality of life (qol) [5].the multidimensional qol has been accepted nowadays [6].each woman spends at least one-third of her life during the menopausal period [8].in 1998, more than 477 million women were living in the postmenopausal period, and this rate will reach 1.1 billion people by 2025 [9].according to some newer statistics in the us, 6000 people reach menopausal age every day, and about 50 million will be menopausal over the next decade.iran is also expected to have about 5 million women with menopausal by 2021, leading to demographic changes and population aging [10].based on the results of various studies in iran, the qol of menopausal females has been reported at a moderate level [11, 12].also, given the role of women as the key members of family health and transmitting the model of education and promoting a healthy lifestyle to the next generation, acquiring knowledge of this period’s symptoms and enhancing their qol is of great necessity [5, 14].materials and methods study type the current descriptive-analytical cross-sectional work was performed on 270 women aged 45–60 years covered by hamadan health centers in 2018.the word informed consent was obtained from all women; they were informed about the confidentiality of the information and the project’s purpose, and only if they would like, they were enrolled in the study.survey questionnaire the data collection tool used in this study consists of two parts completed by the women through the self-reporting method.a “zero” is identical that the woman has not experienced this specific symptom in the previous month; score “one” demonstrates that the woman experienced the symptom, but it did not bother her; scores “two” through “seven” demonstrate an increasing level of the bothering symptom, so that these scores indicate increasing levels of bother experienced from the symptom and correspond to the ‘‘1’’ through ‘‘6’’ checkboxes on the menqol [6, 20].it is essential to be mention, with increasing menqol scores, levels of bother experienced from the symptom are increased as well.according to the definition of who, menopausal health was considered menstruation cessation without medication for at least 12 months [4].before the study, necessary explanations were provided for the subjects regarding the study’s objectives, and their informed consent was obtained.the demographic characteristics questionnaire and qol questionnaire of postmenopausal females were completed through a face-to-face interview.results demographic characteristics the mean age of the women was 52.19 ± 3.98 years with a range of 45–60 years old.the menopausal quality of life, and associated factors according to the association between quality of life and demographic variables, there was a significant association between the total qol score and job, economy status, smoking, exercise, supplemental omega-3 s intake, and postmenopausal stage (p < 0.01).however the lowest score was menopausal females who had taking supplemental omega-3 s than those who hadn’t taken it (mean = 2.15 ± 1.06 vs. 2.65 ± 0.97; p < 0.001).though women who had postmenopausal stage less than 5 years stage (mean = 2.28 ± 0.87) had significantly lower menqol scores from those who had postmenopausal stage 5 or more years (mean = 2.63 ± 1.16; p < 0.001).although, there was a significant association between the psychosocial dimension of menqol and education, job, economic status, smoking, exercise, supplemental omega-3 s intake, postmenopausal stage (p < 0.01), and marital status (p < 0.05).clerk women (mean = 1.67 ± 1.24) had significantly lower psychosocial scores from housewife women (mean = 2.68 ± 1.07; p < 0.05).while menopausal females who exercised more often had psychosocial scores (mean = 3.68 ± 0.95) lower than those who exercised less than 3 times per week (mean = 0.85 ± 0.55); p < 0.001).also, married women (mean = 2.14 ± 1.26) had significantly lower psychosocial scores than divorce women (mean = 2.94 ± 1.15; p < 0.05).by way of, clerk women (mean = 2.22 ± 0.87) had significantly lower physical scores than housewife women (mean = 2.68 ± 1.07; p < 0.05).while menopausal females who exercised more often had physical scores (mean = 1.79 ± 0.9) lower than those who exercised less 3 times per week (mean = 2.37 ± 0.92); p < 0.001).by way of, clerk women (mean = 1.17 ± 1.49) had significantly lower sexual scores than housewife women (mean = 2.23 ± 1.79; p < 0.001).educated women (mean = 1.40 ± 1.69) had significantly lower sexual scores than women with low levels of education (mean = 2.44 ± 1.50; p < 0.001).while menopausal females who exercised more often had sexual scores (mean = 1.65 ± 1.54) lower than those who exercised less 3 times per week (mean = 2.43 ± 2.05); p < 0.05) (table 2).the mean score of menqol in menopausal women was 2.45 ± 1.04.discussion the present work was performed to determine the prevalence of menopausal symptoms among postmenopausal females and factors associated with the quality of these women’s lives in hamadan city.on the other hand, based on the results of fallahzadeh et al., being employed, and high social interaction with other people can reduce the physical symptoms of menopausal [6].in this sense, li et al. have previously reported low economic status as the main factors contributing to the incremented severity of menopausal symptoms or a longer duration of the menopausal symptoms [25].however, being employed and having high-income levels may be increased access to healthcare or the level of coping mechanisms for menopausal symptoms.nevertheless, some women experience symptoms earlier during the time since menopause while some experience them at a later time.in the present study, the highest menqol in menopausal women belonged to the vasomotor dimension, and the lowest belonged to the sexual dimension.in this period, dramatic changes occur in hormone levels, including reduced severe estrogen, which results in vasomotor symptoms (vms) such as hot flushes and night sweats.in agreement with the findings in the current work, ceylan et al. indicated that the most frequently experienced symptoms during this period were aching muscles and joints [39].conclusion in general, the results of the present work revealed that vasomotor symptoms were the most dominant symptom.
number of words= 1042
[{'rouge-1': {'f': 0.493482782995052, 'p': 0.9172222222222222,'r': 0.3375438596491228}, 'rouge-2': {'f': 0.35430452357892617, 'p': 0.6298885793871867,'r': 0.24647058823529414}, 'rouge-l': {'f': 0.4832795618806348, 'p': 0.7623076923076924,'r': 0.3537837837837838}}]
-----------------------------------------------------------------------------------------------------------------------------------
p43:
Extractive Summary:
for this reason, it is essential to develop efficient health communication strategies aimed at the lay public.in recent times, social networking sites (snss), which are web-based services that allow users to create a profile and connect with other individuals within the system, have emerged as powerful health communication platforms [6].users can share content without any verification by editors, reviewers, or factcheckers.often, such content reaches a number of users similar or even greater than traditional media [9].the aim of this study is to analyse news stories about breast cancer shared on social networks from varied perspectives.we seek to understand the characteristics regarding the narratives in our sample, including the credibility of the content with more public engagement.data collection was conducted between 17 june 2019 and 17 june 2020. to collect our sample, we used an online tool called buzzsumo [23], which monitors the web and social media feeds to show the most popular content in any niche.we searched for the keyword "breast cancer", in quotation marks, so that we only have results displaying this exact term, and not the words separately.in 2020, facebook has more than 2.7 billion monthly active users [24]; twitter has 321 million [25]; pinterest has 442 million [26], and reddit, 430 million [27].our search was limited to pages in english, with no country restrictions.the news stories filtered by buzzsumo were exported to an excel table containing the following information: total shares (sum of shares across all analysed social networks); total facebook shares; twitter shares; pinterest shares; total reddit engagements; and published date.2. coding: coding is the step in transforming raw data from corpus, making use of records to be grouped in the future [29].3. categorization: each news story was considered by us as a unit of our corpus.in this way, we used different dimensions to categorize each column of the coding schedule (table 1).two researchers dedicated themselves to the analysis of the material and its classification.after analysing this initial sample, one of the researchers completed the categorization of the entire corpus.credibility analysis and types of rumours we initiated our content analysis by classifying news stories according to its credibility.for this, we first separated them into the following categories: "verified" (scientifically accurate) and "rumours" (scientifically inaccurate or false) [10].the following categories were established: (1) misleading content: describes stories which are not entirely false yet lead the reader to misinterpret the data; (2) false connection/ context: this encompasses wardle’s categories of manipulated content, false connection, and false context.we classified a rumour in this category when the headline does not support the content of the news story, or when genuine images, videos, photos, and audios were used outside their original context, or were manipulated.(3) fabricated content: news stories without any trace of genuine information (both in the textual and non-textual parts) were classified in this category.as an example, we can mention radio or television networks, newspaper publishers, book publishers and movie studios.yes’ or ’no’ (table 1).statistical analyses data were analysed using ibm spss statistics (version 26.0) predictive analysis software, and microsoft office excel (version 16).depending on the sample size, fisher’s exact test or chi-square test was used to determine the relationship between two categorical variables.among the twenty stories with the most shares, thirteen were published by traditional media outlets: four times by fox news and once by nine other media entities, such as the epoch times, metro, cnn and nbc news.in consideration of the number of shares in relation to the credibility of the content (fig. 1), we see the content classified as "rumours" tends to be more shared than those scientifically correct, both in traditional and digital media.“real life stories” are 58.3% (1.4 times more) “rumours” and 41.7% “verified”.” content type regarding rq2, when we examine the distribution of content type categories in our sample (fig. 2), we see most stories are classified as ‘real-life story’ or ‘solidarity’ (67.69%).during the period studied, we also noticed a recurrence of the same news among the stories with the most total shares (table 2).the death from covid-19 of a mom of 6 who survived breast cancer was also noteworthy, being repeated four times.mentions prevention and early detection/screening to answer rq3, most analysed news stories do not address ways of preventing or early detecting breast cancer (table 3).there is an extraordinarily strong statistical connection (cramer’s value = 0.435; fisher’s exact test, p < 0.001) between content type and prevention; and between content type and early detection (cramer’s value = 0.355; chi-square test: p < 0.001).stories whose theme are "opinion", "educational" and "risk factors" have the highest proportion of references in prevention.breast cancer awareness month regarding rq4, we compared the news published in october (known as the “breast cancer awareness month” or “pink october” in several countries around the world [5]) with the other months.there are several variations in relation to the topics covered and in relation to the credibility of the most shared content (chi-square test: p < 0.001; cramer’s value = 0.300, extraordinarily strong connection).the majority (69.7%) of our sample could not be classified as to credibility, as they do not address topics related to science, technology and treatments.this trend has previously been observed.in a study that evaluated the accuracy of the most popular articles on snss relating to genitourinary malignancies [33], there was a significantly higher average number of shares for inaccurate and misleading articles, compared to accurate ones.the same tendency was observed in a study dedicated to examine the spread of information related to zika virus on the internet [10].it was classified as ‘false connection/context’ because the title implies the vaccine is a reality, since a patient has been cured of cancer.however, the text of the article shows the story is more complex than it may seem at first: the vaccine is still a trial, and this patient was the first one to be tested.we concur that the title is sensationalist, since it leads readers to conclude something that is not yet realistic.this same story, with the same or remarkably similar titles, was reproduced thirty-two times in our sample.”the most shared “content types” in the sample of this study were “real-life stories” and “solidarity” (67.69%).these stories focus on the daily life, or personal narratives of cancer patients, their family members, or friends.there is no educational or awareness objective regarding risk factors and prevention of breast cancer.moreover, a study conducted in hungary revealed most respondents were unaware of the fact that breast cancer selfexamination should be initiated two decades earlier than mammography, when women turn twenty years old [42].a systematic review of breast cancer screening discourse on social media [46] indicates there is a substantial presence of unscientific statements shared by lay individuals about the topic, such as “mammography causes breast cancer”, or “breast cancer can be prevented by organic food”.on the other hand, there is an increase in stories regarding solidarity.a second limitation is the fact that our sample is limited to stories in english.if we analysed other languages, we might have discovered differences in the topics covered and in the credibility of the news.the same should be done during pink october, when there is an increase in internet searches on the topic.
number of words= 1183
[{'rouge-1': {'f': 0.3582477867204377, 'p': 0.7710309278350516,'r': 0.23333066453162532}, 'rouge-2': {'f': 0.20963389757125137, 'p': 0.38724137931034486,'r': 0.14371794871794874}, 'rouge-l': {'f': 0.3446453584112057, 'p': 0.6200000000000001,'r': 0.2386541737649063}}]
-----------------------------------------------------------------------------------------------------------------------------------
p44:
Extractive Summary:
ten, the former frst lady of ethiopia, roman tesfaye abneh, publicly championed cervical cancer prevention, garnering attention and funding for the cause as well as establishing a national cancer control taskforce.since the national screening program was started in 2015, more than 250 health facilities across ethiopia have started screening and the ministry plans to increase that number to 800 in the next phase of scale-up (personal communication, ministry of health, august 9, 2019).still, uptake of cervical cancer remains low.facilityand community-based surveys have found screening utilization ranging from 0 to 24.8% in populations across the country [4–23].barriers to uptake of screening such as low community awareness, lack of experience with screening, accessibility of screening services, long distances to health facilities and transportation issues, fear or embarrassment associated with cervical exams, other negative attitudes about screening, and misconceptions about cervical cancer have been documented, primarily through questionnaires distributed to women [4, 11, 24–27].despite the challenges, women indicate a high willingness to be screened; 86.2% of hiv-positive women in an addis ababa facility-based study were willing to be screened if free of cost and yet only 17% reported that they had ever received a provider recommendation for cervical cancer screening [11].providers anecdotally report many challenges to implementing screening, particularly if screening has never been or is currently not ofered at their health facility.aside from the addis tesfa implementation report and another quality improvement project, there are no studies to the best of our knowledge documenting provider perspectives on screening program implementation in ethiopia to date [3, 28].te aim of this study, therefore, was to describe provider perspectives on cervical cancer prevention and control eforts in ethiopia, with a focus on cervical cancer screening using visual inspection with acetic acid.key informants, including screening program managers and cancer prevention focal persons, were interviewed to explore barriers and facilitators to screening implementation.methods eighteen semi-structured key informant interviews were conducted to elicit health workers’ perspectives on cervical screening as a cancer prevention strategy in one geographic region of ethiopia from may to july 2019.study setting te study was conducted in several administrative zones of the oromia region of ethiopia: shewa, west arsi, arsi, and bale.oromia is the largest administrative region of ethiopia by land mass and by population, accounting for about 35% of the country’s total population [29].study sites are located in eastern oromia, 90 to 500 km to the south and east of the capital city, addis ababa, along major transportation corridors as seen in fig.  1.te locations were purposively selected due to presence of cervical cancer experts (see participant and recruitment below) and feasibility of data collection (site accessibility with public transportation) while also attempting to include a variety of perspectives from locations with well-established vs. new screening programs.additionally, key personnel were recruited in addis ababa, where the country’s only oncology center and the federal ministry of health are located, to represent a national-level perspective.participants and recruitment key informants with specialized knowledge of cervical cancer screening, prevention, and treatment were selected.participants were any cadre of health worker or health administrators that could be described as cancer experts given their positionality directly ofering cervical cancer screening or treatment or overseeing the provision of cervical cancer services.tis included cervical screening program managers, other workers ofering cervical cancer screening for at least two years, gynecologic and clinical oncologists, medical directors, hospital chief executive directors, and town, zonal, or national cervical cancer focal persons.te participants could have been employed in primary, secondary, or tertiary government health facilities or health ofces.other inclusion criteria were that the participant must be aged 18 years or older, could have any level of formal education or any degree, and could identify as any race, ethnicity, gender, or religion.informants were recruited face-to-face in their places of employment.since few individuals in ethiopia meet the inclusion criteria, purposive snowball sampling was used to identify key individuals with the appropriate qualifcations and then solicit the help of study participants to identify additional participants [30].first, the in-country research team (bel, abh, dwk, ad, tk) made site visits to health facilities known to ofer cervical cancer screening.no list of health facilities ofering screening is publicly available and incomplete and inaccurate reporting led to signifcant discrepancies between communities that reported presence of screening services and those actively performing screening at the time of the study.trough consultation with local experts (abh, dwk, ad, tk), a community-based hear-say approach was deemed most accurate for screening community identifcation.meetings with hospital administrators and the screening program were used to identify specifc health workers at each facility that may be eligible to participate as well as additional facilities or participants at other facilities that may be contacted.vertical recruiting occurred, where we worked our way up and down the administrative chain of cervical cancer screening oversight in a particular location with strong social ties between participants as well as horizontal recruiting where weaker social ties or hear-say were used to direct us to other screening sites [31].a recruitment  script was used to share information about the study’s purpose, expected benefts and risks, and participant eligibility.no individuals were excluded or refused to participate.recruitment continued until data saturation was reached, as evidenced by informational redundancy in latter interviews [32].te researchers refexively discussed key themes at the conclusion of each interview and the decision to cease recruitment was subjectively made when themes became increasingly similar between participants.interviews semi-structured interviews were conducted in-person at participants’ places of employment or other private, ofsite locations at the convenience of the participant.an interview guide with pre-determined interview questions was used(additional fle  1: appendix  1).te interview questions were created in consultation with a gynecologic oncologist (dw) and a screening program manager for content validity.probing questions were also used to elicit more detailed information from informants, at the discretion of the researchers, and evolved throughout the study based on emergent themes from previous interviews [33].interviews were always conducted by at least two co-authors (bel and abh) who both have previous training in qualitative research methods.at some research sites, additional co-authors also participated in interviews (ad, tk).interviews were conducted in three languages, according to participant preference: amharic (ofcial, national language), afan oromo (ofcial, regional language), and english.participants and interviewers may have used multiple languages in the same interview, as preferred to fully communicate meaning.tis multi-lingual approach to the data collection, which involved use of a multi-cultural research team rather than use of interpreters and language translation enhances the integrity of the qualitative data [34].all interviews were audio-recorded, with participant permission.interviews ranged from 20 to 60  min.participants were compensated for their time, with a prepaid mobile phone calling card at the conclusion of the interview valued at 100 ethiopian birr (~$3 usd).analysis tematic analysis was conducted to identify key emergent themes from the interviews [35].two researchers (bel and abh) jointly listened to the audio-recordings for familiarization and used open-coding to create meaning around excerpts.ten the codes were categorized into emergent themes using discussion and consensus (bel and abh).data were analyzed in the language in which the interviews were conducted.supporting quotes were transcribed in english using meaning-based translation; the combination of oral and written transcription is used to systematically validate key concepts [34, 36].te integrated behavioral model (ibm) was used as a framework during analysis to understand how specifc barriers and facilitators could ultimately impact a provider’s intent to ofer screening or actual ability to implement cervical cancer screening [37].emergent themes were mapped to the ibm constructs and are presented in a table.findings are presented using the key emergent themes, supporting quotes, and are presented in text and tables as appropriate.tis study is reported in accordance with consolidated criteria for reporting qualitative research (coreq) [38].ethical considerations tis study was approved by the university of arizona institutional review board and the oromia regional health bureau.results participants were mostly male (67%) and were working at cervical cancer screening programs as screeners or managers (50%), were cancer focal persons at town, zonal, or federal levels (22%), treated cancer as oncologists (17%), or were in decision-making positions as hospital administrators (11%).te majority of participants worked at general hospitals (39%) or teaching and referral hospitals (44%).participant characteristics are reported in table 1.attitudes about cervical cancer and appropriateness of screening methods all screening programs were using visual inspection with acetic acid (via), while some had the availability of a cryotherapy machine to provide treatment on site and others had to refer to another facility for cryotherapy.participants felt that via was an appropriate screening method for the context in which they were providing screening services, citing reasons of low-cost, low-tech, quick and easy interpretation, and ability of all cadres of health professionals to ofer via screening with some training.tey expressed hope to be able to address cervical cancer in ethiopia through cervical screening.motivation to prioritize this health issue stemmed from undeniably high cervical cancer incidence and mortality, which are receiving more attention recently, as well as the fact that most cervical cancer is preventable.many had witnessed the poor state of women afected by advanced stage cervical cancer and held deeply personal convictions to prevent other women from experiencing the same thing.cervical cancer can be preventable.if it is screened and if the vaccine is given on time, it is a preventable disease.but if people are not screened  and don’t receive cryotherapy, it is such a drastic disease, you know.– participant 13.if we focus on prevention, we reduce the burden on black lion (oncology center).two, we might keep so many mothers from dying and we reduce their painful suffering.– participant 15.via is so good for low-income countries.we are serving it freely so cost-wise, it is good.also, it takes little time for screening and treatment even with cryotherapy.they don’t have to come back.it is simple, it is cheap.– participant 4.visual inspection is the way forward, at least for now.i think it’s impossible to implement nationwide any other method like pap smear or hpv testing but at large health facilities it is possible.– participant 1.cervical cancer and screening awareness participants repeatedly cited the newness of cervical cancer as a health topic in their country, both as a motivating factor driving innovation and action as well as a barrier since the awareness of the issue is low among the community and even health professionals.in fact, the greatest barrier, most cited by participants, was lack of awareness of cervical cancer.within the community, an overall sense of ignorance about cervical cancer was compounded by perceived fear of cancer as a deadly disease with no hope of prevention or recovery and therefore no chance of medical intervention.
number of words= 1743
[{'rouge-1': {'f': 0.3191105769230769, 'p': 0.7585714285714287,'r': 0.20205479452054795}, 'rouge-2': {'f': 0.16697110404739668, 'p': 0.3049570200573066,'r': 0.1149561403508772}, 'rouge-l': {'f': 0.2872926782832062, 'p': 0.5648979591836735,'r': 0.19262958280657397}}]
-----------------------------------------------------------------------------------------------------------------------------------
p45:
Extractive Summary:
in ethiopia, violence against women and girls continues to be a major problem.spatial statistical techniques give a prominent benefit for examining the distribution of health problems.as far as my literature research is concerned, no article has been found which shows the distribution of domestic violence in ethiopia using this technique.methods study design and setting the study uses data that was extracted from edhs 2016 dataset.ethiopia is an ancient country located in the horn of africa from 30 to 140 n and 330 to 480e.ethiopia has a total area of 1,100,000 square kilometers and with over 110 million population and this makes it the second populous country in africa.ethiopia is a nation with around 80 ethnic groups.from this, the four largest are oromo, amhara, somali, and tigrayans.figure 1 shows map of ethiopia where the study has been conducted.in the first stage, a total of 645 enumeration areas (eas) were selected with probability proportional to ea size and with independent selection in each sampling stratum [7].the questionnaire was translated into local languages (amharic, tigrigna, and oromiffa) to appropriately collect the information needed.information about ipv was obtained by asking ever married woman a 13 item question; of which, 7 measures physical ipv, 3 emotional ipv, and 3 sexual ipv.data processing and analysis statistical analysis of the data was performed on spss version 25.cross-tabulations and summary statistics have been carried out to describe populations by age, level of education, place of residence, and region.finally, a model comparison between the models was performed based upon the log-likelihood ratio test to choose the best-fitted model.spatial autocorrelation (global moran’s i) statistics and anselin local cluster analysis was performed to show the spatial distribution of domestic violence among woman aged 15–49 in ethiopia.global moran’sglobal moran’smoran’s i index close to -1 means domestic violence cases are dispersed whereas, close to 1 indicates that domestic violence cases are clustered.anselin local moran’sbernoulli’s model was fitted to identify statistically significant locations of domestic violence clusters.the bernoulli model was selected because the structure of the data shows the binomial [0/1] distribution.women who have experienced domestic violence were considered as case and labeled1 whereas, those who do not experience as control and labeled 0.the commonly used parameter for evaluation of model fitness is the log-likelihood ratio test that compares the deviance (-log likelihood) of the models by subtracting the smaller deviance from the larger one.similar to the log-likelihood ratio test, the model with a small aic and bic value is considered as the better model.the majority 3219 (83.7%) of the respondents were from the rural part of the country and 1532 (39.8%) of them were from the oromia region.the mean age of the respondents was 27.76 ± 9.1sd years and the majority, 2361 (61.1%) of the respondents do not attend formal education.domestic violence from women included in the analysis, 24% (95% ci 22.9%, 25.4%) of them have experienced emotional violence, 23.7% (95% ci 22.5%, 25%) physical violence and 10.1% (95% ci 9.3%, 11.1%) sexual violence by their intimate partner.in addition, 34% 95% ci (32.6%, 35.4%) women have experienced either type of violence by their intimate partner.result of logistic regression a binary logistic regression analysis was used to examine the association of predictive variables with domestic violence.the experience of domestic violence was increased with increasing in woman’s age.the odds of experiencing domestic violence among women who witnessed family violence during childhood were 2.2 times higher than those who do not saw family violence (aor = 2.24, 95% ci 1.81, 2.58).the odds of domestic violence was 4.4 times higher among women who were afraid of their partner most of the time and 2.3 times higher among those who were sometimes afraid of their partner when compared to those who don’t afraid of their partner (aor = 4, 95% ci: 3.45, 5.61) and (aor = 3.21, 95% ci 1.83, 2.81), respectively.the result from the null model shows that the variance of random factor is 0.716 with its calculated z statistics of 7.35 and p value of 0.000.the icc value shows that 21.4% of the variation in the outcome variable was explained by the grouping variable and the rest was by predictor variables.the second model is a random intercept model that has a random intercept component and a fixed coefficient of individual and relationship level factors.the third model (full model) was developed by including community-level variables in model two.the study also reveals that the spatial pattern of ipv was not random in ethiopia.hotspot clusters are zones where high values are surrounded by high values and cold spots are zones where low values are surrounded by similar values.on the other hand, clusters are called outliers when high values are enclosed with low values or vice versa.in this study, hot spot clusters of ipv were observed in amhara, oromia, and snnp regional states.of the amhara region, particularly in the east and west gojam areas, the north and south gondar areas, and the south wollo area.in the oromia region (west arsi, guji, bale, and jimma areas), and in the snnp (sidama, gedio, dawro, and gamo gofa areas) were the principals.this suggests that the magnitude of the problem is high and requires the attention of the responsible agencies.the majority of respondents from the amhara region reported that their husbands drunk alcohol.and, those from oromia and snnp regions accept wife-beating as justified action for the husband.the result from sat scan analysis of the data identifies primary and secondary most likely clusters.the primary significant cluster was located in oromia (guji and borena zones), somali (liben and afder zones), and snnp (sidama zone) regions of the country.the secondary cluster was located in the amhara region east gojam zone and in oromia in the jimma zone.the results from previous studies conducted in foreign countries show that spatial variation in the distribution of intimate partner violence clusters was mainly attributed to neighborhood-level characteristics [9, 10, 15].in the current study, sufficient community-level variables (neighborhood level) were not included.therefore, future studies may incorporate community-level predictors when conducting similar studies.this result is consistent with an ecological study conducted in brazil [16] and nigeria [17].the result of this study is lower than a study conducted in southeast oromia [21] and northwest ethiopia [22].the relationship between marital controlling behaviors and experience of domestic violence can be explained as: if one partner exhibits repetitive marital controlling behaviors, good feeling and effective communication will vanish between them and, this could lead to disputes and the occurrence of violence.afraid of partner was considered as the result of many hostile behaviors and, it is mostly associated with many violent activities [29].gender norms are informal social rules and expectations that distinguish males from females whereas gender norms perpetuating violence against women are norms that normalize violence within a specified community [30].women who live in a community where wife-beating for husband is highly acceptable were 3.6 times more likely to experience domestic violence when compared to those who do not accept wife-beating.this result shows that the existence of a permissive social norm in the community plays a significant role in facing domestic violence by a woman.therefore, it may not answer the reason why the distribution of ipv varies across communities properly.this may be one direction for future studies.conclusion the output from the spatial analysis shows significant clustering of domestic violence cases in ethiopia.all parties including governmental organizations, non-government organizations, the scientific community, community leaders, and every individual should be involved.• the scientific community needs to expose the hidden reality behind closed doors by conducting scientific research
number of words= 1238
[{'rouge-1': {'f': 0.3706354305859608, 'p': 0.8200000000000001,'r': 0.2394277108433735}, 'rouge-2': {'f': 0.1958060656075501, 'p': 0.35762541806020065,'r': 0.13480783722682743}, 'rouge-l': {'f': 0.3649609748643035, 'p': 0.6853846153846155,'r': 0.24869415807560138}}]
-----------------------------------------------------------------------------------------------------------------------------------
p46:
Extractive Summary:
background parental efficacy acts as one of the most powerful predictors of future success, as it not only plays a part in the goals a person sets in parenting and which activities that person becomes involved in, but also influences the coping strategies the person will adopt under difficult circumstances [1].on the other hand, teenage pregnancy is a universal phenomenon affecting both developed and developing countries [3, 4], with approximately 16 million births to mothers aged 15–19 years, and two million to girls under the age of 15 years, annually [5].furthermore, honikman, et al. [11], report on perinatal mental health project indicated that 49% of teen mothers are pregnant again within 24 months.findings of a report focusing on the needs of teenage parents suggested that these teen girls face various challenges related to stigma from peers, community members as well as their family, while the men who have impregnated these girls often deny responsibility [12].in addition, rafferty, griffin and lodise [13] and mollborn and dennis [14] found that unmarried teenage mothers are more vulnerable than married ones because in many cases the unmarried pregnant girls are rejected by their parents as they have added shame and an additional burden on the family.the interaction between parental efficacy beliefs, parenting and social support is likely to vary by environmental and family contexts [17].according to amoateng et al. [15] families operate as a central form of social support to individuals, in addition to forming an intrinsic component of collective networks and ecologies.with a decreased support system, single teen mothers may find themselves lowering their evaluation of the quality of their parenting; as exposure to what quality parenting entails might be low.according van den berg [18], greater satisfaction with support networks may result in a greater sense of parental efficacy for teen mothers.also, michalos [19] asserts that social support from family members plays a role in the facilitation of teen mother’s parental efficacy across the sphere of her parenting role.however, there is paucity of research on the relationship between social support and parental efficacy, particularly in low socioeconomic communities.for this study, the correlation design was necessary to determine if a relationship between parental efficacy and social support systems exist.these areas were selected because of the high concentration of teenage pregnancy, low levels of skills and education, high levels of unemployment, poverty and substance abuse within them.eligibility criteria was set as:—(1) be a single teen mother, (2) have given birth between january 2009–january 2015, (3) single mothers who were aged between 13 and 19 years when they had their first child, (4) single teen mothers should for a period of one year have resided with or is currently residing with family, caregiver or members thereof and (5) the child should be age 5 and younger.the sps examines the degree to which participant’s social relationships provide various dimensions of social support.the scale has been used within african american samples, including low-income mothers [25, 26].the psoc is used to measure parents’ satisfaction with parenting and their self-efficacy in the parenting role.factor analysis had revealed two sub-scales within the entire measure: the skill-knowledge scale and value-comforting scale [28].efficacy, the degree to which the parent feels capable, had an alpha of 0.76 and satisfaction, an affective measure targeting feelings of frustration, anxiety and motivation, had an alpha of 0.75 [28].since the emphasis of this study is the relationship between quantitative variables, pearson’s correlation coefficient was used to explore the extent of linear relationships among the variables, and to quantify the strength and direction of the relationship.results this section provides the results of the statistical analysis conducted for the study.the results are presented as (1) descriptive information about single teen mothers, and parental efficacy (2) the relationship between the variables, and (3) the comparison of the variables between the different family forms (groups).secondly, when requiring parental consent from prospective participants’ parents, either the participant herself was not comfortable with involving her parent/ parents in the study or the parent refused to give consent, as they wanted nothing to do with the concept of their daughters being a teen mother.table 2 illustrates participants’ (n = 160) age at the time of the survey, age at birth of first child, number of children in household and the family forms identified by single teen mothers.the majority of the participants indicated their living arrangements as staying with one parent [65 (40.6%)].in addition, results in table 1 shows that 119 participants, (74.38%) single teen mothers take care of their child/children on a full-time basis.single teen mothers’ own mothers [64 (40%)] sought to care for the child/children, when she is unable to, a sister [17 (10.6%)], a family friend [13 (8.1%)], an aunt [7 (4.3%), the father of the child [4 (2.5%)], a nanny [3 (1.9%)], a foster mother [2 (1.3%)] and or a neighbour [2 (1.3%)].additional file 1: table s2 results suggest that the most perceived support across the total sample (n = 160) as attachment (m = 2.61, sd = 0.64) as reported by single teen mothers.conversely, single teen mothers indicated reliable alliance (m = 2.53, sd = 0.81) to be least supported.in addition, participants somewhat disagreed (m = 2.35, sd = 1.59) to…parent is manageable, and my problems are easily solved.table 3 suggests that for single teen mothers residing with two parents, (m = 3.07, se = 0.08), greater levels of parental efficacy was experienced.table 3 was perceived as single teen mothers residing with extended family, (m = 2.90, se = 0.10), ss informed greater levels of support.table 3 suggest that for single teen mothers residing with guardian-skip generation families, are engaged more with reassurance of worth (m = 2.36, se = 0.14) and attachment (m = 2.81, se = 0.14) under the subscales of ss.anova analysis below in table 4 the output of the anova analysis and whether a statistically significant difference between groups means are presented.one-way anova for parental efficacy (f (4, 154) = 0.790, p = 0.534) and social support (f (4, 155) = 1.848, p = 0.122).the following anova’s represent the subscales of social support; guide (f (4, 155) = 2.087, p = 0.085), reassworth (f (4, 155) = 1.367, p = 0.248), socintegr (f (4, 155) = 1.391, p = 0.240), attach (f (4, 155) = 0.942, p = 0.441) and nurture (f (4, 155) = 1.611, p = 0.174).the p values reported are greater than α level 0.05, thus no statistically significant difference exists.however, anova for subscale reliable (f (4, 155) = 2.572, p = 0.040), this value is less than 0.05, concluding that a statistically significant difference does exist.determining associational aspects of the variables of the study this section reports on the correlation scores for pe, ss and ss subscales; guide, reassworth, socintegr, attach, nurture and reliable.a pearson product-moment correlation was computed to assess these differences.the results in table 5 provides an indication that there is a relationship between parental efficacy and social support (r = 0.636**) within one parent (n = 64), this correlation coefficient is highly significant from zero (p < 0.001).when looking at the variable a bit further, there was also a positive correlation between parental efficacy across all subscales of social support; guide (r = 0.596**), reassworth (r = 0.577**), socintegr (r = 0.610**), attach (r = 0.596**), nurture (r = 0.597**) and reliable (r = 0.485**) within one parent (n = 64).when computing for two parents (n = 51), a positive correlation was indicated for parental efficacy and social support (r = 0.598**).furthermore, the results also show that there is a positive relationship across all subscales of social support; guide (r = 0.504**), reassworth (r = 0.571**), socintegr (r = 0.546**), attach (r = 0.508**), nurture (r = 0.576**) and reliable (r = 0.582**) within two parents (n = 51).however, findings from this study and a study by reiner hess et al. [31] indicated that the participants had the necessary skills to be a good mother.similarly, when it came to troubling situations with their child/ children they were able to find solutions on their own.in addition, a study by hoven [35] investigating 77 parents of children 2 to 5 years who had not yet started kindergarten, reported a significant positive relationship between social support and parental efficacy.nevertheless, the following results within the study showed that parental efficacy and social support was higher in extended family forms when compared to other family forms.one study, did report findings on extended family, johnson [40].this is very common within the coloured communities, perhaps serving as a possible explanation to the majority of significant difference is found within this family form.the results of the study should be understood with caution as the following limitations were documented.this study only focused on single teen mothers, residing in low socioeconomic coloured communities.in addition, the racial indication for the study was; coloured 98.8%.conclusion single teen mothers whom reported high levels of parental efficacy, may have the confidence and beliefs within themselves that they are able to handle and successfully parent their child/ children.
number of words= 1493
[{'rouge-1': {'f': 0.3341202741970338, 'p': 0.8435191637630661,'r': 0.2083177570093458}, 'rouge-2': {'f': 0.20234976843308108, 'p': 0.42314685314685313,'r': 0.13296758104738154}, 'rouge-l': {'f': 0.33660605389943854, 'p': 0.6839240506329114,'r': 0.22323854660347553}}]
-----------------------------------------------------------------------------------------------------------------------------------
p47:
Extractive Summary:
empowerment is achieved when a woman has the resources, agency, and capabilities to execute decisions on matters of importance [3, 4].gender, as a social determinant of health, is influenced by the “gendered” norms of the roles, personality traits, attitudes, relative power, and influence that society ascribes to it [9, 10].the transition from the millennium development goals to the sustainable development goals (sdgs) in 2015 saw the emergence of target 5 which aims to “achieve gender equality and empower all women and girls” (p. 20) [11].however, as icts have become ubiquitous and grown in both type and access, a digital divide has emerged.this divide widens the inequity and inequality gaps based on gender, age, disability, or socioeconomic status [13, 14].global discussions, such as the 1995 world conference on women: beijing declaration and platform for action, deliberated and advocated for the inclusion of women in the information society in order to fully achieve women’s empowerment in connection with ict.for the purposes of this research, the sixth step was not performed.this helped to ensure an unbiased approach to the search protocol and to enhance rigour [27].these are available upon request.the primary reviewers included the lead and co-authors, as well as one research assistant.any and all potentially relevant citations identified throughout all stages were imported into endnote™, a reference management software, where duplicates were removed by the program and then double checked, and manually removed by the lead author; the list of citations was then imported into a web-based electronic systematic review management platform, distillersr™.any discrepancies were brought forward to the co-author who made an independent decision whether to include or exclude the article.as per arksey and o’malley [27], the following avenues were reviewed as part of the search strategy: searching relevant electronic databases, reviewing reference lists of pertinent articles to identify additional sources, and manually searching key journals.to ensure the search was comprehensive, the following databases, available through the university of saskatchewan library, were searched on november 30, 2016 and updated on january 1, 2018: scopus, embase, abi inform, soc index, sociological abstracts, gender studies, springer link, psychinfo, science direct, and academic search complete.the cochrane library was also searched for any relevant trials in the trial registry.”an illustration of the search term strategy is presented in table 1.search terms were drawn from the research question, as well as from lengthy discussions with the university librarian and expanded upon based on a cursory search of two databases.these databases were determined in consultation with the university librarian and included scopus and gender studies.these papers were then analyzed for similar keywords, definitions, analogies, and index terms that were relevant synonyms to the initial search words [28, 29].these additional terms were added to a master list that informed the final search strategy.specifically, for the term empowerment, keywords were chosen that could provide results that included a lack of empowerment as well, thus the inclusion of “barrier” and “disempower”.an additional term that was used interchangeably with “empower” was “agency”, however, as this term is used more frequently in conjunction with organizations and not empowerment, it was removed from the search term list.the ability of the electronic database search to identify all relevant primary research was verified by hand searching the reference lists of eight key peer reviewed articles and nine key electronic journals that were flagged through the initial test search as well as the main search.the journals were chosen based on their relevance to the research question as well as their scholarly nature.subsequent journals were identified and selected for a hand-search once the initial search was completed.these journals were then reviewed for additional articles potentially not identified through the database search; this included entering the general search into journal databases.the initial pool of results included a total of 4481 citations.an initial set of inclusion and exclusion criteria were developed a-priori to screen abstracts and titles of citations which were refined during each review of the pool of articles.the results were also filtered to include english only content.first screen: inclusion criteria the inclusion criteria created for the first level of study selection were driven by the review topics, specifically, women, empowerment, and icts.the inclusion criteria used in the first level of selection were country of publication, date of publication (2012–2017), and the use of both of the following concepts in the title or abstract of the publication: women’s empowerment and/or information and communication technology.all articles excluded by the criteria were sent to the research assistant who confirmed the exclusion.additionally, if an article could not be excluded based solely on the title or the abstract, the full article was reviewed for relevance to the research question and inclusion criteria.these latter two points did not prove to be an issue as there were no disagreements.as such, this set of inclusion criteria focused on technology as an intervention and women as active participants in the study instead of just the word “women” found throughout the first set of criteria.a subsequent review resulted in 14 of the 59 articles being eliminated from consideration as they did not meet the inclusion/exclusion criteria.rather than focus on a range of these determinants, the authors decided to include all 45 articles and to then review the implications of this finding in the analysis (fig. 1).a total of 573 articles were found in all 10 of the main electronic databases.using the inclusion and exclusion criteria previously described all but six articles were eliminated through the first and second stages in the review process.study characteristics, extraction, and charting the final step in the arksey and o’malley’s [27] scoping review framework was to collate and summarize the results for presentation and discussion.the goal of this step was to determine and chart factors to be extracted from each article to help answer the research question [26, 27, 30].the age of participants was frequently reported although there were inconsistent age groupings across the studies.empowerment definition in the included studies, the concept of empowerment was used incongruously with terms like self-concept, self-esteem, and self-worth, sometimes by the same author in the same study, which further limited our ability to achieve a uniform definition for the purposes of this research.the remaining studies described empowerment in a more indirect way, never including the term “empower” or “disempower.outreach ten of the 51 articles reviewed described supportive ict interventions as a means of outreach or connecting with clients in the community.common themes in this section included supporting women where they are at in the community, in terms of their social position, to enhance positive health behaviours with technological assistance, as well as overall enhanced accessibility to icts.this was accomplished through cognitive behavioural therapy using computers [46], and web-based decision aid for understanding fetal anomalies [47].[64].one intervention focused on the prevention of sexual and reproductive illness using education information [57].these studies focused on the perceived barriers and understanding of the role of mobile phones, [42, 66] the awareness of gender-based barriers in telemedicine [68], the development of women through mobile phones [32, 40], as well as the connection with women in the community apps [50].from the outset of the review, search terms had to include words beyond simply “empower[ment]” as much of the initial searching revealed synonyms including selfefficacy, self-worth, self-concept, and/or capacity.the lack of specific measures of empowerment reflects a barrier, not only regarding how strategies for empowerment are understood and implemented, but how researchers know whether empowerment has been achieved.the finding underscores a need for a standardized tool for measuring the level of women’s empowerment.icts to improve empowerment empowerment through icts has the potential to cross multiple sectors, both private and public.alternatively, governments should support and encourage private mobile operators through tax exemptions and other benefits to facilitate better mobile services and infrastructure in rural, remote, and urban areas.these strategies not only help in improving the overall status of girls and women but also influence overall empowerment and development of the community.scoping studies also lack a thorough evaluation of the quality of results, instead producing a narrative account of all available evidence [26, 27].conclusions the diversity of technological interventions utilized to support empowerment is infinite and there is no limit to how icts can be implemented in daily lives.this study is novel and essential as it comprehensively describes efforts to use icts to empower women, and the imperative for collaborations between researchers, program implementers and policy makers to address the persistent gender disparities in the access to and use of technologies.
number of words= 1396
[{'rouge-1': {'f': 0.34460489364264246, 'p': 0.7615887850467289,'r': 0.22268225584594223}, 'rouge-2': {'f': 0.19182570981428299, 'p': 0.35125,'r': 0.13194081211286993}, 'rouge-l': {'f': 0.32622083136590146, 'p': 0.588918918918919,'r': 0.22559157212317668}}]
-----------------------------------------------------------------------------------------------------------------------------------
p48:
Extractive Summary:
in sweden, as well as in many other western countries, most women diagnosed with bc are cured from their disease or live with it as a chronic condition.nevertheless, studies have shown long-term consequences for employment and sickness absence (sa) [5], and the proportions of women returning to work one year after bc varies signifcantly between countries with diferent social insurance systems, from, e.g., 43% in the netherlands to 98% in the us, while around 60% has been reported in sweden [6].tis highlights the importance of gaining in-depth nation-specifc knowledge about factors that women themselves perceive to be of importance for their return-to-work (rtw) process or for avoid long-term sa.te study base for the cohort study were 970 women who had undergone a frst bc surgery in one of three hospitals in stockholm, sweden when aged 20–63 years.two other inclusion criteria were: living in stockholm county and literate in swedish.exclusion criteria were known distant metastases, pre-surgical chemotherapy, and/or a previous bc diagnosis.te women were screened for eligibility consecutively after surgery at their frst visit for discussion of further treatment.all methods were carried out in accordance with relevant guidelines and regulations.tis open question was placed in the later part of the very comprehensive questionnaire including 388 diferent questions.te questions before this one in the questionnaire, as well as questions in the fve previous questionnaires, concerned diferent aspects of health, morbidity, healthcare, life situation, life quality, and paid and unpaid work.tus, the women had already  before this question considered many such aspects when responding.analysis te analysis of the written statements was made with a inductive manifest content analysis [25, 26], using nvivo 9 [27].one of the authors (awl) frst coded the whole material of individual responses while another author (ef) independently coded parts of the material.tereafter, all authors went through the written statements assigned to specifc categories, and if not agreeing, this was discussed until agreement was reached.from an international perspective, a very high proportion of working-aged women are in paid work in sweden, also in higher ages [29].te sa benefts can be granted for several years and for full- or part-time (25%, 50%, 75%, or 100%) of ordinary work hours [32].tere were no diferences between the groups concerning primary treatment.in the analyses of the written statements, fve categories were identifed covering a wide range of areas, each holding several sub-categories.some statements had a clear positive or negative relation to being in paid work, yet, it was not always possible to interpret such directions in a conclusive way from the statements.a.1.a.2.a.2.a.mental or emotional symptoms or problems in this group, diferent words were used, such as psychological, mental, and emotional symptoms or problems.te most mentioned were related to fatigue and/ or exhaustion, lack of energy or strength.a.2.b.physical symptoms or problems statements regarding physical and bodily symptoms or problems often concerned physical condition in general, e.g., physical shape or physical health.pain was the most frequently mentioned physical symptom and was often referred to in general terms such  as having pain, ache, or tenderness.others specifed when the pain occurred, e.g., “pain when lifting”.other physical problems or symptoms mentioned were susceptibility to infections, nerve injury, or anemia.a.3.tere were also statements describing how treatment afected their mental health, e.g., “chemo made me slow and stupid”.a.4.disease prognosis or progress in this sub-category, the statements concerned the medical status or experiences related to the prognosis or progress of the bc.a.5.a.6.some expressed this in general terms, others were more specifc like “important work, i’m thriving, i have a lot to give but can become tired because of too much cancer.newlyweds, work at our summer house, worried husband due to cancer progression and spread”.some women stated that other health conditions than the bc had hindered them from being in work, “my otherwise bad health is the reason for not being able to work”.b.1.at the workplace from the workplace, colleagues were most often mentioned, sometimes just by one word, but also described as supportive, good, or understanding.others stated that support from customers, clients, or from children and parents if working in a nursery or school.b.2.family, relatives, and friends family, relatives, and friends is another sub-category, by most women these persons were described as supportive, understanding, helpful, or loving.other positive statements were having found a new partner, having good social contacts, becoming a parent, gaining a better relationship with the children, or becoming a grandparent.b.3.some women described good support from healthcare in general, while others mentioned certain professionals, such as physicians, nurses, or medical social workers.c. flexibility and adjustments possibilities te category flexibility and adjustment possibilities covered several areas in life and the following four subcategories were identifed; c.1.work content, scheduling, and place, c.2.about one ffth of the codes fell in this category.c.1.flexibility was by most expressed as something that was supported by, e.g., employers, managers, and colleagues, while some also mentioned having been encountered with non-fexibility.another aspect mentioned was if not having the possibility to have a substitute, e.g., making it more difcult to be sickness absent, “my knowledge is unique/cannot easily be substituted”.c.2.one factor mentioned was the possibility to reduce work hours by being on parttime sa (e.g., 25%, 50%, or 75% of ordinary work hours) or by combining sa benefts with vacation to get a longer leave.te possibility to dispose the time on sa or at work according to ones needs both during, after, or to ft treatment was also stressed, e.g., “the sia was fexible and allowed me to work when i could”.would have coped better and felt better if having been on part-time sa for a longer time”.c.3.c.4.further, some women mentioned that they lived close to the hospital or to their work why that such adjustments were not necessary.sometimes this was related to colleagues but also to third part, e.g., customers or the children that one was teaching in school.d.2.economic security or economic stress another factor related to consequences from working was economy.contrary, a few statements expressed an own desire or will to work less.some expressed that the bc had led to a changed view of life that impacted their preferences or strength of will, e.g., “if possible, i have an even more positive view of life which also makes work more fun”, or “i think more ‘do i want this?is it important?’”.what is interesting in the results from the women´s open answers are the comprehensive picture they give over diferent type of factors.still the large number of statements in various felds is a strength of the study showing the complexity of what is of importance for being able to work or not work following breast cancer.conclusion te results give a comprehensive overview over a variety of diferent types of factors for being able to return to/remain in work or to not work for women of working ages during the two years after a frst breast cancer surgery.te results may facilitate the support for women after breast cancer from diferent stakeholders that are encountering them during the period of treatment and rehabilitation measures.
number of words= 1154
[{'rouge-1': {'f': 0.40057074850810365, 'p': 0.7904610951008646,'r': 0.2682553528945282}, 'rouge-2': {'f': 0.2122521192400659, 'p': 0.36190751445086705,'r': 0.15015873015873016}, 'rouge-l': {'f': 0.3437822182627796, 'p': 0.6075722543352602,'r': 0.23970802919708029}}]
-----------------------------------------------------------------------------------------------------------------------------------
p49:
Extractive Summary:
yang et al. found a close correlation between reduced urogenital hiatus diameter in the sagittal plane and the modified oxford grading scale [12].the modified oxford grading scale is defined as follows: 0 = no contraction; 1 = flicker; 2 = weak; 3 = moderate; 4 = good; and 5 = strong [15].second, vaginal pressure was measured with the peritron and mizcure perineometers.the order of use of the two vaginal manometers and the two test positions were each performed randomly.testing was conducted with the women in two positions: the supine position, with flexed and slightly abducted legs, and the standing position, with straight and slightly abducted legs.before data were acquired by the perineometers, the participant inserted the probe, which was covered with a condom and lubricated with hypoallergenic gel, into her vaginal cavity.the participants were instructed to place the probe inside the vagina to a location where 0.5–1.0 cm of the probe was visible from the outside of the introitus.pfm strength was then evaluated by a maximum voluntary contraction, as measured by squeeze pressure.the instruction used for each contraction was ‘squeeze and lift the pfm as much as you can’.vaginal pressure testing was performed with three repetitions of maximum voluntary contractions that each lasted for 3 s, with a 3-s rest between contractions.a 2-min rest break was then taken.visible co-contraction of the transversus abdominis muscle was permitted, as long as there was no pelvic tilting [16, 17].examiner 1 was a physiotherapist with 11 years of clinical experience.examiner 2 was a physiotherapist with 5 years of clinical experience and 4 years of educational experience in a university institution.test 2 all women were evaluated twice.after the testing protocol was completed in the first session (test 1), all subjects repeated the protocol 2–6 weeks later (test 2).in test 2, vaginal pressure measurements were performed using only the mizcure perineometer.the order of the two test positions (supine, standing) and the two examiners were each assigned randomly.statistical analysis within- and between-session intra-rater reliabilities in vaginal pressure values (maximal voluntary contraction) were analyzed using intraclass correlation coefficients (icc) (1, 1), and inter-rater reliability was analyzed using icc (2, 1).validity was assessed by pearson’s productmoment correlation coefficient and spearman’s rank correlation analysis of the vaginal pressure values of the peritron and mizcure.pearson’s product-moment correlation coefficient was used when the data were normative, and spearman’s rank correlation coefficient was used when the data were non-normative.statistical analyses were performed using the free statistical analysis software r, version 2.12.0 (https ://perso nal.hs.hiros aki-u.ac.jp/pteik i/resea rch/stat/s/), with the level of significance set at 5%.results the median age of the 20 healthy female participants was 26.5 years (range 23–45 years), and their median body mass index was 19.4 (range 17.5–23.4) kg/m2.none of the subjects included in the analysis had done pfm training before participating in the research project or between the two evaluation points.table 1 summarizes within and between-session vaginal pressure values obtained by examiners 1 and 2 (mizcure and peritron perineometers).all raw data are referred to as additional file 1.within‑session intra‑rater reliability table 1 shows the within-session intra-rater reliability using three repetitions of each maximum voluntary contraction by the mizcure and peritron perineometers for both tests 1 and 2. for both examiners 1 and 2, all vaginal pressures in tests 1 and 2 had icc (1, 1) values of 0.90–0.96.between‑session intra‑rater reliability between-session intra-rater reliability values for the miz- cure perineometer for examiners 1 and 2 are shown in table 2.the between-session intra-rater reliability of examiner 1 was icc (1, 1) = 0.72 for the supine position and 0.79 for the standing position.the between-session intra-rater reliability of examiner 2 was icc (1, 1) = 0.63 for the supine position and 0.80 for the standing position.within‑ and between‑session inter‑rater reliabilities table 3 shows the inter-rater reliability analysis for vaginal pressure values for tests 1 and 2.the inter-rater reliability for test 1 was icc (2, 1) = 0.96 for both the supine and standing positions for the peritron.the icc (2, 1) for mizcure was 0.91 for the supine position and 0.87 for the standing position.the inter-rater reliability of the miz- cure in test 2 was icc (2, 1) = 0.69 for the supine position and 0.95 for the standing position.validity significant correlations between the peritron and miz- cure perineometers in the measurements of vaginal pressure were found in the supine position (pearson’s correlation coefficient of 0.68, p < 0.001) and in the standing position (spearman’s correlation coefficient of 0.82, p < 0.001).more details about these results are presented in table 4.discussion pfm training has been shown to improve stress urinary incontinence and pelvic organ prolapse [18, 19] and is recommended by the international continence society as grade a [20].however, about 30% of women report failure to contract the pfm correctly [21].incorrect pfm contraction is not expected to have a training effect.therefore, it is recommended that proper pfm training should always include an objective assessment of correct contraction.in the present study, the reliability and validity of pfm strength assessment using the mizcure perineometer were examined in healthy women.using transvaginal devices, it is known that the measurement of pfm strength depends on the size and placement of the probe, the subject’s cooperation, and the examiner’s experience and skills [22, 23].
number of words= 864
[{'rouge-1': {'f': 0.4639308628072723, 'p': 0.9184848484848485,'r': 0.310343347639485}, 'rouge-2': {'f': 0.32099011003597483, 'p': 0.5985171102661597,'r': 0.21930182599355533}, 'rouge-l': {'f': 0.44953457857363377, 'p': 0.7877419354838711,'r': 0.3145054945054945}}]
-----------------------------------------------------------------------------------------------------------------------------------
p50:
Extractive Summary:
according to eurostat’s 2019 edition of ageing europe report, the oldest old (i.e. patients≥80  years) are within the group of elderly the fastest growing segment of the population at large.te aim of this prospective, controlled observational study was to compare the time course of neuromuscular blockade after rocuronium 0.6  mg/kg in patients≥80 years old with patients between 20-50 years.all patients gave written informed consent, and all clinical data were obtained in our department.35 patients scheduled for a surgical procedure under general anesthesia were included between 15 may 2020 and 16 march 2021.after preoxygenation, total intravenous anesthesia (tiva) was induced in all patients with 1.5–2.5 mg/kg propofol and 10 μg sufentanil and maintained with 4–10  mg/kg/h propofol and bolus doses of sufentanil (5–10  μg).tereafter the device was set to deliver tof stimulations at a 15 s interval and the following time intervals were measured as defned: onset time, defned as the time from start of injection of rocuronium to no response to tof stimulation (tof count=0); clinical duration defned as the time from start of injection of rocuronium to the return of the 4th response of the tof (tof count=4) and recovery to the tof-ratio of 0.9 and 1.0, defned as the time from start of injection of rocuronium to a tof ratio above 0.9 and 1.0, respectively.secondary outcomes were onset time, maximum depth of neuromuscular block, clinical duration and recovery to a tof ratio≥1.0.under intravenous anesthesia mean time to a tof ratio recovery≥0.9 for adult patients is 60 min with a standard deviation of 11 min [15].with an alpha risk of 5% and a power of 90%, 14 patients needed to be included in each group.in order to compensate for possible dropouts, 35 patients were included in total.discrete variables were compared using a pearson chi-square test.when paired diferences were tested for continuous variables, a t-test for paired data was used on variables for which a normal distribution is observed for the diference between the two variables and data are presented as mean±standard deviation.otherwise, a wilcoxon test for paired data was used and median and inter-quartile range are presented.results tirty fve patients were enrolled between may 2020 and march 2021.patients in the group 80+ were 85 years old [82–88] and patients in the group 20–50 were 39 years old [28–42].te female/male ratio was 4/12 in the group 80+ and 6/10 in the group 20–50.te weight of patients in group 80+ was 70.5±14.0  kg and the bmi was 25.3±3.9.te clinical duration was 52 [48–69.5] min in the group 80+.te corresponding values in the group 20–50 was 36 [34–41] min, respectively; p<0.001.te corresponding values in the group 20–50 were 53.5 [49.5–55.5] min and 59.5 [56.5–70.25] min, respectively; p < 0.001.tus, a shift from rocuronium as a rapid onset, intermediate acting compound to a slower onset and long-acting compound can be observed in patients ≥ 80 years old.recent observations by blobner et al. confrmed this limitation of an acceleromyographic tof ratio of 0.9 for clinical decision-making [17].hence, in the present study both parameters were assessed: a tof ratio recovery to 0.9 facilitating comparison with data in the literature, and a tof recovery to 1.0 indicating acceptable neuromuscular recovery when using acceleromyography.tis amg device has recently been approved for research purpose [14].tis increased incidence of residual paralysis in the elderly was associated with a higher incidence of hypoxic events, airway obstruction, and postoperative pulmonary complications.moreover, even pacu and hospital length of stay was increased in the elderly.terefore, strategies to prevent residual paralysis in this patient population needs to be revised and the poor tolerance of even small degrees of residual paralysis in the elderly should be considered in the context.neuromuscular monitoring and pharmacological reversal are key elements in any strategy to prevent postoperative residual paralysis [4].of interest in this context, mcdonagh et  al. observed a rapid and complete reversal of rocuronium neuromuscular block in the elderly when sugammadex was given at a tof count of 2.sugammadex doses were similar to nonelderly adults although reversal was slightly slower in the elderly cohort: 2.2  min in patients<65  years, 2.6  min in patients 65–74  years old and 3.6 min in patients≥75 years [25].tat’s why rocuronium may be an interesting alternative for rapid sequence induction (rsi) in the elderly.in the present study, a tendency to deeper maximum level of neuromuscular block could be observed in the elderly (table 2).moreover, as the present study was designed to assess the impact of increasing age on the pharmacodynamic properties of rocuronium, it does not allow detailed insights in the underlaying mechanisms.
number of words= 749
[{'rouge-1': {'f': 0.40385578058840316, 'p': 0.77995670995671,'r': 0.2724691358024691}, 'rouge-2': {'f': 0.2167774126231363, 'p': 0.3656521739130435,'r': 0.15405438813349814}, 'rouge-l': {'f': 0.39864736215787383, 'p': 0.6859999999999999,'r': 0.280958904109589}}]
-----------------------------------------------------------------------------------------------------------------------------------
p51:
Extractive Summary:
background cancer is a major human health problem and remains the second leading cause of death worldwide, after cardiovascular disease [1].tumor heterogeneity and cancer cell polymorphism and differentiation in various developmental stages in the same tumor make it very difficult to choose the most efficient and suitable therapy.buiķis and colleagues have previously shown that human sarcoma cell cultures treated with thiophosphamide (thiotepa), a chemotherapeutic agent used to treat breast, ovarian and bladder cancers [3, 4] exhibit the development of unusual small-sized cells, called microcells.a population of microcells has also been observed in triple-negative breast cancer histological samples obtained from patients [7].dox is a widely used anticancer drug that intercalates within dna base pairs, causing damage to dna strands, inhibiting both dna and rna synthesis and thereby arresting cell proliferation [11, 12].in this study we have identified microcell formation in three different cell lines: human melanoma (sk-mel-28), human skin fibroblast (hs-68) and cervical carcinoma (hela) cells.sk-mel-28 is a dox- and ptxresistant human melanoma cell line [6, 22, 23].methods cell lines human cervical carcinoma (hela), sk-mel-28 melanoma, and hs-68 (human skin fibroblast) cell lines were acquired from the american type culture collection (atcc) and maintained in a culture medium consisting of dulbecco’s modified eagle’s medium (dmem; thermo scientific, il, usa) supplemented with 10% fetal bovine serum (fbs).therefore, we used three cell lines to determine if microcells could be forming in other cancer (hela, sk-mel-28) and non-cancer (hs-68) cell lines.in this study we used the hela cell line, to which is known that microcells formation is initiated by applied stress factors.the second used cell line is a human fibroblast cell line (hs-68) described as normal cells, and the third cell line is melanoma cells which are quite aggressive skin cancer.stress factors stress factors were applied to cancer microcell formation.cell lines hela, hs-68, and sk-mel-28 were treated for 24 h with doxorubicin (dox; 50 mg; teva) at a final concentration of 2.5 μm or with paclitaxel (ptx; 6 mg/ml; teva) at a final concentration of 0.7 μm at 37 °c in a 5% co2 atmosphere.after treatment, the dmem was replaced with fresh medium, and the cells were cultivated for another 24 and 48 h at 37 °c in a 5% co2 atmosphere.ans is a staining protein at hydrophobic site of a protein, it fluoresces in a blue light [26, 27], while etbr fluoresces in a red light, thereby revealing dna and rna [28].when cell density was 80–100% on the monolayer, a final concentration of 7.5% methanol (sigma-aldrich, mo, usa) was added.ans–etbr fluorescence was visualized using a three-band blue, red, and green (leica brg) optical filter and a leica dm1000b microscope qualitatively scored blinded by two independent observers as negative or positive staining.sequentially neutral red (nr, sigma-aldrich, taufkirchen, germany) was added to control and treated cells for 3 h incubation.coverslips were removed from the 24-well plate coated with cv  ultra-mounting medium and analyzed under a microscope.a transfection mix for one sample was prepared using 1 μg of plasmid pcdi-egfp (6837 bp) containing gfp gene diluted in 100 μl of serum-free dmem with an additional 2 μl of turbofect™ reagent (thermo scientific, il, usa), according to the manufacturer’s protocol [30].then the medium was changed for a fresh medium without dox.the confocal laser microscope transmitted light and the fluorescent light detectors were equipped at the argon laser line of 488 nm, the time lapse function was used for time lapse imaging.after that samples were rinsed with pbs three times for 5 min and one time with pbs/tween 20 for 5 min.samples were then washed trice with pbs for 5 min and once with pbs/tween 20 for 5 min and covered with alexa fluor 488 (ab150077, abcam, ca, usa) goat anti-rabbit secondary antibody diluted 1:200 in 1% bsa/ pbs, and incubated for 1 h in a humidified chamber in the dark.samples were washed trice with pbs for 5 min and once with pbs/tween 20 for 5 min.determination of cells number the cells count was determined using a leica dm1000b microscope (leica microsystems) with a 40× objective (dry, plan apochromatic, with a numeric aperture of 0.85).the cell line 4/21 was seeded in 24 cm2 karell flasks with a density of ~ 3 × 105 cells per well and grown in eagle medium supplemented (sigma-aldrich, mo, usa) with 10% fbs and 1% penicillin/streptomycin (growth medium) in a humidified atmosphere containing 5% co2 at 37 °c. cells were treated with thio-tepa at a final concentration 20 μg/ml for the 24 h. after treatment, the medium was removed and washed with medium without serum, and fresh growth medium was added and incubated in for the 24 h. the incubation medium was removed and 4 °c 2.5% glutaraldehyde solution prepared in pbs at 4 °c was added for the cell sample preparation for electron microscopy.the pellet was dehydrated in 70% ethanol and embedded in epon (epoxy embedding medium; sigma-aldrich,mo, usa).statistical significance of difference of means were calculated where appropriate.the percentages were counted, assuming a total cell count of 1060 being 100%.the hela cell line was chosen as cell line to which is known that microcells formation is initiated by applied stress factors, such as chemotherapeutic drugs thio-tepa [4].we observed small, round, and intensively stained cells, which formed after ptx therapy in the sk-mel-28 cell line (fig. 1b).in contrast, when therapy was not used, microcells were not seen at all (fig. 1a).the microcells showed an increasing tendency in the cell population of the hela cell line.sk-mel-28, the human skin melanoma cells, were treated with ptx.methanol stimulates apoptosis, and it was used to compare if the microcell formation observed as well as after chemotherapeutic drugs in hela cell line (known that microcells formatting).nadph test shows the metabolic activity of the cell.hydrogen ion is transferred through the cytochrome system (cytochrome 450) and cell viability is shown [31].the untreated sk-mel-28 cells (fig. 6a) presented nr up take via active transport – endocytoses, meaning these cells are viable.otherwise, ptx treated sk-mel28 (fig. 6b) cell presented lack or no nru at all, that means cells are non-viable.in turn, the microcell (fig. 6b, red arrow) presented intensively nr uptake.antigen is expressed in cell cytoplasm (fig. 7a, phc) near the nucleus (fig. 7a, dapi/tsg101/phc).in the cell cytoplasm are ribosomes, seen as dark spots.discussion in this work we induced microcells in human cancer cell (sk-mel-28 and hela) lines and human fibroblasts (hs-68) used chemotherapeutic drugs dox and ptx acting to cells 24 h. we observed that microcells also occur hela cells after 6 h 7.5% methanol treatment.the microcell formation in this work was observed after cancer cell exposure to chemotherapy or, described by cell morphological changes.similarly to microcells, raju cells, nucleolar aggresomes (noas), and bonghan microcells (bh-mcs) have also been observed in cancer cell lines after anticancer treatment.sundaram et al. have observed the cell formation of mitotic colonies on the monolayer in a cell culture after treatment with etoposide (vp-16) or x-rays [38].on day 14 after treatment initiation, from the mother cell in a process called neosis, the number of raju cells raised to approximately 10, with a diameter of 6–10 μm; however, only 8% of them were unable to survive [38].raju cells are able to live for 8 weeks after mitotic crisis, after new raju cell formation from polyploid cells [38].another study showed that after treatment with vp-16, noas are formed on day five; these noa cells were shown to contain fibrillarin, rdna, and pericentric heterochromatin [39].a bh-mc is a small-sized cell with the ability to divide and pluripotent differencing features, similar to adult stem cells [40].the differences between microcells, raju cells, and noas are that microcells develop from macrocells 24– 48 h after antitumor treatment [4], whereas raju cells and noas develop from the polyploid cell tree up to 7 days after anticancer treatment [8, 38, 39].elevated microcell phagocytic ability has observed using carmine red and indian ink, suggesting their metabolic activity [24].numerous anticancer drugs, such as paclitaxel and doxorubicin, which were used in this study, as well as methanol, cause cell stress, which creates dna damage and damage to cellular membranes, interrupting cell homeostasis, and inducing apoptosis [42–44].to avoid damage, cells transform their architecture to a spherical shape during mitosis; this process is controlled by changes in the actin cytoskeleton.although an enormous amount of data indicate that cytotoxic drugs induce and activate apoptosis initiation machinery and the cellular stress response, many questions remain unanswered.for instance, the opinion that apoptosis represents the principle of the mechanism by which tumor cells are killed as a result of cancer therapy may not be universally true, as previously proposed by herr and debatin [48].there is a hypothesis that a microcell forms from a perished macrocell [8].we showed that in the early stages of microcell formation, cells mainly contain proteins (fig. 2).the microcell formation under applied therapy is observed in the other research, where is shown that the microcell contain ribosome like particles, nucleus with pronounced functional activity [50].for the nadp h test we used 1 min pre-fixation with 4% formaldehyde solution to reduce activity of nadph diaphorase activity as nadph activity in the microcell is very strong.this indicates the rising metabolism of the mircocell after the applied stress.abid et al. described nadph oxidase requirement for endothelial cell proliferation and migration activity [56].nadph has an essential role in reducing ribonucleotides into deoxyribonucleotides by ribonucleotide reductase; accordingly, it is involved in dna synthesis by implication [15].in some research, cancer cells uptake dna molecules from the cultivation medium to which they are added, more so than noncancer cells [57, 58].however, there is a subpopulation that could be a rare mutation causing drug resistance, and these cells can proliferate during anticancer drug effects [19, 59].in further research, it is important to pay attention to this cell type as a feature of the mechanism of drug resistance or drug-tolerant cells.therefore, analysis of the microcell population in a tumor undergoing anticancer treatment could be a strong prognostic factor for a patient’s survival, and could be a potential regenerator of cancer cells after the death of the tumor itse
number of words= 1659
[{'rouge-1': {'f': 0.30273962855811803, 'p': 0.761275167785235,'r': 0.18893764434180138}, 'rouge-2': {'f': 0.15003904251896052, 'p': 0.2686531986531987,'r': 0.10408434430964761}, 'rouge-l': {'f': 0.26375483660751864, 'p': 0.5288235294117647,'r': 0.17569105691056913}}]
-----------------------------------------------------------------------------------------------------------------------------------
p52:
Extractive Summary:
in severe hypothermia, intrinsic heat production by means of active movement and shivering, disappeared, leading to further progression in the decrease in body temperature.frailty is characterized by a decline in functioning across multiple physiological systems, accompanied by an increased vulnerability to stressors [6].for the purpose of verifying this hypothesis, we analyzed the japan’s nationwide registry data on hypothermia.material and methods study design and setting we performed a prospective, observational, multi-center registries of hypothermia: the hypothermia study 2018&2019.the study has been approved by the ethics review board of teikyo university hospital in japan (approval no: 17–090).the requirement for informed consent was waived due to the observational nature of the study by the ethics review board of teikyo university hospital in japan.the following data were collected: age, sex, any preexisting conditions, activities of daily living (adl), lifestyle, location of hypothermia incidence, mechanism underlying hypothermia (acute medical illness [stroke, ischemic cardiac disease, infectious disease, malnutrition, arrhythmia, diabetes mellitus, renal disease, hypoglycemia, cardiac failure, endocrine disease and gastrointestinal disease], trauma [submersion, distress], alcohol intoxication, other [including drugs]), charlson comorbidity index (cci) [10], glasgow coma scale (gcs) [11], sequential organ failure assessment (sofa) score [12], laboratory data, temperature, blood pressure, heart rate, respiratory rate, cardiac arrest during pre-hospital, intubation, hospital length of stay, mortality, and cerebral performance category (cpc) [13] score at 30 days after admission, and complications.the temperature was recorded as the core temperature from the rectum, urinary bladder, or esophagus if available; otherwise, the peripheral temperature from the axilla or ear was noted.the severity of hypothermia was classified according to the temperature as mild (35–32 °c), moderate (32–28 °c), or severe (< 28 °c) with reference to previous studies [1] [3].the ph value in principle was evaluated by an arterial blood gas analysis, and the ph value measured using the venous blood gas was adjusted as described in a previous study [14].pneumonia was defined as an obvious shadow on chest radiography or computed tomography (ct).pancreatitis was defined as cases meeting at least two of the following conditions: 1) abdominal pain, 2) elevation of pancreatic enzyme levels in the blood, and 3) edema of the pancreas or peripancreatic effusion on ultrasound/ct.the secondary outcomes were the comparisons of the length of intensive care unit (icu) stay, hospital stay, cpc at 30 days after admission, and complications between the frail and non-frail patient groups.data analyses data are expressed as the number (%), median (interquartile range) or the mean ± standard deviation, as appropriate.missing data were managed with multiple imputation by chained equations [17, 18].the variables included in the imputation model were those from the multivariable model.twenty-five datasets were imputed with 10 iterations each.a cox proportional hazards analysis was applied to the 25 imputed datasets, and final estimates were obtained by averaging the 25 estimates according to rubin’s rules.all statistical analyses were performed with ezr (saitama medical center, jichi medical university, saitama, japan), a graphical user interface for the r software program.multiple imputation was performed using the mice package in r (version, 4.0.3 r foundation for statistical computing, vienna, austria).the frail patient group was older, had higher cci values, and included a higher percentage of ah cases that occurred indoors in comparison to the non-frail patient group.clinical and laboratory data among the 920 patients, the core body temperature was measured in 585 (63.6%).primary outcome as shown in table 3, the overall 30-day mortality rate was 23.5% (n = 216).a sensitivity analysis performed using the complete dataset of cases excluding cases with missing values (n = 679) confirmed the robustness of the results.secondary outcomes among the 920 total patients, the median length of icu stay was 3 days, and the median length of hospital stay was 13 days.discussion the present nationwide study showed that frail patients with ah had a significantly higher risk of mortality in comparison to non-frail patients with ah, even after adjustment for important confounders.recently, frailty has been shown to be associated with mortality and adverse outcomes in patients with various conditions [7], including patients with chronic obstructive pulmonary disease [19], patients with inflammatory bowel disease [20], patients with aids [21], patients awaiting liver transplantation [22], hip fracture patients [23] and patients undergoing elective vascular surgery [24], independent of chronological age.previous studies showed that prognostic factors in ah include the potassium level, ph value, lactate level, and age [2, 3, 25–27].although these factors may be useful for predicting the prognosis and selecting an appropriate rewarming intervention, these factors cannot be controlled and do not help improve the prognosis of patients with ah.however, in contrast to the other factors, frailty is a factor that can be avoided with preventive intervention [28] [29].although the rates at which ecmo or a warmed blanket were used in the frail patient group were lower in comparison to the non-frail patient group, the results were also similar in the subgroup analysis that excluded cases in which ecmo or a warmed blanket were used.in the present study, the finding that the cpk level was lower in the frail patient group may support this mechanism.thus, further studies are needed to address this problem.the rate of early mortality within 30 days was higher in the frail group than in the non-frail group.the complications defined in this study (arrhythmia, pneumonia, pancreatitis, electrolyte abnormality and coagulopathy) occurred infrequently, which may have contributed to the lack of a significant difference.the present study was associated with some limitations.in this regard, a comparative study regarding the accuracy of the cfs score is currently in progress [40].
number of words= 907
[{'rouge-1': {'f': 0.40412870191949407, 'p': 0.7322073578595318,'r': 0.2790813093980993}, 'rouge-2': {'f': 0.19394360172356442, 'p': 0.30154362416107383,'r': 0.14293868921775899}, 'rouge-l': {'f': 0.34769918660258614, 'p': 0.5907100591715977,'r': 0.24635270541082166}}]
-----------------------------------------------------------------------------------------------------------------------------------
p53:
Extractive Summary:
malnutrition is common among hospitalized patients and has been reported to be associated with worsened prognosis among patients with chronic diseases such as cancer [5] and renal failure [6].gnri could thus be important for risk stratifcation even in patients with cardiovascular disease.stable cad patients with myocardial damage display poor prognosis compared to those without myocardial damage [10].methods study population we evaluated a retrospective cohort in a single center, investigating 241 consecutive patients with stable cad and myocardial damage admitted to kagoshima university hospital between january 2015 and august 2018 for pci.levels of serum albumin, cholinesterase, high-sensitivity c-reactive protein (hs-crp), high-density lipoprotein cholesterol (hdl-c), lowdensity lipoprotein cholesterol (ldl-c), total cholesterol (t-cho), triglycerides, creatinine, uric acid, and fasting plasma glucose were measured, and estimated glomerular fltration rate (egfr) was calculated using the modifcation of diet in renal disease equation with coefcients modifed for japanese patients as follows: egfr (ml/min/1.73  m2 )=194×serum creatinine (mg/ dl)−1.094×age (years)−0.287 (×0.739 for female subjects) [11].patients were divided into a malnourished group (gnri<92) and a non-malnourished group, then macce after pci was compared between groups.mean gnri was 100±13, and participants comprised 55 malnourished patients (23%; gnri<92) and 186 non-malnourished patients (77%).malnourished patients were older (77±9  years) than those without malnutrition (67±11 years, p<0.001), and sex (male) had a signifcant diference between malnourished and non-malnourished patients (p=0.009).tirty-four patients (14%) died after pci, and the frequency of all-cause death was signifcantly higher in the malnourished group (31%) than in the non-malnourished group (9%; p<0.001).te egfr was synonymous with hemodialysis, and the 95%cis of cholinesterase and lvef included 1.00.multivariate analysis showed malnutrition (hr 2.30; 95% ci 1.13–4.67; p=0.020) and hemodialysis (hr 2.17; 95% ci 1.19–3.93; p=0.011) correlated positively with macce (table  3).furthermore, cox proportional hazards models using malnutrition and hemodialysis revealed that patients with both malnutrition and hemodialysis showed greater risk of macce after pci as compared to patients with neither malnutrition nor hemodialysis (hr 6.91; 95% ci 3.29–14.54; p < 0.001) (fig. 3).discussion in this retrospective cohort study of stable cad patients with myocardial damage, we showed that: (1) patients with malnutrition displayed a higher incidence of macce as compared to patients without malnutrition; (2) malnutrition and hemodialysis were independent risk factors for macce in stable cad patients with myocardial damage; and (3) patients with both malnutrition and hemodialysis were associated with an additive increase in the risk of macce.bmi or albumin alone may thus be insufcient for assessing nutritional status.gnri, as an index combining bmi and albumin [7], has recently been widely used in simple screenings of nutritional status and may be useful to overcome the shortcomings of individual indicators such as serum albumin and bmi.intensive care is therefore needed to prevent recurrent cardiovascular events in cad patients with myocardial damage compared to those without myocardial damage, and total management including prevention of energy and protein waste and feeding energy based on nutritional status may improve the prognosis for stable cad patients with myocardial damage.in the present study, hemodialysis was also independently associated with macce.chronic kidney disease has been established as a risk factor for cardiovascular morbidity and mortality [23].tose previous reports may support the results of the present study.in patients with end-stage renal disease, malnutrition has been thought to have a close relationship with infammation and atherosclerosis, and the concept of malnutrition-infammation-atherosclerosis (mia) syndrome has been suggested [26, 27].mia syndrome was then recognized to be an important clinical issue which should be addressed in patients with end-stage renal disease.tese mechanisms may cause protein-energy malnutrition [29].although nutritional status is afected by the interaction of multiple other factors, infammation is recognized as a central factor in malnutrition.chronic infammation is known to lead to increase oxidative stress and the development of severe endothelial dysfunction, in turn leading to cardiovascular disease [27].indeed, we showed that patients with malnutrition and hemodialysis had additively elevated incidences of macce in this study.statin therapy has not been found to achieve any obvious efect on prevention of cardiovascular events in patients with advanced kidney disease [30].hence, a novel strategy for preventing cardiovascular events may be needed in patients with advanced kidney disease.in recent years, some antiinfammatory therapies that do not afect lipid levels have demonstrated the ability to reduce cardiovascular events in cad patients [31, 32].tis study showed several limitations.in addition, the present study assessed the gnri only on admission and did not assess changes in gnri.tird, we were unable to evaluate another malnutrition index such as the nutritional risk index, mini nutritional assessment shortform scale or maastricht index, because the present study is retrospective, and we do not have the data those indexes need.tus, hemoglobin a1c was not included in the analysis.conclusions te current study revealed malnutrition as assessed by gnri ofered an independent risk factor for macce among stable cad patients with myocardial damage.
number of words= 789
[{'rouge-1': {'f': 0.4865609045726958, 'p': 0.7252631578947368,'r': 0.36607609988109396}, 'rouge-2': {'f': 0.30102383406289573, 'p': 0.42883905013192614,'r': 0.23190476190476192}, 'rouge-l': {'f': 0.45627640609669207, 'p': 0.6250239234449761,'r': 0.3592768079800499}}]
-----------------------------------------------------------------------------------------------------------------------------------
p54:
Extractive Summary:
some of the extracts and fractions showed rbc precipitation (hi+) until a certain dilution which showed dose-dependent responses for their ha physical interaction.but some other samples showed rbc precipitation (hi+) in all dilutions which is indicative of ha physical interaction in all dilutions (table 7).preliminary phytochemical analysis results te existence of secondary metabolites was investigated by diferent preliminary analyses for the crude extracts, and chloroform and methanol fractions of fve potent anti-infuenza virus plants including g. glabra, m. communis, m. ofcinalis, s. alba and c. sinensis (fermented).phytochemical analysis data (table  8) confrmed the presence of alkaloids, cardiac glycosides, tannins, favonoids, triterpenoids and steroids in the active crude extracts of g. glabra, m. ofcinalis and s. alba.te efective chloroform fractions of m. communis and c. sinensis (fermented) were rich in alkaloids, cardiac glycosides, triterpenoids and steroids.methanol fractions of m. communis and m. ofcinalis with potential antiviral activities against infuenza virus contained high amounts of favonoids, tannins, triterpenoids and steroids.discussion medicinal plants have progressively been noticed as suitable alternatives to the synthetic antiviral agents [22–25].in the current research, based on the antiviral properties of medicinal plants against iav and other viruses, and their traditional and folklore usage in iran, the antiviral efcacy of the crude extracts, and chloroform and methanol fractions of some iranian native medicinal plants including g. glabra, m. communis, m. ofcinalis, h. perforatum, t. platyphyllos, s. alba, and c. sinensis (fermented and non-fermented) were evaluated against iav with more details.in  addition, other compounds of licorice showed signifcant inhibition on infuenza a neuraminidase in a computerbased approach [27].te efects of polyherbal formula containing licorice were confrmed for the prevention and treatment of infuenza-like syndrome, clinically [28, 29].m. ofcinalis essential oil could inhibit avian infuenza virus (h9n2) through various replication cycle steps especially direct interaction with the virus particles [30].also, its extract demonstrated a signifcant anti-infuenza efect against h1n1 strain of infuenza virus [31].te extract of h. perforatum showed anti-iav efect both in  vitro and in  vivo.te ec50 of the extract was 40 μg/ml against iav while its cc50 in mdck cell line was 1.5 mg/ml [32].in an experiment, it was observed that h. perforatum extract had signifcant efcacy for the treatment of mice infected with iav [32].in another study an opposite response occurred.te consumption of oral h. perforatum extract in the mice infected with infuenza a virus, enhanced transcription of the suppressor of cytokine signaling 3 (socs3) and led to the impaired immune defense and higher mortality [33].te anti-infuenza activity of green tea (camellia sinensis) against h1n1 virus was equivalent to green tea by-products (ec50 equal to 6.72 and 6.36μg/ml, respectively).also, hexane-soluble and ethyl acetate-soluble fractions of green tea by-products possessed strong anti-infuenza activity in chickens [34].te other studies demonstrated that dimeric polyphenol molecules in green tea display more potent antiviral efects against both infuenza a and b viruses than monomers.in addition, the existance of c-4′ hydroxyl group in the b ring of planar favonols is necessary for the anti-infuenza b virus activity [35, 36].it was confrmed clinically that formulations containing c. sinensis or green tea metabolites including catechines and theanine could prevent infuenza infection [37, 38].according to the results, in terms of the selectivity index (si), the extracts and fractions of all tested herbs were considered safe for the antiviral treatments except chloroform fraction of g. glabra.its si value was the same as conventional drugs.in addition, c. sinensis, s. alba, h. perforatum and m. ofcinalis were categorized as the safest plants in cellular studies.in hi test, the extracts of t. platyphyllos, s. alba and c. sinensis showed rbc precipitation in all tested dilutions which indicates the strong physical interaction of these compounds with the ha surface glycoprotein of the virus.however, amongst the others, methanol fraction of m. ofcinalis showed the weakest interaction (3rd dilution), and on the opposite side m. communis and h. perforatum showed stronger interaction (5th dilution).te results of this study confrmed dose-dependent response for most of the extracts and fractions.in a previous research, the formation of complexes between tannins and proteins was confrmed [39].also it was demonstrated that antiviral inhibitory efects of hydrolyzable tannins were related to the intractions blocking between viral glycoproteins and cell surface glycosaminoglycans (gags) [40].it is interesting that in our experiment, m. ofcinalis, unlike the others, did not contain any tannin which justifcated the weakest intraction.with a thorough scrutiny on the tables 3, 4, 5 and 6 and comparing their results with the si values in table 2 and the fact that si values higher than 3 are safe compounds, it was concluded that except one almost all tested samples were safe but not efective against iav titer.moreover, general linear model (glm) analysis which estimated marginal means of all the respective values for diferent exposure ways (combined treatments) confrmed all the outcomes.te data are shown in the supplementary figs.  1, 2, 3 and 4.terefore, most of the above mentioned plant species might be promising alternatives to decrease fu unfavorable efects by afecting the viral and cellular receptors.te data were considerable because the conventional antiviral drugs; amantadine and oseltamivir showed promising efects against virus infection, however, growing drug resistance has caused a signifcant challenge [41, 42].te phytochemical analysis of the potent anti-iav extracts/fractions demonstrated that they were rich in favonoids, tannins, triterpenoids and steroids.natural products have diferent mechanisms against viral infections from interfering with entry, transcription, replication and translation of the virus, nuclear export of the virus, viral assembly, packing and budding to enhance the host responses [43].tere were many reports that showed favonids act as anti-iav compounds with various mechanisms [44, 45].flavonoids are natural phenolic compounds of plants with potent antioxidant and antiviral properties.tey can help viral-infected cells to fx their biochemical imbalance resulting from oxidative stress [46].also they have shown potential inhibition on the neuraminidase active site of infuenza virus.te potency of na blocking reduced from aurones to favon(ol)es, isofavones, favanon(ol)es and favan(ol)es, respectively.te structure activity relationship (sar) studies of favonoids against infuenza virus demonstrated that the presence of 7-oh, 4′-oh, c2= c3 and c4=o functionalities were necessary, but the existance of a sugar group reduced the efects [47, 48].triterpenoids and steroids are natural components elucidated from plants and other organisms which have various biological activities including antiviral activities.mechanistic studies revealed triterpenoids bind tightly to the viral hemagglutinin (ha) and disrupt the attachment of viruses to the cell receptors [49–51].
number of words= 1049
[{'rouge-1': {'f': 0.3736087601988057, 'p': 0.776959706959707,'r': 0.24593436645396535}, 'rouge-2': {'f': 0.18076569708730172, 'p': 0.3052941176470588,'r': 0.1283941605839416}, 'rouge-l': {'f': 0.32509367886534757, 'p': 0.5423926380368098,'r': 0.23210526315789473}}]
-----------------------------------------------------------------------------------------------------------------------------------
p55:
Extractive Summary:
we hypothesized that there would be barriers relevant to the planned trial including care pathway implementation, and that understanding those barriers would facilitate development of strategies to improve implementation.the trial will enroll 444 patients at 20 sites to demonstrate a difference of 3 points in total sspedi scores between intervention and control patients, assuming alpha 0.05, power at least 80%, intraclass correlation coefficient 0.021 and sspedi score standard deviation 8.8.care pathways are tools that can improve cpg implementation.each of the questions was rated on a 5-point likert scale consisting of 1 = “strongly disagree”; 2= “disagree”; 3= “neutral”; 4= “agree”; and 5 = “strongly agree”.we dichotomized those who agreed (score of 4 or 5) vs. those who were neutral or disagreed (score of 1, 2 or 3).these were rated on a 5-point likert scale representing the degree to which they were a barrier: 1 = “not at all”; 2 = “a little”; 3= “somewhat”; 4= “a lot”; and 5 = “extreme”.we focused on any barrier defined as those that were somewhat, a lot or extreme barriers, and severe barriers defined as those that were a lot or extreme barriers.third, given the timing of the covid-19 pandemic related to full grant funding (r01 notice of award july 2020), we also asked about the impact of the pandemic across the spectrum of clinical research activities.it was disseminated to the participating sites by email and completed in redcap.the respondents were the site principal investigators; they could consult with other institutional personnel to facilitate survey completion.reminders were sent weekly up to three times in the event of non-response.statistics descriptive statistics were used to summarize baseline characteristics and potential barriers to symptom management care pathway implementation.analyses were performed using r studio version 3.6.1, the r foundation for statistical computing.results of the 25 sites who supported grant submission, one withdrew because of institutional resource constraints and one did not complete the survey, leaving 23 institutions included in the analysis.the survey was completed between august 5, 2020 and september 9, 2020.table 1 describes institutional, patient and healthcare professional characteristics.within all institutions, health care professionals create orders in the electronic health record for symptom prevention and management.the median number of new pediatric cancer patients diagnosed annually was 90 (interquartile range (iqr) 63 to 200).all institutions were described as not-for-profit.the median number of physician full time equivalents was 9 (iqr 5 to 13).table 2 shows the results of the inner setting measures from the cfir.respondents from 35% of institutions indicated that “staff members often showed signs of stress and strain”.table 3 shows additional barriers to implementing symptom management care pathways among respondents.the most common barriers (somewhat, a lot or extreme barrier) were as follows: lack of person-time to create care pathways and champion their use (35%), lack of interest from physicians (30%) and lack of information technology resources (26%).severe barriers (a lot or extreme barrier) were rare and the most common was the lack of person-time to create care pathways and champion their use (9%).table 4 illustrates the impact of the covid-19 pandemic on research activities at the institutions.most sites reported no negative impact of the pandemic across research activities.the most common activities that were a lot more difficult or almost impossible were executing contracts (9%), study activation (9%) and accessing patients in person (9%).sites with fewer newly diagnosed cancer patients and those with fewer physician, nurse practitioner and physician assistant full time equivalents were significantly more likely to agree that their staff are supported.we found that unfavorable implementation climate may be a potential barrier to care pathway implementation.conclusions in conclusion, respondents at pediatric oncology institutions expected few barriers to symptom management care pathway implementation at their institutions.
number of words= 609
[{'rouge-1': {'f': 0.5285516702776238, 'p': 0.7941379310344827,'r': 0.39608695652173914}, 'rouge-2': {'f': 0.3141063492204272, 'p': 0.4506228373702422,'r': 0.24107309486780715}, 'rouge-l': {'f': 0.4771560590882797, 'p': 0.7138356164383561,'r': 0.3583435582822086}}]
-----------------------------------------------------------------------------------------------------------------------------------
p56:
Extractive Summary:
it is produced endogenously in small amounts as a byproduct of the catabolism of heme molecules [1].it can also be inhaled when hydrocarbon-containing fuels are not completely burned [1, 2].the qt interval has long been known to vary significantly among the individual leads of a surface 12-lead electrocardiogram [3].a potential clinical application of this inter-lead difference was proposed in 1990 by day et al., who suggested that the inter-lead difference in the qt interval might provide a measure of repolarization inhomogeneity, which they called “qt dispersion” [3, 4].methods the sample group of this retrospective study comprised patients suffering from carbon monoxide poisoning who had been admitted to st.mary’s hospital, kurume, japan for treatment between june 2013 and september 2019.encephalopathy was defined as the development or recurrence of symptoms such as difficulty concentrating, dementia, psychomotor retardation, parkinson’s disease, and amnesia, which were either diagnosed by mri at time of discharge or up to 6 weeks after admission.the qt intervals for each lead were measured and corrected for heart rate (qtc) using bazett’s formula (qt/ √rr) [6].univariate and multivariate nominal logistic regression was performed to examine the relationships between patient prognosis and the level of carboxyhemoglobin, troponin i, and ecg parameters.results patients altogether, 70 patients (42 men, 28 women; mean age 52 ± 18 years) were included in the study, and their clinical characteristics are presented in table 1.fifteen patients were diagnosed with encephalopathy either at or after discharge, whereas no patients exhibited cardiomyopathy at or after discharge.correlation between carboxyhemoglobin and qt intervals the correlation between maximum qt intervals and the carboxyhemoglobin levels was not significant.conversely, the correlation between the maximum qtc interval and the carboxyhemoglobin level was significant (p = 0.0072, r2 = 0.1017; fig. 1), as were the relationships between qt dispersion and carboxyhemoglobin (p < 0.001, r2 = 0.2358; fig. 1) and the qtc dispersion and carboxyhemoglobin (p < 0.001, r2 = 0.2613; fig. 1).comparison between patients with and without a sequential disability death, carbon monoxide encephalopathy, and cardiomyopathy at or after discharge were all considered to be sequential disabilities, although none of the patients in this study had cardiomyopathy.roc curve analysis was performed on qtc dispersion and troponin i levels (fig. 3).carbon monoxide reduces the oxygen-carrying capacity of blood and binds with cardiac myoglobin, causing a rapid decrease in myocardial oxygen reserves [6].furthermore, both qtc dispersion and troponin i are predictors of sequential disability.tanba et al. reported that the normal value of qtc dispersion in japanese patients is 31 ± 11 ms [10].in this study, the qtc interval and qtc dispersion correlated with the carboxyhemoglobin level.note that the multivariate analysis did not find that carboxyhemoglobin was related to a poor outcome.hampson et al. found that the level of carboxyhemoglobin in the blood was a poor predictor of the clinical prognosis of patients with carbon monoxide poisoning [12].moreover, mortality was associated with the absolute difference in carboxyhemoglobin [12].satran et al., however, reported that moderate-to-severe carbon monoxide poisoning causes myocardial injury when assessed by electrocardiography or biomarkers [5].although there were no patients with cardiomyopathy at/after discharge in this study, the indices of myocardial injury could lead to neurological injury or even mortality.the mechanisms described influence this result.the findings must be confirmed in prospective, multicenter studies with larger populations.
number of words= 538
[{'rouge-1': {'f': 0.49190276976303393, 'p': 0.781206896551724,'r': 0.35896672504378285}, 'rouge-2': {'f': 0.29966426680521535, 'p': 0.45095238095238094,'r': 0.22438596491228072}, 'rouge-l': {'f': 0.436436739053908, 'p': 0.7154545454545456,'r': 0.3139862542955326}}]
-----------------------------------------------------------------------------------------------------------------------------------
p57:
Extractive Summary:
while several studies have documented ample diagnostic variation regarding patients with pss in general practice [30, 31], it is not well documented which codes or other methods gps use for registration of pss and if they fnd their current approach to registration satisfactory.te primary aim of the present study was, therefore, to explore how gps currently register pss.ten gift cards of 25 euro were allotted to gps who participated and provided us with their email address.to address conceptual diferences between gps regarding pss and to ensure that both the medical and psychological domain of pss was captured, separate questions were added which specifcally addressed pss in patients with a(n explained) chronic medical condition (i.e., ‘patients presenting with more or more severe symptoms than you would expect’) and/or the b-criteria of ssd (i.e., ‘patients who have maladaptive cognitions, emotions and/or behaviours related to the somatic symptoms’).at the start of the survey, gps were asked to fll in non-identifying demographic questions.below you fnd a description of the survey questions (for an exact outline of the survey, see additional fle 1).subsequently, (3) gps were asked whether they mention pss-related terminology in the (3a) episode name or (3b) free text area (two 4-point scale items (ranging from ‘never’ to ‘always’)).te responses on the four single complaints were analysed both combined and as separate complaints.for the question regarding the use of pss-related syndrome codes, the two ‘yes, …’ answering options were combined and the two ‘no, …’ answering options were combined to construct total scores.results of the approximately 12,000 active gps in the netherlands, an estimated 2,000 gps were reached through our distribution method.in total, 259 gps (13%) fully completed the survey, with exception to the fourth item (4) which was completed by 189 gps.table  1 displays the characteristics of the total sample.gps years since graduation is reasonably evenly distributed over 5-year periods, varying between the smallest group of gps graduating between 26 to 30 years since participating in the survey (8.5%) and the largest group of gps graduating between 6 to 10 years before participation (17.8%).combining the preferred frst choices of code for the four pss case examples, the general trend indicates that gps were most likely to register pss on a symptomspecifc level (89.3%).te frequency of choosing general codes increased from 6.9% as a frst choice to 31.1% for the second and 45.5% for the third choice.te choice for icpc code p75 (somatization disorder) increased from 1% as a frst choice, to 5.1% and 8.0% for the second and third choice respectively.when presented with fatigue complaints, more than 35.7% chose to report the complaint with p75 (as a second or third choice).for a more detailed description of the choices of icpc codes per presented symptom, see additional fle 3.some gps mentioned in the comment section that a fear of stigmatization was the reason for avoiding pss-related terms.of all gps who answered the question regarding icpccodes for recognized pss-syndromes (n=189), 91.5% indicated that they use the codes for ibs (d93), cvs (a04.01) and fm (l18.01) (not shown in table).te answering options given in the survey are depicted in table 3. while 68.3% of gps reported diagnosing the syndrome themselves, several gps commented that this was only the case for ibs (which was also the case in most gps who selected the answer ‘other, namely…’).gps needs table  4 shows the results relating to the second aim of this study.of those who did not see the lack of a specifc pss code as a problem (52.1%), many commented they sometimes describe pss in the available free text area when registering the patient’s somatic complaint.of all gps, 32.8% reported that they would like to be able to express pss-related components – i.e., specifc thoughts, feelings and behaviour conforming to the b-criteria of ssd – in a code.additionally, gps indicated a need for training (56.7%) and/or an – (online) classifcation and/or risk assessment – tool (58.3%) and/ or other support (69.7%).other pss-related needs mentioned by gps in the elective comment sections regarded clearer or more referral options, more time and fnancial compensation for consultations and better guidelines (although others explicitly mentioned that they found the current guidelines adequate).pss are primarily coded at a specifc somatic symptom level and gps often avoid using terminology related to pss in their ehr.in addition, gps indicate they us the codes for well-known pss-syndromes as ibs, cfs, or fm, although ibs is coded more often than cfs and fm.overall, half of gps are unsatisfed with current registration options for pss.still, while gps provide several suggestions for improvements of the classifcation system, there is little consensus on this matter.on the other hand, respondents did indicate more frequently diagnosing the pss-related syndrome ibs, compared to cfs and fm, which is in line with previous research indicating that gps are more profcient in diagnosing ibs [11].tese gps reportedly require a specifc code for pss, in combination with training, tools, a widely accepted guideline, and referral options.in contrast, the literature shows that there are a variety of training options [38], concise and validated screening questionnaires [26, 39, 40], and referral options [3, 13, 41] available to gps.in line with this guideline, some gps suggested coding of pss should be done by severity, which is also in line with studies which propose the introduction of codes that specify severity to improve documentation of mild pss [8, 9, 29].it is therefore conceivable that gps do indeed need training, and knowledge of the availability of training, to increase their understanding of pss.still, although adequate reference data is limited, responses appear fairly representative for the population of gps in the netherlands [35, 42].nonetheless, this came with the limitation that it is unclear if the more frequently coded ‘p75-somatization disorder’ in case of fatigue compared to other complaints is a true fnding, or whether it demonstrates the limitations of the icpc coding system itself to facilitate classifcation of pss, or if it is related to a lack of potential alternative codes (see additional fle 2).besides, this may promote timely treatment of the cognitive, emotional and behavioural components of pss, which, in turn, may decrease the burden of pss and reduce the risk of iatrogenic harm.conclusion registration of pss in primary care is currently ambiguous.specifc complaints presented by patients with pss are primarily coded on a symptom-specifc level.since many of the latter already exist, improvements should be directed at new options for registration, specifcally coding, and increasing and spreading knowledge about pss, guidelines, available tools and referral optio
number of words= 1075
[{'rouge-1': {'f': 0.3652203118528291, 'p': 0.7603914590747332,'r': 0.24032484635645304}, 'rouge-2': {'f': 0.17444864992122278, 'p': 0.2914285714285714,'r': 0.12448154657293498}, 'rouge-l': {'f': 0.33774053741693155, 'p': 0.5976073619631901,'r': 0.2353846153846154}}]
-----------------------------------------------------------------------------------------------------------------------------------
p58:
Extractive Summary:
background te burden of cancer incidence is increasing worldwide and is regarded to be a global and serious public health problem [1].gynecological cancers were reported to be major sources of cancer related mortality in moroccan women.according to globocan 2018 statistics, cervical cancer is ranked second and ovarian cancer is ranked in the ffth position among cancers in moroccan women [2].hysterectomy and or oophorectomy are known to be the main surgical treatments after the diagnosis of cancer of the cervix, uterus and ovary.in many cultures, besides breast, the uterus and ovaries were considered as symbols of femininity, fertility and especially motherhood.indeed, the loss of these organs after surgery makes end to reproductive capacity and afects the ultimate goal of procreation in some countries [3, 4].after undergoing radical surgery, gynecological cancer patients become vulnerable to several disorders impacting their quality of life, such as anxiety, depression, body image disorders and low self-esteem [5, 6].next, sleep quality is considered to be an important indicator of quality of life [7].among women with breast cancer, higher scores on depression and fatigue were associated with lower sleep quality [16].recently, higher hopelessness predicted higher sleep disturbance in women with breast cancer [9], while no such data were reported for women with gynecological cancer.to our knowledge, and to date, there is no published study specifcally devoted in sleep disorders in north african women with gynecological cancer after radical surgery.tese studies showed that sleep disorders were signifcantly correlated with anxiety-depressive disorders and impaired self-esteem [7, 8, 17–19].however, these studies were not specifc to women with gynecological cancer who have undergone radical surgery.further, these studies did not address the relationship between self-esteem, body image disturbances and sleep disturbances in women gynecological cancer after radical surgery.however, to answer to these questions is important for the following reasons: exploring the predictors of sleep quality, a prognostic indicator of survival in women with cancer, and managing them improves the quality of life of women with gynecological cancer [7].t1–t3 tumor stages, followed by hysterectomy/ oophorectomy for more than 3  weeks and not more than 6  months; 4. signed written informed consent (for unschooled patients a verbal consent followed by a fngerprint were used as a consent proof); 5. willing and able comply with the study conditions.te researcher verbally presented the study to the patients and woman who did not wish to participate were excluded.patients with a total score of 11 or higher for hads-a or hads-d were diagnosed with clinical anxiety or depression, respectively.it refects also the impact of cancer treatment, including surgery, on body image of gynecological cancer patients.subsequently, a pilot study was conducted on a group of 30 cancer patients excluded from the study.statistical analyses descriptive statistics were performed to describe sociodemographic and medical characteristics participants.demographic variables were included in the frst step.te medical coverage system was related to the quality of sleep in our sample and was therefore included as a potential social confounding factor in the second step.in the third step, we entered the scores of anxiety, depression, dissatisfaction with body image and self-esteem.all p values equal to or less than 0.05 were considered statistically signifcant.te statistical analysis was carried out using the spss® version 20.0 (ibm corporation, armonk ny, usa) for windows®.microsoft excel 2010 was used for inserting fgures.results te socio-demographic and clinical characteristics of the participants were represented in the table 1.tirty-nine % reported very poor subjective sleep quality, and 14% reported somewhat poor quality.for sleep onset latency, 35% needed more than one hour to fall asleep, and 20% needed between 31 and 60  min to fall asleep.te sleep duration did not exceed 5  h per night for 42% of the participants, with a median of 5  h per night (table 2).forty-seven % of participants had usual sleep efciency less than 65%, while the latter exceeded 85% in 18% of participants.te sleep disorders in the participants were explained by several reasons, mainly citing: waking up in the middle of the night or early in the morning (68%), getting up to use the bathroom (62%), having nightmares (42%), feeling hot (38%), having breathing problems (35%), having pain (26%), feeling cold (18%) and coughing and snoring loudly (16%).tis result is consistent with data of a descriptive study conducted on women with breast cancer and gynecological cancer, for which the psqi indicated that 80% of patients had sleep disorders during their stay at the hospital [29].te reasons which are basically responsible for hindering the quality of sleep and mostly cited by the patients were especially: waking up in the middle of the night or early in the morning and getting up to use the bathroom.in the present study, age was a factor associated with sleep disturbances, this factor was also cited by other study seeking predictors of poor sleep quality in patients with cervical cancer during adjuvant therapy [17].according to our data, poor patients sufered from various sleep disorders.tis fnding is inconsistent with a study showing that there was no correlation between psqi and time since diagnosis in ovarian cancer patients [7].it has been reported also that, in cancer patients, the level of emotional distress was signifcantly associated with the levels of self-esteem [31].however, our study is the frst in morocco and elsewhere, in which the efect of dissatisfaction with body image on the quality of sleep in patients with gynecological cancer is evaluated after radical surgery.in addition, our work is conducted at the university hospital center which welcomes patients coming from both urban and rural sector of the casablanca-settat region, the country’s economic capital.
number of words= 914
[{'rouge-1': {'f': 0.44384918584244515, 'p': 0.7646107784431138,'r': 0.31267782426778246}, 'rouge-2': {'f': 0.24532505461384796, 'p': 0.3853153153153153,'r': 0.1799476439790576}, 'rouge-l': {'f': 0.3451814484050083, 'p': 0.5351162790697674,'r': 0.25475750577367207}}]
-----------------------------------------------------------------------------------------------------------------------------------
p59:
Extractive Summary:
in this article, we offer an approach for developing specifications for process improvements using rapid, remote, consensus-building methods, and we illustrate it using a case study conducted during the covid-19 pandemic.we start by noting that there is no consensual definition of process improvement, but it is distinguished by its focus on how to improve the underlying processes (such as workflows, task design, role allocations, communication techniques, resources required, and so on) for delivering care – rather than, as in the case of clinical guidelines, defining ideal clinical standards.process improvement has a particular role in ensuring that work systems are optimised, for example by helping to define the activities from beginning to end of a clinical process or pathway; to explain how these activities can most effectively be undertaken; to clarify tasks, roles, and skills needed; to characterise the decisions to be made and the support needed to make and implement those decisions; and to identify the equipment, resources and other tools required [7, 8].some, such as the model for improvement, lean and six sigma, amongst others [11], have been adapted from industry techniques [12].specification requires identifying and defining the changes that need to be made (for example, an amendment to task or role design, or use of a new piece of equipment) to bring about improvement.the work of process improvement specification has instead remained largely locally-led, conducted within individual organisations.local leadership of process improvement has, of course, many advantages, including the potential to customise a solution to local circumstances and to imbue a sense of local ownership, but it is also associated with some disadvantages.the deficiencies in some of the current infrastructure for process improvement have been vividly surfaced by the covid-19 pandemic, which has caused massiv disruption in the organisation and delivery of healthcare [23, 24].given the challenges of both top-down and bottom-up quality improvement, a more collaborative alternative is likely to be of value.these approaches are commonly recommended and used for developing clinical guidelines [55, 65] and reporting guidelines [66, 67], but application of consensus-building in process improvement has remained much more limited, not least because of the tendency (discussed above) to see process improvement as the domain of local teams.new methodological approaches are needed to use the method effectively for large-scale remote consensus-building to specify process improvements, while adhering to a participatory ethos and minimising time and effort required of participants.(2) produce a draft or prototype of the proposed process improvement specification rapid consensus-building on the specification of process improvements can be facilitated by producing a draft or prototype version, which might be informed by rapid literature reviews, existing guidelines or practices, ideas sourced from specialist groups, or stakeholder surveys, interviews and focus groups [70, 75].importantly, for many process improvement activities, patients and the public are key stakeholders whose expertise and perspectives should be included [42, 84].(4) design and conduct a remote consensus-building exercise ethics the ethical principles for participatory consensusbuilding exercises have much in common with any other quality improvement activity [92, 93], or indeed guideline development, in that they may often be classified as service improvement activities not requiring specific ethical review.where these exercises are conducted as research studies rather than improvement activities, different considerations may apply, such as the requirement for oversight and/or approval by an institutional review board or research ethics committee [92, 93].this might be done by inviting participant feedback on the draft/prototype of the process improvement specification, which can then be synthesised in a set of propositions to be rated in the subsequent delphi rounds.delphi rounds the propositions derived from the idea generation phase can be used in two or more subsequent delphi rounds of iterative surveys in which participants state their level of agreement with propositions on a numeric scale.responses are aggregated and participants have the opportunity to revise their judgments in the light of feedback that includes their own and the group’s judgment, with the aim of exploring or reaching group consensus [98].two rounds is often sufficient to reach consensus and may reduce burden for participants [70, 75], though more rounds may sometimes be used.use of visually appealing forms of feedback, such as interactive graphs that show the distributions of ratings across one or more stakeholder groups, can facilitate response across rounds.one advantage of the collaborative approach is that it is likely to facilitate engaged dissemination and implementation by those who have participated [99].the need for process improvement specification in maternity care during the covid-19 crisis was particularly urgent given that, in contrast with some other areas of care, it is not possible to defer or reschedule births [100, 101].this meant that many existing areas of care where good clinical practice was well-established (e.g. through clinical guidelines) required process improvement to adapt to the need for: infection control, the challenges of communication and teamwork likely to be posed by use of personal protective equipment (ppe), and other demands of making clinical processes covid-safe.pph is an emergency that complicates 1.2% of births in highincome settings [105, 106].to employ a consensus-building exercise, we used thiscovery (https://www.thiscovery.org/about), an online research and development platform created and developed by this institute at the university of cambridge.results we applied the five steps of the framework to the case study.many maternal deaths related to pph in healthcare settings can be avoided through effective clinical management [106, 109], including prompt initiation of several simultaneous actions such as uterine massage, intravenous fluid resuscitation, and administration of medication (tranexamic acid to treat major haemorrhage and uterotonics to contract the uterus).treatment delay can result in poor outcomes [113, 114], so delivering these clinical interventions requires highly optimised underlying processes, including effective teamwork, coordination, communication, and access to appropriate supplies.all of these processes require adaptation for a covid-19 scenario, which might demand, for example, donning and doffing of personal protective equipment, changes in the tasks undertaken and their sequencing, and forms of communication suitable for a situation where masks and visors may inhibit verbal and nonverbal exchange.this problem appeared well-suited to consensusbuilding that could rapidly generate a consistent approach.target audiences included healthcare professionals working in maternity care in uk nhs trusts, including midwives, obstetricians, and managers of maternity services.(2) draft of the proposed process improvement specification we started by producing version 1 of a video that showed a simulation of a maternity ward team managing pph in a woman with suspected or confirmed covid19.the video illustrated processes that included: how the team communicated with each other, the woman and her partner; ppe donning and doffing procedures; and use of obstetric-specific procedures (e.g. pph ‘grab bag’, treatment algorithms) in a covid-19 context.the processes illustrated were based on: the emerging national guidance on covid-19 infection prevention and control in a clinical setting (april 2020); clinical guidelines for managing obstetric emergencies such as pph [e.g. 111, 112]; and ways of working established in one of the safest maternity units of the uk [115, 116].the use of a video format to elicit suggestions for improvement was intended to enhance participant engagement [76, 77], minimise cognitive burden (e.g. not having to study a written manual) [117], and align maternity professionals’ desire to have more covidspecific e-resources available [104].the video format was also expected to work well for the study participants using a range of different technologies, including mobile devices [76, 118].(3) participant recruitment strategy eligibility we drew on the expertise of three expert groups: maternity teams to provide clinical and practical views; infection prevention and control staff for specialist knowledge on infection guidance; and healthcare human factors specialists for their perspective on the interaction between people and work systems in healthcare settings [21, 119].all exercises were user tested to optimise participant experience on computer, tablet and smartphone platforms.data collection and analysis for the consensus-building exercise (including the recommendation generation exercise and delphi rounds) was completed in 6 weeks.ethics participants registered for an account on thiscovery after consenting to the platform’s privacy policy and terms of use (https://www.thiscovery.org/register/).participants confirmed their consent before each new round of the consensusbuilding exercise.all data was captured and processed without any personal identifiable information.recommendation exercise we started with a recommendation generation exercise to inform the subsequent delphi consensus-building rounds (fig. 1).this initial exercise consisted of asking participants for their feedback on a draft of good practice in undertaking the process, as illustrated in a video.after seeing the video, participants were asked, using open-ended questions, to draw on their professional expertise to provide recommendations to improve the practice illustrated in relation to: 1) donning ppe; 2) management of the emergency in the context of covid-19, e.g. use of pph “grab bag”; 3) doffing ppe; and 4) any other areas.there were 912 responses from 103 participants (two participants did not provide recommendations).the risk of attrition bias was low, as ratings in the first delphi from participants completing both delphi rounds and from participants who did not respond to the second round were similar.in the second round, the participant was presented with their original rating for each respective recommendation, along with the distribution of ratings from each stakeholder group [122, 123], as shown in an interactive bar chart (see supplement 2).the participant was asked to consider the ratings of the others, and whether they would like to change their rating or stay with their original rating from the first round.(5) final specification of process improvements the recommendations that reached consensus were reviewed by the project team and used to inform the specification of process improvements for optimised management of obstetric emergencies during the covid-19 pandemic.the video and other resources were endorsed by leading organisations who had supported the project, including royal colleges, specialist societies, and quality improvement bodies, and were widely shared.our case study shows that it is possible to deploy the approach successfully to specify process improvements in an area of pressing need during the covid-19 pandemic.this capability can help to enhance the currently limited infrastructure for collaborative building of specifications for process improvement in quality and safety of healthcare.it thus may have potential to address many of the problems of duplication of effort, waste, de-standardisation, and inability to engage sufficiently diverse expertise that currently characterise many quality improvement efforts in healthcare [17, 36].our case study showed that the proposed methodological approach can be used successfully to develop specifications for the process improvements needed to ensure high quality care, and may support the production of the kinds of high quality resources that professionals particularly value [104].the systematic approach to participatory consensus-building that we propose is rich in potential for use in other areas that would benefit from specifying process improvement for clinical scenarios.using this approach is likely to reduce waste in process improvement, since it not only produces a solution that can be used at scale within the maternity care community, it also generates many core elements of a solution that can be customised for different clinical scenarios outside of maternity.over time, the approach may facilitate further work to strengthen the infrastructure for participatory approaches in process improvement, similar to efforts over the last decades for building the infrastructure for clinical guidelines and core outcome set development [1, 2, 54, 72].yet results of quality improvement in healthcare are typically mixed [18], suggesting the need to improve how improvement is done.the participatory ethos on which our approach is built may increase acceptability, uptake and impact of process improvement, but that remains to be tested.finally, there will be an ongoing need to evaluate the process improvements specified through our proposed approach, for examining impacts on implementability, efficiency, staff and service user experience, acceptability, sustainability of change, impact on clinical outcomes, and any unintended consequences.it also showed the feasibility of rapidly gaining feedback and reaching consensus on the process improvement (< 6 weeks), even during the first peak of the covid-19 pandemic in the uk, with relatively little attrition (≤; 20%) between the delphi rounds.this risk was managed by close involvement of clinical expertise from the project team, and by ensuring that process improvements were specified such that they could have enduring relevance (e.g. referring to principles and national guidance on donning and doffing procedures rather than rigidly specifying them).we were not able to evaluate the implementation or impact of the specified process improvements in the time available or compare our approach with alternative approaches to the specification of process improvements.conclusion we developed and tested a methodological approach to specifying process improvements that employed a participatory ethos and remote consensus-building methods.
number of words= 2054
[{'rouge-1': {'f': 0.32031767529784316, 'p': 0.8509798270893372,'r': 0.19728980742132457}, 'rouge-2': {'f': 0.2025956147148653, 'p': 0.44572254335260114,'r': 0.13109022556390978}, 'rouge-l': {'f': 0.3132520913599694, 'p': 0.6731746031746031,'r': 0.20411764705882354}}]
-----------------------------------------------------------------------------------------------------------------------------------
p60:
Extractive Summary:
background approximately 234 million major surgical procedures are undertaken worldwide every year [1].however, since thoracic surgery requires a differentiated ventilatory approach, those patients were excluded from the primary analysis of the las vegas study.in thoracic surgery, conventional methods to prevent and treat hypoxemia during one lung ventilation (olv) can be harmful to the lung tissue: high fraction of inspired oxygen (fio2) and low (or no) positive end–expiratory pressure (peep) both can promote atelectasis, whereas high tidal volume (vt) can cause baro- and volutrauma [4].the type of thoracic surgery (open or endoscopic) as well as the intraoperative mechanical ventilation settings may also influence ppcs.when low vt was used in abdominal surgery, high peep combined with recruitment maneuvers, as compared to low peep without recruitment maneuvers, did not add to the protection against ppcs [7].methods study design and sites the present work is a post hoc analysis of the ‘local assessment of ventilatory management during general anesthesia for surgery and effects on postoperative pulmonary complications’ (las vegas trial) [3].the las vegas trial protocol was first approved by the institutional review board of the academic medical center, amsterdam, the netherlands (w12_190#12.17.0227) and registered at clinicaltrials.gov (nct01601223).the protocol of this trial was published elsewhere [9].these data have not been considered in previous analyses.reasonable parameters of baseline characteristics, intraoperative data and preoperative risk factors for ppcs were identified from previous studies [10–13].the definition of protective mechanical ventilation is still under debate.for this analysis it was based on recent recommendations [8, 14–16].the occurrence of ppcs is presented as a collapsed composite of ppcs in the first five postoperative days.the following ppcs were scored daily from the day of surgery until hospital discharge or postoperative day 5: 1) need for supplementary oxygen (due to pao2 < 60 mmhg or spo2 < 90% in room air, excluding oxygen supplementation given as standard care or as continuation of preoperative therapy), 2) respiratory failure (pao2 < 60 mmhg or spo2 < 90% despite oxygen therapy, or need for non-invasive mechanical ventilation), 3) unplanned new or prolonged invasive or non–invasive mechanical ventilation, 4) acute respiratory distress syndrome, 5) pneumonia.severe ppcs were defined as the occurrence of one or more of the complications 2–5.patient data were anonymized before entry onto a password secured, web–based electronic case record form (openclinica, boston, ma, usa).the ventilatory data, which were collected hourly, were first averaged for each patient according to the number of observations (median of the value).in a longitudinal analysis, this data is presented for the first, second, third, fourth and last hour of surgery.all data are presented for the whole population and for the subgroups.cut-offs of 6 ml/kg pbw for tidal volume, and 5 cmh2o for peep were chosen to form the matrices.these cut-offs were based on widely accepted values of each variable, or according to normal daily practice.statistical significance was considered to be at two-sided p < 0.05.all analyses were performed with r version 3.4.1 (http:// www.r-project.org/).mechanical ventilation patients were ventilated with vt of 7.4 ± 1.6 ml/kg pbw, peep of 3.5 ± 2.4 cmh2o, and driving pressure of 14.4 ± 4.6 cmh2o (table 2).values of ventilator settings along time are shown in supplemental figures 2 through 4 (additional file 1).the combinations of vt and peep according to subgroups are shown in supplemental figures 5 through 7 (additional file 1).discussion in this population of patients undergoing thoracic surgery: 1) mechanical ventilation differed from those recommended for lung protection in 85.2% of all patients; 2) patients under olv received lower vt, higher peak, plateau and driving pressures, higher peep levels and respiratory rate, and received more recruitment maneuvers compared with tlv; 3) the overall incidence of ppcs was as high as 45.7%; 4) ppcs were more common among patients with higher ariscat score or comorbidities, but not increased following open vs. endoscopic procedures, or olv vs. tlv; 6) ppcs were associated with increased los.the main strengths of our study are that data was stored, analyzed and reported according to international standards [21].in a mixed surgical population without surgery involving cardiopulmonary bypass, 10.4% of patients developed ppcs within the first postoperative 5 days; values ranged from 6.7% in plastic/cutaneous procedures to 38.2% in transplant surgery [3].in average, 10.7% of patients at increased risk, for example obese patients, developed ppcs [29].the observation that patients who developed ppcs had more comorbidities and longer los is in line with previous studies addressing intraoperative tlv [3, 33].third, the definition of protective mechanical ventilation was based on recommendations that are still under debate.
number of words= 743
[{'rouge-1': {'f': 0.4821389033399898, 'p': 0.711095890410959,'r': 0.3647103274559194}, 'rouge-2': {'f': 0.2272558727465487, 'p': 0.309010989010989,'r': 0.17970996216897855}, 'rouge-l': {'f': 0.4358452639687892, 'p': 0.6039805825242719,'r': 0.340935960591133}}]
-----------------------------------------------------------------------------------------------------------------------------------
p61:
Extractive Summary:
experience from our tertiary center is that the extubation could be safely performed immediately after cervical surgery.while for some major orthopedic surgery, it might take 24-48 h for safe extubation.in the current case, the delayed airway obstruction still existed 2 weeks after surgery and required tracheostomy.the upper airway edema is obvious from postoperative ct imaging and gradually dissipated after 1 month.for pediatric patients, large adenoids/tonsils and obese are important indications for difficult airway management [8].considering the patient’s past medical history of sleep dyspnea in supine position due to tonsil hypertrophy, the failed tracheal intubation and the canceled tonsillectomy, we speculated that this might constitute as an important factor for prolonged extubation when complicated with airway edema.moreover, the occipito-cervical alignment has significant impact on the oropharyngeal space.sporadic case reports revealed that the angle of occipito-cervical (o-c) fusion would be critical for postoperative upper airway obstruction [9].a retrospective clinical study showed there’s a difference in the o-c2 angle (the angles between the inferior endplate of c2 vertebrae body and the mcgregor’s line).the percentage changes in the crosssectional area of the oropharynx before and after surgery were linearly correlated.deceased o-c2 angel may lead to dyspnea and/or dysphagia after surgery [10].for patients with occipital-cervical fixation surgery, extubation should be performed with caution.the boy’s congenital cervical skeletal deformity including the basal invagination and kfs, as well as the internal fixation may interfere with the accurate measuring of o-c2 angel.therefore, we could only made a rough estimation of the o-c4 angel.however, from lateral view of the cervical x-ray in neutral position, a slight decrease of the o-c4 angel was noticed (fig. 3).the o-c4 fusion immobilized the head and neck movement, and forced the patient’s head in a slight flexion position when compared with preoperative imaging, which further compressed the laryngeal space and resulted in the failed extubation.moreover, pediatric patients with large tonsils, adenoids or obesity were prone to airway collapse during anaesthesia [11].the decreased oropharyngeal space caused by the surgery were further compressed by the postoperative laryngeal edema and tonsil hypertrophy which ultimately resulted in difficulty breathing and tracheostomy.chiari malformation (cm) is the downward herniation of the caudal part of the cerebellum and/or medulla oblongata into the spinal canal, and is associated with basilar invagination and can alter neurological functions such as upper airway motility and respiratory control, not only central, but also obstructive respiratory events [12].in this case, the tonsil hypertrophy and cm may not directly lead to airway obstruction, but may complicate the difficulty airway management.the compensatory range of the upper airway space in this patient was limited with the pre-existing tonsil hypertrophy as well as changed upper airway muscle tension caused by the chiari deformity.the post-operative decreased o-c4 angel further worsened the condition and might be an important constitutional factor in postoperative dyspnea [13].extubation should be performed with caution.however, fob examination and cuff leak test could only reflect subglottic airway condition.it is reported from a multicenter evaluation study that several cuff leak tests display limited diagnostic performance for the detection of post-extubation stridor.given the high rate of false positives, routine cuff leak test may expose to undue prolonged mechanical ventilation [14–17].inappropriate selection of tracheal tube would lead to confused result.therefore, we did not solely depend on this test.for the narrowing of the oropharyngeal space, more thorough examination including ct scanning should be performed.multi-disciplinary consultation is important for deciding appropriate extubation time.anaesthesiologists should get well prepared for the emergent airway condition.however, there is no pediatric-specific universal extubation guidelines or experts consensus.current algorithms are modifications of adult approaches which are often inappropriate.we reported a rare case of a pediatric patients with cervical osseous deformity undergone orthopedic surgery and need tracheostomy as a result of failed tracheal extubation.the prolonged upper airway obstruction after occipito-cervical fusion have never been reported in pediatric patients undergone osseous torticollis surgery.the causes of prolonged airway obstruction after occipito-cervical fusion are multifactorial.the upper airway edema constituted the major reason, and the hypertrophic tonsil and the congenital cervical malformation may further complicated the airway condition with limited occipital-cervical range of motion and decreased compensatory space of the oropharyngeal cavity.cautions should be taken during extubation process in pediatric patients who undergone major osseous torticollis surgery.evaluation of general clinical factors that may produce an adverse impact on ventilation after tracheal extubation should be comprehensively considered and optimized.
number of words= 712
[{'rouge-1': {'f': 0.40456196182314913, 'p': 0.7836150234741783,'r': 0.27266666666666667}, 'rouge-2': {'f': 0.2127987817638599, 'p': 0.35773584905660377,'r': 0.1514419225634179}, 'rouge-l': {'f': 0.3910581452659649, 'p': 0.6493650793650794,'r': 0.27977011494252874}}]
-----------------------------------------------------------------------------------------------------------------------------------
p62:
Extractive Summary:
a detailed analysis, described below, was performed to assess the optimum number of nodes as well as the number of layers.the following hyper parameters were used: the batch sized equaled the sample size, i.e. fourty-five patient data sets; a learning rate of 0.01 was chosen since speed optimization was not a concern; a log-sigmoid transfer function was used for node activation.an analysis was performed to address the potential of over-fitting by assessing the classification accuracy and the classification error as a function of the number of network nodes and network layers.since training of a given nn amounted to gradient searches in very high dimensional spaces, some searches would terminate in local minima, with commensurately poor classification as reflected in low auc values and significant classification errors.other training runs would avoid local minima and yield good, in rare cases perfect, classification.five hundred training runs were performed for each dosage threshold with randomized initialization of network weights.for each run, training data sets, validation data sets and test data sets were randomly chosen based on the ratios, respectively, of 0.7, 0.15, and 0.15.the optimum number of training runs was determined by extending their number until the standard deviation in the errors of a series of runs was approximately 1/10 the maximum range of errors observed at a fixed phenylephrine dosage.each run was terminated once the validation score did not improve for 6 epochs.the error definition used here is the mean absolute classification error, where the classification error, with a continuous range from −1 to 1, represents the difference between the nn output and the target designation, i.e. whether a given patient’s total phenylephrine dosage is below (target = 0) or above (target = 1) the discrimination threshold.the absolute error range is continuous between 0 and 1, in contrast to the binary target designation, as the nn classification estimate is continuous.in the context of assessing the categorization capability of the nn the optimum number of nodes and layers was determined, based on categorization error and mean auc.figure 6 presents three-dimensional graphs of the error (a) and auc (b) evolution as a function of the number of nodes of a single hidden layer nn as well as the phenylephrine dosage.the network node axis is logarithmic to better reveal the dependence of the classification error and classification accuracy (auc) for single-digit network nodes.the surface plots clarify that the classification error and the classification accuracy, after initially respectively decreasing/increasing with an increasing number of nodes, level off at approximately 12 nodes.this indicates that further increases in the number of nodes would only increase computational load but not enhance discrimination capability, providing the motivation for limiting the node number to 12.the results of assessing the effect of including more nn layers on categorization performance are presented in figs. 7 & 8, which present, respectively, the difference between the performance of a two-layer and a threelayer 12-node nn and that of the single-layer 12-node nn shown in fig. 6.specifically, figs. 7 & 8 present the subtraction of the categorization error (a) and of the mean auc (b) of the respective 2-layer/3-layer nn from that of the single-layer nn.this is indicated by the larger errors, i.e. for both fig. 7a and fig. 8a the difference in error in the range < 12 nodes is negative, meaning the subtracting higher-level network error is larger than the single-layer network’s corresponding error, and by the positive auc difference ranges displayed in fig. 7b and fig 8b, meaning the subtracting higher-level network auc is smaller than that of the single-layer network, indicating the higher/better discrimination capability of the single-layer network.the general leveling-off response characteristic for the number of nodes > 12 is observed for all phenylephrine dosage levels, however, a distinct minimum/maximum in the classification error/classification accuracy (auc) is observed for 450 mcg, where these quantities level off at, respectively, 0.28 and 0.89 for the single-layer network.for this threshold, auc = 0.89 (p < 0.001) with specificity and sensitivity, respectively, equal to 0.91 and 0.84.the resulting roc is presented in fig. 5, solid black curve.an attempt was made to predict hypotension based on patient baseline information and cuff-based pre-op systolic blood pressure.the results of correlating these parameters with phenylephrine dosage administered are presented in table 2.none achieved statistical significance, precluding any effort to build a predictive model.discussion the significant findings of this study are: (1) it appears to be possible to assess the likelihood of significant post-spinal hypotension through the evaluation of coherencies in as autocorrelation spectra and (2) this significant hemodynamic information can be obtained from non-invasive and readily obtained arterial pressure pulse information.the results presented here add to the previous research in that multiple studies have identified changes in as in comparisons of pregnant versus normal women as well as over the course of pregnancy.a prior study by osman determined that arterial stiffness changes sinusoidally during pregnancy with an overall mean pulse wave velocity (pwv), the gold standard surrogate parameter, of 7.81 m/s significantly lower than the measured 10.0 m/s in non-pregnant women [13].different mechanisms have been proposed to explain the changes in pwv throughout pregnancy.while the initial drop may be due to changes of vaso-active substances such as nitric oxide (no) [23, 24], progesterone, relaxin, the changes may also be related to volume expansion [25], while the inhibition of no, an increase in cardiac output or increased circulatory volume [26] could be responsible for the increase that is observed from midtrimester to term [27].a question in the context of these findings, which were obtained during normal gestational evolution, is whether these changes in as, and probably other hemodynamic parameters, indicate that certain pregnant women are more prone to developing hemodynamic instabilities in response to a significant stressor, such as spinal anesthesia?our results suggest that increased coherence in as may be such a marker.if increased coherence is interpreted as decreased spontaneous variability, the concept of a predictive model of this specific case of hypotension becomes more plausible.variabilities in different physiological parameters, heart rate variability for example, have been shown to decrease prior to hemodynamic crises, such as a hypotensive event, as compensatory mechanisms in the cardiovascular system try to maintain stability [28].comparing the classification performance of a discrete parameterization of a physiological data set such as the autocorrelation spectra used here with that of a generalized pattern recognition approach such as nns is useful because the generalized approach can provide an assessment of the totality of available features that would aid successful classification.this is likely why the nn approach was more successful at classification than the discrete approach, although the small degree of improvement is also indicative, as is discussed below.the nn utilized a set of distinguishing features in its optimization of classification, as opposed to a single measure.as an example, we investigated utilizing the first crossing of the autocorrelation spectrum as a basis of classification.however, as a single-variable classification attempt the approach yielded only an auc = 0.6.other features to add to the discrete model could include, for example, distinct frequency components in autocorrelation spectrum.the nn likely identified a combination of these multiple features and others.however, given that the discrete feature classification approach yielded comparable performance results with those of the nn suggests that the feature that the absolute value integration was designed to quantify, primarily the amplitude of the as modulations, represents a significant portion of the classification potential available in the as autocorrelation spectra.in fact, and this point was examined as part of the validation analysis for the number of nodes and layers used in the nn, the lack of significant improvement in the classification performance of the nn over that of the discrete feature suggests a dearth of additional available features overall.specifically, increasing the number of nodes past the threshold value of 12 did not improve performance, i.e. both classification error and accuracy remained flat and significantly different from what would indicate perfect classification, addressing over-fitting concerns.the same consideration applies to the results of expanding the nn to include multiple layers, which potentially provides access to more refined categorization opportunities in the combinations of inputs.the fact that these network expansions yielded no improvement in categorization capability again suggests that hidden features that could be used to improve performance are not present in the input as autocorrelation spectra.these considerations in turn suggest that the addition of complementary data sources, such as for example heart rate variability spectra, will be required to significantly enhance the classification capability of either the nn or the discrete approach.the results presented here suggest that the nn approach, at least at the implementation level of a clinically relevant prediction algorithm, is somewhat superior to the discrete feature approach, providing implicit access to a plurality of features and, presumably, combinations thereof.in addition, the expansion of the approach to include the submission of other physiological data signals to the network can be readily envisioned and has been implemented in the context of general hypotension prediction by others [7].in the context of gaining understanding of the underlying physiological mechanisms, however, discrete feature identification will continue to be relevant for reasons we present below.the results presented here fit into the larger picture of the evolving and increasingly successful attempt to predict impending hypotension in critical clinical settings from arterial waveforms [6, 7].however, these recent studies have focused on using the discrimination capability of nns primarily because traditionally hemodynamic parameters such as heart rate variability [29, 30], stroke volume variability [31], arterial stiffness [32] or pulsatility indices [33] have been usually obtained as static single measurements that do not lend themselves to continuous, discrete monitoring.
number of words= 1585
[{'rouge-1': {'f': 0.30065966336028627, 'p': 0.790754716981132,'r': 0.18561743341404358}, 'rouge-2': {'f': 0.1967143936042402, 'p': 0.42984848484848487,'r': 0.12754088431253785}, 'rouge-l': {'f': 0.3533896295538098, 'p': 0.6950000000000001,'r': 0.2369316375198728}}]
-----------------------------------------------------------------------------------------------------------------------------------
p63:
Extractive Summary:
background nausea and vomiting after surgery is one of unpleasant, trouble, and the most common side effects with general anesthesia.given the higher baseline risk (females, laparoscopic surgery, use of postoperative opioids, etc.), a single prophylactic measure is often not sufficient to achieve a satisfactory ponv prophylaxis.recent meta-analysis showed that the perioperative lidocaine administration reduced risk of nausea but not vomiting during the first 48 h after operation [15].materials and methods the present study was ratified by ethics committee of the anqing municipal hospital (approval number: aq042) and prospectively registered at www.clini caltr ials.gov (nct03809923, date of registration: 18/01/2019).the inclusion criteria included american society of anesthesiologists (asa) physical status i and ii, 40–60 years of age, not taking the antiemetic drug which has an effect on the incidence of ponv within 24 h before surgery, and scheduled for elective laparoscopic hysterectomy.the exclusion criteria in the current study included obesity with bmi (body mass index) > 30 kg.m−2, preoperative atrioventricular block and bradycardia, history of allergy to local anesthetics, history of preoperative opioids medication and psychiatric, severe respiratory disease, and impaired kidney or liver function.to obtain sufficient oxygenation, 100% oxygen was given to each patient via facemask for 3 to 5 min before induction of anesthesia.patients in the four groups were induced with target-controlled infusion (tci) of propofol and remifentanil.cis-atracurium 0.15 mg/kg was injected intravenously when the patients lost consciousness, and an endotracheal tube (ett) with an internal diameter of 6.5 mm (female) was inserted into the trachea after adequate muscle relaxation.mechanical ventilation was implemented using fabius draeger machine.respiratory parameters were set as follows: tidal volume and respiratory rate were set 6–8 ml/kg and 12–14 beat/min (bpm) to maintain the petco2 between 35 and 45 mmhg during the intraoperative period, respectively.a supplemental dose of cis-atracurium was administered intermittently according to train of four (tof) to maintain muscle relaxation during the anesthesia period.the operations were performed by two high-experienced surgeons under a co2 pneumoperitoneum, and the pressure of pneumoperitoneum was maintained between 10 and 12 mmhg for all patients.outcomes variables our primary outcome was the incidence of nausea, vomiting, and ponv within the first 48 h after surgery.if patients underwent the following conditions such as sustaining nausea (more than 30 min) or vomiting or retching (great than or equal to 2 times), rescue antiemetics (ondansetron 8 mg or droperidol 1 mg) were given intravenously.the secondary outcomes included the incidence of total 24 h ponv after surgery, the occurrence of bradycardia, dry mouth, agitation, and shivering during the pacu stay period, postoperative pain vas scores, postoperative fentanyl consumption, as well as propofol and remifentanil dose during the anesthesia period, which were not registered outcomes in the clinicaltrials.gov.the intensity of pain after operation was estimated with a 10-cm vas in the pacu and the ward (0 for no pain, 10 for the most imaginable pain).bradycardia was defined as heart rate < 50 beats/min or a decrease more than 20% of baseline.a sample size of 180, 203, and 216 was respectively needed, 54 subjects was allocated to each group, and considering rate of dropout, therefore, 62 patients were enrolled in each group for the present study.oneway analysis of variance (anova) was used for continuous data analysis in all four groups.repeated-measures anova compared difference of the pain vas scores and fentanyl consumption in the four groups during the first 24 h after surgery.if group differences were found by anova to be significant, and tukey’s post-hoc test was performed for further analyzed.if heterogeneity of variance was found, dunnett’s t3 test was performed for further analyzed.there was statistical significant when p value < 0.05 apart from the post-hoc pairwise comparisons in which p-values were adjusted by bonferroni correction.results of 285 patients screened for eligibility, 37 subjects were excluded because of preoperative bradycardia and refusing to participate the research.the intraoperative requirement of propofol and remifentanil the intraoperative requirement of propofol was lower in groups d and ld than groups c and l (p < 0.001, p < 0.001, p < 0.001, p < 0.001, respectively).the intraoperative requirement of remifentanil was lower in groups l, d, and ld than group c (p < 0.001, p < 0.001, p < 0.001, respectively).the intraoperative requirement of remifentanil significantly decreased in groups d and ld compared to group l (p < 0.001 and p < 0.001).in addition, the occurrence of total 24 h ponv was significantly decreased in group ld compared with group c (20 (33.3%) and 36 (60.0%), p = 0.003, bonferroni-corrected α = 0.0083 [0.05/6]).there was not significant differences regarding incidence of nausea, vomiting and ponv during the 2–24 and 24–48 h after surgery, and the use of rescue antiemetics in the four groups.the consumption of fentanyl was lower at 6 h after surgery in group l than group c (p = 0.024).the consumption of fentanyl was much less at 6, and 12 h after surgery in group ld than group l (p = 0.003 and p = 0.033) (table 4).however, it did not significantly reduce the incidence of nausea, vomiting, and ponv at 2–24 and 24–48 h after laparoscopic hysterectomy.the lower overall opioid consumption may decrease the incidence of ponv.intraoperative and postoperative use of opioids may induce the incidence of ponv.in the present study, our results were explained by several probably reasons.beloeil h et al. reported that dexmedetomidine-based, opioid-free anesthesia had higher rates of severe bradycardia [28].clinical trials proved that dexmedetomidine administration decreased the rate of agitation [32, 33] and shivering [34].in addition, although the incidence of agitation and shivering decreased in group ld, it was not significant difference between the two groups.therefore, the combination regimen of lidocaine and dexmedetomidine had lower the rate of ponv, it might be due to better control postoperative pain and minimize opioid, which help to improve the quality of recovery and minimize opioid-related adverse events for facilitating enhanced recovery after surgery (era
number of words= 967
[{'rouge-1': {'f': 0.5093909335909712, 'p': 0.7048314606741573,'r': 0.39880698351115423}, 'rouge-2': {'f': 0.278860744321332, 'p': 0.36831144465290805,'r': 0.22436893203883496}, 'rouge-l': {'f': 0.40658609278578156, 'p': 0.6215463917525774,'r': 0.3021041214750543}}]
-----------------------------------------------------------------------------------------------------------------------------------
p64:
Extractive Summary:
the main cause of intensive care unit (icu) admission remains lung failure.baseline was defined as the day of icu admission.outcome analysis started at the time of the baseline lu exam.icu length of stay (los), length of mechanical ventilation, and all-cause icu mortality were the endpoints of the study.lung ultrasound lu was performed by the icu physician on duty supervised by a senior physician with expertise in lu recording and interpretation with the same equipment (venue, ge healthcare).each exam takes between 3 and 5 min (min) with the patient in a supine position.no change in position was needed for the exam.a normal lung will have a total score of 0 points.all patients were divided into two groups depending on the lus at admission: a low (0–12 points) and a high (13– 24 points) lus group.statistical analyses continuous normally distributed data were presented as means ± standard deviation (sd) and compared using the student’s t-test.normal distribution was assessed by the shapiro-wilk test.non-normally distributed variables were compared using the mann-whitney-u test.the values of ph (7.42 ± 0.09 vs 7.35 ± 0.1; p = 0.047) and pao2 (107 [80–130] vs 80 [66–93] mmhg; p = 0.034) were significantly reduced in patients of the high lus group.pleural effusion was rare (n = 4; 9.5%).the mean total lus was 11.9 ± 3.9.eleven of whom worsened over the course (after 2 (1–7) days) with the need for intubation and invasive ventilation, see fig. 2.in this patient group, lus worsened mostly with increasing evidence of b-lines, pleural thickening, and consolidations in the anterior zones, see also table 3.the change in lus from baseline (icu admission) to clinical deterioration (day of intubation) was significant (p = 0.02), see fig. 3. in our cohort icu mortality was 29% (n = 12).baseline characteristics grouped by icu mortality are shown in supplemental table 1.non-sur-vivors had a significantly increased sofa (6.2 ± 3.4 vs 10 ± 3; p = 0.001) and apache ii score (18 ± 7.6 vs 24 ± 7.1), p = 0.022).there was no difference in preexisting comorbidities in both groups.non-sur-vivors had significantly increased values of leucocytes (8.0 [5.5–10.3] vs 11.9 [9.3–15.4] g/l; p = 0.019), creatinine (0.9 [0.6–1.0] vs 2.0 [0.8–3.0] mg/dl; p = 0.047), lactate dehydrogenase (381 ± 116 vs 483 ± 119 u/l; p = 0.016), interleukin-6 (79 [18.6–171] vs 233 [73–280] pg/ml; p = 0.019), high sensitive troponin (0.012 [0.01–0.02] vs 0.053 [73–280] ng/ml, p < 0.001), and brain natriuretic peptide (482 [174–1454] vs 1725 [797–11,652] pg/ml; p = 0.023).furthermore, lymphocytes (8 [5.0–10.8] vs 3 [2.3–7.3] g/l; p = 0.039), and albumin (3.0 ± 0.5 vs 2.6 ± 0.3 mg/dl; p = 0.008) were significantly decreased in non-sur-vivors.values for pao2/fio2 ratio (171 ± 61 vs 118 ± 65, p = 0.017) and ph (7.42 ± 0.08vs 7.34 ± 0.13, p = 0.017) were significantly lower in the non-sur-vivor group.during the covid-19 pandemic, several hospitals used lu to determine the severity of lung failure and to support treatment decisions [8, 12].as expected, ph and pao2 were significantly reduced in the high lus group confirming that lu reflects the severity of lung failure which is in line with the findings of zhao et al. [18].this mortality corresponds with the mortality predicted by sofa (20–40%) [19] and apache ii (25–40%) [20] score.there was no difference in mortality between the two lus groups.none of the covid-19 patients in our study had a normal lus at icu admission.therefore, it is plausible that no patient had a normal lu.main findings of lu at admission were pleural thickening and subpleural consolidations, pleural effusions were rare, homogenous blines over all 8 zones were not seen.interestingly, presence of pleural effusion and subpleural consolidations at baseline ultrasound examination were each significantly increased in the group that died.
number of words= 620
[{'rouge-1': {'f': 0.4752142738464923, 'p': 0.7083647798742139,'r': 0.35753541076487255}, 'rouge-2': {'f': 0.2793431238507285, 'p': 0.3949211356466877,'r': 0.21609929078014184}, 'rouge-l': {'f': 0.43418924990353563, 'p': 0.5781967213114754,'r': 0.34761194029850745}}]
-----------------------------------------------------------------------------------------------------------------------------------
p65:
Extractive Summary:
background implementation of electronic health record (ehr) systems and ubiquitous presence of monitoring devices in acute care environments have led to inpatient care being associated with large streams of continuously updating information.these electronic systems have the advantage of providing comprehensive patient data anywhere at any time, which can be used to improve care by keeping providers appraised of patient’s status.all inpatients require active management to insure safe, effective and efficient care.the critical care and monitored environments uses continuous physiologic and device monitoring with parameter specific incorporated alarms to improve safety.unfortunately, this large volume of continuous data may produce an overwhelming number of alarms or alerts, with the potential to harm due to alarm fatigue [1–3].integrating these multiple streams of rapidly changing data into a system which can prioritize and display a large amount of data in an easily understood manner, may help address this issue.this could be considered analogous to the development of the multifunction flight displays in the modern aircraft that take data previously represented across many dials or indicators and present a single reference screen and includes prioritized alerts to pilots [4].when considered across multiple patients it is necessary to have a readily prioritized and easy to understand way of reviewing large amounts of clinical data to enable identification of patients in most need of immediate attention [4].this may be considered analogous to a “flight control tower”.alertwatch, inc. has developed software which provides live, real-time decision support alerts [5, 6].the displays synthesize multiple laboratory, history and physiologic parameters into an intuitive icon-based display for easy identification of organ system problems and alert when systems/parameters are out of pre-specified, patient specific customizable ranges [5, 6].to try to minimize the problem, awac employs scrolling median values to trigger alerts.the median values of each variable is determined over a 5-min period which is updated every one minute.the scrolling median time period is configurable.this method will remove short term “outlier/artifact” values reducing these measurements producing an alarm, but will delay the alert by a few minutes depending on the scrolling period selected.mobile use awac has been configured and tested for use with mobile devices; tablets and smart phones.an application is available in the apple app store by searching “alertwatch inc.” vpn access is required for out of hospital use.it opens to the census view and when a patient is selected, it is designed to swipe to view all the sections: patient view, patient information and alerts, provider contact and waveforms.the provider can be called directly if their phone numbers are in the ehr.discussion automated decision support systems such as awac, may be utilized as back-up surveillance to support nursing care at the bedside or as part of a remote tele-icu service.it has long been determined that even highly trained and motivated personnel’s ability to detect adverse events deteriorates over a matter of hours.this phenomenon was originally demonstrated in the context of assessment of the vigilance of sonar operators to detect enemy submarines [26].the aviation industry has addressed this issue as the complexity of the aircraft’s monitoring systems increased.information from multiple dials were integrated into a single multifunction display [4].this primary flight display shows a horizon with few numbers but in the background processes multiple streams of information and provides alerts to the pilot in order of importance when concerning or dangerous situations occur.in aviation these systems have been implemented to reduce data overload causing the pilot to lose situational awareness.broad implementation of this technology reduced the commercial crashes from one in a million flights to less than one in 16,000,000 flights despite increasing complexity of underlying systems [27].introduction of systems which summarize multiple sources of clinical information may support provider awareness of changing clinical status and support delivery of high complexity care.multifunction displays have been developed and deployed within routine anesthesia care [5, 7].in a sixyear retrospective study comparing users versus nonusers of awor it was found that use of this system was associated with improved process of care compliance in management of blood pressure, tidal volume and fluid management [7].its use was also associated with a $3,500 decrease in patients encounter charges.additional study has associated the use of alerting display with improved compliance with glucose management guidelines [28].further work has demonstrated utility of an automated monitoring system in labor and delivery (l&d) environments.life threatening post-partum hemorrhage (pph) is a rare event which necessitates immediate intervention to save the mother’s life [29].pph is associated with significant maternal mortality.the american college of obstetrics and gynecology (acog) and others have published guidelines for risk assessment and surveillance of mother; maternal early warning system (mews) [29].in an attempt to improve maternal surveillance another version of alertwatch was developed for l&d; awob [6, 8].in a recent observational study comparing awob to mews with the assumption that compliance with mews was 100%, awob had a better positive predictive value (ppv) for severe postpartum hemorrhage [8].in addition, because awob retrieves vital signs directly from the monitoring network, awob detected nine severe cases of hemorrhage that mews did not detect [8].an overview display mode used in the anesthesia, obstetrics and nursing workrooms allowed the simultaneous monitoring of multiple patients, perhaps in a manner more analogous to a control tower maintaining vigilance of multiple aircraft [30, 31].awob has been well accepted by clinicians in labor & delivery with a majority of providers feeling the system should remain in use and that it improved patient safety [9].this current manuscript describes an acute care focused version of alertwatch which has similarities to prior operating room and labor and delivery specific versions.when considered in an icu deployment, much like or patients, the population is seriously ill and at high risk of further deterioration and therefore, have extensive monitoring with skilled providers – icu nurses.but additionally the patient’s primary nurse has multiple distracting tasks which requires them to leave the bedside.for this reason, the monitors have high/ low alarm triggers meant to alert the nurse to come to the bedside, but have been well documented to produce alarm fatigue [32].furthermore, in icu environments the provider team is required to manage multiple patients dispersed over a geographic unit.the role of remote surveillance and communication systems and services is well established in critical care [33].these systems mostly rely on consultation by request and/ or surveillance by another layer of provider viewing the ehr and monitors [33, 34].which again relies on human vigilance.a preliminary version of awac was developed and implemented in 2017 to monitor floor patients at a hospital by alerting a surveillance consultant at another hospital [10].safavi et al. studied the feasibility and utility of alerting for 6 physiologic and lab values for a 24 bed surgical floor.the nearly 1.6 million vital sign and labs electronically reviewed resulted in 2.6 alerts pre week (0.3 per shift), 88% of which were actionable and 68% resulted a in change in patient management [10].they concluded that electronic remote surveillance can provide actionable alerts without alarm fatigue.the awac system may have some advantages in that it can automatically send notifications to specific providers; rn, rt, md depending the management protocols without depending on human vigilance.as noted above, awac is not limited to the icu.the system can be applied to step-down beds, floor units and even the emergency department [10].the data feeds are the same for every patient in the hospital.if there are no data available for a specific field, that organ is just gray.the impact of awac on icu patient care remains to be determined.improvement needs to be demonstrated in both process of care, patient outcomes and acceptability by providers.implementation of systems like awac may help expand to opportunities for remote surveillance, increased monitoring and advance individualized care plans.para-ehr systems which aggregate information may offer opportunities for deploying sophisticated care algorithms derived from artificial intelligence (ai).douville et al. recently employed ai techniques to predict the need for mechanical ventilation in covid-19 patients [35].they found that the calculated variable of spo2/estimated fio2 (non-intubated patients) was the most predictive and could be continuously determined with spo2 monitoring.conclusion lessons learned from the aviation industry may offer opportunities for improvement in medical care—checklists, crew/team resource management, the multifunction display and the flight tower have led to substantial improvements in aviation safety.this paper describes a system which retrieves, integrates, analyses and displays medical data and alerts providers to possible issues regarding the patient’s condition.studies utilizing awac are required to determine its effectiveness in improving safety and quality of patient ca
number of words= 1396
[{'rouge-1': {'f': 0.34052648788784523, 'p': 0.7687179487179487,'r': 0.21870395634379264}, 'rouge-2': {'f': 0.15605538958625997, 'p': 0.2629260450160772,'r': 0.11095563139931741}, 'rouge-l': {'f': 0.3303963369155132, 'p': 0.6125531914893616,'r': 0.22620214395099542}}]
-----------------------------------------------------------------------------------------------------------------------------------
p66:
Extractive Summary:
postsurgical management of liver transplantation is of great importance in critical care medicine, because large amounts of fluid administration is frequently necessary to maintain intravascular volume and adequate portal vein blood flow to the grafted liver.several studies reported on the benefit of lactate reduction as a clinical parameter for monitoring early graft function following liver transplantation [18–20].so, far, no study has assessed the clinical application of q-crt in liver transplantation, although we previously evaluated qcrt in septic patients.intraoperative porto-caval shunt was never created in this series.all 18 years old or older patients were eligible, and patients who did not agree to participate and/or who demonstrated missing data were excluded.quantitative crt was defined as the time in seconds from the release of the pressure to the time when the blood flow reached 90% of the original flow, which was measured for 5 s at the beginning of the test before applying pressure.abdominal drain tubes were routinely inserted near the surface of the donor graft, behind the graft hilum, and into the rectovesicular (douglas) pouch.in patients with pleural effusion, drainage was performed by the placement of thoracic catheter.the total amount of discharge was recorded, including pleural effusion.the estimated difference of the mean and standard deviation were 0.55 and 0.41, respectively, using logtransformed data.on this basis, the sample size was calculated as being 10 patients with massive ascites and 10 patients with non-massive ascites, assuming a type i error rate of 0.05, a power of 0.8, an anticipated effect size d = difference of means/ standard deviation = 1.34.statistical analysis continuous variables were presented as medians with interquartile ranges, and categorical variables were presented as percentages.categorical data were compared by the chi-square or fisher’s exact test, while continuous data were compared by student’s t test or wilcoxon’s rank-sum test.all continuous parameters with a skewed distribution were entered into these models as log-transformed variables using the natural logarithm to the base e. a multivariable logistic regression model was used to evaluate the independent contribution of q-crt or δab to the outcomes by adjusting predefined preoperative (meld score) and operative (blood loss during surgery) factors.postoperative data four patients developed early allograft dysfunction (ead) [21], and one patient had graft failure 6 months following transplantation.serial measurements of q-crt and δab quantitative crt and δab were measured at pod1.due to missing data, three patients were excluded from the analysis.the absolute changes in q-crt from icu admission to pod1 failed to show any significant association with these outcomes.we also evaluated a possible correlation between q-crt and δab and the change of meld score before and after the transplantation surgery.no significant correlation was found in perioperative changes of the meld score with q-crt and δab (data not shown).maximum lactate values during surgery, icu admission, 12 h after admission, and pod1 are shown in fig. 4a.no significant correlation between the absolute changes in q-crt and δab from icu admission to pod1 with lactate clearance was observed (fig. 4b-c).no significant correlation was observed between the lactate clearance and 14 days total discharge, length of icu stay, and length of hospitalization after surgery (data not shown).the enrolled patients were divided into two groups by the change in q-crt and δab from icu admission to pod1, but no significant difference in the outcomes between the two groups was observed (table 4).discussion this study evaluated tissue perfusion in patients with postoperative liver transplantations by a quantitative crt method using a pulse oximeter.the newly developed parameter of δab, which is expected to reflect the total oxygen delivery to the peripheral tissues, was also significantly associated with these outcomes.these observations suggest that the newly developed q-crt method might be helpful to detect tissue perfusion abnormalities influencing organ function of the grafted liver in postoperative period.this suggestion may be supported by the observation that qcrt and δab were significantly correlated with portal and hepatic vein blood flow rate measured using ultrasound.q-crt technique is expected to be accomplished more easily than blood flow measurement using ultrasound.it should be addressed that a single measurement of peripheral tissue perfusion was associated with the outcomes; however, many related mechanisms are presumed to be involved in these significant associations.crt is a simple and non-invasive test used to assess peripheral perfusion at the bedside.although crt can be used without any equipment, intra-examiner differences and poor reproducibility, even by the same observers, were reported [9, 10, 25].crt can be affected by the surrounding environment such as temperature and lightning [10, 26], unifying measurement condition is an important point.to overcome this problem, several investigations were conducted.kawaguchi and colleagues developed a device that can be adjusted for pressing strength and time using an electric actuator and strength and color sensors [27, 28].shinozaki and colleagues used fingernail video recording and image analysis software for calculating crt [29].we developed a new device that can measure crt quantitatively by using the pulse oximeter.another study also evaluated quantitative crt using a pulse oximeter in ed patients [31].our device demonstrates the advantage of measuring not only q-crt but δab, an index of integrated peripheral oxygen delivery status, because pulse oximetry enables the measurement of transmitted light quantities under red and infrared light [12].in this study, both qcrt and δab showed significant associations with post liver transplantation outcomes.liver transplantation is expected to reduce portal hypertension and associated ascites transudation.in this study, all the patients survived for the observation period of 1 year.several possible mechanisms may explain this finding.it is known that a reduction in systemic vascular resistance due to primary arterial vasodilatation in the splanchnic circulation is observed in patients with liver cirrhosis [41, 42].second, q-crt and δab may indicate the progress of normalization of portal blood flow, and detect the reduced perfusion caused by endotoxin.it is recognized that portal vein blood flow is normalized as the function of the graft liver is restored [45].a high portal pressure is strongly associated with poor liver transplantation outcomes [46, 47].it is known that a reliable analysis of respiratory changes in arterial pressure is possible in patients who are sedated and mechanically ventilated with conventional tidal volumes [57, 58].conclusion this prospective observational study found that q-crt with a pulse oximeter was significantly associated with postoperative outcomes in patients receiving liver transplantations.
number of words= 1026
[{'rouge-1': {'f': 0.3786720280916277, 'p': 0.8209881422924901,'r': 0.24608897126969417}, 'rouge-2': {'f': 0.1880858653684741, 'p': 0.3319047619047619,'r': 0.1312244897959184}, 'rouge-l': {'f': 0.3865249142261699, 'p': 0.6991390728476821,'r': 0.26709543568464733}}]
-----------------------------------------------------------------------------------------------------------------------------------
p67:
Extractive Summary:
anesthesia leads to impairments in central and peripheral thermoregulatory responses.this is exacerbated by cool ambient operating room temperatures and exposed body cavities, resulting in inadvertent perioperative hypothermia in unwarmed surgical patients [3].in tandem with increasing recognition, an array of options have become available for perioperative patient temperature monitoring and warming.postoperatively, temperature monitoring is considered standard of care, and active warming is indicated when patients are hypothermic [2, 7, 18].methods we conducted a cross-sectional survey on anesthesiologists and anesthesia trainees in six countries in the asia–pacific, namely singapore, malaysia, philippines, thailand, india and south korea.written informed consent was waived, and return of anonymous completed questionnaires implied consent to participate.it was then progressively rolled out over an approximately one-and-a-half-year period in the six study countries.this weblink was disseminated to local anesthesiology societies, conferences and hospitals in the surveyed countries.all physicians practising or undergoing training in anesthesiology were invited to participate in the survey.this was especially important as at least half of the countries surveyed had a disproportionately large proportion of small hospitals [22], which may be challenging to obtain direct audit data from.participant information from this lucky draw was entirely separate from the study questionnaire, could not be linked back to survey responses in any way, and was not used in the study.questionnaire development creation and hosting of the online questionnaire were performed with the web-based survey tool survey- monkey [23].predominantly closed-ended questions were used, which were a combination of dichotomous, checkbox, multiple select and likert-scale questions, although options for open-ended responses were provided.phrases such as “majority of patients” were used when it was recognised that the variable of interest may not be clinically appropriate in all circumstances and patients.attempts were made to use forced-answer questions where possible, within the limitations of the survey tool, to improve data integrity.questions were based on currently published literature as well as the authors’ own experiences, and was jointly constructed and reviewed by authors across the surveyed six asia– pacific countries.the primary outcome was to determine the proportions of participants who monitor temperature perioperatively, and actively warm their patients in the preoperative, intraoperative, and postoperative phases.the secondary outcome was to determine the factors that affect compliance to perioperative temperature.finally, participants were asked regarding the availability of patient warming options and temperature measuring equipment in their hospital, as well as any hospital-specific protocols or training courses.univariate analyses were performed to identify correlations between demographics and primary variables, and conducted with logistic regression for categorical and ordinal variables, linear regression for continuous variables, and kruskal–wallis test for ranked ordinal data.a proportion of questionnaires were largely empty or more than 50% incomplete (7.6%), likely from premature closure of the webpage, and were excluded from the study via case deletion to ensure data integrity.these hospitals range widely in terms of number of beds, number of operating theaters, and number of patients anaesthetized annually.on univariate analysis, the availability of active warming devices in the operating room (p < 0.001, or 10.040), absence of financial restriction (p < 0.001, or 2.817), presence of hospital training courses (p = 0.011, or 1.428), and presence of a hospital sop (p < 0.001, or 1.926) were significantly associated with compliance to intraoperative active warming (table 3).another area which respondents were keen for was more education for staff (73.2%), as well as an implementation of an official hospital standard operating procedure (sop) (65.2%) (table 4).in the exploratory analysis, it was found that countries differed significantly in terms of the number of operating theaters at the respondent’s practice location (p < 0.001).having active warming equipment readily available in the operating room was associated with ten times the odds of performing intraoperative active warming.it must be emphasized that compliance to guidelines leads to a reduction in perioperative hypothermia and associated adverse events, which can result in net cost savings from fewer complications and a shorter hospital stay [8, 9].in the face of significant resource constraints, it can be exceedingly difficult for full compliance to best practices.systematic changes to hospital sops have been shown to improve compliance to guidelines and translate into improved clinical outcomes [35–38].ideally, various stakeholders in hospital management as well as local experts need to be involved for the conceptualization of the most optimal local strategy, and this can be disseminated into individual hospital training courses or sops.as others have found before [43], it appears that the smaller hospitals face more constraints implementing best practices.as these hospitals have the greatest potential for improvement, they should not be neglected in national guidelines and policy-making.however, as this study was based on self-reported data, there are inherent reporting and recall biases.the study had a relatively limited response rate of 14.9%, which is similar to other published surveys of physicians using a weblink-only survey methodology [44].such biases would be expected to artificially inflate compliance rates, although this was not observed in the study results.nonetheless, the results of this study should be verified by local audits where possible, ideally in tandem with changes to institutional policies, followed by efforts to close the audit loop.conclusions in conclusion, this survey found that compliance to perioperative temperature management guidelines is generally poor, especially among smaller hospitals.
number of words= 852
[{'rouge-1': {'f': 0.4333355497767011, 'p': 0.739811320754717,'r': 0.30640399556048836}, 'rouge-2': {'f': 0.2748210671800188, 'p': 0.4390851735015773,'r': 0.2}, 'rouge-l': {'f': 0.40479749301080714, 'p': 0.608860103626943,'r': 0.30318385650224217}}]
-----------------------------------------------------------------------------------------------------------------------------------
p68:
Extractive Summary:
many cases of post-thyroidectomy bleeding occur due to violent cough that develops particularly while waking up from anesthesia and during extubation [2].dexmedetomidine helps decrease emergence agitation and helps keep a patient in a calm state after surgery [5, 6].moreover, a small dose of dexmedetomidine is effective in suppressing cough during emergence from anesthesia without respiratory depression [4].patients with the following conditions were excluded: risk of a difficult airway, history of respiratory disease, chronic cough, cardiovascular disease, or pregnant or breast-feeding woman.patients were advised to fast overnight and were administered intramuscular midazolam (0.05 mg/kg) before being transferred to the operating room (or).the induction of anesthesia was performed by a skilled anesthesiologist who was blinded to the allocation of the patient.for the induction, 2.0 mg/kg propofol was administered and a targeted effect-site concentration (ce) of remifentanil was adjusted as 2.0 ng/ml using a targetcontrolled infusion device (orchestra® base primea; fresenius-vial, france) based on a minto pharmacokinetic model.to maintain anesthesia, desflurane with a 50% o2-air mixture was used, and the end-tidal concentration of desflurane and the ce of remifentanil were adjusted according to the bis score (between 40 and 60) and vital signs (within 20 % of baseline values).when the surgeon performed the subcutaneous suture, which was approximately 15 min before the end of the surgery, the infusion of remifentanil was discontinued, and a code-labeled syringe was prepared and infused at a rate of 3 ml/kg/h until the patient was fully awake and transferred to the post-anesthetic care unit (pacu).when the surgeon ended the suture, desflurane was discontinued approximately 5 min before the end of the surgery and the patient was ventilated with 100 % o2 (5 l/min).to control postoperative pain, fentanyl was administered with a patient-controlled analgesia instrument according to the hospital protocol (basal infusion, 0.625 μg/kg/h without a loading dose; intermittent bolus, 1.0 μg/kg/h; lockout time, 15 min).when the patient regained spontaneous ventilation and consciousness (bis score > 90), careful extubation was performed while avoiding irritation, and the patient was transferred to the pacu.the primary objective of the study was to measure the amount of postoperative bleeding for three consecutive days.secondary outcomes such as vital signs, extubation time, recovery time, cough reflex, ramsay sedation scale (rss), 11-point numeric rating scale (nrs, 0 = no pain and 10 = worst pain imaginable) for pain measurement, etc., were assessed by independent anesthesiologists and surgeons.the patient characteristics, duration of surgery, duration of infusion of study drugs, and amount of fluid administered during the surgery were recorded.vital signs such as mean blood pressure (mbp) and heart rate (hr) were measured according to the time interval as follows: t0, before the administration of the study drugs; t1, 5 min after the administration of the study drugs; t2, 10 min after the administration of the study drugs; t3, 15 min after the administration of the study drugs; t4, just before extubation; t5, 5 min after extubation; t6, after arrival at the pacu.during recovery from anesthesia (time interval from discontinuing desflurane to transfer to the pacu), the cough reflex was measured visually and graded according to the severity (grade 0, no cough; grade 1, single cough with mild severity; grade 2, cough persistence less than 5 s with moderate severity; grade 3, severe, persistent cough for more than 5 min) [7].extubation time (time interval from the discontinuation of desflurane to extubation) and recovery time (time interval from the discontinuation of desflurane to transfer to the ward) were assessed.approximately 5 min after arriving at the pacu, the rss of the patient was measured as follows: 1, patient anxious and agitated or restless or both; 2, patient cooperative, oriented, and tranquil; 3, the patient responds to commands only; 4, asleep or a brisk response to a light glabellar tap or loud auditory stimulus; 5, sluggish response to a light glabellar tap or loud auditory stimulus; 6, no response to a light glabellar tap or loud auditory stimulus [8].the effect size was calculated based on a previous study in which the incidence of cough was 55 % after a single use of dexmedetomidine infusion [4].the odds ratio, relative risk, and risk differences with 95 % confidence intervals (95 % ci) were calculated as a measure to compare the risk of severe cough and agitated state in the pacu according to the rss associated with the use of dexmedetomidine.differences were considered statistically significant when the p value was less than 0.05. results a total of 140 female patients scheduled for elective thyroidectomy were assessed for eligibility.among the 140 patients, none did not meet the inclusion criteria or refused to participate.a total of 140 patients were enrolled, but one patient in group d was excluded because of re-operation according to the biopsy results.there were no significant differences in patient characteristics, duration of surgery, amount of intraoperative fluid, and infusion duration of study drugs between the two groups (table 1).according to the rss classification, the patients in group d maintained a calmer state (36.0 % in group d vs. 29.5 % in group s, p = 0.01) in the pacu.in particular, the patients in group d showed a lower incidence of the agitated state compared to the control in the pacu (7.9 % in group d vs. 20.1 % in group s).discussion in this study, dexmedetomidine infusion during emergence from anesthesia significantly decreased the incidence of severe cough, emergence agitation in the pacu, and the amount of bleeding that was measured by the drainage system.as the thyroid gland is an organ with high blood flow, severe bleeding after thyroidectomy is related to a major life-threatening complication that requires intensive care, although the incidence of significant bleeding after thyroidectomy is as low as 2.0 % [1, 2].moreover, thyroidectomy is associated with postoperative cough, especially in women [11].however, we omitted a loading dose as in previous studies of dexmedetomidine because of the possibility of sudden hemodynamic changes [4, 6].in particular, the incidence of severe cough as grade 3 in group d decreased significantly compared to that in group s (4.3 % in group d and 11.5 % in group s).however, further research on gender-specific dexmedetomidine sensitivity is required.it is well known that the sedative effect of dexmedetomidine is associated with a decreased incidence of emergence agitation.in the present study, we measured the rss score to compare the emergence profiles, which revealed that dexmedetomidine resulted in calm awakening in the pacu (36.0 % in group d vs. 29.5 % in group s, p = 0.01).despite the sedative effects, there were no significant differences in extubation time (p = 0.728) and recovery time (p = 0.604) and there was no event of desaturation after the administration of dexmedetomidine.however, there is disagreement among studies regarding the effect of dexmedetomidine on perioperative bleeding.nevertheless, in the current study, we administered a small dose of dexmedetomidine at the end of surgery when vascular ligation and bleeding control ended.we assessed the amount of postoperative bleeding for three days and revealed a significant decrease in bleeding during emergence and while staying in the pacu (19.0 ml vs. 33.1 ml, p = 0.001), and the decrease of bleeding was confirmed until the second pod.considering that hematoma usually occurs within 24 h after surgery [10], the difference in the amount of bleeding was statistically significant, although the absolute difference was relatively small (48.1 ml vs. 73.1 ml during the first 24 h).however, we only measured the intensity of postoperative pain on the first day in the pacu, which is considered a limitation of our study.the infusion of a loading dose of dexmedetomidine can significantly increase blood pressure and decrease the heart rate [26].however, the infusion rate (0.6 μg/kg/h vs. 0.5 μg/kg/h) and duration of infusion (median 34 min vs. 10 min) of dexmedetomidine were higher and longer than those mentioned in the previous study.this difference in methods may have resulted in a decrease in heart rate without a difference in blood pressure.adjuvant dexmedetomidine is effective in preventing ponv [28].these effects of dexmedetomidine can be helpful in reducing postoperative bleeding after thyroidectomy.
number of words= 1325
[{'rouge-1': {'f': 0.3357432730451411, 'p': 0.880126582278481,'r': 0.2074373657838225}, 'rouge-2': {'f': 0.23166181216135723, 'p': 0.531864406779661,'r': 0.14808022922636105}, 'rouge-l': {'f': 0.3629170837337374, 'p': 0.7391729323308271,'r': 0.24049808429118774}}]
-----------------------------------------------------------------------------------------------------------------------------------
p69:
Extractive Summary:
patients who experience postoperative delirium have worse outcomes including prolonged hospitalization, increased costs, lower odds of home discharge, more readmissions, delayed functional recovery, and increased perioperative and long-term mortality [4, 5].postoperative delirium is probably facilitated by multiple factors which may include severe pain [7], opioid medication [8], sleep disturbances [9], and the stress response and inflammation consequent to surgical tissue injury [10].we included patients aged 65–90 years who were scheduled for elective hip or knee arthroplasties, hip fracture repair, or spinal surgery and who agreed to use patient-controlled intravenous analgesia postoperatively.neuraxial anesthesia included epidural and combined spinal-epidural anesthesia.peripheral nerve blocks included lumbar plexus, sciatic nerve, femoral nerve, and iliac fascial space.all were performed with ultrasound guidance.per routine, epidural catheters, if used, were withdrawn at end of surgery because patients were given prophylactic antithrombotic therapy after surgery.regional analgesia was not used in spine surgery patients.general anesthesia was induced with midazolam (1–3 mg), propofol or etomidate and sufentanil or remifentanil, and maintained with propofol infusion, sevoflurane and/or nitrous oxide inhalation, and sufentanil or remifentanil.sequential randomization numbers were assigned to vials by a pharmacist who was otherwise not involved in the trial.allocation was concealed in sequentially numbered sealed opaque envelopes until the end of the trial.unmasking was allowed only if clearly needed for clinical purposes.postoperative analgesia was primarily provided by patient-controlled intravenous administration of the trial drug (either dexmedetomidine 200 μg or 0.9% saline) and 200 μg sufentanil, diluted with 0.9% saline to 160 ml.patient-controlled analgesia was continued for at least 24 h, but not longer than 72 h after surgery.other analgesics including nonsteroidal anti-inflammatory drugs, acetaminophen, and opioids were administered when the numeric rating scale (nrs, an 11-point scale where 0 indicates no pain and 10 the worst pain) of pain remained > 3 despite selfcontrolled analgesia.patients were transferred to the intensive care unit (icu) when clinically indicated; otherwise, they remained in the post-anesthesia care unit for at least 30 min, and were then sent to a surgical ward.electrocardiogram, invasive or non-invasive blood pressure, and pulse oxygen saturation were monitored continually in critical care and recovery units.non-invasive blood pressure and heart rate were then monitored once or twice daily until hospital discharge.those with unstable hemodynamic were monitored frequently and transferred to an intensive care unit if necessary.non-pharmacological strategies to reduce delirium, including restoring hearing and vision aids, reorientation, cognitive stimulation, early mobilization, sleeppromotion and timely correction of dehydration were all used per clinical routine [17].patients with delirium were initially managed with non-pharmacological measures and treatment of primary diseases.severe agitation (rass score of + 3 or more) was treated with haloperidol and/or dexmedetomidine [18].measurements baseline data included demographic characteristics, surgical diagnosis, pre-operative comorbidities, surgical history, smoking and alcohol consumption, and preoperative medications and laboratory test results.the charlson comorbidity index was calculated [19].during the pre-operative interview, cognitive function was evaluated with the mini-mental state examination score (mmse; scores range from 0 to 30, with higher scores indicating better cognitive function) [20].postoperative data included intensive care unit admission after surgery, study drug and sufentanil consumption during patient-controlled analgesia, supplemental analgesics and hypnotics within 5 days, and other medications.in patients with positive cam assessments, delirium was classified into three motoric subtypes: (1) hyperactive (rass score was consistently positive, + 1 to + 4); (2) hypoactive (rass score was consistently neutral or negative, –3 to 0); and, (3) mixed [24].for each domain, the score ranges from 0 to 100, with higher score indicating better function; minimal important difference 0.5 sd [27].we assumed that delirium would be reduced by 50% in the dexmedetomidine group.with significance set at 0.05 and power set at 80%, the sample size was 676 patients.anticipating about 5% loss-to-follow- up, we planned to enroll 712 patients.for patients who were discharged or died within 5 days, the last delirium assessment results were used to replace the missing data when calculating incidence within 5 days; missing data were not replaced when calculating daily prevalence of delirium.patients who died within 30 days were censored at the time of death; and those who stayed in hospital for longer than 30 days were censored at 30 days after surgery.differences were calculated as dexmedetomidine group vs. or minus placebo group.for all hypotheses, two-tailed p values < 0.05 were considered statistically significant.from that time to december 6, 2019, 2,817 patients were screened for eligibility, among whom 712 patients were enrolled and randomly assigned to receive either dexmedetomidine (n = 356) or placebo (n = 356).surgeries were cancelled in 2 patients, protocol deviation occurred in 12 patients.specifically, study drug administration was modified in 9 patients, age was < 65 years in 2 patients, the surgical procedure changed in 1 patient.a total of 710 patients who were randomized and underwent surgeries were included in the intention-to-treat analysis.hypoactive delirium was most common in both groups (table 3).no sub-group interactions were statistically significant.pain severity during movement was also lower in the dexmedetomidine group across the first 5 postoperative days, with a median difference -1 points, p < 0·001; the differences were clinically significant at all 10 time-points.subjective sleep quality was better in patients given dexmedetomidine than placebo during the initial 3 postoperative days (day 1: median difference -1, 95% ci -1 to 0 points, p = 0.007; day 2: median difference 0, 95% ci -1 to 0 points, p = 0.010; day 3: median difference 0, 95% ci -1 to 0 points, p = 0.003); among these, the improvement on day 1 was clinically significant.at 30 days after surgery, physical (mean difference 3.8, 95% ci 1.6 to 5.9, p = 0.001) and psychological (mean difference 2.8, 95% ci 1.0 to 4.7, p = 0.002) components of the whoqol-bref were both better in dexmedetomidine than placebo patients, but the differences were too small to be clinically important.no severe adverse events occurred during the study period (table 4).discussion results of this blinded randomized trial showed that, in elderly patients following major orthopedic surgery, dexmedetomidine supplemented intravenous analgesia did not reduce delirium within 5 days; however, it improved analgesia and subjective sleep quality without increasing adverse events.postoperative pain, opioids, and sleep disruption each potentially contribute to delirium.there are several factors potentially contributing to the relatively low delirium incidence in our patients.the largest previous trial of dexmedetomidine supplemented analgesia for prevention of delirium was by sun and colleagues who randomized 557 non-cardiac surgical patients to analgesia with opioids alone, or opioids combined with dexmedetomidine (0.1 μg·kg−1·h−1) for the initial 48 postoperative hours [34].both analgesia and sleep quality improved, but the relative risk reduction for delirium was only 15% which was not statistically significant.power was reduced in patients sent to surgical wards because they were presumably healthier and had less delirium than those who stayed in an intensive care unit (13.0 versus 22.4% in control patients) [30, 34–37].however, prudence is necessary when administering dexmedetomidine in postoperative patients, especially those in the general ward.most multimodal regimens do not include dexmedetomidine.in our patients, dexmedetomidine reduced nrs pain scores at rest and with movement; the changes reached or surpassed the minimal clinically important difference [40].as might therefore be expected, we found that dexmedetomidine improved subjective sleep quality during the first 3 postoperative days.dexmedetomidine might have improved sleep by activating endogenous sleep pathways [42].in our results, dexmedetomidine did not cause excessive sedation or hemodynamic fluctuations, suggesting that the drug in current dose is a safe sedative and analgesic adjuvant.our trial was under-powered mostly because the delirium incidence was lower than expected and because the apparent treatment effect was 35% rather than the anticipated 50%.in summary, supplementing sufentanil intravenous analgesia with low-dose dexmedetomidine did not significantly reduce delirium, but improved analgesia and sleep quality without provoking adverse events.
number of words= 1262
[{'rouge-1': {'f': 0.34859315755358855, 'p': 0.806462093862816,'r': 0.22235250186706498}, 'rouge-2': {'f': 0.2012648297403345, 'p': 0.3888405797101449,'r': 0.1357698056801196}, 'rouge-l': {'f': 0.339094876080572, 'p': 0.6724844720496894,'r': 0.22670436187399032}}]
-----------------------------------------------------------------------------------------------------------------------------------
p70:
Extractive Summary:
background metformin (1,1-dimethylbiguanide hydrochloride) belongs to the biguanide class of drugs and is a widely used drug administered orally to treat type 2 diabetes mellitus [1].epidemiological studies have demonstrated that metformin use is associated with decreased cancer incidence and mortality in patients with diabetes [2, 3].however, the underlying molecular mechanism by which metformin reduces tumour incidence and inhibits cancer cell growth in -vitro and in -vivo has not been clearly elucidated.the well-accepted mechanism of metformin action is inhibition of mitochondrial respiratory complex i and activation of amp-activated protein kinase (ampk) in response to energy depletion [7].mtorc1 directly phosphorylates downstream substrates, including ribosomal s6 kinase 1 (s6k1) and eukaryotic initiation factor 4e (eif4e)-binding protein 1 (4e-bp1), to regulate protein synthesis to promote cell proliferation [12].mtorc1 is tightly regulated by multiple upstream pathways.the response of mtorc1 signalling to growth factors is mediated by the small gtpase ras homology enriched in brain (rheb), which is negatively regulated by tuberous sclerosis complex (tsc1/2) proteins [13–15].when the pi3k/akt pathway is activated by growth factors, akt phosphorylates tsc2 and disrupts the tsc1/2 complex [16, 17].energy levels signal to mtorc1 through ampk by two mechanisms [18].firstly, ampk directly phosphorylates the tsc2 on s1387 to activate tsc2 and promote inhibition of mtorc1 inhibition through the rheb axis [19, 20].the second, ampk phosphorylates raptor on serines 722 and 792 to directly inhibit mtorc1 activity [21].however, the molecular mechanisms involved in ampk-independent mtorc1 inhibition by metformin have not been fully elucidated.methods cell culture and reagents h1299 nsclc cells were obtained from atcc (manassas, va, usa) and cultured in rpmi 1640 medium (#lm011–01; welgene, gyeongsangbuk-do, republic of korea) supplemented with 10% foetal bovine serum (gibco; thermo fisher scientific, waltham, ma, usa).metformin, phenformin and thiazolyl blue tetrazolium bromide (mtt) were purchased from sigma-aldrich (merck kgaa, darmstadt, germany).cell viability assay cell viability was assessed by measuring the mitochondrial conversion of mtt.the proportion of converted mtt was calculated by measuring the absorbance at 570 nm.the results are expressed as the percentage reduction in mtt under the assumption that the absorbance of the control cells was 100%.cdna primed with oligo dt was prepared from 2 μg total rna using mmlv reverse transcriptase (in-vitrogen; thermo fisher scientific).the following specific primers were used for pcr: atf4: 5′-agtcgggtttgggggctgaag − 3′ and 5′-tggggaaaggggaagaggttgtaa-3′, 437 bp product; β-actin: 5′-ggattcctatgtgggcgacag- 3′ and 5′-cgctcggtgaggatcttcatg-3′, 438 bp product.the pcr products were visualized on a 2% agarose gel containing ethidium bromide.quantitative realtime pcr was performed using an abi 7500 real-time pcr system (applied biosystems).the fold change in gene expression was determined using the comparative ct (2–δδct) method.sirna transfections in h1299 cells were performed using lipofectamine rnaimax according to the manufacturer’s instructions (in-vitrogen; thermo fisher scientific).western blot analysis proteins from cell lysates were separated using 6–11% sodium dodecyl sulphate-polyacrylamide gels and transferred to nitrocellulose membranes followed by immunoblotting with the specified primary and horseradish peroxidase-conjugated secondary antibodies.the following antibodies were used: s6k (#9202), p-s6k (thr389) (#9205), 4e-bp1 (#9644), acc (#3662), p-acc (ser79) (#3661), ampkα (#2532), and p-ampkα (thr172) (#2535) were obtained from cell signaling technology.the atf4 (#sc-200) antibody was obtained from santa cruz biotechnology.statistical analysis data are expressed as the mean ± standard deviation (sd) of three independent experiments.h1299 cells were treated with metformin at the above mentioned concentrations for 24 h. as shown in fig. 1a, metformin inhibited mtorc1 activity, as shown by the decrease in s6k phosphorylation.phenformin, a metformin analogue also inhibited mtorc1 activity, as assessed by reduced phosphorylation of s6k1 and 4e-bp1.it has been reported that metformin requires ampk to inhibit mtorc1 [26].as expected, metformin and phenformin both induced ampk activation, as evaluated by the activating phosphorylation of thr172 in ampkα and ser79 in the ampk substrate acetyl-coa carboxylase (acc) (fig. 1a).next, we explored the effect of the absence of ampk on metformin-induced mtorc1 inhibition.atf4 sirna almost completely blocked the upregulation of redd1 in the presence of metformin (fig. 3c and d).sestrins are stress-inducible proteins that regulate metabolic homeostasis [29].as shown in fig. 3e and f, sestrin2 protein and mrna levels were upregulated under metformin or phenformin treatment.we first investigated the protein expression of atf4 and its downstream targets redd1 and sestrin2 by treatment with metformin in ampk knockdown cells.to investigate whether redd1 and sestrin2 are involved in cell sensitivity to lapatinib and metformin, we knocked down redd1 and sestrin2 in h1299 cells, followed by lapatinib and metformin treatment.furthermore, we showed that sirna targeted against atf4, redd1, and sestrin2 did not change the ampk activation induced by metformin.
number of words= 736
[{'rouge-1': {'f': 0.3538408151821918, 'p': 0.8247169811320756,'r': 0.22523932729624838}, 'rouge-2': {'f': 0.18385460927785482, 'p': 0.34215189873417723,'r': 0.12569948186528498}, 'rouge-l': {'f': 0.2844177828909524, 'p': 0.6205617977528091,'r': 0.1844859813084112}}]
-----------------------------------------------------------------------------------------------------------------------------------
p71:
Extractive Summary:
although first-line therapy consisting of rituximab, cyclophosphamide, doxorubicin, vincristine, and prednisone (r-chop), cures a substantial proportion of de novo dlbcl patients, 30–40% relapse (~ 25%) or experience primary refractory disease (~ 15%) [3, 4].unfortunately for the patients with primary refractory disease or relapse after first-line treatment, standard salvage treatments combined with autologous stem cell transplantation have limited efficacy and cannot be offered to all patients due to comorbidities or performance status [4, 6].four diagnostic samples were included from rrdlbcl patients (n = 4 matching ddlbcl and rrdlbcl patients) for additional analysis.dna extraction dna and rna were purified as previously described [28] from homogenized biopsies using qiagen’s all prep dna/rna/mirna kit, following the manufacturer’s guidelines.dna from saliva or healthy tissue was purified using: dneasy blood & tissue kit (qiagen, germantown, md, usa) and prepitl2p (dna genotek, ottawa, canada), respectively following the manufacturer’s guidelines.furthermore, cel files were used for differential gene expression with the r-package limma [31] and cibersort [32] analysis, using r version 4.0.3.before the statistical analysis, gene expression data were background corrected and normalized using the rma algorithm implemented in the rpackage affy [33].expression was summarized at the gene level using a brainarray custom cdf for the affymetrix human genome u133 plus 2.0 genechip.sequencing library preparation was performed as previously described [28] using either the accel-ngs 2s hyb dna library kit (swift biosciences, san francisco, ca, usa) or twist library preparation ef kit (twist biosciences, san francisco, ca, usa) [28].for exome capture, either the twist human core exome kit (twist biosciences, san francisco, ca, usa) or clinical research exome v2 (agilent, santa clara, ca, usa) were used and further sequenced by illumina paired-end sequencing producing a minimum of 26 gb and 18 gb of raw sequence data for tumor dna and normal dna samples, respectively.bwa mem v0.7.12 was used to align reads against the gdc grch38.d1.vd1 human reference genome sequence [28].the statistical analysis was conducted using r (version 4.0.3) and graphpad prism (version 7, graphpad software inc., lajolla, ca).for gene expression data analysis, p-values were adjusted according to the benjamini-hochberg procedure.droplet digital pcr (ddpr) the validation of selected variants was performed by ddpcr.emulsion droplets were generated by the qx200 droplet generator (bio-rad), following the transfer of droplets to a 96-well pcr plate.two-step thermocycling protocol (95 °c × 10 min; 40 cycles of [94 °c × 30 s, 60 °c × 60 s (ramp rate set to 2 °c/s)], 98 °c × 10 min) was carried out in c1000 touch thermal cycler with 96 deep well reaction modules (bio-rad).end-point fluorescence within each droplet was measured using qx200 droplet reader (bio-rad).data were processed using the quantasoft analysis pro software program (bio-rad).external validation cohorts for validation of ddlbcl, data from chapuy et al., 2018 (135 ddlbcls, which are a mix of cured and relapsing diagnostic samples) were utilized [40].in rrdlbcls, fifteen patients relapsed within the first two years from diagnosis, and the remaining two patients relapsed after 4.7 and 6 years from diagnosis.mutational profile of immune surveillance genes in ddlbcls and rrdlbcls genetic alterations in at least one of the 36 immune surveillance genes were detected in 22 (73%) ddlbcl and 13 (77%) rrdlbcl patients.antigenpresenting genes were affected in more rrdlbcls than ddlbcls, and most of the patients in both cohorts harbored mutations in genes affecting both antigene presentation and immune suppression and exhaustion simultaneously 40 and 35% of ddblcls and rrdlbcls, respectively.gene mutation frequencies in more than half (60%) of the mutated genes in external rrdlbcl were higher than in external ddlbcl even if the difference was not significant, which is in concordance with observations in our data (fig. 5).findings that differed between our study and the external cohorts were similar numbers of mutated 30) and external ddlbcl (n = 32), demonstrating the important role of cohort size (fig. s4).however, more than half of genes affected in rrdlbcls showed higher gene mutation frequency than in ddlbcls, some near double (hla-a) in our data and external cohorts (figs. 1 and 5).the loss of function mutations in fas gene lead to the suppression of fas/fasl system responsible for activation-induced cell death [45].in contrast, the loss of function mutations in the tnfrsf14 gene leads to bcell autonomous activation as well as extrinsic activation of the lymphoma microenvironment through b and tlymphocyte attenuator (btla attenuator) located on cd4+ t-helper cells [46].specific missense mutations in pim1 are possibly activating.along with pim1 being overexpressed in dlbcl cells compared to normal bcells, tumor cells are prevented from undergoing apoptosis inactivating proteins such as apoptosis signaling kinase 1 (ask1), preventing further activation of fas ligand [47, 48].however, neoantigen presentation is necessary for immune surveillance, and lack of expression of mhc molecules might be the reason for failed anti- pd1 immunotherapies [10, 49].in particular, if the cell is unable to present neoantigens in association with mhc molecules, there is no need for pd1/pd-l1 interaction [10].nevertheless, it is interesting to speculate that clonal selection of genetic variants in antigenpresenting genes occurs during or after the treatment resulting in the development of rrdlbcl (fig. 4) even if we cannot distinguish single cell double genetic events from polyclonal tumor formations.recently, several algorithms have been developed, providing a refined classification of dlbcl into five to seven distinct subtypes based on genetic features [24, 52, 53].as these genetic classes are based on global genetic analysis, and we use only a sub-selected set of genes in our analysis, we did not include refined genetic classification.however, it is observed that 73% of mcd genomes acquired genetic variants in genes affecting immune surveillance, thus becoming invisible to the host immune system, suggesting a crucial role in dlbcl pathogenesis, which is in agreement with cluster 5 described by chapuy et al.,2018 [24, 40].also, an important observation in our ddlbcls and rrdlbcls is that 16 and 29% of the patients, respectively, harbor mutations in antigene presenting genes excluding genes in immune suppression and exhaustion, while 13 and 12% harbor mutations in genes related to immune suppression and exhaustion but not in antigenpresentation, respectively (table s2).similar features are observed in external ddlbcl and rrdlbcl cohorts where 15 and 28% of patients, respectively, are affected by mutated antigen-presenting genes and 25 and 25% affected by genes involved in immune suppression and exhaustion, respectively.thus, a higher mutational rate of antigene presenting genes in rrdlbcl than in ddlbcl can be observed, though findings are not significant in neither our nor external cohorts – perhaps due to small cohort sizes it may suggest their possible role in the development of resistance toward therapy.
number of words= 1077
[{'rouge-1': {'f': 0.3134928081859884, 'p': 0.7079310344827587,'r': 0.2013220940550133}, 'rouge-2': {'f': 0.16350707294999928, 'p': 0.28645021645021645,'r': 0.114404973357016}, 'rouge-l': {'f': 0.2755626104552357, 'p': 0.5315384615384615,'r': 0.18599297012302285}}]
-----------------------------------------------------------------------------------------------------------------------------------
p72:
Extractive Summary:
in contrast, some scholars concluded that not all patients with icc could benefit from a wide margin hepatectomy (wmh) [15].given this, we conducted this multicenter study to investigate the impact of surgical margin width on long-term outcomes in icc patients.patients who underwent palliative resection and patients with positive surgical margin, mortality within 1 month of surgery, peritoneal seeding, distant metastasis and incomplete information were excluded.follow-up patients were regularly followed up every 3–6 months after surgery, during which serum carbohydrate antigen 19–9 (ca19–9) and abdominal ct or mri were routinely performed.os was defined as the interval between the date of surgery and the date of death from any cause or the date of the last follow-up.since patients who underwent wmh and narrow margin hepatectomy (nmh) were not randomly distributed, propensity score matching (psm) was used to minimize selection bias.results patient characteristics figure 1 presented the flowchart of patients’ enrollment.wide margin resection was more frequently performed among patients had a small, single and ca19–9 level raised tumor, and more frequently performed by laparoscopic approach.after 1:1 psm, there were 79 of the 195 wmh patients were matched with 79 of the 283 nmh patients, and all baseline characteristics were compared between the groups.the 1-, 3-, and 5-year os in wmh were also higher significantly than in the nmh (76.10%, vs 66.22, 56.10% vs 39.86, 50.24% vs 37.16%, all p<0.05, respectively).meanwhile, patients underwent wmh had a longer median dfs compared with patients undergoing nmh (16 vs 8months, p<0.001; fig. 2 b).after 1:1 psm, the median, 1-, 3-, and 5-year os of patients in the wmh were still better than that in nmh (40 vs 21 months, 81.51% vs 67.23, 63.87% vs 40.34, 57.14% vs 40.34%, all p < 0.05, respectively; fig. 2 c).univariate and multivariate cox analyses of os and dfs in patients with intrahepatic cholangiocarcinoma before psm, univariate analysis identified surgical margin width was associated with os and dfs (all p < 0.05).additionally, multivariable analysis showed that surgical margin width was an independent prognostic factor affecting os and dfs (table s1).after psm, univariate analysis identified surgical margin width was associated with os and dfs (all p < 0.05).however, multivariable analysis showed that surgical margin width was an independent prognostic factor affecting os but not dfs (table 2).subgroup analysis based on clinicopathologic feature to identify the optimal icc patients for wmh, subgroup analysis was conducted based on clinicopathologic feature.as shown in fig. 3, the following factors may benefit dfs of the patients who underwent wmh: ca199 ≤ 200 u/ml, cea ≤ 5μg/l, no lymph node metastasis, mf type, mild tumor differentiation, no mvi, and no perineural invasion (fig. 3).subgroup analysis based on ajcc staging system to comprehensively understand the relationship between clinicopathological features and surgical margin, subgroup analysis was further conducted based on the 8th ajcc staging system.therefore, it is necessary to further explore and discuss the prognostic value of wmh in icc patients with different characteristics and stages.a meta-analysis showed a consistent result that wmh could benefit long-term survival in patients with icc [21].in patients with stage ii of ajcc, multiple tumors usually reflect intrahepatic metastasis, and a study of european network for the study of cholangiocarcinoma (ens-cca) has shown that the prognosis of these patients is as poor as that of patients with extrahepatic metastasis [24].besides, icc with vascular invasion is also classified as stage ii, which is also reported to be the independent risk factor of prognosis for patients with icc [18, 25].similarly, we found patients with lymph node metastasis had no benefit from wmh.this reflects that lymph node metastasis was a factor that played a fatal role for the outcome of patients with icc and wmh is not enough to improve the prognosis of these patients.in this study, we found wmh had a longer os and dfs than nmh in patients with ca199 ≤ 200 u/ml, cea ≤ 5μg/l, mf type, mild tumor differentiation, no mvi, and no perineural invasion.besides, approaches such as extended resection and vascular reconstruction were considered to improve outcome further [22, 27].of note, aggressive approaches used to achieve a wmh may lead to an increase in adverse events, such as liver failure and massive bleeding [15].in this study, a higher rate of intraoperative blood loss, transfusion, and postoperative complication were observed in wmh group.as for patients with stage ii or iii, wmh alone is not sufficient to improve the survival, and adjuvant therapy and other effective treatments may still needed.there are several limitations that should be acknowledged when interpreting this study.first, this was a retrospective study and selection bias may have been present.as for icc patients with ajcc stage ii or iii, wmh alone could not improve the survival and more effective treatments are still need
number of words= 780
[{'rouge-1': {'f': 0.34739985392943323, 'p': 0.7170588235294117,'r': 0.2292279855247286}, 'rouge-2': {'f': 0.20705151281907686, 'p': 0.37049261083743845,'r': 0.14367149758454106}, 'rouge-l': {'f': 0.37043436709712363, 'p': 0.6211811023622047,'r': 0.2639058171745152}}]
-----------------------------------------------------------------------------------------------------------------------------------
p73:
Extractive Summary:
to date, over 20 mmp members have been identified in humans, which are divided into different subtypes according to their substrate specificity such as collagenase: collagenase-1 (mmp- 1), collagenase-2 (mmp-8), collagenase-3 (mmp-13) and collagenase-4 (mmp-18); gelatinase: gelatinase a (mmp-2) and gelatinase b (mmp-9); and stromelysin: stromelysin-1 (mmp-3) and stromelysin-2 (mmp-10) [1].dysregulation of mmps has been found to be involved in diverse pathological conditions including arthritis, fibrosis and neoplasia [9–23].cutaneous basal cell carcinoma (cbcc) and cutaneous squamous cell carcinoma (cscc) account for approximately 80 and 20% of nonmelanoma skin cancer (nmsc), respectively [24, 25].while cbcc is a locally destructive cancer that rarely results in metastasis or death [27], cscc is the main contributor of nmsc deaths.however, serum mmp-13 as a diagnostic marker for cscc has not been explored.diagnosis of cscc was confirmed by pathological analysis of excised tumor tissues.staging of cscc was done according to the eighth edition of american joint committee on cancer (ajcc) cancer staging system: t1, tumor diameter < 2 cm; t2, tumor diameter ≥ 2 cm and < 4 cm, t3, tumor diameter ≥ 4 cm, or minor bone erosion, or perineural invasion, or deep invasion; t4, tumor with gross cortical bone/bone marrow invasion [35].histology typing of invasive cscc and cscc in situ, and further subtyping of invasive cscc into well-differentiated, moderately-differentiated and poorly-differentiated were done at our pathology department.patients with other skin disorders, connective tissue disease, renal disease, other tumors, hepatic disease, severe cardiovascular or pulmonary disease were excluded.briefly, 100 μl of each standard and sample were added into appropriate wells of the 96- well elisa plate and incubated for 2.5 h at room temperature.categorical data were analyzed by chi-square test.results a total of 77 patients (49 males and 28 females) and 50 healthy individuals (33 males and 17 females) were included in this study.the ratio of male to female, and the age in the two groups were not significantly different (table 1).histologically, there were 17 cases of cscc in situ and 60 cases of invasive cscc, and the latter had remarkably higher serum mmp-13 levels than the former (fig. 2 b).patients with nonmetastatic cscc had significantly higher serum mmp- 13 levels than healthy controls (p < 0.001).post-surgery measurement of serum mmp-13 was performed in 12 patients, which showed a marked decrease of serum mmp-13 concentrations after the removal of cscc (523.0 ± 231.4 pg/ml prior-surgery versus 296.4 ± 92.6 pg/ml post-surgery, p < 0.001).mmp-13 expression in excised samples from all 77 patients and 30 healthy subjects were analysed by ihc.ihc microphotographs are shown in fig. 3. was examined.there were no substantial differences in serum mmp-13 levels between these two groups (table 2).when serum mmp-13 levels were compared between male and female patients, no significant differences were observed (table 3).moreover, the age and the percent of patients in each stage were not significantly different between male and female patients (table 3).the roc curve analysis revealed that serum mmp-13 predicted the presence of invasive cscc with an auc of 0.87 (95% ci [0.78 to 0.95]) for sensitivity and specificity of 81.7 and 82.4%, respectively for a cut-off value of 290 pg/ml (fig. 4 a).the same group also showed that mmp-13 protein was expressed in cscc tissues as assessed by immunohistochemistry [32].these findings suggest that analysis of mmp-13 expression in small biopsy samples may be used to determine the invasive capacity of scc at an earlier stage.jiao et al. showed that patients with esophageal scc had significantly higher serum mmp-13 levels than healthy controls; furthermore, serum mmp- 13 levels were found to be associated with tumor progression and survival [42].riedel et al. discovered the elevation of serum mmp-9 levels in patients with head and neck scc [43], which is also observed by stanciu et al. [44].choudhry et al. revealed that serum levels of mmp-1, − 8, − 10, − 12 and − 13 in oral scc patients were substantially elevated as compared with healthy controls [48].in the present study, we found a significantly higher level of mmp-13 in cscc tissues compared with control tissues.these results suggest tumor-derived mmp-13 contributes to the elevation of serum mmp-13 seen in our patients.conclusions serum mmp-13 levels show high sensitivity and specificity for the differentiation of invasive cscc and cscc in situ, and the prediction of lymph node metastasis, suggesting serum mmp-13 might serve as a valuable biomarker for early detection of cscc invasiveness and monitoring of cscc progressi
number of words= 726
[{'rouge-1': {'f': 0.5451240144781541, 'p': 0.8171264367816091,'r': 0.4089830508474576}, 'rouge-2': {'f': 0.3331961113249589, 'p': 0.4792219020172911,'r': 0.25537859007832897}, 'rouge-l': {'f': 0.46294038839792456, 'p': 0.7031360946745562,'r': 0.3450642673521851}}]
-----------------------------------------------------------------------------------------------------------------------------------
p74:
Extractive Summary:
using the re-aim framework [26], this paper describes the outcomes of a text message-delivered, extended contact intervention for healthy living after cancer (hlac).it was hypothesised that, on average, those who had received hlac+txt would maintain or improve weight, diet and physical activity behaviour outcomes, while those who did not receive hlac+txt would regress towards baseline levels, resulting in a significant between cohort difference in intervention effect at the end of hlac+txt.methods study design a historical control design was used to evaluate the addition of hlac+txt to the hlac intervention.four of the five ccs in australia took up the initial hlac program and 786 eligible cancer sur-vivors participated in hlac (88.7% overall uptake), with outcomes reported elsewhere [28].cc delivery staff recruited participants for the hlac+ txt trial during their final hlac coaching call or the post-hlac assessment, and if participants agreed, recorded verbal or written consent was obtained.the content, frequency and timing of the texts were tailored based on information collected from participants during two scripted, telephone tailoring interviews conducted by cc delivery-staff.all participants were asked to rate overall satisfaction with and usefulness of the texts for meeting goals on a 5-point scale from “not at all” to “extremely” useful/satisfied, and to provide feedback on the program in an open-ended question.non-significant findings can indicate either no change/difference in outcome or an insufficient sample size to show a conclusive finding.therefore, we only described outcomes as “maintained” or cohorts as being “similar” when the finding was both non-significant and the likely true effect size for the change/difference (as seen by the 95% confidence interval) was less than the minimum difference of interest (mdi) [23].results adoption outcomes all four ccs delivering the hlac program were approached and agreed to deliver the hlac+txt intervention (cc victoria, cc south australia, cc new south wales and cc western australia).these decisions were based on the resources available at each cc reach outcomes of the participants approached to join hlac+txt across the four ccs (n = 189), 96% (n = 182) were eligible and 64% of these (n = 115) consented to participate (fig. 2).participants in the hlac+txt trial (n = 282) were mostly female (n = 253/90%) sur-vivors of breast cancer (n = 181, 64%), who were on average 1.9 years (sd ±3.0) since diagnosis and had a mean age of 58.3 (sd ± 10.9) years and at the pre-hlac assessment, had a mean bmi of 27.7 kg/m2 (sd ± 5.7).implementation outcomes staff training all 16 staff (100%) who delivered the tailoring interviews attended the first 30-min telephone-delivered tailoring interview training session and four staff (25%) attended the second 12-week tailoring interview training session (the two hlac telephone coaches with motivational interviewing training and two research assistants who had no motivational interviewing training).intervention delivery the mean number of weeks participants received the text message intervention ranged from 18.5–22.2 weeks across ccs (mean 21.1, sd = 1.7) (additional file 8) with some cc adapting the 24-week intervention based on their perceptions at the 12-week tailoring interview of participants’ needs.participants received a mean of 83 text messages over the length of the intervention ranging from 40 for cc4 to 112 for cc1.the percent of goal checks participants responded to was a median (25th, 75th percentile) of 70% (50.0, 85.0) and a median (25th, 75th percentile) of 30% (10.0, 55.6) required a goal check response to be triggered by the researcher.at the 12-week tailoring interview 84% (n = 97) of participants changed their preference for text message content, frequency and/or timing.no participant changed the timing or frequency of the texts via a text message to the coach, apart from 10 participants notifying via text message that they no longer wished to receive the texts.participant satisfaction with program at the post-hlac+txt trial assessment most participants were ‘satisfied’ or ‘extremely satisfied’ (77%, 90/ 110) with the text message program and found the texts ‘useful’ or ‘extremely useful’ for supporting them to meet their behaviour goals (68%, 75/110).participants perceived that hlac+txt provided: reminders for maintaining their diet and physical activity behaviours established during hlac and provided a continuing connection with the program.a common theme throughout the qualitative interviews with participants were reported personal stressors or barriers to achieving their diet, physical activity and weight goals, such as: social issues (employment, finances, family); ongoing treatment side effects and cancer-related symptoms (disturbed sleep, fatigue, cravings, taste changes, mental health issues and joint pain); and chronic conditions such as arthritis.effectiveness outcomes within-cohort changes both hlac+txt and control cohorts had significantly worsened outcomes for body weight, mvpa, vegetable intake, and fat and fibre index scores between the pre- and post-hlac+txt trial assessments (table 5).maintenance outcomes (participant level) within-cohort change the hlac+txt cohort maintained outcomes for weight, fruit intake, and fat and fibre index scores between post-hlac+txt and followup (after 6-months no contact) (table 5).results during this same time period were inconclusive for waist circumference, mvpa, physical and mental quality of life outcomes for the intervention cohort as the confidence intervals included the mdi.sensitivity analysis the results of the multiple imputation analyses (additional file 11) supported the main analyses results for changes during 6-months no contact for the hlac+txt cohort.maintenance outcomes (setting level) to date, the initial hlac program is being adapted and offered by three cc at a reduced scale or in a web-based format [28].discussion using the re-aim framework, this implementation trial evaluated a text message-delivered, extended contact intervention targeting healthy weight, diet and physical activity, for cancer sur-vivors.importantly, this program was delivered by the major community-based cancer support, non-profit organisation in australia.all four ccs adopted the hlac+txt intervention and the program was feasible to implement, however reach and implementation results varied greatly across ccs.the intervention was not effective when outcomes were compared to a historical control cohort at the completion of the intervention, although data collected at the hlac+txt trial follow-up assessment (6 months after text message completion) suggests that the benefits of the extended contact intervention may have been delayed.three key reasons are suggested for the lack of effectiveness findings at the end of the intervention: 1) the variability between ccs in adoption, reach and implementation, 2) the lack of intervention focus on holistic support for the social and mental health requirements of this cohort of cancer sur-vivors, and 3) the disconnect in accountability and rapport between the initial hlac and the hlac+txt programs.variations in hlac+txt program delivery across the ccs were driven by differences in cc resources leading to variations in the qualifications and experience of the intervention delivery staff [43–45].ensuring delivery staff are skilled in motivational interviewing [46], and techniques to provide participant support for coping with barriers to maintaining diet and physical activity [47] may be a key area for consideration with future implementation.further, a protocol for a shorter intervention delivery period (i.e. 12 weeks) could be explored by comparing the effectiveness of a 12-week and 6-month intervention trial period using a randomized controlled trial.as well as a previous cancer diagnosis, participants had an average of two co-morbidities, and many reported mental health conditions similar to those seen in cross sectional australian data of cancer sur-vivors [49–51].the most frequently reported unmet need of cancer sur-vivors, following treatment completion is support for psychosocial issues [52].incorporating the option for mental health support into the text message frameworks and the option for participants to choose mental health goals targeting stress, depression, anxiety and sleep may improve intervention acceptability for this population [53] and have potential for supporting the mental health of cancer sur-vivors.alternatively, a program that incorporates text messages which are supplemented with additional telephone support for emotional and social wellbeing may address this deficit and may be a more appropriate extended contact modality for some participants [47, 54, 55].furthermore, with recent developments, this support may be feasible through triaging levels of intervention intensity via artificial intelligence [56, 57].the loss of connection, rapport and accountability established between the coach and client during hlac, may have further exacerbated the lack of support participants experienced for social and mental health issues.the text messages were signed-off with a researcher’s name (rather than the hlac coach’s name).a previous review of extended contact interventions suggested that the contact with the interventionist is a key component of the success of these interventions [16].other researchers have hypothesized that established relationships enhance the effectiveness and implementation of lifestyle interventions into practice and reduce attrition [58–60].the existing program is a way to “step down” the intensity of the relationship and wean the participant on to a more cost-effective means of communication.however, continuity of care between the coach and client may still be required for such programs to be effective.after 6 months of no contact following the end of hlac+txt weight, fruit intake and fat and fibre outcomes were maintained by the intervention cohort.this suggests that the text messages may have influenced participants’ ability to maintain changes in diet and exercise behaviour in the longer term.a similar effect has been reported in breast cancer sur-vivors [19] and adults [61] who received a text message-delivered, extended contact intervention following a lifestyle intervention.the text messages may have promoted longer-term maintenance of self-regulation skills such as self-monitoring and encouraged bcts including engaging support, goal setting and techniques for forming habits [62, 63].strengths evaluating this extended contact, text message-delivered intervention in a service delivery setting adds to the broader evidence on dissemination and implementation outcomes, where interventions are delivered in real world, rather than optimal research conditions [64].this research was conducted in partnership with the ccs who took ownership of the program.participant engagement with the intervention was positive and there was low participant attrition.the qualitative feedback added to our understanding of the important components of extended contact interventions for this population, including: the background and experience of the delivery staff; the importance of the continuity in the delivery staff across telephone and text modalities to maintain rapport with participants; and the additional social, health and mental support requirements of this target group [65].limitations hlac+txt was not included in the initial research protocol for hlac [27] and a historical control study design was used due to timeline restrictions.outcome assessments were completed at 18 months from baseline for the hlac+txt intervention cohort, but not the control cohort, due to limited researcher resources.maintaining support for participants based on ongoing relationships with cc coaching staff skilled in motivational interviewing may improve accountability and outcomes.
number of words= 1711
[{'rouge-1': {'f': 0.34937095057329925, 'p': 0.8263025210084034,'r': 0.22151515151515153}, 'rouge-2': {'f': 0.19057143598001777, 'p': 0.3649438202247191,'r': 0.12895564289724876}, 'rouge-l': {'f': 0.3701726673950864, 'p': 0.6774766355140187,'r': 0.2546590909090909}}]
-----------------------------------------------------------------------------------------------------------------------------------
p75:
Extractive Summary:
background glioma is a common tumour originating in brain [1].glioblastoma is the most aggressive subtype and the most common in adult glioma [1].other than glioblastoma, diffuse gliomas include astrocytomas and oligodendrogliomas.and anaplastic astrocytomas and anaplastic oligodendrogliomas show poor prognosis compared in each subtype [1].these subtypes had been classified mainly by histological diagnosis [2].recent intensive genomic and molecular biological analyses of gliomas have identified several significant driver gene mutations in idh, braf, or h3f3 [3, 4].dysregulations in the p53, rb1, and mapk / pi3k pathways have also been suggested to be involved in the molecular pathogenesis of glioblastoma [5, 6].the importance of the molecular information to an understand the biological properties and pathogenesis of glioma is well recognized.the new 2016 world health organization (who) classification for central nervous system tumours has introduced the concept of multi-layered integrated diagnosis using a combination of traditional histopathological classification and information obtained from modern molecular analytical methods; therefore, the necessity for molecular information will increase in the neuro-oncological field [7].ras genes including kras, hras, and nras are wellknown oncogenic genes, and are involved in the erk pathway, a subgroup of the mapk pathway.ligandmediated activation of receptor tyrosine kinases, such as epidermal growth factor receptor (egfr), activate ras proteins and initiate the cascade of the erk signalling pathway.activated ras proteins activate the raf, which can activate mek just upstream of erk [8, 9].through these several pathways, ras genes promote cell proliferation, survival, and growth.mutations in ras genes have been found in various cancer cells and lead to dysregulation of cell proliferation to promote oncogenesis [11, 12].ras proteins are bound to gdp in a stable state, and switch to an activated state when bound to gtp [12, 13].gtpase switches gtp-bound ras back to gdp-bound ras [13].ras mutations have an impaired intrinsic gtpase and are insensitive to gtpase-activating proteins; therefore, inhibiting the conversion of gtp to gdp resulting in dysregulated cell proliferation and oncogenesis [11–13].ras mutations are mainly observed in codons 12, 13 and 61, and often in pancreatic, colorectal, lung and thyroid cancers [14, 15].kras-activating mutations are widely effective as predictors of resistance to anti-egfr monoclonal antibodies in colorectal and lung cancer patients [15–18].anti-kras drugs have been under development [19, 20], and some clinical trials are ongoing [21].ras mutation is now an important biomarker and therapeutic target in these solid cancers.in terms of central nervous system diseases, a recent study showed an important relationship between ras mutations and cerebral arterio-venous malformations as a non-neoplastic pathology [22].although several reports have found a small number of cases bearing ras mutations in various gliomas, the clinicopathological properties of these mutations have not been fully addressed [23–26].this study analysed ras mutations and their clinical significance in gliomas.methods patients and samples inclusion criteria for the present study were the local initial diagnosis of gliomas according to the 2007 who classification of central nervous system tumours, and frozen or fresh tumour tissues available for genetic analysis.the exclusion criteria were insufficient quality of results of genetic analysis, or clinical data, but no case was excluded.a total of 242 cases were enrolled, including 167 tumours operated on from july 2008 to october 2017 in kyoto university hospital, and 75 tumours operated on from may 2006 to march 2017 in osaka national hospital.clinical data collected from each institution included age, sex, tumour location, extent of resection, clinical course including treatment protocol and dates of surgery, recurrence or progression, and death.ki-67 index were analysed in 167 tumours which was operated in kyoto university hospital.sanger sequencing tumour dna was extracted from tumour specimens with nucleospin® tissue (macherey-nagel, düren, germany).regions of interest for driver genes [23, 27–30] were amplified by pcr with gene-specific primers (supplementary table 1) and takara ex taq® (takara bio, shiga, japan) (idh1/2, h3f3a, and hist1h3b) or ampli- taq gold 360 (thermo fisher scientific, waltham, ma) (tertp, kras, hras, and nras) using applied biosystems geneamp pcr system 9700 (thermo fisher scientific).in the presented case series, recurrent tumours of case 1 (ao) and case 4 (ganglioglioma) showed no ras mutations which were shown in their primary tumour.this fact may imply that tumour with ras mutation was disappeared by treatment.through direct comparison of the genomic landscape of gliomas at initial diagnosis and recurrence, a previous study showed that full set of mutations found in the initial tumour do not maintain in the recurrences and suggested that recurrent tumours are originate from cells derived at a very early stage of the evolution of tumours [58].while idh1 and tertp mutations, and 1p/19q codeletion assigned as the truncal events during tumour evolution [3], ras mutations in glioma may be an additional alterations to development.about the primary tumours, sanger sequencing revealed tp53 mutation in one of these aas, and methylation assay showed amplification of pdgfra and loss of cdkn2a/b and tp53 in the other.this fact proposed that ras mutation have a potential to be a driver gene of glioma development, but its effect may be supportive compared with major truncal driver mutations like as idh mutation, tertp mutation and 1p/19q codeletion.because ras mutation could switch at glioma recurrence, the molecular analysis is thought to be essential for recurrent as well as primary tumours when anti-ras treatment are conducted.0.07% of cases in the project genomics evidence neoplasia information exchange (aacr genie) launched by the american association for cancer research [59].kras g12a has been identified in lung, colon, colorectal and rectal adenocarcinoma, and uterine endometrioid carcinoma, while kras q61k has been found in colon, colorectal and pancreatic adenocarcinoma.kras g12a and kras q61k are predictive biomarkers for the use of erlotinib, gefitinib, cetuximab, and panitumumab in patients [16–18, 60, 61].non-small cell lung carcinoma and colorectal carcinoma have the greatest number of therapies targeting kras g12a and kras q61k or related pathways.kras e76d has not been reported in other types of cancer, and further study was needed whether if it has a role of an activating mutation.nras q61r is present in 0.73% of aacr genie cases [59], and has been identified in cutaneous melanoma, melanoma, papillary thyroid cancer, poorly differentiated thyroid gland cancer, and colon adenocarcinoma [59].nras q61r is a predictive biomarker for the uses of cetuximab and panitumumab in patients [60, 61].further, for nras-mutant melanoma, binimetinib reportedly improves progression-free survival compared with dacarbazine [62].lower grade astrocytomas in our cohort contained a large number of idh-wild type tumours.this fact partially results from high frequency of tertp mutation.in our idh-wild type tumours, 8 out of 18 das and 15 out of 32 aas showed tertp mutation.nowadays, idhwild type astrocytomas with tertp mutations are known as a group of astrocytomas with poor prognosis, and these tumours are supposed to be a different group from the group of common lower grade astrocytomas [63].the diagnosis of lower grade astrocytoma without idh mutation needs further discussion.conclusions we found 4 ras mutations in various types of 242 gliomas.all cases involved younger adults.no clear association was identified between ras mutations and clinical or genetic characteristics of tumours.
number of words= 1144
[{'rouge-1': {'f': 0.339164506697296, 'p': 0.8010924369747898,'r': 0.2151209341117598}, 'rouge-2': {'f': 0.18409892217471355, 'p': 0.34848101265822784,'r': 0.12509181969949917}, 'rouge-l': {'f': 0.3215902778335938, 'p': 0.6434265734265734,'r': 0.2143661971830986}}]
-----------------------------------------------------------------------------------------------------------------------------------
p76:
Extractive Summary:
background lung cancer still is the leading malignancy in the global cancer spectrum of morbidity and mortality [1].in addition, accurate tumor-lymph node-metastasis (tnm) staging means that the prognosis of the patients is accurate [10, 11].the prognoses of diseases have an effect on treatment selection and patients’ management.this study majorly included two parts, incidence-rate analysis (step 1) and survival analysis (step 2).we retrospectively recruited patients who were histologically diagnosed with malignant tumor in the lungs as their first primary malignancy from 2004 to 2015 in surveillance, epidemiology, and end results (seer) database, which contains clinicopathological and survival data of cancer patients from 18 registries.there were 52,472 eligible patients (including 2353 occult lung cancers) for survival analysis.the detailed information was presented as fig. 1.all patient records were anonymized before analysis.information collected from the seer database included sex, race/ ethnicity, survival time, cause on disease, age at diagnosis, tumor size, approach of treatment (including surgical treatment, radiotherapy, and chemotherapy), tumor differentiation, histological subtype, tumor location, tnm stage, and marital status.follow-up cancer-specific survival, which was the duration from the date of diagnosis to death caused by lung cancer, was regarded as our observational endpoint.for survival analysis, follow-up duration ranged from 1.0 to 155.0 months, with a median of 27.0 months.statistical analysis all statistical analysis was performed using spss statistics 25.0 software (ibm spss, inc., chicago, il, usa), and graphpad prism 8 (https://www.graphpad.com/ scientific-software/prism/).risk ratios (rrs), hazard ratios (hrs) and 95% confidence intervals (cis) were calculated using multivariable logistic regression analysis and cox regression analysis, respectively (regression method was enter selection).the average value of each covariate was calculated by the multivariable cox regression model, and estimated the adjusted survival curves of t classification.statistical tests were considered statistically significant with two-sided p value < 0.05.results patient characteristics in the step-one case selection, there were 305,054 patients (including 2958 occult lung cancers) for calculating incidence.the detailed information of patient characteristics was shown in table 1.after adjusting for other confounders, patients with tx had a poorer prognosis than patients of t2b (adjusted hr 1.186, p < 0.001), nevertheless better long-term survival outcomes than patients with t4 (adjusted hr 0.845, p < 0.001).besides, the prognosis for patients of tx was not statistically different from that of t3 patients (p = 0.121).in this cohort, there were 2206 death events in the 2353 occult lung cancers.female patients showed a better survival than male patients (table 6, adjusted hr = 0.796, 95%ci 0.726–0.876, p < 0.001).next, data on 52,472 eligible patients were analyzed by cox regression analysis including univariable and multivariable analyses.according to the results, we found that occult lung cancer patients didn’t have satisfactory survival outcomes.the prognosis of occult lung cancer was between t2b’s and t4’s.besides, there was no significant difference in the prognosis of patients with t3 classification or occult lung cancer.besides, hui mai et al. performed a study about characteristics of occult lung cancer-associated ischemic stroke, and suggested that occult cancer should be considered in the setting of multiple and recurrent embolic strokes within the short term in the absence of conventional stroke etiologies [9].because malignant tumors may cause the blood to hypercoagulable state, which leads to the occurrence of thrombosis [17], the previous researchers began to investigate the incidence of occult lung cancer in stroke patients.alejandro daniel babore et al. analyzed data of over 800,000 patients, and uncovered that the prevalence of occult lung cancer was 5.3 per 1000 patients in the stroke patients, and 2.6 per 1000 patients in the control group [6].however, the results of the present study revealed that crude incidence rate of occult lung cancer was 10.0 per 1000 patients, which was much higher than the findings from above study.our study cohort focused on lung cancer, which led to a higher incidence rate of occult lung cancer in the present study.though, this present study paid more attention to the incidence rate of occult lung cancer in entire lung cancer cohort, and illustrated that the incidence rate over time was reduced between 2004 and 2015.the present study found that the prognosis of occult lung cancer patients was poorer than that in patients with t2 disease.however, the sample size was relatively small in their research [20, 21].second, cases with second primary lung cancer were excluded from the study.
number of words= 702
[{'rouge-1': {'f': 0.45922293423533483, 'p': 0.763950177935943,'r': 0.32827814569536423}, 'rouge-2': {'f': 0.24541814676943088, 'p': 0.37357142857142855,'r': 0.18273209549071617}, 'rouge-l': {'f': 0.4074456508245576, 'p': 0.6008641975308642,'r': 0.30822714681440444}}]
-----------------------------------------------------------------------------------------------------------------------------------
p77:
Extractive Summary:
advances in imaging technology and an increase in physical check-ups have led to a higher incidence of rcc detection, which enables early clinical intervention [2].despite new approaches to rcc treatment, including new pharmacological inhibitors, rcc remains chemotherapy-resistant, and patient survival largely depends on surgical treatment at early stages [3].discovery of a novel therapeutic target to improve prognosis in rcc patients is an urgent clinical need.clarification of the molecular mechanisms underlying rcc will provide evidence to improve clinical strategies.tripartite motif (trim) family proteins contribute to cancer development by mediating cell growth, metastasis, and oncogenesis [4].the ring finger domain, which is responsible for e3 ubiquitin activity, is directly responsible for ubiquitination [6].as an oncogene, trim27 promotes cancer cell growth in various cancers, including esophageal, lung, and colorectal cancers [7, 8].trim44 is another trim member that exhibits oncogenic roles in rcc, and higher trim44 levels are associated with poor prognosis [9].nuclear factor kappa b (nf-κb) is an essential signaling transcription factor that regulates diverse cellular processes.nf-κb promotes proliferation, regulates cell death, stimulates migration, and mediates inflammatory processes depending on the cell type, developmental stage, and pathological state [10, 11].the nf-κb complex is inactivate in the cytoplasm, and upon stimulation, the nf- κb dimer moves to the nucleus, where it binds to various target genes [12].iκbα is a well-characterized nf-κb inhibitor, which acts by removing nf-κb from dna in the nucleus and relocating it to the cytosol [13, 14].the activation/ translocation of nf-κb requires the removal of iκbα by ubiquitination and degradation.in lung carcinoma, nf-κb and iκbα signaling is regulated by trim family members [15, 16].methods cell culture hk-2, 786–0, a498, achn, caki-1, and caki-2 cells were purchased from shanghai biology institute (shanghai, p.r. china).cells were maintained in dmem media (trueline, kaukauna, wi, usa) containing 10% fbs (thermo fisher scientific, waltham, ma, usa), lglutamine (2 mm) and 1% penicillin-streptomycin (solarbio; beijing, p.r.china).cells were placed in a 37 °c incubator with 95% air and 5% co2.the nf-κb inhibitor was dissolved in dmso (d2605, sigma, usa).qrt-pcr total rna was extracted using trizol (in-vitrogen, waltham, ma, usa).cdna was generated using a cdna synthesis kit (thermo fisher scientific, waltham, ma, usa).real-time pcr was conducted with following cycling scheme: 95 °c for 10 min, 40 cycles of 95 °c for 15 s, and 60 °c for 45 s on the real-time pcr equipment (abi-7300, abi, usa).gapdh was the internal control.the 2−δδct method was used for gene expression analysis as mentioned in a previous report [18].all experiments were performed using three replicates.western blotting.proteins were extracted in ripa lysis buffer (jrdun, shanghai, p.r. china), in the presence of protease inhibitor cocktail (edta-free; roche, heidelberg, germany).the concentration was calculated by a bca protein assay kit (thermo fisher scientific, waltham, ma, usa).a total of 25 μg of proteins were separated by 10% sds-page gels before being transferred to nitrocellulose membranes (millipore, billerica, ma, usa) overnight.primary antibodies were incubated at 4 °c overnight, followed by incubation of secondary antibody anti-mouse igg (1: 1000; beyotime, shanghai, p.r. china) for 1 h at 37 °c. the bands were detected using an enhanced chemiluminescence system (tanon, shanghai, p.r. china).ikba (ab32518, abcam, usa).protein expression was quantified according to the gray value after normalized to gapdh, nuclear extract was normalized against that of histone h3.lentiviral plasmids (plvx-puro) with full length human trim27 or a mock plasmid (negative control) were generated and transiently transfected into caki-1, caki-2, and 786–0 cells using lipofectamine 2000 (thermo fisher scientific, waltham, ma, usa).cells were harvested 48 h post-transfection.cell proliferation cell counting kit-8 (cck-8) assay kits (sab, college park, md, usa) were purchased for the cell proliferation assay.briefly, cells were transfected and seeded in 96-well plates for various time points.cck-8 solution (1: 10) was incubated for 1 h before harvest.signals were detected by anti- iκbα antibody (#4812; cell signaling technology, danvers, ma, usa) and anti-ubiquitin antibody (ab7780; abcam, uk).tunel tunel assays were performed on sections using an apoptag kit (11,684,817,910; roche, switzerland) principally according to the supplier’s instructions.results increased expression of trim27 in rcc correlates with poor prognosis to define trim27 expression in rcc, we first compared the trim27 mrna from rcc tissues (n = 535) and normal renal tissues (n = 72) from the cancer genome atlas (tcga).consistent with rcc patient data, trim27 was selectively upregulated in human rcc cell lines (fig. s1a).we conducted a chi-square test to investigate any correlation between trim27 and rcc clinical characteristics.since disrupted apoptosis pathways play essential roles in neoplastic progression, trim27 might be a future biomarker to predict rcc prognosis.all three sirnas effectively inhibited the mrna and protein expression of trim27 (fig. 2a, b).caki-2 cells transfected with sinc or sitrim27 were injected into nude mice and the tumor tissues were measured on day 33.moreover, increasing iκbα ubiquitination improves nf-κb activation [20].trim44 and trim59, identified as oncogenes, are enriched in cancers and involved ininvasion stimulus, which predicts poor prognosis [9, 21].the results of our study provide novel evidence that trim27 is another example of a trim superfamily member that has an oncogenic role in cancer.indeed, trim27 has been shown to promote tumor invasion and growth in colon cancer through its promotion of the epithelial-mesenchymal transition and activation of the akt pathway [5].unlike other cancer types, the exact molecular mechanisms underlying the oncogenic action of trim proteins in rcc remain unclear.many downstream signaling pathways of trim27 have been identified in cancer, including pten/akt, p38, and stat3 [7, 8, 29].
number of words= 889
[{'rouge-1': {'f': 0.3822528294446624, 'p': 0.7790163934426229,'r': 0.2532627118644068}, 'rouge-2': {'f': 0.17373059552529918, 'p': 0.2839917695473251,'r': 0.12514316012725346}, 'rouge-l': {'f': 0.3057976917758735, 'p': 0.5844927536231883,'r': 0.20706563706563708}}]
-----------------------------------------------------------------------------------------------------------------------------------
p78:
Extractive Summary:
procedures were based on clinical guidelines for cancer screening and early diagnosis and treatment in china [17].the whole esophagus was visually examined.lugol’ iodine staining in esophagus was applied to the diagnosis of suspicious lesions.the subjects with suspicious lesions (unstained and inflammatory lesions) were needed biopsy.the number of biopsies depended on the size of the lesion (> 1 cm, ≥ 2 biopsy; > 2 cm, ≥ 3 biopsy; > 3 cm, ≥ 4 biopsy).in addition, if multiple scattered lesions were found, biopsy of all suspicious lesions should be taken as much as possible.subjects without suspicious lesions did not receive a biopsy.the biopsy slides were read by two pathologists independently.if there were any inconsistencies, a third pathologist would give advice through discussion.doctors reported the worst biopsy diagnosis to participants with multiple lesions.the histological criteria were as previously described [18–20].to ascertain severity, esophageal mucosa was ranked using 5 categories: normal esophageal mucosa, minor mucosa changes, esophagitis, esophageal squamous simple hyperplasia (essh) or esophageal squamous dysplasia (esd) [21].esd was further categorized into 3 levels including mild, moderate, and severe.according to who tumor histological classification [7], mild and moderate esd combined fall under lgin.severe esd and squamous cell carcinoma in situ (cis) are considered as the hgin.in this study, we grouped the participants into 4 groups: normal control, esophagitis, lgin and hgin/escc (lesions including severe esd, cis, and higher-grade lesions).statistical analyses basic, descriptive statistics show categorical variables as frequencies and percentages, while continuous variables are shown as mean and standard deviations.the appropriate tests for significance were applied, χ2 or anova test, respectively.in this report, we use the unconditional univariable logistic model to calculate crude odds ratios (ors) and 95% confidence intervals (cis) [22].the variables with a p value ≤0.2 in unconditional univariable logistic analysis were selected for unconditional multivariable logistic analysis.we considered three models to calculate the adjusted or, including ageadjusted only, gender-adjusted only and adjusted age, gender, education, bmi and income at the same time.these adjusted confounders were selected based on the previous studies of esophageal cancer.besides, models of adjustment for age or gender only were conducted as they may influence the socioeconomic status.in this study, dependent variable was the diagnosis outcome of disease.the independent variables included environment, lifestyle, diet habits, and medical history.we used spss software (version 24.0) to perform χ2 or anova test, and glm function of r software (version 3.6.1) to perform logistic regression analysis.all tests of significance were two-sided with a p value of 0.05 was considered as statistically significant, expect univariable logistic regression analysis.results participant characteristics a total of 44,857 subjects aged 40 to 69 were incorporated in the analysis, including 37,656 (83.95%) subjects with a normal esophagus, 4890 (10.90%) people with esophagitis, 1874 (4.18%) cases of lgin and 437 (0.97%) cases of hgin/escc.because the complete statistical description table was too long (additional file 3), we only selected a few main variables to show in table 1.the proportion of male in normal group, esophagitis, lgin and hgin/escc was 41.4, 46.8, 48.7 and 56.5%, respectively.however, the normal group (54.41 ± 7.26) was composed of a slightly younger population than the esophagitis (57.29 ± 6.85), lgin (59.22 ± 6.25) and hgin/escc (60.85 ± 5.80) group.risk factors for different esophageal lesions we applied two steps to identify risk factors.firstly, the unconditional univariable logistic regression analysis was used to test each variable independently and the estimated ors with 95%ci are listed in additional file 4.secondly, the statistically significant variables in unconditional univariable logistic analysis were selected for unconditional multivariable analysis and the results were shown in additional file 5.tables 2, 3, 4 displayed the significant factors of 3 lesion groups from unconditional univariable logistic analysis and multivariable logistic analysis adjusted for age, gender, education, bmi and income.after final unconditional multivariable analysis, we identified a few important risk factors of 3 esophageal lesions.for the hgin/escc patients, some unhealthy lifestyle and dietary habits (table 3) were risk factors, including alcohol (adjusted or 1.60, 95%ci 1.23–2.09), smoking (adjusted or 1.46, 95%ci 1.14–1.89), drinking well and surface water (adjusted or 1.55, 95%ci 1.28– 1.89), salty diet (adjusted or 1.36, 95%ci 1.07–1.77), drinking tea (adjusted or 1.32, 95%ci 1.08–1.61).although the p value of drinking hot/burning hot tea was no statistically significant, the adjusted or value was 1.34.in addition, pesticide exposure (adjusted or 1.44, 95% ci 1.11–1.84), loose teeth (adjusted or 1.48, 95%ci 0.99–2.12), history of chronic hepatitis/cirrhosis (adjusted or 1.91, 95%ci 1.03–3.22), and family history of cancer (adjusted or 1.64, 95%ci 1.34–2.00), also positively related to the risk of hgin/escc (tables 2, 3, 4).based on the results of the adjusted analysis, we also found that some risk factors associated with lgin were the same as hgin/escc, consisting of drinking alcohol (adjusted or 1.30, 95%ci 1.12–1.51), drinking well and surface water (adjusted or 1.19, 95%ci 1.08–1.32), salty diet (adjusted or 1.57, 95%ci 1.38–1.81), drinking tea (adjusted or 1.34, 95%ci 1.26–1.54), family history of cancer (adjusted or 1.33, 95%ci 1.20–1.47), pesticide exposure (adjusted or 1.20, 95%ci 1.05–1.37) and loose teeth (adjusted or 1.23, 95%ci 1.00–1.51).in addition, occupation as a farmer (adjusted or 1.20, 95%ci 1.07– 1.35) was also positively related to the risk of lgin (table 3).drinking hot tea still did not find statistical significance (adjusted or 1.10, 95%ci 0.92–1.33).on the contrary, eating poultry meat (adjusted or 0.73, 95%ci 0.66–0.81), fruits (adjusted or 0.89, 95%ci 0.80– 0.98), vitamins (adjusted or 0.49, 95%ci 0.27–0.82), milk (adjusted or 0.82, 95%ci 0.71–0.94), acid suppressants (adjusted or 0.51, 95%ci 0.39–0.66), antibiotics (adjusted or 0.33, 95%ci 0.14–0.80) and drinking improved water (adjusted or 0.58, 95%ci 0.46–0.72) were negatively related to the risk of lgin.although esophagitis is a benign disease, there are more risk elevating and risk reduction factors than the diseases mentioned above.
number of words= 938
[{'rouge-1': {'f': 0.2903417556478623, 'p': 0.7136170212765958,'r': 0.1822448979591837}, 'rouge-2': {'f': 0.18513283706353686, 'p': 0.37481283422459893,'r': 0.12292479108635099}, 'rouge-l': {'f': 0.31393522944550667, 'p': 0.6048837209302325,'r': 0.2119753086419753}}]
-----------------------------------------------------------------------------------------------------------------------------------
p79:
Extractive Summary:
background globally, primary liver cancer is the sixth most prevalent malignant tumor and the fourth leading cause of cancerrelated death [1], and approximately 85–90% of primary liver cancers are hepatocellular carcinoma (hcc) [2].various treatment strategies, such as hepatectomy, transplantation, ablation, and interventional therapy, have improved survival benefits for hcc patients [3], but the high recurrence rate has led to an unsatisfactory prognosis and poor overall survival (os) for these patients [4].the copper metabolism murr1 domain (commd) protein family consists of 10 family members, all of which have a highly structurally conserved c-terminal motif [6].increasing evidences show that commd proteins play important roles in tumorigenesis, progression, invasion, and metastasis.commd1, the first characterized commd protein, participates in multiple processes, such as copper metabolism, sodium excretion, and inflammatory responses [6–8].despite these data, only commd7 has been investigated in hcc.however, little is known about the function of all commd family members in hcc.transcriptional expression data of commd1–10 in cancer and normal tissues were downloaded from the oncomine database.ualcan ualcan (http://ualcan.path.uab.edu) is an interactive web portal that can be analyzed based on rna-seq and clinical data of 31 cancer types for in-depth analysis of tcga gene expression data [15].in this study, ualcan was used to analyze the mrna expression of the 10 commd family members in hcc tissues and the association of these proteins with clinicopathological parameters.information about the number-at-risk cases, median values of the mrna expression level, hrs, 95% cis and p-values can be found on the k-m plotter website.a statistically significant difference was considered when a p-value < 0.05.genetic mutations in commd proteins and their association with os, disease-free survival (dfs), and progression-free survival (pfs) of hcc patients are presented as k-m plots.a log-rank test was performed to determine the significant difference between the survival curves, with a p-value < 0.05 indicating a statistically significant difference.immune scores estimation of stromal and immune cells in malignant tumor tissues using expression data (estimate) is an algorithm that uses gene expression signatures to predict the ratios of stromal and immune cells in tumor samples.we divided the hcc cases into two groups based on the median value of the immune score, with the high-score group comprising individuals with higher immune scores and the low-score group comprising individuals with lower immune scores.tumor immune estimation resource (timer) is a web resource for the comprehensive analysis of tumorinfiltrating immune cells (https://cistrome.shinyapps.io/ timer/) [22].gepia2 and string databases the gene expression profiling interactive analysis 2 (gepia) web server is a resource for gene expression analysis based on tumor and normal samples from the tcga and gtex databases [23].lipofectamine rnaimax (thermofisher, usa) was used to transfect negative control (nc) and commd3 sirnas (ribobio, china) into hcc cells as the manufacturer’s protocol suggested.target sequences for sirnas were tggtgaccttaagtgtaca (commd3 sir1), gcagatctctccctcatat (commd3 sir2) and cgcttggaatatcagataa (commd3 sir3).cells were seeded into 48-well culture plates (1 × 104 cells/well).after 12 h cultivated, the cells were transfected for 24 h and were cultivated for 40 h. then the cells were incubated in edu working solution for 2 h, fixed with 4% paraformaldehyde, permeabilized, washed, and stained with 1x apollo solution and 1x hoechst33342 solution according to the manufacturer’s instructions.statistical methods cox regression analysis in r 3.6.1 software was used to evaluate the association of mrna expression of the commd genes with patient survival.the effect of clinical parameters and mrna expression of the commd genes on the survival of hcc patients was evaluated by univariate cox regression; genes with p-value ≤ 0.05 in the univariate analysis were then included in the subsequent multivariate analysis.in the chen liver dataset, commd4 was higher in hcc tissues than in normal tissues (fold change 1.669, p = 3.22e-15) (table 1) [26].wurmbach et al. observed a 1.663-fold increase (p = 5.37e-7) in commd4 mrna expression in hcc tissues compared to normal tissues [27].commd4 mrna expression was reported to be 1.576-fold higher (p = 2.36e-43) and 1.793-fold higher (p = 2.36e-43) in the first and second roessler liver datasets [28].the mrna levels of commd tended to be higher as the tumor grade increased.these results indicated that the mrna expression of commd1/2/3/4/7/ 8/9 was markedly associated with prognosis in liver cancer patients and that the mrna expression levels of commd3/4/5/7/8/9/10 were significantly related to prognosis in patients with grade 3 hcc.these results demonstrated that commd3 is an independent prognostic factor for os in patients with grade 3 or 4 hcc.after adjusting for purity, commd2/3/10 expression was positively correlated with the majority of gene markers in different functional t cells (cd8+ t, t helper 1 (th1), th2, etc.), b cells, dcs, neutrophils, and nks, which was consistent with the results in fig. 7.commd3 had a strong correlation with markers of monocytes, tumor-associated macrophages (tams), m1 macrophages, neutrophils and tregs in hcc.commd3 was closely correlated (|correlation coefficient| > 0.25) with monocytes (cd14), m1 macrophages (irf5), dcs (bdca-4 (nrp1)), th1 cells (stat1), th2 cells (stat6), and tregs (stat5b).commd proteins were altered in 188 samples from 369 hcc patients (51%); commd5 had the highest mutation rate (14%).predicted functions and pathways affected by the changes in commd and closely associated genes in hcc patients we used gepia2 to detect the five most closely genes for each commd family member.the string database was then applied to construct a network for comprising commd and the 50 identified genes (fig. 8e).the results showed that the most enriched kyoto encyclopedia of genes and genomes (kegg) pathway was the ribosome, and ribosome-related genes, including rps27a, rps29, rpl37a, rps3a, rpl21 and mrps11, were closely associated with the commd genes (fig. 8e).rps27a may be the key node for the commd proteins in hcc.finally, the linkedomics database was used for gsea of the commd proteins to investigate potential biological processes and pathways.the k-m curve and log-rank test analysis indicated that the mrna levels of commd3 were significantly associated with os (fig. 9b).the present results demonstrated that si-commd3 suppressed the proliferation of human hcc cells.discussion although the function of commd7 has been partially confirmed in the tumorigenesis and prognosis of hcc [12], the function of other commd family members in hcc has not been explored.a network was constructed for the commd genes and their 50 most closely related genes.the most enriched kegg pathway was the ribosome, which was in agreement with the gsea results.in addition, the prognostic value of commd proteins in advanced hcc has been identified.commd1 is an important negative regulator of nf-κb, while activation of canonical nfκb signaling increases prostate cancer cell survival [29].moreover, higher mrna levels of commd1 significantly corresponded to longer os in hcc patients.to date, the expression or function of commd2 in human cancer remains unknown.in addition, we found that commd2 showed a significant positive correlation to the infiltration of cd8+ t cells, cd4+ t cells, macrophages and dcs in hcc, suggesting that commd2 contributes to the recruitment and regulation of immune infiltrating cells to influence prognosis in hcc.high expression of commd3 was found in prostate cancer, and could promote tumor cell migration/invasiveness that associated with tumor recurrence and poor survival [32].in addition, the mrna expression of commd3 was associated with all immune cells and some immune cell markers, including cd14 (monocytes), irf5 (m1 macrophages), bdca-4 (nrp1) (dcs), stat1 (th1 cells), stat6 (th2 cells), and stat5b (tregs), in hcc.one reason for this is commd3 could stimulate interferon expression to influence immunity [34], while macrophages can strengthen their killing effect on tumor cells by acting interferon.commd4 maintains genomic stability via regulating chromatin remodel at sites of dna double-strand breaks to support cell survival [35].the depletion of commd4 markedly reduced cell proliferation and enhanced cell death after exposure to dna damaging agents [36].thus, it’s different from other tumors that higher commd4 levels indicate favorable os in hcc patients.these results indicated that commd5 may play a different role in the tumorigenesis and malignant progression of hcc.commd6 is the most well-studied member of the commd proteins in human cancer.the mrna expression of commd6 is higher in 20 types of human cancer, such as hcc, colorectal cancer (crc), and lowgrade glioma than in their corresponding healthy tissues [39].eleven types of cancer, including adrenocortical carcinoma, pheochromocytoma and paraganglioma and ovarian cancer, showed lower levels of commd6 expression than their healthy counterparts.further investigations are needed to explore the oncogenic role of commd6 in hcc.zheng et al. reported that the expression levels of commd7 are higher in hcc tissues and hcc stem cells (hcscs), and silencing commd7 inhibited cell proliferation, migration, and invasion via suppression of nf-κb p65 [13].in addition, they also found that hcc cell apoptosis was increased when commd7 expression was knocked down [40], and higher commd7 expression was associated with a significantly poorer prognosis [41].the stability of commd8 depends on commd3, and the commd3/8 complex functions as an adaptor that can selectively recruit a specific g protein-coupled receptor kinase (grk) to chemoattractant receptors and promote lymphocyte chemotaxis [42].multivariate analysis indicated that high mrna expression of commd3 was an independent prognostic factor for shorter os in all hcc patients.commd2/ 3/10 were associated with tumor-induced immune response activation and immune infiltration in hcc.finally, we validated that knockdown of commd3 inhibits human hcc cell lines proliferation in -vitro.
number of words= 1506
[{'rouge-1': {'f': 0.3978826201985446, 'p': 0.9076068376068376,'r': 0.2547894406033941}, 'rouge-2': {'f': 0.2535679936933386, 'p': 0.5157142857142858,'r': 0.16811320754716982}, 'rouge-l': {'f': 0.3539920076202023, 'p': 0.7226946107784431,'r': 0.23440422322775264}}]
-----------------------------------------------------------------------------------------------------------------------------------
p80:
Extractive Summary:
in recent years, several  registries have evaluated the use of omt after pci.moreover, several observational studies found that the  application of omt after pci was suboptimal in the real  world [11, 12].limited data are available on the use of  omt after pci in china.accordingly, our study aimed to investigate the long- term trend in the utilization of omt, evaluate the  utilization and impact of omt on the main adverse car- diovascular and cerebrovascular events (macces) in  post-pci patients and analyze the predictive factors of  omt after patient discharge in china.methods patient enrollment   our study was a single-center prospective observa- tional study.figure 1 showed the study flow chart.the inclu- sion criterion was successful pci.the following were  exclusion criterion: (1) a definite history of allergy and  allergies to or intolerance of the drugs recommended  by the guidelines (including statins, antiplatelet drugs,  β-blockers, and various blood pressure and glucose- lowering drugs); (2) a diagnosis of malignant tumors or  a life expectancy < 1 year; (3) a diagnosis of an immune  system disease and/or taking hormones therapy; (4)  a serum creatinine level ≥ 265  mol/l or renal fail- ure detected in the past or during hospitalization; (5)  lack of autonomy or a diagnosis of a mental illness; (6)  incomplete clinical data or coronary angiography data;  and (7) in-hospital death after pci.data collection demographic data included age, sex, height, weight,  occupation, education, and type of medical insurance.  other clinical data included admission diagnosis, family  history of cardiovascular disease and history of hyper- tension, diabetes, hyperlipidemia, chronic heart failure  (chf), chronic renal insufficiency, stroke, old myocar- dial infarction, angina pectoris, pci, cabg, smoking and  other concomitant diseases.all clinical data  were obtained from the electronic medical record system.    follow‑up, definitions, endpoints in this study, medication use was recorded before dis- charge and at 1 month, 6 months and 1 year after dis- charge.macces and the occurrence of individual  events were also recorded.follow-up was mainly per- formed by telephone (contact with the patient or the  patient’s family), outpatient clinic visits and readmis- sion.omt was defined as the combination of dual  antiplatelet therapy (dapt), statins, β-blockers, and  angiotensin-converting enzyme inhibitors (aceis) or  angiotensin receptor blockers (arbs) during follow-up  after pci.the primary endpoint was the occurrence  of any macces, defined as a composite of death from  any cause, nonfatal myocardial infarction, stroke, or  target vessel revascularization (tvr).the secondary  endpoints were each of the individual clinical events  described above.tvr was defined as repeat pci or  cabg in the vessel treated during the index pci or any  coronary artery segment containing the target lesion.statistical analysis all statistical analyses were performed with spss  22.0 (spss inc., chicago, illinois).continuous vari- ables with a normal distribution are expressed as the  means ± standard deviations (`x ± s).independent  samples t-tests were used to compare the differences  between groups.nonnormally distributed data are rep- resented as the median (quartile ranges) [m(q1–q3)].  the mann–whitney u test was used for intergroup  comparisons.logistic regression was used to calculate  the odds ratio (or) with 95% confidence intervals (ci)  to find out the factors on omt status after pci.omt  status was regarded as the dependent variable.socio- demographic and medical variables were independent  variables.chi-square test was used to compare the cat- egorical variables between groups.multivariable cox regression model was devel- oped to calculate the hazard ratio (hr) with 95% ci  for significant differences and clinically significant vari- ables between groups on macces.macces status  were regarded as the dependent variable.p value below 0.05  was considered statistically significant.the follow- up period was from admission to the day when the  macce was observed or until 1 year after discharge  from the hospital.see table 1 for details.there were significant differ- ences in omt based on all the aforementioned vari- ables (p < 0.05).a cox hazard  ratio model found that adherence to omt at the 1-year  follow-up (p = 0.001, hr = 0.486, 95% ci 0.312–0.756)  was a significant prognostic factor for a lower incidence  of macces (see table 3).medication  adherence was independently associated with a favora- ble prognosis [15–17].adherence to omt is a crucial factor affecting the pro- gression and prognosis of chd.the get with the guidelines project [24]  (gwtg) in the united states advocated for increasing  the prescription of omt to acs patients during hospi- talization because it could increase the long-term out- of-hospital medication adherence among these patients.  finally, although we extensively adjusted for some  covariates, it remains possible that unknown confound- ers influenced the association between medication non- adherence and macces and the factors influencing the  utilization of omt.the 1-year rates of adherence to  antiplatelets and statins were generally good.
number of words= 755
[{'rouge-1': {'f': 0.44141696061379415, 'p': 0.7191803278688524,'r': 0.31843161856963614}, 'rouge-2': {'f': 0.24776427444458657, 'p': 0.3726315789473684,'r': 0.1855778894472362}, 'rouge-l': {'f': 0.37220124721439035, 'p': 0.5672067039106146,'r': 0.2769767441860465}}]
-----------------------------------------------------------------------------------------------------------------------------------
p81:
Extractive Summary:
while there was no clear trend over the years (table 1), the prevalence was higher toward the end of the study (16%) than in the beginning (14%).the average annual change in age- and sex-adjusted prevalence was 0.23% (0.15%, 0.30%; p < 0.001).predictors of age of diagnosis table 3 shows the results of the prevalence of risk factors by age tertiles: ≤ 55 (n = 28,390), 56 to 65 (n = 30,343), and ≥ 66 (n = 31,361).the results show that younger patients are more likely to be men; have a higher bmi; have higher total cholesterol, higher ldlc, higher triglycerides, and higher risk of dyslipidemia; and much more likely to be current smokers and opium users.in other words, younger patients had more risk factors, which is perhaps why they were diagnosed with cad at earlier ages.table 4 shows the predictors of age of diagnosis.younger than women with a mean difference (95% ci) of − 3.12 (− 3.27, − 2.97).after adjusting for other variables in the model, being a male was still associated with earlier age at diagnosis, but only less than one year younger, with a mean difference (95% ci) of − 0.89 (− 1.09, − 0.68).in the adjusted model, one sd increase in bmi (4.5 kg/m2) was associated with approximately 1.5 years of earlier at diagnosis, with a 95% (ci) of − 1.52 (− 1.60, − 1.43).other factors associated with younger age at diagnosis and their estimated coefficients (95% cis) were: serum ldl-c (each sd, 38.7 mg/dl) with − 0.43 (− 0.52, − 0.35) years; serum triglycerides (96.9 mg/dl) with − 1.35 (− 1.45, − 1.26) years; dyslipidemia (yes vs. no) with − 0.81(− 1.00, − 0.62) years; fbs (53.3 mg/dl) with − 0.32 (− 0.42, − 0.22) years; ever cigarette smoking with − 2.76 (− 2.95, − 2.57) years; and ever opium use with − 2.20 (− 2.44, − 1.96) years.factors associated with an older age at diagnosis in the adjusted models and their respective coefficients (95% ci) were higher hdl-c (10.6) with 0.71 (0.62, 0.79) years; diabetes mellitus with 1.05 (0.84, 1.26), and hypertension with 3.54 (3.37, 3.71).to further investigate the effect of opium use, we evaluated the association in various subgroups (table 5).it was associated with earlier age of cad diagnosis in nearly all subgroups, except for those who were 55 or younger.discussion this study is the largest hospital-based serial crosssectional study of patients with established cad thus far conducted in iran.despite its limitations, the study results give us unique insights about comparing the prevalence in cad patients with the general population, comparison of risk factors among men and women, trends of risk factors among cad patients, the impact of neglected risk factors such as opium, and the impact of these risk factors on age of cad diagnosis.the associations of traditional risk factors with cardiovascular disease in iran are well-established [10–15].in 2005, hypertension was responsible for 80,000, hypercholesterolemia for 34,000, diabetes for 34,000, and smoking for 11,000 deaths in iran in 2005 [11].nationwide, the risk of cardiovascular disease attributed to hypertension, high ldl-c, diabetes, and smoking were estimated as 36%, 24.1%, 9.9%, and 5.5%, respectively [10].comparisons with general population the prevalence of cad risk factors have been studied in cross-sectional samples of the iranian general population, such as the steps studies [16].as expected, the prevalence of cad risk factors was higher in cad patients of our study compared to the general population; for example, the prevalence rates of hypertension, dyslipidemia, cigarette smoking, and diabetes mellitus were, respectively, 58%, 64%, 38%, and 34% in our study, compared with 17%, 33%, 12%, and 10% in general population; and 55%, 48%, 16%, and 21% in older adults (55–65 years), according to steps study 2007 [16].in 2016, a systematic review and pooled analysis reported the national agestandardized hypertension prevalence of 24% and 29%, among iranian adults (> 25 years) in men and women, respectively [17].another national study reported the hypertension prevalence of 23% and 50% in 30–55 and > 55 year-old populations, respectively [18].a systematic review demonstrated that the estimated national prevalence and 95% confidence intervals for hypercholesterolemia (≥ 200 mg/dl), hypertriglyceridemia (≥ 150 mg/dl), high levels of ldl-c (≥ 130 mg/dl) and low levels of hdl-c (< 40 mg/dl in male and < 50 mg/ dl in female) in iranian people are 41.6% (36.1–47.0), 46.0% (43.3–48.7), 35.5% (24.0–47.1) and 43.9% (33.4– 54.4), respectively, among ≥ 15 year-old population [19].according to the international diabetes federation 2019 report, diabetes national prevalence in iranian adults 20–79 years was 9.4% (7.4%-12.3%) and more than 5 million iranian people had diabetes.the middle east and north africa region has the highest age-adjusted diabetes prevalence worldwide and the number of people with diabetes in this region, by 2045, will increase by 96.5%.the prevalence of diabetes among > 50-year-old population in this region is almost 25% [20].comparing men and women with cad being a man is an established risk factor for cad.in our study, 68% of all patients were men, whereas approximately 50% of the general population are men.men were diagnosed with cad approximately three years earlier than women.even after adjustment for a host of risk factors, being a man was still associated with a oneyear earlier diagnosis.this may reflect either incomplete adjustment for risk factors, or an inherited tendency for higher risk of cad in men.as expected, the prevalence of ever tobacco smoking (51.8% vs. 7.6%) and ever opium consumption (19.2% vs. 2.0%) was substantially higher in men than in women.these numbers are consistent with findings from population- based studies.a recent systematic analysis from the global burden of disease study reported that the prevalence rates of smoking in iran were 5% and 25% in females and males aged 15 years or older, respectively.almost 1.6 million women and 8.7 million men are current smokers in the country [22].this gender difference is primarily because consuming opium and smoking tobacco was a more acceptable behavior for men than for women.these two risk factors explain a substantial amount of the difference in age of diagnosis between men and women.unlike the global prevalence of diabetes in men and women (9.6% vs. 9.0%, respectively) [23], the prevalence of diabetes in iranian women is 1.7% higher than men [24] mainly because a greater obesity rate among iranian women [25].therefore, our results about the higher bmi and diabetes prevalence in women are in line with other national studies [16, 24–27].other risk factors, however, were more common in women than in men in our study.for example, compared to women, men had a lower prevalence of dyslipidemia (59.1% vs. 75.3%) and hypertension (49.2% vs. 76.2%).these differences do not reflect differences in the general population.for example, the prevalence of diabetes mellitus in the general population is higher in men than in women [12, 13].rather, observed pattern is because our study participants all had cad.women who were less likely to be cigarette smokers and opium users had other risk factors to expose them to cad.in recent reports from china national stroke screening and prevention project (cnsspp) in 2019, similar to our results, prevalence of dyslipidemia was higher among woman than men (54% vs. 46% in urban and 52% vs. 48% in rural area) [28].likewise, in a study from neighboring azerbaijan, zeynalov and colleagues reported that the prevalence of hypertension among cad population was significantly higher in women than men (66.2% vs. 58.4%, p < 0.05) [29].the trends of risk factors over the study period over the 11-year study period, the average serum concentration of total cholesterol, ldl-c, hdl-c, and triglycerides declined substantially.these trends are similar to those found in population-based serial crosssectional studies in iran.for example, in the steps (2007–2016) studies in iran, total cholesterol, ldl-c, and triglycerides decreased substantially over the course of the study [30].decreasing trend of non-hdl-c was also observed in most western countries, japan, and south korea [31].this decrease could be mainly attributed to statin prescription and dietary modifications, e.g., replacement of saturated with unsaturated fats and reduction in trans-fats.in a recent study on non-communicable disease in iran, approximately half (46.5%) of 40–70-year iranians were eligible for moderate- to highintensity statin therapy based on american college of cardiology and the american heart association (acc/ aha) guideline [32].serum hdl-c levels, however, changed only modestly.hdl-c has mostly genetic determinants but lower physical activity, smoking, and carbohydrates consumption may also lower hdl-c levels [30, 31].the prevalence of several other risk factors, i.e., higher bmi, diabetes, and hypertension was increasing in our study population.while studies conducted in the general population also show increasing trends of bmi and fbs, trends of hypertension have been on the decline.data from the steps studies in the iranian general population [30] show that, from 2007 to 2016, bmi was increasing in non-diabetic men population; but there was no significant trend in non-diabetic women, nor in the diabetic population.
number of words= 1455
[{'rouge-1': {'f': 0.341712164834622, 'p': 0.786012084592145,'r': 0.21831038798498123}, 'rouge-2': {'f': 0.1734967759257423, 'p': 0.31242424242424244,'r': 0.120093926111459}, 'rouge-l': {'f': 0.304633666218552, 'p': 0.5477777777777778,'r': 0.21098360655737705}}]
-----------------------------------------------------------------------------------------------------------------------------------
p82:
Extractive Summary:
background atrial fibrillation (af) is the most common cardiac arrhythmia with adverse clinical outcomes and diverse pathophysiological background [1].af affects approximately 20.9 million men and 12.6 million women worldwide, representing a 1.5-fold and two-fold increase in all-cause mortality, respectively [2].aortic stenosis is the most prevalent valvular disease, with aortic valve replacement (avr) surgery remaining the gold standard treatment for severe symptomatic aortic stenosis, improving both quality of life and overall survival [3].however, the pathophysiology of af in aortic stenosis is poorly understood.af pathophysiology begins with ectopic firing and re-entry, which depend on several mechanisms: (1) ion channel dysfunction; (2) ca2+- handling abnormalities; (3) structural remodelling; (4) autonomic neural dysregulation.regarding ion channel dysfunction, cardiomyocytes return to their resting potential after depolarization through an equilibrium between if (pacemaker) and ik1 (inward rectifier k+) currents, which might be dysfunctional in af.this is the first study aimed to characterize atrial fibrosis in af patients with aortic stenosis submitted to avr surgery, as well as evaluate the gene expression profile of several extracellular matrix proteins and quantify differentially expressed targets in the serum of af patients.serum quantification of target extracellular matrix proteins was performed to establish potential af biomarkers (timp1 and timp2).between 2014 and 2019, 56 patients with severe aortic stenosis submitted to avr surgery in a tertiary hospital were selected.reoperations and emergent cases were excluded, as well as patients with severe aortic regurgitation.new york heart association (nyha) functional classification was used to characterize heart failure symptoms.patients underwent an echocardiographic evaluation performed by experienced operators up to 6 months before surgery.reduced ejection fraction was considered below 40%, according to the european society of cardiology guidelines [12] left atrium enlargement was considered when left atrium diameter was equal or superior to 40 mm.all patients had an electrocardiogram (ekg) performed up to 6 months prior to surgery and af was defined according to international guidelines as absolutely irregular rr intervals and no discernible, distinct p waves, with an episode lasting at least 30 s being the threshold for diagnostic purposes [2].definitions of paroxysmal, persistent, long-standing persistent or permanent af followed the european society of cardiology guidelines.biopsy and serum sample collection during surgery, an atrium appendix myocardial biopsy was collected, and the sample was either immediately fixed in formalin (formalin solution, neutral buffered, 10%) and processed for histological analysis or flashfrozen in liquid nitrogen and stored at -80ºc for posterior gene expression analysis.gene expression analysis a total of 19 patients (sr n = 11, af n = 8) were evaluated for the gene expression of several extracellular matrix proteins: collagen i and iii, timp1, timp2, timp4, mmp2, mmp9, mmp16 and transforming growth factor β1 (tgf-β1).prior to gene expression quantification using the 2− δct method, pcr efficiency of each gene including the internal control gene (18s rna) was determined and it was assured that they were identical.target gene expression was normalized to the 18s gene.initial mrna expression data were logarithm transformed using the 2− δc t method [13].serum quantification after gene expression analysis, a total of 24 patients (sr n = 17, af n = 7) were selected for assessing serum quantification of proteins whose genes were shown to be increased by reverse transcription (rt)-pcr in af patients (timp1 and 2, see below).results patient demographics participants had a mean age of 70.85 ± 10.00 years, with males representing 49.2% of patients.the most common comorbidity was hypertension, representing 64.4% of patients, while 20.3% had type 2 diabetes mellitus.symptoms of heart failure (nyha > i) were present in 55.9% of patients, with nyha class ii being the most prevalent stage (54.2%); 15.3% of patients suffered from angina, while only 5.1% had complaints of syncope or lipothymia.solely one patient had decreased left ventricular ejection fraction.concerning sr and af patient subgroups, af patients were older (af 75.47 ± 7.71 vs sr 68.98 ± 10.23 years, p = 0.023).atrial fibrillation histomorphometric analysis fibrosis was significantly increased in the af group when compared to sr patients, with a mean ± sd percent fibrosis of 38 ± 6% and 25 ± 2%, respectively (p = 0.024).moreover, cardiomyocyte area was significantly higher in af patients versus sr patients—334.1 ± 18.31 μm2 versus 289.3 ± 6.18 μm2 (p = 0.008) (fig. 1).the collagen ratio i/iii was, nonetheless, similar between af and sr patients: 0.24 ± 0.05 versus 0.23 ± 0.06 (p = 0.928).mmp9 and mmp16 were similar between patients, showing no differential atrial expression: 1.27 × 10–6 ± 2.77 × 10–7 versus 1.57 × 10–6 ± 3.25 × 10– 7 (p = 0.509) and 4.35 × 10–6 ± 7.08 × 10–7 versus 5.86 × 10–6 ± 6.66 × 10–7 (p = 0.141), respectively.on the other hand, timp1 was less expressed in the atria of af patients: 3.69 × 10–4 ± 5.82 × 10–5 versus 5.72 × 10– 4 ± 7.41 × 10–5 (p = 0.052).timp2 presented similar results, demonstrating a significantly decreased expression in the af group: 2.37 × 10–4 ± 3.04 × 10–5 versus 3.75 × 10–4 ± 4.37 × 10–5 (p = 0.026).af patients were older, had increased cardiomyocyte area and atrial fibrosis on histologic quantification, increased collagen type iii gene expression, as well as decreased timp1 and timp2 gene expression.patients with af had increased fibrosis, as demonstrated by histology sections, as well as increased cardiomyocyte size, suggested by the higher cell area.furthermore, the increased cardiomyocyte cell area in af patients might correspond to a glycogen accumulation, with depletion of contractile material, as demonstrated in a study of sustained af in goats with a substantial proportion of atrial myocytes with marked ultrastructure changes, including loss of myofibrils and accumulation of glycogen [21].studies on dogs with mitral valve stenosis have reported atrial cardiomyocyte hypertrophy with decreased myofibrils [21, 22].despite the absence of studies on aortic stenosis patients, it has been reported similar degenerative cardiomyocyte changes in patients submitted to cardiac surgery, which correlate with atrial size and pressure, along with diastolic dysfunction [23].in addition to the increase in atrial fibrosis in histology sections, mrna expression of collagen type iii was higher in af patients, although collagen i and collagen i/iii ratio did not present differences between groups.conversely, yoshihara et al. found a decrease in mrna expression of both collagen type i and iii in patients with af submitted to kosakai’s modified maze procedure [26].tgfβ1 is an established positive regulator of cardiac fibrosis [27], is upregulated in af patients with mitral valve disease submitted to valve replacement, and [28] postoperative af in patients submitted to myocardial revascularization [29]. nevertheless, studies on aortic stenosis are scarce.however, timp1 and timp2 were decreased in af (p = 0.052 and p = 0.026, respectively), with no differences in gene expression for timp4.conversely, mmp2/ timp4 demonstrated a tendency towards significance, being decreased in af patients, while mmp16/timp4 was significantly lower in this patient subgroup.conversely, there were no differences in mmp9 atrial expression in af patients with heart failure with reduced ejection fraction [32].despite a similar expression of timp4 between groups, mmp2/timp4 and mmp16/timp4 ratios demonstrated a decrease in af patients.remarkably, mmp16/timp4 was overwhelmingly decreased in af patients.this is the first study that suggests mmp16/ timp4 as a marker of disease, specifically, a marker of chronic af in aortic stenosis patients.the increase in matrix inhibitors when comparing with proteolytic enzymes (decreased mmp/timp ratios) may explain the collagen accumulation seen by histology, in addition to the increased mrna collagen type iii expression.the obstruction caused by aortic stenosis leads to pressure overload and, ultimately, to ventricular hypertrophy and cardiac fibrosis [37].this study indicates an atrial matrix remodeling in aortic stenosis patients with chronic af submitted to valve replacement while suggesting timp1 and timp2 as biomarkers of disease, readily available with peripheral blood sampling.conclusions atrial fibrillation patients with severe aortic stenosis present increased atrial fibrosis and collagen type iii synthesis, with extracellular matrix remodelling demonstrated by a decrease in the mmp16/timp4 ratio, along with an increased serum timp1 and timp2 proteins.timp1 could be a marker of adverse outcomes in aortic stenosis, in addition to conventional echocardiograp
number of words= 1319
[{'rouge-1': {'f': 0.35289895175119296, 'p': 0.9661038961038961,'r': 0.21587737843551796}, 'rouge-2': {'f': 0.2740158735349612, 'p': 0.6917391304347826,'r': 0.17084626234132583}, 'rouge-l': {'f': 0.33993902597477993, 'p': 0.8399115044247787,'r': 0.2130921052631579}}]
-----------------------------------------------------------------------------------------------------------------------------------
p83:
Extractive Summary:
treatment of hf include pharmacological therapies, such as beta blockers, diuretics, ace inhibitors, and angiotensin receptor-neprilysin inhibitors (arni) [4], and device-based therapies, such as implantable cardiac defibrillators and cardiac resynchronization therapy (crt) [5].for patients with more advanced stages of hf, treatment may include left ventricular assist devices (lvad) and heart transplantation.a novel approach using baroreflex activation therapy (bat) has shown promising results in clinical trials in these patients.health system or integrated delivery network.the model also uses data abstracted from recent hf literature [9–12].the working hypothesis is that bat therapy is cost-saving; though it is a more resource-intensive in the short run, it results in better clinical outcomes over time, which in turn results in downstream cost savings for hf patients.in past clinical studies of hf patients, bat has been attributed to improvements in hrqol, activity tolerance, nyha functional class and nt-probnp levels [7, 8, 16, 17].key cost-related outcomes considered were bat-specific serious adverse events, hf and non-hf cv hospitalizations, hf medication utilization, and progression to lvad or heart transplant.hf medication utilization is based on trial data for the first 6-months of the model, and it was assumed that subjects taking medication remained on that medication throughout each time interval.medication use in the gdt group was assumed to increase 4% each time period [11]; given disease progression, medication utilization was assumed to increase for both groups but at a slower rate in the bat + gdt group.the rates for hf and non-hf cv hospitalizations in bat + gdt were estimated based on assumed relative reductions in hospitalizations attributable to bat.this hazard ratio was applied as a 25% relative reduction in the number of hf hospitalizations in bat + gdt compared to the gdt alone rates.the probability of bat + gdt receiving an lvad or a heart transplantation were both assumed to be 0.50% in years post-baseline.heart transplantation probabilities in gdt alone are calculated from the one-year probability of heart transplantation among hf patients reported in the literature (i.e., approximately 1.0% per year over 5 years) [10].lvad and heart transplantation were estimated for gdt alone by converting the initial probability into an instantaneous rate.dollars (table 2).cost of bat was provided by the manufacturer and includes the cost of the device and the associated implantation surgery.annual gdt costs were based on the monthly whole acquisition medication costs for hf patients, which reflect approximate payer reimbursement amounts.model savings are attributable primarily to three factors.first, trial results associated with bat + gdt imply that there will be fewer cv hospitalizations (e.g., fewer adverse events requiring hospitalization for hf) among bat + gdt.second, clinical benefits of bat + gdt reported in beat-hf imply that, based on existing clinical practice guidelines, the need for lvad and heart transplant may in some cases be delayed.third, bat + gdt requires an initial device and implantation cost, which means that the initial cost when spread over a 3-year period is negligible in terms of savings.the findings are generally consistent with the working hypothesis.while there are currently no published cost-impact analyses for the treatment of chronic heart failure with bat, a preliminary cost-effectiveness analysis in the german setting in patients with advanced chronic heart failure demonstrates that bat is a cost effective treatment option, adding more evidence to support the use of bat for the treatment of heart failure symptoms [20].though the prediction model was quite different (based on mccabe et al. [21]), the results were virtually identical.second, model results were benchmarked with the existing literature.in the u.s., annual hf costs are estimated to be about $26,000 (2018 dollars) but can also range as high as $40,000 depending on assumptions [22].these ad hoc analyses suggest that the results of the cim are consistent with prior literature and across methodologies.there are some potentially important limitations to note.first, the model relies extensively on preliminary 6-month clinical data from beat-hf.however, this is not expected to significantly limit the interpretation of the analysis, as the strength of preliminary trial data shows a robust relative benefit between the two randomized arms.this is supported by a meta-analysis of 11 trials involving 2000 hf patients [28].third, the model does not consider other important health outcomes, such as “quality-adjusted” life years (qalys).however, given the relatively short timeline considered, discounting is unlikely to have a meaningful impact on the results.conclusion despite its initial device and implantation costs, bat + gdt becomes the low-cost alternative treatment less than 3 years from implantation, based on observed and extrapolated clinical trial outcomes.these results are broadly consistent with other studies of bat.
number of words= 748
[{'rouge-1': {'f': 0.40156864295243183, 'p': 0.7025757575757576,'r': 0.2811251580278129}, 'rouge-2': {'f': 0.17626050615504701, 'p': 0.26011406844106466,'r': 0.13329113924050634}, 'rouge-l': {'f': 0.36688363464119367, 'p': 0.5453086419753086,'r': 0.2764343163538874}}]
-----------------------------------------------------------------------------------------------------------------------------------
p84:
Extractive Summary:
these observations may occur because blood vessel atherosclerosis decreases the blood supply to periodontal tissues which impairs the ability to resist anaerobic bacteria based on oxygen concentration.p. gingivalis, one of the major periodontal bacteria, induces platelet coagulation, invades the coronary and carotid endothelial cells, increases the possibility of macrophage infiltration, and causes endothelial dysfunction through stimulation of blistering cell conversion and atherosclerosis progression [5].coupled with the 21.2% increase in medical examination fees for periodontal disease, there has been a noticeable decrease in people’s quality of life, increasing both individual and societal burden [8].similarly, more than a quarter of the global death rate (31.3%) is due to chronic cardiovascular disease [9, 10].death due to coronary artery disease (cad) affects 31 males and 21 females for every 100,000 individuals in the korean population—a number that has been continuously increasing in korea over the last 10 years [11].oral health hygiene practices—non-smoking, maintaining appropriate weight, proper toothbrushing, use of dental floss and interdental brushes, as well as regular exercise and periodic dental examinations—are required to promote periodontal health [14].however, people with chronic diseases such as hypertension and osteoporosis have lower oral health behaviors than those without such diseases [15].sabounchi et al. [16] also reported on the significant negative effects of smoking and positive effects of physical exercise on periodontal health.in cohort follow-up study of 941 in-patients with cad, cardiovascular disease recurrence was decreased by 81%in patients who used dental floss or interdental brushes [17].therefore, attention must be paid to the oral health of patients with preceding chronic disease as primary prevention, as well as those with cad requiring secondary prevention due to the possibility of recurrence.among lifestyle risk factors, cigarette smoking and low physical activity were associated with periodontal health, respectively [16].therefore, using national representative data, this study aimed to confirm the association of oral health and hygiene behavior in adults and the elderly with preceding chronic disease or cad through comparison with a healthy group.results can be used as basic data to confirm the importance of oral hygiene management for primary and secondary prevention in adults and the elderly with cardiovascular risk factors.setting and sample we used a nationally representative database obtained from the 7th knhanes, a government-approved statistical survey by the korean centers for disease control and prevention.the knhanes includes data on participants’ demographic, social, health, and nutritional status using three component surveys: health interview, health examination, and nutrition survey.the samples included 16,489 participants from the 1st and 2nd years of the knhanes raw data (2016–2017).the knhanes sampling method included a two-stage stratified sampling approach with sampling districts and households as the first and second sampling units.a systematic sampling method in the 192 sampling areas was used in the selection of the 23 appropriate sample households.facilities such as nursing homes, military facilities, prisons, and foreign households were excluded [21].of the 10,344 patients, 38.2% (n = 3947) had a history of one or more of having hypertension, diabetes, dyslipidemia, acute myocardial infarction, or angina, and were classified as risk groups, and the remaining 61.8% were classified as healthy controls.for lifestyle-related characteristics, the selected variables were smoking and body mass index.in this study, the risk group was considered to have been diagnosed with cad such as myocardial infarction and angina in the past, or one of hypertension, diabetes, or dyslipidemia.to determine the number of remaining teeth, 0 is healthy teeth, 1 is caries, 3 is caries-experience treatment, 4 is loss of caries-experienced, 6 is full-color teeth, 7 is caries-non-experienced treatment, and 8 is unerupted teeth, 9 is a non-recordable surface to investigate the condition of each tooth.oral hygiene behavior five questions were used to assess oral hygiene behavior.preventive dental treatment was reclassified as “yes” if sealants, fluoride application, or scaling were received, and “no” if not.in elderly group, 40.6% of the risk group reported of a chewing problems, showing a statistical significance compared to 34.8% of the elderly control group (p = 0.039).no other characteristics regarding oral hygiene behavior such as the use of oral products, dental examinations, toothbrushing frequency, preventative treatment, treatment history and number of remaining teeth, permanent dental caries, and periodontal disease prevalence were significantly different in the two elderly groups (tables 3, 4).discussion in this study, more differences in lifestyle and oral healthrelated characteristics were identified between the adult risk and healthy control groups than in the elderly groups before psm.it was demonstrated that person having ms had significantly higher odds of having periodontitis than normal people [26].experience of gingival treatment related to oral bacterial infection was significantly higher in the adult risk group after psm than in the healthy control group.accordingly, it is necessary to improve education and awareness about dental treatment and preventive management in the adult population with cardiovascular risk rather than the elderly population.after psm, the experience rate of tooth extraction or oral surgery among the oral health status variables was significantly higher in the risk group than in the healthy control group in the elderly.however, there was no significant difference in oral hygiene behavior across all adult and elderly groups in the current study.while oral hygiene behavior is important in oral health and preventing cardiovascular disease, it has yet to be confirmed whether it can be a risk group predictor for both adult and elderly subjects after psm.in addition, improved oral hygiene behavior has been shown to prevent periodontal disease, tooth decay, and tooth loss, as well as reduce the risk of cardiovascular disease [30].moreover, further research on the variables to confirm the effectiveness of cardiovascular disease prevention in the practice of oral health and oral hygiene behavior is also necessary.conclusions this study was conducted to determine the association between oral health status, hygiene behavior, and cardiovascular risk including preceding chronic diseases in adults and the elderly, respectively, when sociodemographic and lifestyle-related factors were controlled.on the other hand, chewing and speaking problem were found to significantly higher in elderly risk group than in the healthy control group.
number of words= 982
[{'rouge-1': {'f': 0.36121583067878354, 'p': 0.7525396825396826,'r': 0.23764132553606238}, 'rouge-2': {'f': 0.21443024363045218, 'p': 0.3887250996015936,'r': 0.1480487804878049}, 'rouge-l': {'f': 0.34815578976277933, 'p': 0.6033333333333333,'r': 0.24467248908296943}}]
-----------------------------------------------------------------------------------------------------------------------------------
p85:
Extractive Summary:
mets was present in 952 (37.2%) participants.the prevalence of mets was 27.4%, 35.9%, 42.6%, and 44.1% in groups i, ii, iii, and iv, respectively.groups ii, iii, and iv showed a significantly higher prevalence of mets than group i (fig. 2a).carotid plaque was observed in 856 (33.4%) participants.the prevalence of carotid plaque was 34.3%, 28.1%, 32.8%, and 39.5% in groups i, ii, iii, and iv, respectively.only group ii showed a significantly lower prevalence of carotid plaque than group i (p = 0.014) (fig. 2b).carotid plaque was more frequently observed in participants with mets than in those without mets (39.8% vs. 29.7%, p < 0.001).in the univariate logistic regression analysis, age (odds ratio [or] 1.090, 95% confidence interval [ci] 1.077–1.104), male sex (or 1.826, 95% ci 1.538– 2.168), mets (or 1.568, 95% ci 1.326–1.856), and smoking (or 1.995, 95% ci 1.667–2.388) were associated with an increased risk of carotid plaque (all p < 0.01).among the individual components of mets, increased bp (or 2.410, 95% ci 2.023–2.872, p < 0.001) and fasting glucose (or 1.709, 95% ci 1.447–2.019, p < 0.001) were associated with an increased risk of carotid plaque.the risk of carotid plaque was significantly lower in group ii than in group i (or 0.750, 95% ci 0.596–0.943, p = 0.014), but was not significantly different between group i and groups iii (or 0.935, 95% ci 0.738–1.185, p = 0.580) and iv (or 1.248, 95% ci 0.992–1.571, p = 0.590) (table 2).the results of multiple logistic regression models for the association of hb with the risk of carotid plaque are presented in table 3.compared with group i, only group ii was consistently associated with a decreased risk of carotid plaque in all logistic regression models.discussion in the present cohort of relatively healthy adults without a history of major adverse events, we investigated the beneficial range of hb with respect to the risk of carotid plaque compared with the lowest category of hb level, after adjusting for mets and its individual components.low hb level is a well-established risk factor for cv disease [20].furthermore, it is a substantial predictor of adverse clinical outcomes, independent of the cv risk status.kalra et al. reported that a low hb level was an independent predictor of mortality, cv events, and major bleeding in 21,829 patients with stable coronary artery disease [21].similar results have been reported in patients with acute coronary syndrome [1, 22, 23] and heart failure [24–26].however, tanne et al. observed that the association between the hb level on admission and mortality was not linear and the risk of mortality increased at both extremes of hb levels in patients with acute stroke [4].under conditions of high hb levels, blood viscosity increases with elevated peripheral resistance and diminished cardiac output.an increase in blood viscosity affects coronary, cerebral, and peripheral blood flow as well as perfusion [27–30].in addition, high hb levels could stimulate atherogenesis through erythrocyte aggregation, leading to platelet aggregation and adhesion to the arterial wall [30–32].amoris (apolipoprotein mortality risk study), a study in 114,159 healthy men and women by holme et al., suggested that high hb levels are a risk factor for major atherosclerotic cv events [33].these results suggest that there may be a clinically beneficial level of hb even within the normal hb range.however, there is a paucity of data on the association between serum hb levels and the risk of subclinical atherosclerosis, especially in the healthy general population.mets is an important predictive factor for the development of diabetes and atherosclerotic cv disease.initially, we identified that (a) the prevalence of mets steadily increased with increasing hb level and (b) mets was significantly associated with an increased risk of carotid plaque.given the relevance of hb to oxygen- carrying capacity, oxidative stress, inflammatory processes, and blood viscosity, we hypothesized that a beneficial range of hb for the risk of subclinical atherosclerosis compared with the lowest category of hb level could be present in a relatively healthy adult population.
number of words= 650
[{'rouge-1': {'f': 0.46440304446598857, 'p': 0.7557142857142858,'r': 0.33519337016574585}, 'rouge-2': {'f': 0.3307166651095658, 'p': 0.5180286738351254,'r': 0.24289073305670816}, 'rouge-l': {'f': 0.5101865989473838, 'p': 0.6849425287356321,'r': 0.40647798742138364}}]
-----------------------------------------------------------------------------------------------------------------------------------
p86:
Extractive Summary:
introduction atrial fbrillation (af) is a common arrhythmia associated with a prothrombotic or hypercoagulable state, which may increase the risk of cerebral and systemic embolism [1].it is well known that a hypercoagulative state is demonstrated by high levels of c-reactive protein (crp) and d-dimer [2].increased d-dimer levels indicate hyperactivation of secondary fbrinolysis, meaning a tendency for intravascular thrombosis [3].d-dimer is a product of cross-linked fbrin degradation, and it is a circulating marker of thrombogenesis and thrombus turnover [4].increasing d-dimer levels may refect atrial thrombus formation and higher embolic risk in patients with af.catheter ablation is a well-established treatment option for patients with symptomatic af [5].rivaroxaban is not included in this study because the standard dose of rivaroxaban (15  mg/10  mg  s.i.d.) in japan difers from the international standard dose (20 mg/15 mg s.i.d.).written informed consent was provided by all patients before participation, and the study protocol was approved by the institutional ethics committee.peripheral venous blood samples were collected immediately before the ablation procedure.for the clinical ablation procedure, a conventional single transseptal puncture was performed using an sl-1 sheath and a brk-1 needle (st. jude medical inc., sunnyvale, ca, usa).tte parameters at baseline included left ventricular diastolic dimension (lvdd), ejection fraction (ef), left atrial diameter (lad), and left atrial volume index (lavi).tee parameters at baseline included left atrial appendage (laa) peak emptying velocity and presence of spontaneous echo contrast (sec).te following plasma/serum biomarkers were analyzed centrally at baseline: (1) thrombogenesis/fbrinolysis biomarkers: d-dimer, fbrinogen, prothrombin fragment f1+2, protein c, thrombomodulin, and (2) infammation biomarker: crp.for the quantitative measurement of d-dimer, a latex-enhanced photometric immunoassay (lpia, mitsubishi chemical medience corporation, tokyo, japan) was used with an automatic analyzer (lpia-s500).te detection limit of this assay was 0.3 μg/ ml.te association of demographic data and medical history on biomarker levels at baseline was investigated.in addition, we assessed the relationship between anticoagulants, biomarker levels, and echocardiogram characteristics.statistical analysis data are expressed as mean and sd or median with 25th to 75th percentiles for normally distributed and skewed variables, respectively.normality was assessed using the shapiro–wilk test.we used t-tests and chi-squared tests to compare continuous and categorical variables.to assess diferences among the three groups, categorical variables were compared with the χ2 test, while continuous variables were compared using the kruskal–wallis test.propensity score analysis was performed to determine the efect of the drug on d-dimer levels.a p value<0.05 was considered signifcant.results during entry periods, 169 patients with atrial fbrillation were initially screened, and fnally 141 patients (on dabigatran n=30, on apixaban n=47, on edoxaban n=64) were recruited (fig.  1).te study population included 84 (60%) men, and the mean age at the baseline examination was 68.1±10.1  years.te mean chads2-vasc score was 2.4±1.4.tere were signifcant diferences among the three groups in terms of gender, age, and history of heart failure.tere was no signifcant diference in the frequency of use of angiotensin-converting enzyme inhibitors/angiotensin receptor blockers, β-blockers, statins, and anti-platelet drugs among the three groups (table 1).laboratory and echocardiographic data of all patients are shown in table 1.serum creatinine and crp levels, and brain natriuretic peptide levels in peripheral blood were not signifcantly diferent between the three groups.echocardiographic parameters (lvdd, ef, lad, lavi, and sec) did not difer among them.tere were signifcant diferences among the three groups in hemoglobin concentration and laa fow velocity (table 2).(2) tere were no diferences in peripheral d-dimer levels among patients with dabigatran, apixaban, and edoxaban.af patients with a high degree of la structural remodeling may have low baseline lvef and have la fbrosis [20].doacs have been shown to prevent thromboembolism in patients with non-valvular af.in the apixaban group, la d-dimer level was lower than in the edoxaban group, suggesting that the anticoagulant efect of apixaban is better than that of edoxaban in patients with nonvalvular a
number of words= 620
[{'rouge-1': {'f': 0.4057469205647822, 'p': 0.6749382716049384,'r': 0.290059880239521}, 'rouge-2': {'f': 0.2173360592411263, 'p': 0.326198347107438,'r': 0.1629535232383808}, 'rouge-l': {'f': 0.35445726271630834, 'p': 0.55,'r': 0.26148936170212767}}]
-----------------------------------------------------------------------------------------------------------------------------------
p87:
Extractive Summary:
adequate selection of antithrombotic therapy for stroke prevention is critical for improving the clinical outcome of patients with nvaf.several clinical practice guidelines have been developed to guide the management of af patients, providing clinicians with recommendations on individualization of treatment based on the patient’s characteristics [16, 17].oac is still underused in af patients who are at high risk of stroke, and many patients are instead treated with antiplatelet agents or do not receive antithrombotic treatment [18, 19].based on this background, we conducted a cross-sectional study to describe the demographic, functional, and clinical characteristics of patients with nvaf attending internal medicine departments in spanish hospitals and the potential factors associated with antithrombotic treatment patterns.methods study design and patients a multicenter, cross-sectional observational study was conducted in internal medicine departments from 93 hospitals distributed across spain.the patients must have been diagnosed with nvaf before the study inclusion, and disease-related data (type of nvaf, disease duration, etc.) was collected from medical charts.statistical analysis in order to describe the demographic, functional, and clinical characteristics of patients on each antithrombotic treatment approach, patients included in the study were categorized into five groups according to the antithrombotic therapy used for their clinical management as follows: no treatment, vkas (acenocumarol or warfarin), doacs (apixaban, rivaroxaban or dabigatran), antiplatelet drugs (aspirin and/or other antiplatelets), and antiplatelet plus anticoagulant drugs.only patients who fulfilled the eligibility criteria and could be categorized in any of these treatment groups were considered as evaluable for the study analysis.thus, factors associated with oac (vkas, doacs, or any oac plus antiplatelet therapy) versus antiplatelet therapy and with vkas versus doacs were assessed.overall, 88.7% of patients received oac (vkas, doacs, or any oac plus antiplatelet therapy).most patients were not institutionalized (> 90%) and had no cognitive impairment according to the pfeiffer questionnaire (> 60% of patients in all groups).compared with autonomous patients, non-autonomous patients were older (84 [80–88] vs. 78 [72–83] years; p < 0.001), predominantly female (62.3% vs. 43.7%; p < 0.001), and with higher risk of stroke and bleeding according to cha2ds2- vasc (5.6 ± 1.5 vs. 4.4 ± 1.5; p < 0.001) and has-bled (3.4 ± 1.1 vs. 2.8 ± 1.1; p < 0.001) scores.similarly, patients with cognitive impairment had a more advanced age (83 [79–88] vs. 79.0 [73–84] years; p < 0.001), were mostly women (61.2% vs. 45.1%; p < 0.001), and had higher cha2ds2- vasc (5.2 ± 1.6 vs. 4.6 ± 1.6; p < 0.001) and has-bled (3.1 ± 1.2 vs. 2.9 ± 1.1; p < 0.001) scores compared with patients without cognitive impairment.additionally, the median age of patients with < 4 comorbidities was 64 (54– 67) years while it was 81 (75–86) years in patients with ≥ 4 comorbidities (p < 0.001) in whom cha2ds2- vasc and has-bled scores were higher (vs. < 4 comorbidities) ( cha2ds2-vasc: 4.9 ± 1.5 vs. 1.9 ± 1.0; p < 0.001; hasbled: 3.1 ± 1.1 vs. 1.2 ± 1.0; p < 0.001).there were statistically significant differences in terms of age and educational level between the different treatment groups.in addition, there were statistically significant differences in estimated prognostic characteristics, including comorbidity (charlson comorbidity index score), 1-year mortality risk (profund index) and life expectancy (investigator’s clinical judgment) among the patients treated with the different antithrombotic treatment strategies.most patients were diagnosed with permanent nvaf (70%), with a median time since diagnosis of over four years.the differences in clinical characteristics between the patients undergoing different antithrombotic treatment strategies are shown in table 2.no significant differences were found in the percentage of patients with high thromboembolic risk ( cha2ds2-vasc ≥ 2) who were treated with each antithrombotic therapy.factors associated with antithrombotic treatment patterns the results of the multivariate analysis performed to identify the independent factors associated with the use of oac versus antiplatelet therapy and of doacs versus vkas are summarized in table 3.factors associated with the use of doacs versus vkas factors significantly associated with the use of vkas over doacs were longer time since diagnosis (or, 1.045) and institutionalization (or, 7.744).as expected, this elderly population had a high comorbidity burden, particularly cardiovascular risk factors and disease, a very high thromboembolic risk and a moderate to high risk of bleeding.patients with worse functional status and worse prognosis in terms of survival were those with more advanced age and with a higher risk of stroke and bleeding.of note, antiplatelet therapy alone was prescribed in less than 7% of patients with nvaf in routine clinical practice.older age is a common reason given for not prescribing anticoagulation [20, 28].this study has therefore highlighted that age alone was not considered as a contraindication for anticoagulation.however, the benefit of stroke prevention outweighs the potential increased risk of bleeding in these patients [30, 31], in whom a higher net clinical benefit has been demonstrated compared to the younger population due to their higher thromboembolic risk [32].we found a substantial increase in the use of oac in elderly patients compared to prior studies conducted in octogenarian af patients in spain, where only half of the patients received anticoagulation [33].although a history of prior bleeding has traditionally been associated with anticoagulation under-prescription in elderly patients [39], bleeding history was not identified as a factor contributing to anticoagulation underuse in our study.antiplatelet therapy has traditionally been used instead of anticoagulation to prevent the risk of bleeding despite the demonstrated greater efficacy of anticoagulant treatment over antiplatelet agents without increasing major bleeding [42].of note, institutionalized patients and those with total or partial dependence for daily activities, who were older and with higher thromboembolic and bleeding risk, were primarily treated with antiplatelets, which were also preferentially used in patients with functional disability and worse prognosis.these findings, therefore, suggest that there might be functional and prognostic factors driving the selection of antiplatelet therapy alone over oac.indeed, the univariate analysis showed that institutionalization, dependence for daily activities, 1-year mortality risk assessed by the profund index, and life expectancy (physician’s clinical judgment) were factors significantly associated with preferential use of antiplatelet therapy over oac.however, among these factors, only 1-year mortality risk was retained in the multivariate model, which showed that poor prognosis (intermediate or high risk) was a factor contributing to oac non-prescription in favor of antiplatelet therapy alone.this real-world assessment has also highlighted that the type of nvaf appears to be a factor that has played an essential role in the underuse of oac in nvaf.however, contrary findings regarding its impact on thromboembolic risk have been reported [47, 48].regarding the type of oac, we found that a significant proportion of patients still received vkas over doacs despite guideline recommendations [16].the use of doacs among patients receiving oac (24.4%) was similar to that reported in spanish studies conducted at the regional level during 2015 (24%) and 2011–2014 (25%) [49] and the nationwide fantasiia registry (22%) [50].thus, educational level was identified as an independent factor associated with the use of doacs, with a preference of these agents over vkas in more educated patients, probably also with a higher income, in line with prior reports [55].this may be explained by better access of patients with a high educational level to the available information on new oral anticoagulants, which may influence treatment decision for stroke prevention.in addition, patients with longer disease duration were also more likely to receive vka therapy over doacs probably because prescription of doacs may have only increased over the past few years due to current guidelines recommendation [16].in addition, we must take into account the obvious limitations of a retrospective chart review that uses patients’ data already recorded in the medical charts for reasons other than research, including incomplete or unrecorded information.the strengths of this study include a highly representative population of non-selected elderly patients with nvaf in clinical practice, including inpatients and outpatients, attended in the internal medicine departments of more than 90 hospitals distributed homogeneously throughout the country.to our knowledge, this is the first and the most extensive study to assess the real-world characteristics of nvaf patients according to antithrombotic treatment after approval of doacs in the internal medicine setting in spain.conclusion this study showed that the vast majority of elderly patients with nvaf, with high comorbidity burden and high thromboembolic risk, received oac in the realworld setting, with vkas as the most frequently prescribed treatment, while doacs remained underused.
number of words= 1371
[{'rouge-1': {'f': 0.2779057309389846, 'p': 0.7474193548387096,'r': 0.17068493150684932}, 'rouge-2': {'f': 0.173727090957345, 'p': 0.3662962962962963,'r': 0.11386566141192599}, 'rouge-l': {'f': 0.3164977217239173, 'p': 0.6414285714285715,'r': 0.21007782101167316}}]
-----------------------------------------------------------------------------------------------------------------------------------
p88:
Extractive Summary:
with this regard, international guidelines recommended the combined use of cardioprotective drugs: platelet aggregation inhibitors (antiplatelets), β-blocking agents (β-blockers), angiotensin-converting enzyme inhibitors (aceis)/angiotensin receptor blockers (arbs) as well as statins in combination with ezetimibe and pcsk9-inhibitors [9, 10].thus, adherence and persistence to chronic poly-therapy is a key factor in secondary prevention since it is associated with a lower risk of mortality and recurrent events [11–15].cr programs offer a substantial contribution to achieving and maintaining lifestyle changes and medication adherence [16–19].hence, cr implementation is strongly recommended by clinical guidelines [20–23].the aim of our study was: (1) to evaluate the impact of the application of in-hospital cr program on adherence to chronic polytherapy following ami and (2) to investigate the long-term effects of cr on three cardiovascular outcomes (all-cause mortality, hospital readmission for cardiovascular and cerebrovascular event, and admission to the emergency department) at three-year follow-up in the setting of the regional health service of lazio region (italy).the data were extracted from the lazio regional health information systems, which contain, for example, information on hospital admissions, in-hospital cardiac rehabilitation participation, mortality and drug claims.setting and study cohort this is an observational study based on the population living in the lazio region, italy.in case of multiple hospital admissions, the first admission during the study period was defined as the index admission.only incident cases of ami were included: patients with hospitalizations for ami or related causes (i.e., percutaneous coronary intervention-pci, bypass, ischemic heart disease, surgery of the heart and great vessels) in the 9 years before index admission were excluded from the analysis.in-hospital cardiac rehabilitation (ih-cr) consists of programs for prevention of deconditioning and recovery of daily activity in the acute phase, as well as supervised exercise therapy and patient education in the early recovery phase.patients candidates for access in ih-cr are those with in prolonged unstable conditions, undergoing evaluation for cardiac transplantation or verification of persistence of the indication, patients at high risk of new cardiovascular events and/or clinical instability, patients with a long in-hospital stay discharged after a prolonged stay in intensive care or intensive respiratory/cardiac care; patients with event-related complications, such as stroke, cognitive impairment, renal failure, pulmonary embolism, re-surgery, pleural or pericardial effusions requiring evacuation therapy, infections, complicated wounds, or those with presence or exacerbations of a severe comorbidities.ih-cr, by its multifactorial care structure based on a multidisciplinary team that involves not only the cardiologist but also the social, psychological, and behavioural dimensions of the disease is in a privileged position as regards the approach to the clinical complexity of multimorbidity and frailty.drug exposure information was collected from the regional registry of all drugs dispensed by public and private pharmacies.all drugs in this study were included in the patients’ health care plans and were equally available to all residents, in accordance with the universal health care coverage provided to residents of italy.information about prescriptions of antiplatelets (atc: b01ac04, b01ac05, b01ac06, b01ac22), β-blockers (atc: c07), acei/arbs (atc: c09), and statins (atc: c10aa) were retrieved for all patients.secondary outcomes were (1) all-cause mortality; (2) first hospital readmission due to major adverse cardiovascular and cerebrovascular events (macce), defined as a composite of death, recurrent myocardial infarction, ischemic stroke, and hospitalization for heart failure; (3) first admission to the emergency department (ed), occurring within a 3-year follow-up period.follow‑up adherence and persistence to polytherapy after the index event, were evaluated by analyzing prescription patterns during the 6- and 12-month following discharge from the index admission.the end of individual follow-up for measuring drug adherence coincided either with the end of 6-month or 12-month follow-up, the date of death or with the date of all-cause hospitalization whichever came first.regarding to the secondary outcomes, patients were followed from the 30th day after hospital discharge (the first 30 days after discharge were used to define exposure) to the occurrence of individual events, censoring at the date of non-cardiovascular mortality or the end of 3-year follow-up for macce admissions, and censoring at the date of all-cause mortality or the end of 3-year follow-up for admissions to the emergency department.of the 28.395 patients discharged from hospital with a first diagnosis of ami between january 1st 2013 and december 31th 2015, 13.540 (48%) met the inclusion criteria and were enrolled in the present study.among the entire study population, 1.101 (8.0%) patients began ih-cr at 33 regional sites, during the 30 days following hospital discharge from the index admission.however, the proportion of patients that have participated in ih-cr program strongly changes according to the subgroup of ami patients identified.participation in ih-cr ranged from 3 to 17%.table 1 lists the main characteristics of ami subpopulations, grouped by ih-cr participation status.ih-cr participants were more frequently male, slightly older, and had a greater burden of comorbidities and interventional procedures.in fact, the participants were more likely to have a history of heart failure, conduction disorders or arrhythmias or other cardiac diseases, were more likely to be seen with cerebrovascular diseases or more likely to be treated with other operations on heart and pericardium, compared with those who did not participate in ih-cr program.however, regarding these percentages, more or less evident differences can be observed across the 4 subgroups of patients.in addition, the median length of ih-cr stay ranged between 20 to 22 days.impact of ih‑cr on adherence to chronic polytherapy the adherence to eb medications by ami subpopulations and ih-cr participation status is reported in table 2.using the logistic models with term-interaction between ih-cr participation and 4 subpopulations of ami patients, the impact of ih-cr participation on adherence to chronic poly-therapy was determined.the “global” interaction term was statistically significant (p value < 0.001), for both 6- and 12-month follow-up.as visualized in fig. 2, the effect of cardiac rehabilitation on adherence to medications differed considerably between subpopulation groups.ih-cr participation was associated with a statistically significant increase in adherence to chronic poly-therapy at 6-month (or 1.63; 95% ci 1.06–2.50; p value: 0.02) for stemi-no-pci patients, and this improvement was also maintained at 12-month follow-up assessment (or 1.63; 95% ci 1.07–2.47; p value: 0.02).a similar but even stronger effect was observed for nstemi-no-pci patients, who attended the ih-cr program compared with non-attenders at 6-month (or 1.85; 95% ci 1.51–2.27; p value < 0.001) and at 12-month follow-up assessment (or 2.13; 95% ci 1.74–2.60; p value: < 0.001).in contrast, the effect of ih-cr intervention on adherence was not observed in stemi-pci group (or 0.90; 95% ci 0.70–1.16; p value: 0.427), (or 0.83; 95% ci 0.65–1.07; p value: 0.157), at 6-and 12-month follow-up, respectively.impact of ih‑cr on the occurrence of secondary outcomes during the 3-year follow-up, there were 1.635 all-cause deaths in our population cohort.the 3-year follow-up rates of these outcomes were 12%, 51%, and 24%, respectively.as regards the poisson regression models, the global interaction term between ih-cr participation and the patient’s subgroup membership was statistically significant (p value < 0.001) for each of the three secondary outcomes.as shown in fig. 3, the participation in cardiac rehabilitation would appear to have a “protective effect” on the occurrence of the adverse secondary outcomes, for three out of the four sub-cohorts identified.specifically, for the nstemi-no-pci group, this protective effect of ih-cr was statistically significant for all three adverse outcomes.surprisingly, in the first group of ami patients (stemi-pci), the effect of ih-cr participation on the occurrence of secondary outcomes switched from protective factor to risk factor.we found for this specific group, significantly higher incidence rates for all secondary outcomes, among patients who participated at ih-cr compared with non-participants.discussion this study provides novel ‘real world’ data on the referral and impact of in-hospital cardiac rehabilitation programs in patients suffering ami.the study results showed that ih-cr participation rates are alarmingly low, ranging from 3 to 17% in relation to the groups of the ami population identified.sometimes, it may be that patients do not receive enough information and encouragement to participate in cr by their physician or other health professionals.interestingly, polytherapy adherence was maintained for stemino- pci participant patients (63%), and even increased for nstemi-no-pci patients at 12-month follow-up, suggesting that patients who could not undergo revascularization by coronary angioplasty may have been more often referred to ih-cr than those who received coronary angioplasty.by contrast, the effect of ih-cr intervention on adherence was not observed in the other two subgroups of ami patients (stemi and nstemi) who underwent pci during the index event.as previously mentioned, the participation in ih-cr had a “protective effect” on the occurrence of these three adverse secondary outcomes, for three out of the four subpopulations identified.the study results showed that ih-cr participation, was associated with 24% of risk reduction of mortality (irr 0.76; 95% ci 0.60–0.95; p value: 0.021), 22% of macce readmission (irr 0.78; 95% ci 0.65–0.94; p value: 0.011), and 20% of ed admission (irr 0.80; 95% ci 0.70–0.91; p value: 0.001).moreover, in absolute terms, patients with a diagnosis of nstemi who did not undergo pci, represented the 50% of all ami patients that have participated in ih-cr program in this research.for the above-mentioned reasons, it is reasonable to expect a strong disparity in the characteristics of patients who did or did not participate in ih-cr even for all variables that cannot be measured.strengths and limitations of the study a strength of the study is the possibility to integrate different health information systems of the lazio region of italy, in order to achieve a well-defined (e.g., chronological, demographical, clinical) healthcare-related patient profile and to involve a large number of patients.ih-cr participation was strongly associated to a significant improvement of adherence to evidence-based therapies both at 6- and 12-month follow-up among ami patients who did not undergo pci during the index hospitalization.these findings highlight the benefits of ih-cr, even in an unfavourable context due to the smallness of referral of ih-cr patients and support the clinical practise guidelines that consider cardiac rehabilitation an integral part in the treatment of coronary artery disease.
number of words= 1629
[{'rouge-1': {'f': 0.3607985870890592, 'p': 0.8573563218390805,'r': 0.2284731058415269}, 'rouge-2': {'f': 0.2289947865413203, 'p': 0.4734582132564842,'r': 0.15101851851851852}, 'rouge-l': {'f': 0.41230390521306914, 'p': 0.7305504587155964,'r': 0.2871945701357466}}]
-----------------------------------------------------------------------------------------------------------------------------------
p89:
Extractive Summary:
the adjusted p value was further  increased after adjusting for age, weight, waist circum- ference, drinking, smoking, hypertension, and diabetes.  as a result, carriers of the rs4808278-a allele have a  higher risk of developing high pp compared those car- rying the rs4808278-tt genotype after adjustment  for confounding factors (or = 0.400; 95% ci: 0.217– 0.737; p = 0.003).no significance was observed for the  remaining polymorphisms under all models (see addi- tional file 2: table s2).haplotype analyses of brd4 and pin1 snps shesis software was used to perform haplotype analy- ses.compared to the control group, no significant dif- ference was found in haplotypes of the pin1 gene in the  case group (see additional file 2: table s3).gmdr analysis of brd4 and pin1 snps and high pp risk the gmdr model was used to investigate the best  interaction combination among the four snps and  smoking on high pp risk.table  3 summarizes the  results obtained from gmdr analysis for gene–gene  and gene-diabetes interactions.the best three-locus  models for gene-diabetes interaction (p = 0.0107) were  rs4808278, rs2287838, and diabetes after adjusting for  the covariates (age, sex, smoking, drinking and bmi),  of which the cross-validation consistency was 9/10.  however, we did not find a significant any-locus model  among the four snps.table 1 comparison of baseline clinical characteristics among groups of normal pulse pressure and high pulse pressure data are expressed as mean and standard deviation (sd).p values were corrected using fdr bmi body mass index, sbp systolic blood pressure, dbp diastolic blood pressure, pp pulse pressure, dr false discovery rate indicators normal pulse pressure group (n = 438) high pulse pressure group (n = 460) p value sex  males 275 243 0.004  females 163 217  age (year) 56.94 ± 13.38 65.01 ± 11.36 < 0.001  height (cm) 164.70 ± 7.51 163.36 ± 7.92 0.008  weight (kg) 64.93 ± 11.58 66.34 ± 11.40 0.065  bmi (kg/m2) 23.88 ± 3.67 24.79 ± 3.42 < 0.001  waist circumference (cm) 84.87 ± 10.23 88.30 ± 9.66 < 0.001  sbp (mmhg) 138.05 ± 19.64 174.93 ± 23.83 < 0.001  dbp (mmhg) 87.69 ± 17.56 93.28 ± 17.31 < 0.001  pp (mmhg) 50.36 ± 8.14 81.65 ± 16.16 < 0.001  obesity (%) 156 (35.62) 244 (53.04) < 0.001  diabetes (%) 103 (23.52) 198 (43.04) < 0.001 smoking (%)  smoker 275 (62.79) 324 (70.43) < 0.001  former‑smokers 118 (26.94) 71 (15.44)  nonsmoker 45 (10.27) 65 (14.13) drinking (%) 63 (14.38) 38 (8.26) 0.005  mapping of associated snps at expression quantitative  trait loci (eqtls) rs4808278 was annotated using the regulomedb, a  database that identifies dna features and regulatory  elements in non-coding regions of the human genome.  known and predicted regulatory dna elements  include dnase hypersensitivity regions, transcription  factor (tf) binding sites, and promoter regions that  have been biochemically characterized to regulate tran- scription [17].therefore, we investigated the function  of rs4808278 in gene expression regulation by exploring  the regulomedb.the regulomedb score of rs4808278  in brd4 was 4, suggesting that rs4808278 may be part  of a tf binding site or dnase hypersensitive region.discussion scholars are paying more attention to heredity as an  important factor in high pulse differential pressure [18,  19].to investigate a potential association between five  promising polymorphisms in the brd4 and pin1 genes  and high pp risk in a southeastern chinese population,  we designed this pilot case–control study that includes  666 hypertensive patients and 232 normotensive sub- jects according to exclusion and inclusion criteria.after  excluding one snp (rs2233679) that was not in hwe,  we analyzed the remaining four snps.brd4, a member of the bromo and extra-terminal fam- ily, plays a role in various vascular pathologies.knocking  down brd4 with sirna mitigates neointima formation  and vascular remodeling [20, 21].the protein also regu- lates vascular endothelial growth factor (vegf)-induced  angiogenesis by activating phosphorylation of vascu- lar endothelial growth factor receptor 2 (vegfr2) and  p21 activated kinase 1 (pak1) [22, 23].to some extent,  these pathologies are related to changes in vascular  elasticity, which may affect pp variation.through bind- ing with acetylated rela, brd4 enhances activation of  nf-κb signaling to induce natriuretic peptide precursor  a (nppa) and natriuretic peptide precursor b (nppb)  transcription [24].our previous research also found  that a snp in the nppb gene is associated with high pp  [14].brd4 plays a critical role in propagating inflam- mation and promoting vascular calcification in particu- lar [7, 8].more importantly, brd4, a transcriptional  regulator, mediates inflammatory transcription, ath- erogenic endothelial response, and atherosclerosis [10,  11].vascular damage, an antecedent to atherosclerosis,  leads to an increase in pp and to atherosclerosis, which  in turn results in large-vessel stiffening and increased  wave reflection.while it is not clear what is the incipi- ent event in this cycle, it is clear that, once initiated, a  vicious cycle promoting disease progression ensues.the  association analysis of pp with different genotypes show  that rs4808278 genotype tt carriers have a lower risk  than at and aa genotypes carriers to have higher pp  (or = 0.405; 95% ci: 0.220–0.745; p = 0.004), which indi- cates that the t allele has a protective effect.although  rs4808278 is located in an intron of brd4, the regu- lomedb score of rs4808278 in brd4 is 4, which suggests  that rs4808278 may be part of a tf binding site or dnase  hypersensitive region.this suggests that rs4808278 may  affect pp by regulating brd4 expression.
number of words= 867
[{'rouge-1': {'f': 0.35539219072463224, 'p': 0.7310878661087865,'r': 0.23475495307612096}, 'rouge-2': {'f': 0.2089094670148644, 'p': 0.37252100840336133,'r': 0.14515657620041755}, 'rouge-l': {'f': 0.323935472041579, 'p': 0.5931788079470199,'r': 0.2228046421663443}}]
-----------------------------------------------------------------------------------------------------------------------------------
p90:
Extractive Summary:
after the treatment of eep, the average length of smcs was significantly in- creased in a dose-dependent manner (table 1), reflecting an obvious relaxation effect.among the fine extracts of eep, a even stronger effect of relaxation was observed in eaep, and a lower extent in wep (table 2 and table 3).instead, the treatments with peep and dep showed no significant difference on smc contraction (table 4 and table 5).to unravel whether eaep played an interactive role with acetylcholine, smcs were treated either with eaep alone, or with two different doses of acetylcholine (10− 5 and 10− 3 m), or with the combination of eaep and acetylcholine (table 6).as is shown in table 6, acetylcho- line strongly promoted contraction, while this effect can be suppressed by eaep (table 6).our data suggested that eaep is the major functional fraction of eep to relax smc in an acetylcholine-dependent pattern.eaep increases the intracellular calcium concentration calcium ion is one of the most important modulators of various enzymes, channel proteins and signaling table 3 the effects of wep on contractile response of gastric smcs wep concentration (mg/ml) smc average length (μm) contraction rate (%) 0 83.47 ± 8.49 0 20 85.24 ± 11.21 −2.12 40 86.53 ± 9.96 −3.66 60 88.28 ± 10.89 −5.76 80 89.62 ± 14.07 −7.36 100 97.14 ± 9.02* −16.38 n = 50, * p < 0.05, data (mean ± sem) were analyzed by anova table 4 the effects of peep on contractile response of gastric smcs peep concentration (mg/ml) smc average length (μm) contraction rate (%) 0 84.93 ± 8.78 0 20 86.41 ± 10.45 −1.75 40 87.31 ± 8.35 −2.81 60 82.87 ± 6.14 2.42 80 85.63 ± 12.01 −0.82 100 90.30 ± 15.28 −6.33 n = 50, not significant, data (mean ± sem) were analyzed by anova su et al. bmc complementary medicine and therapies proteins.as a second messenger, calcium plays multiple physiological roles.intracellular free calcium concentra- tion can reflect the cellular response to external drug and environmental stimulations.in smcs, the upregula- tion of intracellular free calcium concentration leads to cell contraction [15–17].it is speculated whether pcg extract has an impact on intracellular free calcium accu- mulation.to test this hypothesis, eaep was selected for its best performance in promoting contraction (tables 2, 3, 4 and 5).as measured in fig. 2a, intracellular free cal- cium concentration in smcs was sharply enhanced to the peak upon eaep treatment within 1 min (fig. 2a).the effect was shown in a dose-dependent manner within the range of 20–100 mg/ml (fig. 2a).after 3 min, the calcium concentration was gradually decreased to baseline level (fig. 2a).the decrease is probably due to the negative feedback loop to avoid constant stimula- tion in the cell [16].in the next step, we attempted to understand the mechanisms behind eaep-mediated ef- fect on intracellular free calcium stimulation.motilin is one of the most important factors in controlling the inter-digestive migrating contractions [18].to test if the effect of eaep was mediated by motilin, hek293/mlnr knock-in cell line with overexpressed motilin receptor was used for experiments, and wildtype hek293 cell was used as control.in hek293 cells, the effect of eaep was similar to that of smcs in a dose-dependent pattern (fig. 2b-d).surprisingly, almost no effect on intracellular free calcium concentration was observed in hek293/ mlnr cells compared to wildtype hek293 cells upon eaep treatment (fig. 2b-d).this result indicates that the effect of eaep on intracellular free calcium accumu- lation is independent of motilin pathway, but is driven by other mechanisms.eaep inhibits the contraction of colonic smooth muscle strip colonic smooth muscle stripe is a commonly used tissue model for contractility study.a wide concentration range (0, 10, 20, 40, 60, 80, 100 and 200 mg/ml) of differ- ent extracts were prepared and tested for their effects on the contraction of colonic smooth muscle strips.the re- sults demonstrated that eep inhibited strip contractility in a dose-dependent pattern (fig. 3a).among the fine extracts of eep, eaep demonstrated a similar inhibitory effect of eep (fig. 3b).instead, peep and dep did not change the contraction pattern of the strip (fig. 3c and d, respectively).the results indicated that eaep is the main fraction of eep with contraction-inhibitory activity in colonic smooth muscle tissue.this in-vitro tissue model confirms our findings in cultured smcs, that eaep is the key eep fraction to promote relaxation and inhibit contraction, while peep and dep fractions are non-effective.the inhibitory effect of eep is mediated by several mechanisms to understand the underlying mechanisms how eep promotes relaxation and suppresses contraction, eep was tested in several classic modulating pathways of gastrointestinal motility.first, acetylcholine is a neuro- transmitter and activates smooth muscle contraction [19].the addition of eep was found to impair acetyl- choline treatment-induced contraction with an inhib- ition rate of 35.57 ± 3.35% in colonic smooth muscle stripe model (fig. 4a).in comparison, in control group without acetylcholine treatment, the inhibition rate was 61.17 ± 3.32%.the antagonistic effect of eep to acetyl- choline is consistent with the fact that the major effect- ive fraction eaep from eep stimulates intracellular free table 5 the effects of dep on contractile response of gastric smcs dep concentration (mg/ml) smc average length (μm) contraction rate (%) 0 85.18 ± 5.75 0 20 83.59 ± 9.89 1.87 40 90.17 ± 10.86 −5.86 60 82.45 ± 12.99 3.21 80 81.19 ± 11.82 4.68 100 91.60 ± 8.79 −7.54 n = 50, not significant, data (mean ± sem) were analyzed by anova table 6 the interactive effects of eaep and acetylcholine (ach) on contractile response of smcs treatment smc average length (μm) contraction rate (%) control 87.51 ± 6.06 0 eaep 101.04 ± 6.91* −15.47 ach-5 76.01 ± 4.89* 13.14 ach-3 70.66 ± 6.65* 19.25 eaep+ach-5 93.42 ± 13.73 −6.75 eaep+ach-3 89.05 ± 11.38 −1.76 n = 50, * p < 0.05, data (mean ± sem) were analyzed by anova.eaep: 60 mg/ ml; ach-5: 10−5 m; ach-3: 10−3 m su et al. bmc complementary medicine and therapies calcium concentration (fig. 2a-d), as acetylcholine is known to block calcium channel and reduce calcium in- flux [20].second, prostaglandins are a group of regulating lipid compounds with multiple functions including the stimu- lation of smooth muscle contraction [21].in our experiments, no change of contractility was observed in indomethacin pre-treated strips (fig. 4b).subsequent treatment with eep suppressed the contraction with the inhibition rate of 11.24 ± 8.23%, which was significantly lower than the control group (61.17 ± 3.32%).the data indicated an antagonistic effect of eep to indomethacin.third, nitric oxide (no) is the first messenger mol- ecule to promotes smooth muscle relaxation through no-guanylate cyclase (gc)-protein kinase c (pkc) pathway [23].in our experiments, no effect on contract- ility was found in strips pre-treated with methylene blue, a typical gc inhibitor (fig. 4c).the subsequent addition of eep reduced the contraction with the inhibition rate to 33.64 ± 5.54%, comparing with 61.17 ± 3.32% in con- trol group, showing a similar but milder antagonistic ef- fect to indomethacin.to conclude, eep may inhibit smooth muscle contrac- tion through acetylcholine receptor, cox and no-gc- pkc pathways.identification of quercetin as a bioactive substance in pcg extract it is difficult to control the quality of bioactive compo- nents in plant extracts from batch to batch.therefore, identification and characterization of bioactive com- pound is critical in food and drug development.column chromatography, high performance liquid chromatog- raphy (hplc) and thin-layer chromatography (tlc) are common chromatographic techniques to isolate bio- active compounds from plant crude extracts [24, 25].we attempted to isolate bioactive substance from pcg extracts using chromatographic methods.first, column chromatography and tlc were used to isolate crude fractions from eaep which were proved to be active in relaxing smc and colonic smooth muscle strip.eight of 20 fractions were found to be able to induce relaxation (table 7).the active fractions were combined and fur- ther isolated by hplc with the following conditions: dis- tilled water was used as mobile phase solvent a and methanol as mobile phase solvent b. both solvent a fig. 2 stimulation of intracellular free calcium influx upon eaep treatment in smcs (a) or hek293/hek293 mlnr cells (b-d).0, 20, 40, 60, 80 or 100 mg/ml eaep were used to treat smcs (a).20 mg/ml eaep (b), 60 mg/ml eaep (c) or 100 mg/ml eaep (d) were used to treat hek 293 and hek293 mlnr cells.x-axis: time after treatment (s); y-axis: calcium concentration in μm su et al. bmc complementary medicine and therapies and b were degassed for 30 min with ultrasonication.flow speed was kept at 1.0 ml/min.for gradient elution, 10–80% solvent b was used in 0–60 min, and 80–100% solvent b was used in 60–80 min.the results showed three main peaks within 80 min, among which peak 2 is the highest (fig. 5a).sub-fractions between 1 and 25 min (p-1), 25–35 min (p-2) and 35–80 min (p-3) were collected respectively.only p-2 sub-fraction demon- strated bio-activity in strip relaxation (fig. 5b).p-1 and p-3 sub-fractions were inactive (data not shown).fur- thermore, p2- was concentrated and crystallized to obtain the pure compound p-2-1.
number of words= 1482
[{'rouge-1': {'f': 0.2701070215452674, 'p': 0.7352360515021459,'r': 0.16544334975369457}, 'rouge-2': {'f': 0.154010181767948, 'p': 0.3027586206896552,'r': 0.10327171903881702}, 'rouge-l': {'f': 0.2727912703782987, 'p': 0.6012500000000001,'r': 0.17641627543035993}}]
-----------------------------------------------------------------------------------------------------------------------------------
p91:
Extractive Summary:
background with the popularization of chinese traditional medicine (ctm), ctm has begun to be used worldwide.in ctm, prescriptions are mixtures that contain at least two types of herbs.herb-herb interactions or herb-drug interactions are important factors that affect the pharmacokinetics and metabolism of drugs and even cause toxicity.cytochrome p450 enzymes (cyp450s) are a family of heme-containing proteins that play important roles in the phasei metabolism of most clinical drugs in the liver and intestine.the activities of cyp450s directly affect the biotransformation and metabolism of various drugs.for example, puerarin is widely used in the treatment of cardiovascular diseases and diabetes, and it can inhibit the activity of cyp3a4.several previous studies demonstrated that the inhibitory effect of puerarin on the activity of cyp3a4 resulted in adverse effects on the pharmacokinetics of various drugs, such as triptolide, edaravone, and astragaloside iv [1–3].cornin was also reported to significantly decrease blood pressure, reverse cardiac hypertrophy, and improve heart function [5, 6].previous studies have mainly focused on the pharmacodynamic effects of cornin, and whether cornin can affect the metabolism of co-administered drugs by regulating metabolic enzymes is still unknown.cyp1a2, 2a6, 3a4, 2c8, 2c9, 2c19, 2d6, and 2e1 are major cyp450 isoforms, that are responsible for the metabolism of most clinical drugs.results effects of cornin on the activities of cyp450s as shown in fig. 2a, the activities of cyp3a4, 2c9, and 2e1 were significantly inhibited by cornin compared with the blank control (p < 0.05), while other cyp isoforms were not affected by cornin (p > 0.001).model of cyp3a4, 2c9, and 2e1 inhibition by cornin with the help of lineweaver-burk plots of inhibitory kinetic data, the inhibition of cyp3a4 by cornin was best fitted in a noncompetitive manner (fig. 3a).the co-administration of various drugs leads to adverse reactions, such as toxicity and treatment failure.cornin is one of the most widely used herbs in cardiology and exerts the effects of decreasing blood pressure, reversing cardiac hypertrophy, and improving heart function [5, 6].it was found that cornin significantly inhibited the activities of cyp3a4, 2c9, and 2e1, which are responsible for the metabolism of the vast majority of drugs [13].cyp2c9 and 2e1 are involved in the metabolism of a large number of drugs, thuscontributing to the wide variability in pharmacokinetics in the metabolism of drugs [17, 18].for example, the metabolism of warfarin was inhibited by cannabis due to the inhibition of cyp2c9 by cannabis [19].the lack of in-vivo pharmacokinetic data on cornin is another limitation of this study, and such data could help assess the clinical significance of the obtained ic50 values.the in-vivo interaction and potential drug-drug interaction need to be verified by additional in-vivo experiments.conclusion in-vitro findings in this study indicated an inhibitory effect of cornin on the activities of cyp3a4, 2c9, and 2e1.cornin was identified as a noncompetitive inhibitor of cyp3a4 and a competitive inhibitor of cyp2c9 and 2e1.these results suggested the potential drug-drug interaction between cornin and cyp3a4, 2c9, and 2e1 substrates in the clinically co-administrated prescriptions, and these results require further in-vivo validation in future investigatio
number of words= 499
[{'rouge-1': {'f': 0.4684417205342673, 'p': 0.7173214285714287,'r': 0.3477777777777778}, 'rouge-2': {'f': 0.269446458093126, 'p': 0.38838565022421523,'r': 0.2062763915547025}, 'rouge-l': {'f': 0.41033250329573756, 'p': 0.546923076923077,'r': 0.32833333333333337}}]
-----------------------------------------------------------------------------------------------------------------------------------
p92:
Extractive Summary:
helium was used as the carrier gas at the flow rate of 1.487 ml/min with initial nominal pressure of 1.4902 psi and an average velocity of 44.22 cm/sec.the oven temperature was initially programmed at 40 °c for 1 min then ramped at 12°c / min to 300°c for 10min.run time was 32.667 min with a hold time of 5 °c / min.identification of the chromatographic peaks was based on comparisons of their relative retention times and mass spectra with those obtained from the nist14.l library data.statistical analysis all assays were carried out in three replicates and results are expressed as mean ± standard error of mean (sem).ic50 values were obtained by interpolations from standard curves.data were analyzed using graph pad prism 8.results isolation of endophytic fungi eighteen pure fungal endophytes were successfully isolated from surface sterilized fresh leaf samples of eight medicinal plants: acalypha ornata, albizia zygia, alchornea cordifolia, chrysophyllum albidum, ficus exasperata, gomphrena celosioides, millettia thonningii, and newbouldia laevis (fig. 1).this was followed by the leaves of c. albidum with three isolates: ca 041, ca 042, and ca 043 and f. exasperata with three isolates: fe 081, fe 082, and fe 083 (table 1).two endophytic fungal isolates were obtained from the leaves of a. ornata (cu 061 and cu 062); m. thonningii (mo 211 and mo 212); n. laevis (na 021 and na 022).a. cordifolia and g. celosioides leaf samples hosted one fungal isolate lo 261 and ge 091 respectively (table 1).pictorial representation of isolated endophytic fungi is as shown in fig. 2.acid, 3-oxo-, methyl ester); sterols (campesterol; stigmasterol; and sitosterol); and fatty acids and its amide derivative (methyl stearate; oleic acid; linoleic acid; palmitic acid; and oleamide).figure 9 shows the chemical structures of pyrogallol (phenolic), lupeol (terpenoid), and β-sitosterol (sterol) as representatives of the major classes of secondary metabolites as well as oleamide (amide derivative of fatty acid) and α-tocospiro a (tocopherol) present in the investigated endophytic fungi extracts.fourteen compounds (0.06 to 19.84% percent composition) were identified in fungal extract mo211.pyrogallol was recorded as the most abundant compound with percent composition of 19.84% while dl-alpha-tocopherol (0.06%) was the least identified compound.a total of twelve compounds were identified in za163 fungal extract, pyrogallol (10.72%) was the most abundant compound while dl-alpha-tocopherol (0.15%) was least identified.this was followed by fungal extract fe 082 with eleven identified compounds.alpha tocospiro (4.92%) appeared as the most abundant while linoleic acid (0.07%) was the least identified compound.fungal extracts fe 084 and lo 261 recorded six compounds each with pyrogallol (31.33%) and 9-octadecenamide (11.04%) recorded as most abundant compounds and 9-octadecenoic acid (z)-, methyl ester (0.62%) and linoleic acid (0.05%) as least identified compounds respectively (table 3).discussion in this study, eighteen endophytic fungi were successfully isolated from surface sterilized fresh leaf samples of eight medicinal plants.to the best of our knowledge, this is the first report describing the isolation of endophytic fungi residing in the leaf cells of the selected plant samples.after size separation by agarose gel electrophoresis of the its pcr products, the pcr products of the plant samples exhibited dna band size range of 400–700 bp.detailed taxonomic assignment of fungal isolates remain unresolved due to limitations inherent in fungal its sequences.the results obtained from this study thus confirmed that leaves of the medicinal plants: acalypha ornata, albizia zygia, alchornea cordifolia chrysophyllum albidum, ficus exasperata, gomphrena celosioides, millettia thonningii, and newbouldia laevis are host to endophytic fungi.the crude fungal extracts showed vary degrees of antioxidant activity in the dpph radical scavenging and reduction of ferric ion assays.dpph (2, 2-diphenyl-1- picryl-hydrazyl) is a stable free radical that produces purple color in methanol.antioxidant activity of the fungal extracts was measured by discoloration to yellow color following the formation of non-radical (2,2-diphenyl- 1-hydrazine) molecule [17].fungal extracts za163 and mo211 exhibited significant (p < 0.05) dpph radical scavenging activity.the metal chelating capacity of a compound may also serve as a significant indicator of its potential antioxidant activity [35].the iron chelating activity of all fungal extracts was determined by reaction with ortho-phenanthroline.fe3+ is reduced to fe2+ by an antioxidant and the formed fe2+ rapidly react with phenanthroline to form a stable red orange colored complex [33, 36].isolated fungal extracts za 163, lo 261, ca 041, ca 042, ca 043, fe 082, fe 084, and ge 091 from a. zygia, a. cordifolia, c. albidum, f. exasperata, and g. celosioides medicinal plants demonstrated iron chelating capability.similar observations of antioxidant activity of endophytic fungi isolates have been reported in some medicinal plants such as bauhinia racemosa, distylium chinense, euphorbia hirta, guazoma tomentosa, phoenix dactylifer, phyllanthus amarus, and senna spectabilis [4, 37–41].our results suggest that endophytic fungi that reside in the leaves of a. ornata, a. zygia, a. cordifolia, c. albidum, f. exasperata, g. celosioides, m. thonningii, and n. laevis showed promising antioxidant activity.fungal extracts za163, mo211, lo 261, fe 082, and fe 084 were among the fungal extracts that exhibited effective antioxidant activity with a substantial yield of extract, thus were selected for phytochemical analysis via gc/ms analytical technique.the identified compounds represented phenolic, terpenoid, and sterol classes of secondary metabolites as well as tocopherols and fatty acids.most of these chemical constituents have been reported to demonstrate remarkable antioxidant activity.phenolic compounds act as natural antioxidants that exert therapeutic effects such as anti-inflammatory, antidiabetic, antimicrobial, antiviral and vasodilatory effects and prevent various forms of diseases [42–44].tocopherol plays an important role as a lipid antioxidant in stabilizing subcellular membranes [45].fatty acids are able to reduce oxidative stress from free radicals by exerting an antioxidant role [46, 47].pyrogallol (phenol), alpha tocospiro (tocopherol), and oleamide (amide derivative of fatty acid) are present as the major components in the investigated endophytic fungi extracts.previous studies have reported the isolation and characterization of graphislactone a, a phenolic benzopyranone antioxidant compound from the endophytic fungus cephalosporium sp. inhabiting the medicinal plant trachelospermum jasminoides [11].similarly, isolation and characterization of pestacin and isopestacin, a coumarone antioxidant and antifungal agents from the endophytic fungus pestalotiopsis microspora that resides in the medicinal plant terminalia morobensis was reported in literature [12, 13].these results suggest that these compounds and others identified in fungal extracts za163 from albizia zygia, mo211 from millettia thonningii, lo261 fromalchornea cordifolia, fe 082 and fe 084 from ficus exasperata could be responsible for the antioxidant activity demonstrated by the endophytes.conclusion the results obtained from this study established that endophytic fungi isolated from medicinal plants: acalypha ornata, albizia zygia, alchornea cordifolia, chrysophyllum albidum, ficus exasperata, gomphrena celosioides, millettia thonningii, and newbouldia laevis commonly used in nigeria local herbal remedies may be a potential source of bioactive compounds, which may be used for the development of antioxidant drugs.
number of words= 1093
[{'rouge-1': {'f': 0.37817365380334317, 'p': 0.7167065868263474,'r': 0.2568512110726644}, 'rouge-2': {'f': 0.21593406770843587, 'p': 0.3612912912912913,'r': 0.153982683982684}, 'rouge-l': {'f': 0.38145923392098635, 'p': 0.5913270142180096,'r': 0.2815384615384615}}]
-----------------------------------------------------------------------------------------------------------------------------------
p93:
Extractive Summary:
background naturopathy is a philosophically-defined, distinct system of traditional and complementary medicine (t&cm), rooted in traditional european and north american practices and recognized by the world health organization (who) [1].naturopaths are not defined by their therapeutic tools, but provide primary contact care in accordance with underlying principles: first do no harm (primum non nocere); healing power of nature (vis medicatrix naturae); treat the cause (tolle causam); treat the whole person (tolle totum); doctor as teacher (docere); disease prevention and health promotion; wellness [2].consumers globally are increasingly consulting naturopathic practitioners to enhance well-being, for common health complaints, and serious chronic conditions [3], with exponential growth in use and expenditure over the past 25 years [4, 5].the recent australian prevalence rate for naturopathic consultations was 6.2% [6], and in switzerland, the consultation prevalence rate was similar 7.7% (7.2–8.2), with use markedly increasing when patients had access to health insurance rebates [7].naturopaths constitute an appreciable part of the health care sector [8], and are globally regulated as naturopathic technicians, licensed naturopaths and naturopathic doctors (nd) [9].the world naturopathic federation (wnf) estimates 75,000–100,000 naturopathic practitioners practice in 81 countries, with over 90 educational institutions providing naturopathic training [10], and 20 naturopathic research institutions globally [11].naturopathic education and regulation have been examined primarily in single country studies.these studies have examined the challenges of implementing accreditation and regulation [12], descriptive studies of accreditation standards and training competencies [13, 14], descriptive studies on the scope of naturopathic regulation in individual countries [15–17] and comparison of regulation between one country and another [18, 19] as well as comparisons between naturopathic regulation and regulation of other professions in single countries [20].there have also been studies of the various limitations of existing regulatory models in south africa [21] and australia [22], as well as examination of the impact of disharmonious regulation across the european union [23, 24].despite increasing public use, a significant naturopathic workforce, and calls by the who for national policy development for integration of services, existent frameworks as potential barriers to integration have not been examined at an international level.in response to this significant research gap, this paper reports findings from a first global examination of regulation, education, and practice frameworks that may impact naturopathic professional formation and integration into health care systems.closer examination of the interface between country, regulation, and education frameworks should be considered in future research.regulation diverse regulation is a barrier to consistent education, professional frameworks, and integration our study demonstrated significant heterogeneity and wide-ranging methods of global naturopathic regulation, which may act as a barrier to development of consistent global educational and professional frameworks, and integration.broad terminology (e.g. ‘natural medicine’) or more narrow definitions based on specific therapeutic tools (e.g. ‘herbal medicine’) may be used by governments, as this is often easier to practically implement than regulation of a complex system of traditional medicine.this would explain variations within the who 2019 t&cm report, which reported population use of naturopathy in 98 of 133 countries (73.6%), yet practicing naturopaths in 35 countries (26.3%) [25], and differences with wnf reported presence of naturopathic practitioners in 81 countries [9].the recommendation by the who that national policy development of t&cm practitioners and practices is the responsibility of member states [26], without provision for clear guidance, potentially underpins this issue.an alternative explanation for the results, is habituated denial of naturopathic practitioners’ contribution to healthcare, an exclusionary tactic reported by brazil in our study, but one that has also been reported by other professions [27].inconsistent or delayed regulation of t&cm professions may potentially be less about risk management – control of sub-standard education, practice standards, and public safety, and more about national politics and established state-profession relations – and may reflect professional tensions between biomedicine and complementary medicine, as reported elsewhere [21, 28–30].prejudice against professionalization of t&cm practitioners by established health profession undertaking boundary construction, jurisdictional protection, and expansion of practices, supported by enduring structural arrangements, is well documented in the literature in many of the countries that have been part of this study.in south africa, for example, the medical association of south africa in 1953 declared naturopathy unscientific, and through code of ethics, prohibited collaboration between doctors and naturopathic practitioners, which has been reinforced through regulatory acts [21].inconsistent and delayed regulation by the state enables continued biomedical hegemony and economic self-interest, marginalization of naturopathic practitioners and consumers, and negates broader interests of society.our results confirm and extend findings from the pan- european cambrella project [19, 23], which found eu regulation directing member states to develop t&cm health policy, resulted in diverse regulation that was non-conformant with eu cross-border healthcare directive and out of step with current theory of risk management, risk regulation and patient safety.in this study, patient’s seeking t&cm across borders encountered different training of identical t&cm practitioners, and different reimbursement systems.diverse regulation was found to be a barrier to consistent delivery of t&cm treatment and research [23, 31].our study suggests heterogeneity extends to include inconsistent global delivery of education, development of professional frameworks, and integration.integration of naturopathic medicine into health care systems we found limited reporting of integration of naturopathic services in healthcare systems in 29 countries, although there are other examples of state-funded naturopathic services.in india, the separate ministry of ayush (abbreviation for ayurveda, yoga and naturopathy, unani, siddha and homeopathy) is responsible for mainstreaming this range of t&cm medical systems, and in 2014, forty-two ayush hospitals providing 1107 beds, were reported to provide yoga and naturopathy services [32].in our study, access to private health insurance, government funded public health and medical integration were more frequently reported where ‘title protection’ forms of regulation existed.full access to government funded public health, was only reported in usa (and then only in some states), and full medical integration only reported in congo dr, zambia, and switzerland.lack of integration may create additional difficulties for patients who wish to use naturopathy in addition to other services.individuals with chronic health conditions use t&cm care regularly and frequently for condition management, and limited system interface may mean consumers have to manage and negotiate parallel systems, determining interpretation, and sharing of health information between healthcare providers [33].limited medical integration of naturopathic medicine reported in our study is consistent with findings in chiropractic, where chiropractic integration was limited to several hospital outpatient settings [34].integration of naturopathy into health systems requires political will, potentially driven by a demanding public, as demonstrated by switzerland constitutional referendum in 2009 that resulted in inclusion of complementary medicine in article 118a of the switzerland constitution [35] and in 2017, inclusion of some t&cm in mandatory health insurance when delivered by dual trained conventional doctors [36].consistent with our findings, the last major australian (victorian) government review into the regulatory and legislative requirements for naturopathy, found lack of direct legislation counter to development of consistent standards of education and professional standards, resulting in difficulties in enforcing and sustaining minimum standards of training and practice [37].we argue diverse regulation, and subsequent variation in education and professional frameworks, may not support who t&cm (2014–2023) strategic objectives to: build a knowledge base to support evidence-based practice; strengthen safety, quality and effectiveness; and promote universal health coverage [26].we propose that growth in naturopathic educational programs in the past 25 years, in an environment of limited or inconsistent regulation, with absence of consistent frameworks and minimal who training guidelines, has fostered differences in global naturopathic education and hindered professional formation.moreover, we propose that although naturopathic practice is impacted by varying regulatory dynamics and structural and cultural differences, consistency in education frameworks operationalized within and between countries, will assist professional formation, integration, practitioner transportability and user access.education and practice standards primary contact care demands higher education training in our study, global naturopathic training varied between two to four years in length, with a presence primarily in the higher education sector, although vocational qualifications were reported by educational institutions from belgium, czech republic, france, italy, nepal, slovenia, uruguay, and venezuela.almost two-thirds of educational institutions, mostly private, reported delivery of bachelor’s or postgraduate qualifications via a national qualifications’ framework with almost half delivering four-year programs.a previous study by mccabe found that the degree-level of training is the minimum preparation required for independent, primary contact practice in naturopathy, which requires a culture of enquiry, interaction with other health professionals, and understanding of practice limitations [12].this is consistent with other t&cm professions providing primary-pointof- care practice such as chiropractic [20], and primary care nurse practitioners in private practice [38].on this basis the who naturopathic training guidelines should be considered only the minimum requirement for entrylevel technician roles [1].voluntary organizations lack legal mandate, have potentially inadequate regulatory frameworks, and conflicting interests educational institutions in our study were reported to have significant influence on faculty characteristics and programs, with accreditation bodies influential where there was regulation, and government departments and professional associations where programs were delivered on a national qualifications’ framework.where naturopathic education was delivered outside these parameters, professional associations provided limited oversight based on organizational standards.however, voluntary professional associations may not be equipped for this role, and studies in australia and the united kingdom have shown them to be insufficient in this role as it relates to naturopathy [37, 39].attempts to complement professional associations with voluntary registers has also been largely ineffective [40, 41].in our study, presence of unified national representation of the naturopathic profession was not ubiquitous, and multiple competing national and mixed associations – with differing views on standards and professional priorities – were a reported obstacle to advancement of naturopathic education and the naturopathic profession.professional unity (or lack thereof) was perceived to impact regulation in some unregulated jurisdictions.this finding supports previous qualitative examination of australian naturopaths where factional agendas and conflicts within competing associations was found to delay registration, higher education standards, as well as other professional development activities [42].intraprofessional conflict has been a previously reported barrier to professionalization in portugal, where lack of cohesion and infighting between t&cm groups delayed regulation of naturopathy and other disciplines by 10 years [19] and in ontario, where only those t&cm groups with good internal communication, organizational infrastructures, and fit with the healthcare system, achieved regulation [43].development of global education standards and accreditation agencies a priority in our study naturopathic programs were primarily based on local education standards.we found limited evidence, other than cnme accredited programs in north america, of consistent education frameworks and attempts for harmonization of programs across countries.this is in stark contrast to other t&cm professions such as chiropractic which has harmonized education frameworks across world regions [44].chiropractic established an international umbrella council, council on chiropractic international (cce-international) with representatives from four regional councils on chiropractic education (cce) [australasia, canada, europe and usa] all contributing to development of the accreditation standards for the cceinternational [44].cce-international is the body recognized by who as the source for evaluation of chiropractic education, and guides education development globally with accreditation of educational programs undertaken by four regional accreditation bodies [44].the naturopathic profession has not developed an equivalent body, but the development of the world naturopathic federation may provide an avenue for organizations such as who, to provide a basis for development of global standards potentially for different forms of practice.existing regional standards and competencies for the spectrum of regulated naturopathic disciplines - naturopathic doctor, naturopath and naturopath technician [9], could form the basis of international standards and competencies, indirectly supporting national regulation and naturopathic integration into health systems.our study indicated that much of this work is quite advanced at a national level in australia, brazil, canada, switzerland, and the united states, which could provide a basis for development of global naturopathic standards and competencies for alignment by world regions.limitations findings from this study should be viewed within the context of its limitations.due to the nature of the survey, and overlapping regulatory groups, analysis is largely descriptive.as the survey was limited to the english language there may have been some cultural or linguistic differences in interpretation, reporting and participation.additionally, not every country was represented as educational institution participation was limited to 16 of 25 countries, professional associations to 25 of 44 countries, and regulatory boards to 5 of 7 countries.some countries were not represented by both educational institutions and professional associations, limiting ability to examine education frameworks and accountability mechanisms relative to professional frameworks.the extended timeframe of this survey and the evolving nature of jurisdiction may mean data collected early may not be directly comparable with data collected near the end of the project.this study was also limited to organizations self-reporting and did not examine policy, nor organizational documents for concordance between what was described and praxis, all of which may warrant further more detailed examination.nevertheless, the results of this study form the most comprehensive global examination of the education, regulatory landscape of naturopathic practice to date, and as such offer valuable insights to the naturopathic profession, policy-makers, and other key professional stakeholder groups.conclusion education and regulation of the naturopathic profession has significant heterogeneity, even in the face of global calls for consistent regulation that recognizes naturopathy as a medical system.standards are highest and consistency more apparent in countries with regulatory frameworks.consistent regulation, formalized education frameworks and practice standards, could potentially support increased professional formation, integration of naturopaths into health systems and increased access to naturopathic care by the publ
number of words= 2195
[{'rouge-1': {'f': 0.26118858876933493, 'p': 0.8298425196850394,'r': 0.1549845882870982}, 'rouge-2': {'f': 0.183196919538544, 'p': 0.46525691699604743,'r': 0.11405286343612336}, 'rouge-l': {'f': 0.3132285523819291, 'p': 0.7288235294117646,'r': 0.19947976878612717}}]
-----------------------------------------------------------------------------------------------------------------------------------
p94:
Extractive Summary:
background castration-resistant prostate cancer (crpc), which develops from prostate cancer but is resistant to androgendeprivation therapy [1], is the leading cause of death in prostate cancer [2, 3].the high mortality rate of crpc is mainly due to metastases to the bone and/or brain [4, 5].currently, mainstay treatments for crpc include surgery, chemotherapy, radiotherapy, and immunotherapy [1, 6].among them, chemotherapy is the first choice for oncologists in terms of treating metastatic crpc.the majority of approved chemotherapeutic drugs kill cancer cells by introducing a dna damage response [7].cancer cells are endowed with a similar or even stronger innate dna-repair capacity compared to that of normal cells [8], which can limit the effectiveness of approved agents in treating cancer and/or may lead to chemotherapy failure [9].with these concerns, an inexpensive reagent that enhances dna damage and inhibits dna repair may have a potential advantage as a crpc therapeutic drug, as identification of drugs that target dna damage with low side effects in normal cells remains a challenge in crpc chemotherapy.piperlongumine (pl) is a natural antibacterial compound that is readily available, is inexpensive, and has long been used in chinese herbal and indian ayurvedic medicine [10].recently, pl has received increased attention from researchers due to its anticancer effects [11, 12].mechanistic investigations have revealed that pl achieves anticancer effects via a reactive oxygen species (ros)-dependent pathway [13] and modulates related signaling pathways, including mapk, nf-κb, and stat3 pathways [14–20].in the last decade, pl has been demonstrated to exhibit anticancer efficacies as well as sensitize the anticancer activities of chemotherapeutic drugs (e.g., doxorubicin) against prostate cancer cells [14, 21, 22].hence, our present study investigated and identified the effects of pl on dna damage and repair in crpc pc3 and du145 cells, which are derived from bone and brain metastasis of crpc.methods cell cultures and reagents crpc cells (pc3 and du145), the human normal prostatic stromal myofibroblast cell line (wpmy-1), and the human normal hepatic cell line (lo2) were purchased from the cell resource center of peking union medical college.cells were maintained at 37 °c with 5% co2 in dulbecco’s modified eagle’s medium (dmem; gibco) that was supplemented with 10% fetal calf serum (ppa-ge, marlborough, ma), as well as 100 u/ml of penicillin and streptomycin (hyclone-ge, marlborough, ma).n-acetyl- l-cysteine (nac) was purchased from beyotime biotech (china).pl was obtained from sigma-aldrich (item number: sml0221), with a purity of ≥97%.determination of cell viability the cell viabilities of pc3, du145, wpmy-1, and lo2 cells were assessed via 3-(4,5-dimethylthiazol-2-yl)-2,5- diphenyltetrazolium bromide (mtt) assays [23].cells (4 × 103 cells/well in 96-well plates) were incubated at 37 °c with or without pl treatment for 48 h, after which mtt (0.5 mg/ml) was added at 20 μl/well for another 4 h. the reaction product, formazan, was dissolved in 100 μl of dmso after discarding the culture medium.cell viability was determined by reading the absorbance at 560 nm by a spectrophotometer (dtx880, beckman coulter, ca, usa).cell adhesion assays cell adhesion assays were performed as described previously [24].a 96-well plate was coated with 50 μl of human fibronectin (2.5 μg/ml) in 1 × pbs (millipore, ca) at 4 °c overnight.cells were seeded into a 96-well plate at a density of 4 × 104 cells/well and were cultured for another 1 h at 37 °c in an incubator with 5% co2.cells were then rinsed three times with 10% formalin and stained with crystal violet for 5 min at room temperature.after three washes with double-distilled h2o (ddh2o), stained cells were dissolved in 100 μl of acetic acid (33%).the absorbance at 560 nm was detected by a synergy h1 multi- mode reader (biotek).the relative number of cells attached to the extracellular matrix was calculated using the following equation: mean optical density (od) of treated cells/mean od of control cells.cells treated with vehicle (0.01% dmso) were used as a control.transwell assays transwell assays were performed as described previously [24] and carried out according to a purchased transwell kit (corning costar, ny) following the manufacturer’s instructions.briefly, cells were pretreated with different concentrations of pl for 48 h and were then reseeded into transwell permeable support (insert) pre-equilibrated with serum-free dmem medium.for each group, 1 × 105 cells/ insert were seeded and incubated in 100 μl of serum-free dmem medium.the insert was then placed in a 24-well plate containing 600 μl of dmem medium with 10% fbs.after 24 h of culturing, cells on the upper surface of the insert were removed with cotton-tipped swabs.then, the cells on the backside surface of the insert were fixed with 10% formalin, stained with crystal violet for 5 min at room temperature, and washed three times with ddh2o.in our present study, we demonstrated that pl-treated crpc cells exhibited decreased adhesion to the extracellular matrix (fig. 1a and b).accordingly, a decreased migration speed (fig. 1c-f) and an attenuated ability to traverse a membrane from the serum-free to serum side (10% fbs) in the transwell assay (fig. 1g and h) were observed in pl-treated crpc cells.fak, a non-receptor protein tyrosine kinase, is overexpressed and activated in a majority of cancers, including prostate cancer.fak localizes to focal adhesions and it is activated by extracellular signals including integrin-mediated adhesion, which further leads phosphorylation of tyr397 and activate downstream signaling pathways, contributes to the metastasis and invasion of prostate cancer cells [40].our study showed that pl inhibited the expression and distribution of fak at the leading edge of cells as well as inhibited phosphorylation of tyr397, and pl-treated cells presented a decreased cell-spreading area in a concentration-dependent manner (fig. 2 and supplementary figure 3).given the importance of f-actin and fak in cell adhesion and migration, we propose that pl inhibited the adhesion and migration of crpc cells by suppressing the expression and distribution of fak in crpc cells.therefore, fak is a potential effective target to inhibit migration of prostate cancer.previous studies have demonstrated that pl exhibited potent anticancer effects by activating ros [13, 41] and high ros levels induce substantial dna damage [37].since the integrity of dna is crucial to cell viability and metastasis of cancer cells, sufficient dna damage can cause cell-cycle arrest and cell death [7].the results of our present study indicated that pl suppressed crpc cell proliferation and induced cell death (fig. 3) at low concentrations (i.e., 1.0, 2.0, and 4.0 μm) by provoking intense dna damage and inducing a strong ddr and inhibiting the dna repair process (fig. 4, supplementary figure 7 and supplementary figure 8).furthermore, we found that the different cell fates of pl-treated pc3 (senescence) and du145 (apoptosis) cells may be the result of multiple factors, including a differential p53 status between these cell types (fig. 3k), as well as differential changes in pl-induced protein expression and apoptosis (fig. 3c and d).taken together, these data suggest that pl might achieve anticancer effects through ros-mediated dna damage and inhibition of repair pathways in a p53-independent manner.therefore, the pl-induced fate of these dna lesions in crpc cells is worthy of further investigation.cancer cells have a high tolerance to genotoxicity via dna repair and/or reversal of epigenetic defects [7].in addition, an imbalance in redox equilibrium may cause serious dna damage, and fak inhibition can also result in persistent dna damage [36, 37].as expected, our present results demonstrated that pl induced substantial dna damage in crpc cells (figs. 5 and 6), which may have been attributed to a substantial imbalance in redox equilibrium (supplementary figure 4) and fak inhibition (fig. 2 and supplementary figure 3).in addition, dna damage repair approach including hr and nhej for dsb were impeded in pl treated pc3 and du145 cells (supplementary figure 8, supplementary figure 11a and b).as a result, substantial dna damage and ddr accumulation-initiated inhibition of cell migration (fig. 1) and a cell death response (fig. 3).conclusions in conclusion, our present results found that pl efficiently inhibited the migration and proliferation of crpc cells and induced cell death in a concentrationdependent manner.moreover, the results indicated that pl treatment would regulate the expression and distribution of fak and the intracellular ros levels.
number of words= 1319
[{'rouge-1': {'f': 0.33557330836430593, 'p': 0.7756737588652483,'r': 0.2140984793627806}, 'rouge-2': {'f': 0.20971830894375373, 'p': 0.41519572953736655,'r': 0.14028985507246378}, 'rouge-l': {'f': 0.29465614151290787, 'p': 0.5835135135135134,'r': 0.1970903010033445}}]
-----------------------------------------------------------------------------------------------------------------------------------
p95:
Extractive Summary:
mette (id19) needed no information about cam when her child was sick.she continued: at the hospital, we were told not to google, not to choose cam, and that is what counts.however, if she had needed such information she would like the information to be available at the hospital.she would welcome cam treatment there, such as treatment for nausea, pains, and adverse effects.tom (id11) and trude (id3) wished for controlled, scientific information about cam preferably from the children’s cancer society (for example a link) or from health care providers.tom would prefer quality information, which needed to be objective, and based on research and experience.bente (id8) said that the children’s cancer society could have a link to a page containing objective information.ingrid (id16) was not negative to cam.nevertheless, she was skeptical.sondre, her husband, failed to see the need for offering information about cam in general.type of information marianne (id2) and anne (id1) would want information on whether cam might cure cancer, improve the immune system, information about the effect of cam on the cancer itself (in the absence of conventional treatment) and whether it can increase the chances of survival.anne (id1) thought the information about cam should be objective and focus on reducing adverse effects and information about possible negative interactions with conventional treatment.nora (id22) would welcome further research on cam and cancer and a possible modality with less late effects compared to today’s conventional treatment.children have a developing body, and cancer treatment have more or less strong adverse effects.several of the participants were therefore concerned about late effects of the cancer treatment and would appreciate further research on late effects (id4, id10, id22).jane would like information of long-term effects of cancer treatment such as harvesting of ovarian eggs, and why influenza vaccines are important.several of the participants wanted information about how to increase quality of life.ella (id20) demanded information about things that can make daily life easier for the children.marianne (id2) wanted information about how to increase quality of life, including how to support the family, how to support children who are afraid, and what to do for yourself when you get anxious.she claimed that parents need support to find out about all of this.jane (id12) explained: when your child gets cancer, you don’t know what to ask.therefore, she would want a folder containing information about cam in general, and more specifically about d-vitamins.ella (id20) would also welcome information about which vitamins and minerals that can be used.many doctors advised the parents not to give antioxidants to their children to strengthen their immune system.the rationale was that when suffering from cancer, herbs or supplements that increase the count of white blood cells should not be given, as this count is already too high.many parents found this information important, and they thought that this information should be spread to others parents.sofie (id4) would want information about reflexology and nutritional needs.mette (id19) thought that great caution should be exercised when giving cam to children with cancer.it must be safe to use, mostly to provide relief rather than an additional burden, and approved by conventional doctors, she said.essentially, she was open to cam, but when it came to her own child, she needed to be absolutely sure that it would not hurt, and that it would help.discussion many of the participants (n = 11, 46%) in this study used cam for themselves, and when they disclosed their children’s use of cam, the oncologists were mostly positive.however, some were very skeptical and recommended against such use.the reason for withholding disclosure was that the oncologist did not ask.the participants did not receive any information about cam at the hospital, which they would have appreciated.instead, they were advised not to use cam and not to take into consideration well intended advice from family and friends.the majority of the participants in this study received advice about cam from lay people.many of these recommendations were rejected, as they were not in line with the parents’ health values.the majority of the participants (n = 21, 95%) wanted evidence-based information about cam from authoritative sources such as health care providers at the hospital or from the children’s cancer society.they wanted information about vitamins and supplements, how to reduce adverse effects of chemotherapy, improve the immune system, and fight the cancer.moreover, they wanted information about how to support the families, how to cope with life, and reduce anxiety for themselves and for their children.use of cam the national research center in complementary and alternative medicine (nafkam) study from 2016 [29] found that 5% in an unselected norwegian population visited a cam provider for their children’s health problems during a year.about 15.7% of the parents gave their children supplements, herbs, or natural health products.only a handful of the children used self-help techniques.a norwegian study from 2007 [30], reported that parents were very restrictive about giving supplements and natural remedies to their children with cancer, but more positive regarding their own use.a study from the oslo university hospital showed that 53% of the patients used cam products to strengthen the immune system of the child.only 18% thought that such products would contribute to the inhibition or cure of the cancer, whereas 62% did not know [28].another norwegian study [31] demonstrated that cam treatment was provided either simultaneously with the hospital treatment or when the patients were referred to palliative care.moreover, the parents’ motivation for cam use seemed to be focused on the basic need of actively taking part in saving their children’s lives, knowing they had tried everything [31].advice about cam the majority of the participants (n = 13, 54%) in our study received well-intended advice about cam from family and friends, which was mostly perceived as a burden and therefore rejected.however, according to gilmour [32], the reason for using family and friends as the main source of information was that they did not receive cam information from the oncologists.fernandez et al. [33] also reported that parents received information on cam from families and friends.this is in accordance with gozum et al. [34] who reported that most parents in turkey learn about cam from friends and relatives or other families with children who have cancer.studies about communication of cam parents and patients with cancer highly value the input from physicians about cam [30, 35, 36].ideally, they should feel free to discuss all options without the fear of being rejected.this can best be achieved through open, transparent, non-judgmental, and informed discussions about possible outcomes of combining cam and conventional treatment for cancer [37, 38].between 38 and 60% of cancer patients, however, use cam without informing their healthcare teams [39].the reason for withholding disclosure was that the oncologists did not ask.the parents in this study reported that positive communication about conventional treatment facilitated fruitful conversations about cam.if they talked with the treating oncologists about cam were met in a positive way.compared to previous research in norway [37], this demonstrates a positive change in health providers’ attitude towards cam.health care providers provide ethical care by respecting parents’ choice on using cam for their child [40].the importance of keeping hope alive when the child is serious ill is important [7, 40, 41].in order to try all possible modalities for their child and maintain hope, parents often perceive cam safer and more efficient than research demonstrates [7, 40].gagnon and recklitis [42] investigated how the parents’ preferred level of control in treatment decision making was related to their personal health care involvement and their decision to use cam for their children.they found that most parents using cam preferred active or collaborative versus passive decision-making.studies about information on cam there is a need among parents of children with cancer for information about cam.studies demonstrates that the most common reason for not using cam is lack of information [33, 43] and additional stress for the child [44].this is in line with the findings from our study where the participants had limited capacity to search for information regarding cam because following up the conventional treatment protocols was time consuming.this is in line with fletcher et al. [24] who found in a qualitative interview study that parents wanted information about cam integrated in the services they already received in the hospital, as lack of time hindered them from examining cam modalities themselves.in a survey ben-arush et al. [45] reported that the majority of parents were very interesting in obtaining more information and guidance about these modalities.krogstad et al. [30] found that the participants had not received any information from the oncologists or other health care providers at the hospital about cam products, which they were very interested in obtaining.moreover, information about cam may give the parents a sense of control of the child’s treatment.it may also provide additional ways of helping their child to get through his/her cancer treatment.finally, it may give parents the feeling that they are doing everything possible to support their child’s recovery [43].strengths and limitations qualitative analysis provides insights into how participants understand and interpret situations, but it cannot be used to establish associations [25, 46].this study should therefore be interpreted in light of its strengths and limitations.twenty-two families agreed to be part of an interview.having more than 22 interviews may have resulted in a richer variety in experiences.however, no further substantial variation was added during the final three interviews, leading to the judgment that the information power was sufficient and that a larger number of interviews would not have significantly altered the outcome of the thematic analysis [47].
number of words= 1569
[{'rouge-1': {'f': 0.368535180556244, 'p': 0.9139490445859872,'r': 0.23080097087378643}, 'rouge-2': {'f': 0.22898505810662004, 'p': 0.4885303514376997,'r': 0.14953855494839102}, 'rouge-l': {'f': 0.3681351527709409, 'p': 0.7366666666666666,'r': 0.24537942664418214}}]
-----------------------------------------------------------------------------------------------------------------------------------
p96:
Extractive Summary:
currently, this anthropozoonosis affects more than 7 million people worldwide, and is considered a neglected and endemic tropical disease in 21 latin american countries [2].the treatment of this infection is still considered challenging, as it is restricted to two nitroderivatives, benznidazole (bz) and nifurtimox, both of which have limited efficacy, especially in the chronic phase of the disease, in addition to severe side effects [4].in brazil, bz is the only drug used for the treatment of cd [5, 6].the t. cruzi dtus may present distinct biological properties, including drug resistance, in addition to different geographical distributions [13, 14].furthermore, azole derivatives, including posaconazole, although demonstrating promising results for the experimental infection, had no efficacy in the human infection [16, 17].natural products are promising candidates due to the diversity of their molecular structures and the ease/reduced cost of obtaining them.the eo of ocimum gratissimum as well as its main constituent, eugenol, have also demonstrated leishmanicidal activity on l. amazonensis exposed to the half maximal inhibitory concentration (ic50) for promastigotes and for amastigotes, causing the parasites to undergo considerable ultrastructural alterations [22].again, there were no cytotoxic effects of this eo on mammalian cells, suggesting that it could be used as a source for new antileishmanial drugs [22].the biological activities of natural compounds are related to their constituents, which, in the case of eos, are mainly terpenes and terpenoids.treatment with these eos and eugenol inhibited parasite growth, with the eo of s. aromaticum being the most effective (ic50 = 99.5 μg/ml for epimastigotes and 57.5 μg/ml for trypomastigotes), promoting ultrastructural alterations mainly in the nucleus [20].our group has demonstrated that the oral infection of mice with different strains of t. cruzi (tci, tcii and tciv) is more severe than the infection by the intraperitoneal route using the same inoculation dose [26, 27], and has a worse response to treatment with bz [unpublished data].together, these results justify the continuation of studies regarding eos, alone or in combination, using trypomastigotes of different origins (blood, insect or culture) and other strains of t. cruzi in the infection of the animals.methods ethical aspects the use, maintenance and care of the experimental animals were carried out in accordance with the guidelines of the national council for the control of animal experimentation (concea).at the end of the experiments, the animals were euthanized by deepening anesthetic with thiopental (20.0 to 40.0 mg/kg) associated with lidocaine (1.0 to 2.0 mg/kg) intraperitoneally, according to the concea guidelines.inoculation of the animals male swiss mus musculus mice aged between 21 to 28 days, and with weights between 18 and 22 g, were used.experimental groups two experiments were carried out following the same protocol, with the difference being the eo used for the treatment.the same bz manufactured by the pharmaceutical laboratory of the state of pernambuco (lafepe) was used as the reference drug as well as in associations with eos.essential oils (eos) eos of ginger (z. officinale, lot 09419) and clove (s. aromaticum, lot 09464) commercially obtained (quinari fragrances and cosmetics ltda, brazil), were used according to the manufacturer’s instructions and analyzed using gas chromatography coupled to mass spectrometry (gc-ms, shimadzu qp 2000).fresh blood examination (fbe) parasitemia was evaluated daily in 5 μl of blood collected from the tail vein of the animal from the 3rd day after inoculation, according to brener [30], until negative for three consecutive days.the following parameters derived from the mean parasitemia curve were evaluated: pre-patent period (ppp), the calculated mean from the first day in which positive parasitemia was detected in each mouse; patent period (pp), the mean of the days in which each mouse presented positive parasitemia in the fbe; maximum peak of parasitemia (pmax), the mean number of bt in 0.1 ml of blood, calculated from the pmax detected for each mouse; day of maximum peak (dpmax), the mean of the days in which each mouse had the highest concentration of parasites in the blood.the reaction was performed using the quantinova sybr green pcr kit (qiagen) with 100 ng of total genomic dna for each sample, and the primers tcz-f (5 = −gctcttgcccacamgggtgc-3 =) and tcz-r (5 = −ccaagcagcggatagttcagg-3 =) [35].cure criteria in order to evaluate the efficacy of the different treatments, an animal that presented negative results for all tests (fbe, bc, cpcr and qpcr) before and after immunosuppression with cy, was considered cured [21, 28].infectivity and survival rates the infectivity of the y strain of t. cruzi with an oral inoculum of 10,000 bt/animal was 100% for all groups of both experiments (tables 1 and 2).it can be observed that the treatment with ceo alone had no effect on the parasitemia, with the ceo-treated animals presenting a parasitemia profile similar to that of the nt animals.on the other hand, the animals treated with bz alone or in combination with ceo exhibited total suppression of parasitemia throughout the course of treatment.the mean parasitemia curves for the experiment with the eo of z. officinale (geo) are shown in fig. 2b.in this experiment, the nt animals had a pmax of about 150,000 bt in 0.1 ml of blood around the 14th dai, whereas in animals treated with geo alone, the pmax had a 50% decrease in parasitemia (~ 75,000 bt/0.1 ml of blood), occurring around the 12th dai.the groups treated with bz, alone and in combination, presented complete suppression of parasitemia in most animals.the mean and standard error of ppp varied from 7.6 ± 0.5 days to 11.1 ± 1.5 days for the nt animals in the two experiments with the z. officinale and s. aromaticum eos, respectively (data not shown in the tables).the values of these parameters presented a significant decrease (p < 0.0001) compared with the nt animals, when the four groups were compared at the same time, for both the experiments with s. aromaticum and with z. officinale, independently (tables 1 and 2).in the two-by-two comparison, it was observed that treatments with both ceo and geo alone did not significantly alter the values of these three parameters.for the % + bc (p < 0.05), the increasing order was as follows: bz + eo (0.0%) < bz (20.0%) < geo (42.9%) < nt (100.0%) (fig. 4).the analysis of the qpcr results demonstrated therapeutic failure for all treatments, since this technique was able to detect values as low as 200 fg of dna in all treated animals of both experiments (figs. 3 and 4).however, the treatment with the bz + geo combination resulted in the greatest number of significant reductions of these parameters (8/9).in the experiments with the eo of s. aromaticum, a significant decrease in the values of 2/9 (ceo alone) and 6/9 (bz + ceo) parameters was recorded, and in the experiments with the eo of z. officinale there was reduction in 3/9 (alone) and 8/9 (bz + geo) parameters.however, a statistically higher cure rate of 44.4%, compared to 0% obtained with bz, was obtained in a previous study in which ceo was administered alone in mice inoculated with y strain [21].the difference observed between the cure rates of these two studies may be related to the infective form used in the inoculation of the animals.the molecular analysis in this study demonstrated that qpcr has high sensitivity in the detection of t. cruzi ii dna, even surpassing cpcr, and evidences therapeutic failure in animals that had presented negative results by the other methods (bc and cpcr).dias et al. [26], who also used bt of the y strain for oral inoculation of mice, observed similar results to those of our study, in terms of levels of parasitemia and mortality.mice orally inoculated with the y strain did not respond to bz treatment.as perspectives of this study, further analyzes are required in order to evaluate the toxicity of the essential oils in-vivo, the efficacy of their major constituents separately, such as eugenol, the main constituent of the s. aromaticum eo, and also to evaluate the efficacy of these and of other eos against the genetic diversity (other dtus) of t. cruzi.the results also reveal that the y strain of t. cruzi, when orally inoculated in mice, is resistant to the reference drug, bz.
number of words= 1343
[{'rouge-1': {'f': 0.3574277120185259, 'p': 0.7737037037037038,'r': 0.2323931623931624}, 'rouge-2': {'f': 0.18639952202121352, 'p': 0.330061919504644,'r': 0.12987170349251603}, 'rouge-l': {'f': 0.370497812641424, 'p': 0.6414285714285715,'r': 0.2604761904761905}}]
-----------------------------------------------------------------------------------------------------------------------------------
p97:
Extractive Summary:
the final concentration of the extract samples ranged from 2 to 0.01mg/ml.a 100 μl of 1mm hematin solution was mixed with 0.2 m naoh and 50 μl of the test extract.a 50 μl of glacial acetic acid solution (ph 2.6) was then added to this mixture.this test was carried out at 37 °c for 24 h. the microtube was then centrifuged at 8000 rpm for 10 min, the sediment was then washed with 200 μl of dmso three times at 8000 rpm for 10min.the β-hematin crystalline precipitated was dissolved in 200 μl of naoh 0.1m to form alkaline hematin.a 100 μl of the alkaline hematin solution was transferred to 96-well microplates and the absorbance was read by elisa reader at a wavelength of 405 nm.the effects of each test substance on β-hematin production were calculated and compared with negative controls.results in-vitro antiplasmodial activity test all the extracts, fractions, sub-fractions, and isolated compounds from c. spectabilis dc leaf were continuously screened in-vitro for their antiplasmodial activity against chloroquine-sensitive p. falciparum (3d7 strain), using microtechnique which demonstrated on the previous study [24].the samples for these tests were hexane extract, methanolic extract, ethyl acetate fraction and chloroform fraction.antiplasmodial activity was classified based on the gessler et al. [25], where antiplasmodial activity of extract was considered very good, moderate, and low with ic50 value less than 10 μg/ml, 10 to 50 μg/ml, and more than 50 μg/ml, respectively.during isolation of compound, chloroform was used to fractionate the extract.chloroform fractionation resulted in nine fractions called c.1-c.9.in-vitro antiplasmodial activity test of these fractions showed that fraction c.8 was the most active fraction with ic50 was 0.02 μg/ml (table 2).purification of fraction c.8 resulted in four sub-fractions called sfc.8.1-sfc.8.4, and their antiplasmodial activity is shown in table 3.sub-fraction sfc.8.1 and sfc.8.3 showed antiplasmodial activity against p. falciparum 3d7 strain with ic50 0.012 and 0.015 μg/ml, respectively.the purification of subfractions sfc.8.3 resulted in two compounds called compound c.8.3.1 and c.8.3.2 and their antiplasmodial activity is shown in table 4 and fig. 2.compound c.8.3.1 and c.8.3.2 showed antiplasmodial activity against p. falciparum 3d7 strain with ic50 0.016 and 0.070 μg/ml, respectively.identification using tlc-densitometry showed that compound c.8.3.1 was in the range of λ 200–300 nm, with a value of rf = 0.65.identification using ftir spectroscopy showed an absorption peak at 472.53 cm− 1; 657.68 cm− 1; 786.9 cm− 1; 864.05 cm− 1; 1101.28 cm− 1; 1382.87 cm− 1 (fig. 3).identification using 1h-nmr spectroscopy showed a characteristic signal of two hydroxyl protons at δ 3.60 ppm (1h, s); and δ 3.69 ppm (1h, s), two protons from the ch2 group of benzene at δ 1.91 ppm (2h, s, h-4) and δ 1.68 ppm (2h, s, h-5), one methyl group at δ 2.13 ppm (3h, s, h-12′), and some cassettes of the ch2 groups of the aliphatic chain at δ 1.38 ppm (2h, s, h-1′); at δ 1.22 ppm (2h, s, h-2′- h-8′); at δ 1.51 ppm (2h, s, h-9′), and at δ 2.35 ppm (2h, t, h-10′).identification using 13c-nmr spectroscopy showed the presence of one carbon with a ketone group at δ 179.0 ppm, carbon in the benzene group at δ 56.96 ppm; δ 67.13 ppm; δ 29.23 ppm; δ 25.77 ppm; and δ 48.29 ppm, carbon in the aliphatic chain at δ 34.32 ppm; δ 25.87 ppm; δ 29.23 ppm; δ 29.33 ppm; δ 22.89 ppm; and δ 38.87 ppm, one carbon in the methyl group at δ 30.23 ppm, and one carbon in the hydroxyl group at δ 65.27 ppm.effect of c. spectabilis on parasitic stage development stage-specific activity test of 90% ethanolic extract of c. spectabilis dc leaf against p. falciparum 3d7 strain was performed at different incubation periods of 0, 6, 12, 24, and 48 h to find out the effect of the extract to the growth of parasites at each stage of development.the extract concentration used was 100 μg/ml.the results of this experiment are shown in table 5 and fig. 4.no sharp difference was observed on the growth of each parasitic stage during incubation period of 0–12 h. however, opposite direction of percentage parasitemia was seen at 12 to 48 h post incubation compared with control which moved upward but the tests extract went to 0. at 12–24 h of incubation period, parasite growth decreased but not significantly compared to negative control.whereas at 48 h of incubation, parasite growth decreased significantly (p = 0.005) compared to negative control, with an inhibition percentage of 100% (fig. 5).in-vivo antiplasmodial prophylactic activity test the results of in-vivo antiplasmodial prophylactic activity test of 90% ethanolic extract of c. spectabilis dc leaf against p. berghei anka infection in mice is shown in table 6.the dose of extract of 800mg/kg provided the greatest inhibitory effect (68.61%) compared to other doses.probit analysis resulted in ed50 value was 161.20 mg/kg.in-vivo antiplasmodial suppressive activity test table 7 shows the results of suppressive activity tests by c. spectabilis dc leaf extract combined with artesunate.suppressive effects produced by the three extractartesunate combinations were higher than artesunate alone.moreover, the suppressive effects of group d (ethanolic extract of c. spectabilis dc leaf at 150 mg/kg (three times a day) on d0-d2 and artesunate at 36.4 mg/ kg on d2) was higher (99.18%) than those showed by artesunate alone (82.60%) and artesunate-amodiaquine combination (92.88%).heme polymerization inhibition test the ic50 value of the heme inhibition test by 90% ethanolic extract of c. spectabilis dc leaf was 0.375 mg/ml, while chloroquine as an antimalarial standard compound was 0.682 mg/ml (table 8).discussion anti-malarial activities of c. spectabilis dc leaf was conducted to extract, fraction and pure isolate.the active compounds were identified by tlc-densitometry, uvvis spectrophotometry, ftir spectroscopy, and nmr.in the 1h-nmr spectra, compound c.8.3.1 showed similarities to the compound (−)-7-hydroxycassine (fig. 6) in the presence of several similar chemical shifts [26].the greatest prophylactic inhibitory effect in-vivo of 90% ethanolic extract of c. spectabilis dc leaf against p. berghei anka was 68.61% provided by the mice treated with 800 mg/kg of the extract.this result was lower than that of mice treated with doxycycline (73.54%).the highest dose that can be used in mice is 1000 mg/kg of bodyweight [27].the effect of 90% ethanolic extract of c. spectabilis dc leaf to inhibit heme detoxification process showed the ic50 value of 0.375 mg/ml which was higher than that of chloroquine as a standard antimalarial drug which was 0.682 mg/ml.the potential for 90% ethanolic extract of c. spectabilis dc leaf in the inhibition of hemozoin formation caused morphological and growth disturbances of malaria parasites due to membrane damage and disruption of the activity of several enzymes [28] as seen during the 12-h incubation period where the inhibitory activity 70% increase compared to controls.furthermore, after 24 h incubation, the growth of parasites was 100% inhibited compared to the control (table 5).the advantage of the combination therapy was to increase the effectiveness of extract to prevent or slow the onset of resistance to a single antimalarial drug [29].selection of 90% ethanolic extract of c. spectabilis dc leaf combination with artesunate referred to the basis of malaria treatment which is a standard antimalarial drug recommended by who [30].the dose of 90% ethanolic extract of c. spectabilis dc leaf used was 150 mg/kg given three times [1].artesunate is the artemisinin derivative is a schizonticidal with fast onset of action and gametocytocidal which can reduce malaria transmission in endemic areas [31].artesunate at a dose of 36.4 mg/kg was converted dose from human to mouse.
number of words= 1232
[{'rouge-1': {'f': 0.33169859440111926, 'p': 0.8082812500000001,'r': 0.20866471019809246}, 'rouge-2': {'f': 0.19015388894039123, 'p': 0.3758823529411765,'r': 0.12726872246696036}, 'rouge-l': {'f': 0.3418544323674796, 'p': 0.642463768115942,'r': 0.23288659793814434}}]
-----------------------------------------------------------------------------------------------------------------------------------
p98:
Extractive Summary:
background migraine affects 1 out of every 7 americans annually and is 2 to 3 times more common in females than males [1].the financial burden of migraine in the united states is estimated to be $1533 per patient annually for episodic migraine and $4144 for those with chronic migraine.according to epidemiological studies, 30–82% of people with headache use cim approaches [3, 4], while only 43% of people with headache discuss their cim treatments with their healthcare provider [2].cim is defined by the national center for complementary and integrative health (nccih) as treatments that are separate from mainstream medicine but may be integrated with it [2].ncci h more accurately reflects that americans are no longer using these approaches “alternatively” but rather in conjunction with mainstream medicine [2, 3] examples of cim are commonly divided into two main categories: natural products (herbs, vitamins, minerals and probiotics) and mind-body practices (yoga, acupuncture, chiropractic, meditation and massage therapy) [2, 3, 6].wells et al., used the 2007 national health interview survey (n = 23,393) to compare cim use between adults with and without migraine/severe headache [2].researchers noted that adults with migraine/severe headache used cim more often for treatment because: their provider recommended it, mainstream treatment was ineffective, or mainstream treatment was too expensive [2].these data also show that only 31.3% of patients reported that a provider recommended cim [8] and that fewer than 50% of adults with migraine/severe headaches discuss cim use with their healthcare provider [2].they found social media to be indispensable for recruitment and found it efficient and cost-effective.online recruitment and social media strategies are particularly used for recruiting individuals that are hard to reach, unable to spend extra time in the office to answer research questions or those hard to engage through traditional means.methods study design this was a cross-sectional web-based survey study of people with migraine who have used cim to treat and manage migraine.the amf and yakkety yak, a digital marketing agency, launched the nationwide #moveagainstmigraine campaign in 2017 to mobilize and empower people living with migraine.during january of 2019, the group had approximately 18,000 members.the first part of study questions served to obtain demographic data, confirm a diagnosis of migraine and characterize episodic versus chronic migraine.the second part of study questions focused specifically on the cim approaches used and their perceived effectiveness.anyone part of the (mam) group was able to see the following post, and click on it if they wanted to participate: “are you a migraine patient over the age of 18 who is living with migraine and using integrative remedies not prescribed by your physician (acupuncture, coenzyme q, cannabinoids, massage therapy, etc.)?dr.permission was obtained from the amf, yakkety yak and the yale university institutional review board to post our survey on social media.this study was submitted for review to the irb and was granted an exemption.if any data were missing from participants, the participant survey was excluded using qualtrics.of the patients who use cim, 316 (94.3%) were female and 17 (5%) were male.patients were asked which category of cim (meditation, relaxation, deep breathing exercises, acupuncture, guided imagery, yoga, cognitive behavioral therapy, biofeedback, mindfulness training, craniosacral therapy, or migraine specific supplements/vitamins) they use.164 (48.9%) use cannabidiol oil (cbd) or other cannabis derivative to prevent migraine and 171 (51%) selected no to this question.there have however been other online survey studies.lee et al. administered a 30-min self-report survey on an online migraine headache resource (migraine in america, www.with a 95% confidence interval, our study shows that a true mean lies between the 95% of the values we have acquired, and only a 5% possibility that it does not.liver function must be closely monitored while using this supplement.this finding supports the existing literature that these strategies are favorably used in combination among the general population [13] and in other select populations [14–16].mindfulness training always differs from meditation through the practice of both informal and formal self-reflection during the day, while meditation is defined to be during a specific time of day at a specific place.from our study, 30.3% of patients found perceived manipulation or body-based practices such as acupuncture, chiropractic maneuvers, etc.it is postulated that cannabis shows potential to interrupt glutamate signaling leading to cortical spreading depression, serotonin release from platelets and cranial blood vessel dilation caused by nitrous oxide and calcitonin gene-related peptide [20].it can be helpful to ask about their history with conventional treatments while also avoiding judgement.when reviewing integrative treatment options, it is imperative to counsel patients on their limitations so that they have appropriate expectations.see table 2.selection bias is inherent in all social media – based studies.other biases may be introduced by unforeseen variables.subjective biases are also inherent in self-reported outcome measures.respondents may have misinterpreted the question and may threaten the validity and reliability of measurement.this is an unavoidable side effect of anonymous polling and may impact the reliability of the data.the data presented in the text as well as the tables indicate “2 or more therapies in combination”.we did not investigate which therapies were used in combinations.the study is also limited by a small sample size, a disproportionately large percentage of caucasian female participants, and a predominantly chronic migraine population which is not representative of the larger migraine population within the united states.future research larger survey studies are needed to gain a broader perspective of cim use among people with migraine.90% of people surveyed reported using cim for the treatment of migraine.
number of words= 897
[{'rouge-1': {'f': 0.3948761070624632, 'p': 0.7121404682274248,'r': 0.2731746031746032}, 'rouge-2': {'f': 0.16719466877952677, 'p': 0.2478523489932886,'r': 0.1261440677966102}, 'rouge-l': {'f': 0.31952676891498943, 'p': 0.5106779661016949,'r': 0.2325}}]
-----------------------------------------------------------------------------------------------------------------------------------
p99:
Extractive Summary:
the spectral result of peak 21 showed at m/z 254, which is consistent with the standard chrysin peak [13].in the same way, the study of rojsanga reported that o. indicum seed extract contained 3 major compounds, including baicalein, baicalin, and chrysin [14].thus, a peak 21 could be the chrysin.however, further investigation is needed.effect of oie on inhibition of cell cycle progression the effect of the oie on the cell cycle of 3 t3-l1 cells is shown in fig. 2a and b. when 3 t3-l1 pre-adipocytes were induced to differentiate with adipogenic stimulators, the cells enter into s and g2/m phase within 24 h following induction (fig. 2a).further analysis revealed that differentiated cells transformed from g1 and s phase to the g2 phase (36% cells in g2 phase).in contrast, treatment with oie at concentrations ranging from 50 to 200 μg/ml significantly arrested cells in the g0/g1 phase (10.3, 10.8, 14.4, and 18.7% cells in g2 phase, respectively) compared to controls (p < 0.05) (fig. 2a and b).the impact of the oie on the cell cycle was further explored, and the expression of cdk2 in differentiating cells was affected at the doses of 150 and 200 μg/ml (fig. 3a and b).these results suggest that oie may inhibit cell cycle entry into the g2/m phase of 3 t3-l1 cells.oie has the potential to inhibit the initiation of mitotic clonal expansion (mce), which is a significant step for cell differentiation.thus, this mechanism may play an important role in the suppression of adipogenesis.effect of oie on glucose metabolism the effect of the oie on glucose metabolism was determined by evaluating the expression of glut4, the impact on the 2-nbdg uptake, and the py20 in differentiating 3 t3-l1 cells.these proteins were consistent with insulin activity [15, 16].however, phosphorylation proteins py20(l) and glut4 at 24 h showed no significant difference among the preadipocytes, untreated-adipocytes, and the 200 μg/ml of oie (p > 0.05).after 12 days, there were significantly reduced on the glut4 and py20(h) of oie-treated compared to the untreated-adipocytes.at the same time, the py20(h) level of oie-treated was not different from the pre-adipocytes (p > 0.05), while the level of py20(l) of oie-treated was significantly higher than pre-adipocytes (p < 0.05).it was thought that reduced insulin activity might influence the expression of glut4.by day 12 in untreated adipocytes, there was a significant expression of glut4 on the plasma membrane (fig. 5a and b).treatment with oie resulted in a 40% reduction of the glut 4 expression compared to untreated adipocytes (fig. 4a and b).in a parallel experiment, both preadipocytes- and adipocytes-treated with oie at 200 μg/ ml were approximately two folds significantly increased uptake of 2-nbdg compared to both untreated adipocytes at 24 and 48 h (p < 0.05, fig. 6).however, on day 12, oie-treated adipocytes were significantly reduced compared to the same groups at 24 h and 48 h (p < 0.05).one explanation for this is that oie may diminish the activity of tyrosine kinases leading to a reduction of glut4 expression in the plasma.moreover, the oie may use the other facilitative glucose transporter, glut1 (insulin-independent), which is mostly expressed in pre-adipocytes [17].the jc-1 was used for monitoring the mitochondrial membrane.when the dyes enter into the cytoplasm of cells, it exhibits a green fluorescence color called the jc- 1 monomer.however, when the dyes enter into the mitochondria, it will accumulate and start forming reversible complexes called jc-1 aggregates.these jc-1 aggregates exhibited excitation and emission in the red spectrum.however, jc-1 aggregates in oie-treated-nd, oie-treated-d, and nd groups were less pronounced compared to the d (untreated-adipocytes) group (fig. 7).this result is consistent with the flow cytometry data regarding mmp.however, a noticeable reduction of mmp was found in all samples at day 12 compared to 24 h, especially in untreated adipocytes.furthermore, the cellular atp level was determined on day 12; the highest intracellular concentration of atp level was found in untreated adipocytes.it was significantly reduced in adipocytes treated with oie (p < 0.05, fig. 9).mitotracker dyes have been used for detecting a mitochondrial mass.this fluorescent dye is linking to thiol groups in the mitochondria, and it produces green fluorescence under excitation 488 nm.this observation was similar to pre-adipocytes alone or treated with oie.this study explored the morphology of mitochondria by tem [18].the results indicated that mitochondria of pre-adipocytes with and without-oie were gathered around the nucleus, and the morphology was mainly short, but the cristae were clearly observed (fig. 11).in contrast, the mitochondria of untreated adipocytes had become a more condensed, slender shape with reduced cristae mitochondria and uniformly distributed in the cytoplasm.while oietreated adipocytes could recover mitochondria mass and morphology almost the same as untreated preadipocytes.whereas oie-treated adipocytes showed significantly lower and higher mmp levels compared to the oie-untreated adipocytes at 24 h and day 12, respectively (p < 0.05, fig. 8).however, on day 12, the atp level of oie-treated adipocytes was significantly lower compared to non-treated adipocytes (p < 0.05, fig. 9).discussion previous studies reported that oie had an antiadipogenic effect through inhibiting the expression of pparγ along with a decrease in adiponectin and fas production [8, 9].in this study, we investigated the impact of the oie on cell cycle, glucose metabolism, and mitochondria.oie at doses of 50–200 μg/ml blocked the cell cycle on the g1/s stage at 24 h, and this was subsequently confirmed by a significant decrease in the expression of cdk2, a protein involved in the transition of g1/s to s/g2.phytochemicals, especially flavonoids, have been shown to processes adipogenesis suppressive in 3 t3-l1 cells.pretreatment of 3 t3-l1 adipocytes with flavonoids such as baicalein and chrysin suppressed adipogenesis by inducing cell cycle arrest in the g0/g1 phase [19, 20].in this study, lc-ms analysis showed that baicalein is one of the major components found in oie.in order to understand the effect of the oie glucose uptake and metabolism, this study evaluated the level of tyrosine phosphorylation associated with the ir, glut4 expression, and glucose uptake.whereas, a protein with mw at 91 kda was not different compared to the untreated adipocytes.these results suggest that oie may influence the activity of insulin and, thus, the uptake of glucose.the increasing phosphorylation of irs-1 and irs-2 may activate many different signaling pathways, including phosphatidylinositol-3-kinase (pi3 kinase) and akt pathway [24].the activation of pi3 and akt pathway has been suggested to be a part of the signal transduction pathway for insulin-induced glut4 redistribution [18].besides, in adipose tissue, glut1 is expressed along with glut4.however, glut4, which is insulin-dependent, are more dominantly expressed in adipose tissue than the glut1 [27].furthermore, thiazolidinediones in both basal and insulin stages increase the glucose uptake and cellular glut1 expression with no effect on glut4 expression [28, 29].another study also demonstrated that 3 t3-l1 cells treated with 50 μm of baicalein significantly suppressed the expression of the glut4 gene and decreased the glucose uptake level [30].moreover, it had been reported that 100 mg/kg body weight of chrysin inhibited insulin receptor 1 (ir1) gene expression and contributed to the reduction of ir1 protein in diabetic rats [31].previous studies had demonstrated that during adipogenesis, mitochondria were present at number 19 folds greater than 3 t3-l1 pre-adipocytes [34].similarly, this investigation found that the level of mmp in untreatedadipocytes was significantly increased during the first 24 h compared to the pre-adipocytes (fig. 8).however, on day 12, the mmp level of the untreated-adipocytes was dramatically decreased compared to the pre-adipocytes (fig. 8), while the atp level increased compared to preadipocytes (fig. 9).additionally, confocal microscopy revealed that the adipocytes’ mitochondria displayed pale green fluorescence, which represents the mitochondrial mass.moreover, the morphology of adipocytes’ mitochondria was elongated.although a massive increase in the number and activity of mitochondria beginning in the early adipogenesis [35].
number of words= 1268
[{'rouge-1': {'f': 0.3749384980560716, 'p': 0.8232467532467533,'r': 0.24274758004467611}, 'rouge-2': {'f': 0.22307828113669873, 'p': 0.4250488599348534,'r': 0.1512220566318927}, 'rouge-l': {'f': 0.40573604327503815, 'p': 0.6950000000000001,'r': 0.28649484536082476}}]
-----------------------------------------------------------------------------------------------------------------------------------
p100:
Extractive Summary:
the  main outcome was whether the patient was admitted to  hospital or not.the chi-squared test was performed to  compute the p-values, and statistical significance was set  at p < 0.05.the bonferroni adjustment was used to adjust  for multiple comparisons.unadjusted and adjusted  odds ratios (ors) were drawn through multivariable  binary logistic regression, where the reference category  was the most frequent.the final model was chosen  based on appropriate model diagnostics.statistical analysis  was performed in stata software 15 software [9].   results   the total number of er visits by paediatric patientswas  46,374.the sociodemographic characteristics of patients  are presented in table 1.the sample consisted of 55.4%  males and 44.6% females.the highest number of visits  belonged to the toddlers age group (32.0%), followed by  schoolers (24.9%).the smallest number of visits were  for the neonates (0.9%).most patients were of the saudi  nationality.   the er characteristics of paediatric patients are presented  in table 2. of the total sample, only 2.5% of patients  were admitted.the highest number of complaints  recorded was for fever followed by respiratory symptoms   (27.0 and 24.9% respectively).with regards to triage,  only 0.02% were for triage i and 0.02% for triage ii,  whereas triage iv and v made up 71.0 and 26.2% of the  visits.most visits occurred in the evening shift and the  winter season had the highest rate of visits (31.9%).  when exploring the association between sociodemographic  characteristics and admissions, the age group  variable was found to be highly statistically associated  with admissions (p < 0.001).analysing this further,   comparisons between all age groups were statistically  significant, except for adolescents with other age groups.  further, the rate of admission was not found to be different  between males and females (table 3).   associations between er visit characteristics and admission  are presented in table 4.the presenting complaints,  triage and shifts were found to be highly   statistically associated with admissions at the p < 0.001  level.the endocrine and haematological complaints, as  well as triage group v, were found to have contributed  mostly to the high statistically significant p-value when  compared to other groups following multiple comparison  testing.for shifts, the only statistical difference was  found for the comparison between the evening and the  morning shift (bonferroni adjusted p-value < 0.001).no  significant association was observed for the season.   the unadjusted and adjusted ors of admissions in relation  to the sociodemographic and er visit characteristics  are presented in table 5.the analysis shows that before  and after adjusting for other variables, there is higher odds  of admission for both neonates and infants, although the  risk is almost fourfold for neonates (adjusted or = 3.85,  95%ci = 2.57–5.76 and adjusted or = 1.89, 95%ci = 1.56–   2.28 respectively).with regards to the complaint, the highest  odds of admission were for haematological illnesses  both before and after adjustment, followed by endocrinal  conditions when compared to fever which is statistically  significant.in comparison with triage iv, triage i had a  19.02 or for admission, followed by triage ii (or = 10.66,  95%ci = 1.98–57.23).triage v had the lowest odds of admission  both before and after adjustment and was statistically  significant (adjusted or = 0.30, 95%ci = 0.23–0.38).  morning shifts were associated with a relatively high odd of  admission, although they were not significant after adjustment,  and neither were the seasons of the visit.  figure 1 shows the number of admissions by month of  visits.it can be seen that the lowest numbers of admission  were during july and august, whereas the highest  were for january, february and march.    discussion   this study supports the evidence that paediatric ers are  very much being used by patients as the first point of  contact with the healthcare system, rather than visiting  their local primary health care physician.this is a problem  which should have eased with the current governmental  initiative in promoting the use of primary  healthcare.    adental category dropped in regression analyses due to zero counts    characteristics of paediatric er visits   within this data, it was found that the most common  paediatric age group was for toddlers aged between 1 <  3 years.similar distributions have been found in other  paediatric ers in the us [10], lebanon [11], and in another  saudi hospital located at the capital city of riyadh  [12].
number of words= 687
[{'rouge-1': {'f': 0.5007161196069481, 'p': 0.7146280991735536,'r': 0.38536388140161726}, 'rouge-2': {'f': 0.24834164495322716, 'p': 0.33243093922651934,'r': 0.1982051282051282}, 'rouge-l': {'f': 0.48401011225965246, 'p': 0.5745871559633027,'r': 0.41810126582278484}}]
-----------------------------------------------------------------------------------------------------------------------------------
p101:
Extractive Summary:
major  depressive disorder and substance use disorder have  been the most common mental health problems associated  with suicide [4].    one of  the methods of suicide in this area is self-immolation  (about 27% of all suicides).about 80% of self-immolation  cases are committed by women.self-immolation is more  common in the first years of marriage when there are  many emotional problems and conflicts and low coping  skills between couples.studies attribute issues such as the  decline in the human development index, cultural factors  and negative attitudes toward women, and the reduction  of suicidal stigma [7, 8].the lowest rate is 2.8 per 100,000  related central provinces including isfahan, yazd, semnan  and qom [9].   according to iranian social welfare organization  (iswo), the social emergency of iswo has taken preventive  action in 8400 suicide attempts through mobile  teams (> 3000 cases), social service bases (242 cases), crisis  intervention centers (1300 cases), and contact with  social emergency hotline (3400 cases) in past year [10].   pre-hospital emergency has also been involved as a  first line contact in suicide situations in the country and  has played an important role in this case [11].they are  the most widespread medical emergency facility in  throughout the county and in most cases are the first  line contact with possible suicidal patients and should  provide interventions based on their prioritization, clinical  judgment, and decision-making skills.inadequate  skills and mastery of emergency technicians in  these conditions can lead to serious injury and even  death in a person who threatens to commit suicide.furthermore,  poor decision-making ability in critical situations,  increases psychological stress on emergency  technician [13–16].in the united states, dr.betz described  the important principles of suicidal patient management  (patient safety, attention to physical issues, and suicide  risk assessment) in his article, given the limited time and  the likelihood of impending injury [17].dr.christopher  reviewed existing protocols for managing suicide in a  pre-hospital emergency room in washington to evaluate   and integrate them to achieve a coordinated and comprehensive  protocol in the future [11].in brazil, 15 psychiatrists  and psychologists, after a systematic review of  sources and articles, provided guidelines and recommendations  for the brazilian psychiatric association for the  management of patients with suicide [18].first, we searched pubmed, scopus, and  psycinfo databases for the combination of the following  keywords: 1)"suicide” (or) “suicide prevention”  (or) “suicide risk factors” (or) “self-harm” (or) “suicidal  patients”.2) “management” (or) “protocol” (or)  “pre-hospital care.subsequently,  we used the cochrane checklist to assess the  full text of the articles and select suitable articles for further  analysis.in this way, 20 articles were selected for  the next stage (table 1).we used these 20 articles and  the textbooks for data extraction.   the extracted findings were then categorized according  to the stage of intervention.here, we report  the special and additional points related to suicidal  patients including: ensuring safety in suicidal patients;  evaluation and history taking in suicidal patients; behavioral  and pharmacological management in suicidal patients;  important points to reduce life-threatening risks  in suicidal patients.   a professor  of psychiatry with years of experience in the emergency  department of a training psychiatric hospital, 3.a specialist in emergency medicine  working in the country’s emergency organization  with years of experience of working with emergency  technicians, 7.2. to what extent is this item understandable and  transparent to an emergency technician?  4.considering the existing facilities and conditions in  our society, is this a plausible step to perform?  each question was scored on a likert scale from very  low to very high (1–5).the first session included emergency technicians  and technical and operational deputy of one of the  emergency centers of tehran (about 50 people).the  second session was held with general practitioners  (dispatch) of operations in the pre-hospital emergency  department.prior to the start of both sessions, informed  consent was obtained from the participants.   then, we revised the protocol another time considering  the feedbacks of the sessions and finalized the protocol in  a two-page format that could be easily used in the ambulance  as a quick review of important measures in dealing  with a suicidal patient before confronting him/her.   result   the general measures that should be considered in the  management of suicidal patients are not much different  from those of other psychiatric emergencies, as reported  in “pre-hospital emergency protocol for mental disorders  in iran” [16] and include the following three levels.basic and safety tips (patient, technician and those  at the scene):  the first stage (primary action) are including a) prescene  assessments of site security, escape routes, and  safe locations in the event of violence from the patient,   b) assessment of the patient’s access to weapons and  equipment that could threaten his/her own life, technicians  or attendees [30], c) assessment for risk and need  for back up and the presence of police, which includes  anticipating their entrance method and avoidance of entering  the place alone, d) using family capacities to provide  security [30] and e) assessment of risk factors for  violence and predicting it symptoms of imminent aggression)  [16].in addition, the police should be contacted  from the beginning.dispersal of people present at  the scene should be done immediately, as their presence  may play a provocative role in attempting suicide [22].  furthermore, when the suicide threat is made with dangerous  measures such as firearms, explosives or chemicals,  the presence of crowd can be quite dangerous [30].  verbal threat to commit suicide and history of previous  suicide attempt, especially with dangerous methods,  and a history of aggression are more important risk factors  [41, 42].   in addition to the items listed in table 3, if the patient  threatening to commit suicide is from the northern or  western provinces of the country (especially in young married  women with a history of marital problems, a history  of psychiatric illnesses, especially major depression and  post-traumatic stress disorder, a past history of self- immolation) be aware of the possibility of self-immolation  and examine the evidence [7, 8].   3. transfer the patient to the hospital.  in a person who has attempted suicide:  in a person who has attempted suicide, immediate attention  to the patient’s medical risks is a priority.it is also important to gather evidence and document  findings on the scene, any findings that help to understand  the means of suicide (like empty cans of pills and  ropes, etc.) and evidence that is effective in understanding  the cause of suicide (patient’s will or any manuscript, patient  medical prescriptions, patient medications, etc.) [44].   however, in most studies, more attention has  been paid to the management of these patients in hospital  emergency department than in the pre-hospital emergency   table 4 important points in dealing with committed suicide   1.if the situation does not improve with these initial measures, contact  the dispatch center and get an assignment  department.reducing the risk to himself, patient, patient’s  family and those present at the scene.  assessment of risk    shirzad et al. bmc emergency medicine (2021) 21:47   factors (especially those that increase the risk of  imminent suicide) as well as protective factors are the  next crucial steps.however, according to emergency  protocols in iran, all of the suicidal patients must be  transferred to a hospital, even if the technician considers  the patient to be in a low risk for suicide.if behavioral interventions are not helpful, medications  are considered [27].local religious  and cultural teachings about the sinful act of suicide  and the ambiguity of the fate of person who commits suicide  also increase the pressure and stress of the family and  mandates the intervention by technician as the first member  of the treatment team to enter the scene.   
number of words= 1225
[{'rouge-1': {'f': 0.3365738424995164, 'p': 0.7643396226415093,'r': 0.21580031695721077}, 'rouge-2': {'f': 0.16423053246738073, 'p': 0.2859090909090909,'r': 0.11520222045995243}, 'rouge-l': {'f': 0.27643809131992514, 'p': 0.5239473684210527,'r': 0.18774744027303755}}]
-----------------------------------------------------------------------------------------------------------------------------------
p102:
Extractive Summary:
 key points    the average sensitivity of anterior-posterior lodox-  statscan for spinal injuries was 9% (1–14% depend   ing on the spinal region) with a high overall specificity > 98%     the overall interrater reliability was slight (slight   κ = 0.02); the radiology attending showed the highest   sensitivity for detecting spinal injuries.     the early  identification of spinal injuries is critical in the initial  management of the trauma patient to avoid adverse  events due to incorrect immobilization and mismanagement  [3, 4].according to the guidelines of the national  institute for clinical excellence (nice), spinal injuries  are suspected if a patient has any significant distracting  injuries, a reduced level of consciousness or is under the  influence of drugs or alcohol, which might be associated  with confusion or uncooperativeness.in the quest for improved imaging techniques in the  emergency room, the lodox-statscan (ls), initially used  in the south african mining industry to reduce diamond  theft, has evolved into a promising time-saving diagnostic  tool [6].the ls uses a linear scanning technique with  a highly collimated (laser-like) x-ray fan beam, which  spreads out in only one direction.on the contrary, conventional  x-ray systems use a wide cone-beam around  the primary photons, which causes more room scatters  and increases overall patient radiation.the translating  c-arm of the ls allows imaging angles between 0° (ap  view) and 90° (lateral view).the x-ray tube, x-ray fan  beam, collimating slit and detector all move together  along a linear scanning path, producing images from  100 mm/ 4in square and up to 1800 mm/70 in by 680  mm/ 27 in compared to approximately 400 mm/ 16 in  square by conventional x-ray systems [7].an anterior- posterior (ap) full-body scan by ls with minimal   radiation dose is completed within 13 s [6], imaging in  two planes within 3 to 5 minutes [8].   especially in a setting challenged by high patient numbers  and limited physical and human resources, the high  speed of imaging allows a reduction in resuscitation time  [9].in 2007, routine ls was implemented in the modified  bernese advanced trauma life support (atls), replacing  the conventional radiographs of the lateral  cervical spine, ap-chest, and ap -pelvis [10].a second  plane is usually performed if there are concerns for an  immediate ct scan such as a pregnancy or a highly unstable  patient.while many studies have compared ct  scans and conventional radiographs as a diagnostic tool  [11–13], only a limited number of studies validated the  diagnostic accuracy of the ls.however, in our study, we included a higher number of  patients and also analyzed the interrater reliability.to  our knowledge, no study has analyzed the diagnostic accuracy  of ap-ls for spinopelvic injuries.   therefore, we analyzed sensitivity and specificity and  interrater reliability of ap-ls to detect spinal injuries  specifically for cervical, thoracolumbar and spinopelvic  injuries.   materials and methods   all methods were carried out in accordance with relevant  guidelines and regulations.general consent of patients  was obtained.the institutional review board  (health and welfare directorate of the canton of bern,  switzerland; cantonal ethics committee for research,  project id 2019–02142) waived the need for informed  consent.all methods were carried out in accordance  with relevant guidelines and regulations.all the experimental  protocols were approved by the institutional review  board (health and welfare directorate of the  canton of bern, switzerland; cantonal ethics committee  for research, project id 2019–02142).   patients   the study group was a consecutive series of polytraumatized  patients admitted to our level i trauma center.  the inclusion criterion was solely an injury severity  score (iss) ≥16.   between 02/2009 and 12/2012, 344 patients aged 16  years and older with an iss equal to or greater than 16  underwent ap-ls and a full-body ct scan upon presentation  in the emergency department.data were retrieved  from individual patient records and the picture archiving  and communication system (pacs) image software   (sectra workstation ids7, version 19.3, sectra ab©  sweden).independent variables included age, sex,  mechanism of injury, and iss.   sample size calculation   the study will focus on a total of 335 participants.this is  a diagnostic test accuracy study to estimate the sensitivity  and specificity of ls in detecting fracture.diagnostic measures  will be estimated with 95% wilson confidence intervals.  we expect the prevalence of fracture to be between  20 and 40% [1, 16], sensitivity to be between 40 and 70%   [1] and specificity between 85 and 100% [1].a sample of  335 participants will result in a two-sided 95% wilson  confidence interval around the sensitivity and specificity  as shown in the tables below (supplemental material  table 1a sensitivity table; table 1b specificity table).  radiographic imaging   the ap-ls was performed by an ls (statscan critical  imaging system, lodox systems [pty] ltd., johannesburg,  south africa).the ls c-arm rotates around the  patient with an angle between 0 and 90 degrees and can  provide an ap view within 13 s (138 mm/s) [1].the patient  was positioned with the upper extremities lateral to  the body to avoid an overlay with the thorax, spine and  pelvis.detailed information about the ls linear slot  scanning radiography system can be found at the companies  online presence (http://lodox.com).   a full-body ct scan followed the performance of an  ls.all ct examinations were performed using a 16slice  multidetector-row computed tomography system  (sensation 16, siemens, forchheim, germany) with collimation  of 16 by 1.5 mm and a reconstruction slice thickness  of both 2 mm and 5 mm.   the results of the ct scans were  used as the diagnostic reference.   these were both senior physicians: an experienced  radiologist and an experienced spine surgeon.  the level of the injury was  classified as cervical, thoracic, lumbar, or sacropelvic.   the overall auc was < 0.7 (0.49–0.61), independent  of the injury level, demonstrating a poor value of the  ap-ls as a diagnostic instrument in the case of suspected  spinal injuries (table 2).   results revealed poor interrater reliability  for cervical spinal injuries (κ = − 0.01; 95% ci: − 0.07,  0.06), the interrater reliability was fair for thoracic (κ =  0.22; 95% ci: 0.15, 0.28) and slight for lumbar (κ = 0.19;  95% ci: 0.13, 0.26) and sacropelvic spinal injuries (κ =  0.04; 95% ci: − 0.02, 0.10).injuries were classified as stable (subaxial and thoracolumbar:  a0-, a1-and a2-type injuries; sacropelvic: a-and b-type  injuries) and potentially unstable spinal injuries (subaxial and  thoracolumbar: a3 and a4-type injuries, b-and c-type injuries;  sacropelvic: c-type injuries) according to ao-spine classification  [21].even in  non-polytraumatized patients, cervical spine injuries can  be difficult to diagnose on plain radiographs [28].because lateral planes are not included in our  standard clinical protocol, this plane is missing in most  patients.
number of words= 1056
[{'rouge-1': {'f': 0.3763691456901978, 'p': 0.7671830985915493,'r': 0.24934782608695652}, 'rouge-2': {'f': 0.20150950364783782, 'p': 0.3491519434628975,'r': 0.141622846781505}, 'rouge-l': {'f': 0.32522771310197074, 'p': 0.585527950310559,'r': 0.22514018691588786}}]
-----------------------------------------------------------------------------------------------------------------------------------
p103:
Extractive Summary:
 in most societies today, quality pre-hospital emergency care is an essential component of caring for patients in need of emergency care [1].pre-hospital emergency care consists of all the emergency medical services which are provided to patients outside the hospital before they are transferred to the nearest medical center [2].(p10, 18 years of experience)another issue under the category of challenges of human resources and medical equipment is shortage of equipment.one of the participants stated that: some of the ambulances in inter-city and rural emergency stations lack such medical devices as suction machines, cardiac monitors, pulse oximeter, and ventilators.even now that we have to deal with the covid-19 epidemic, we don’t have access to personal protective equipment, like masks, gloves, and special gowns, and our fear of getting the infection has a negative effect on our diagnoses and, in turn, our decisions and actions.(p24, 13 years of experience) bijani et al. bmc emergency medicine (2021) 21:11 page 8 of 12 according to another participant: when there is no heart monitor, pulse oximeter, or ventilator inside the ambulance, how am i supposed to diagnose a patient’s cardiac or respiratory problem and make a proper clinical decision that won’t be a threat to the patient’s life?(p18, 6 years of experience) ethical issues the final theme of the major challenges and barriers in clinical decision-making is “ethical issues” which consists of categories of respect for patients’ physical privacy and respect for patients’ sexual privacy.respect for patients’ physical privacy regarding respect for patients’ physical privacy, in the iranian culture, it is important that patients’ physical privacy be maintained and that their private parts not seen by caregivers, especially opposite-sex caregivers.according to a participant: sometimes, we must provide care to female patients, especially young females, who have palpitations or chest pains and are in need of immediate cardiac monitoring.it is not ethical for the patients or even for us who are in the iranian culture to see the female patient’s body.it is really a tough job to make a decision at such times and the patients may get very uncomfortable.(p13, 4 years of experience) respect for patients’ sexual privacy the participants also mentioned that, considering the dominant islamic culture in iran, showing respect for the sexual privacy of patients is very important to patients and their families.one of the participants stated that: while we were transferring a pregnant woman, we realized she had broken her water and was in labor.the baby was coming and we had to help her deliver it.the poor woman seemed very embarrassed and uncomfortable.it is so hard to make a decision in such conditions.after all, we are muslim and our ethical principles dictate that at such times, care should be given by a person of the same gender as the patient.but in iran, there are no female staff members in the pre-hospital emergency care.(p21, 3 years of experience) discussion this study explored the major challenges and barriers which affect clinical decision-making from the perspective of ems personnel.the analysis of the participants’ experiences showed that challenges and barriers of the participants’ clinical decision-making consisted of four areas: professional capabilities, occupational and environmental factors, inefficient organizational management, and ethical issues.at the scene of accidents, the emergency care personnel encounter various clinical challenges which affect their clinical decision-making [18].at the same time, it is the ethical and professional responsibility of emergency care personnel to provide quality care based on code of ethics to victims at the scene of accidents [19].the findings of the present study concern prehospital emergency care personnel’s perception of the major challenges which affect clinical decision-making at the scene of accidents.professional capabilities, including clinical knowledge and skills, were found to play an important part in prehospital emergency care personnel decision-making.similarly, in their study of the barriers to nurses’ clinical decision-making in the university hospitals of the south of iran, mousavi et al., report that having knowledge, clinical experience, and clinical skills are essential to making good clinical decisions—the clinical decisionmaking mean score of the nurses who had poor clinical knowledge, experience, and skills was unsatisfactory [20].another important component of professional capabilities which has a significant impact on clinical decision-making is clinical judgment.according to the study of engebretsen et al., clinical judgment is a necessary skill for all nurses, especially pre-hospital emergency care nurses, as it enables them to make accurate diagnoses and make proper clinical decisions [21].likewise, the results of the study of anderson et al., show that clinical judgment and critical thinking play a key role in making clinical decisions in all situations, in particular the critical and complicated conditions which prevail in pre-hospital emergency care [22].in the present study, the participants stated that some of their colleagues do not have adequate clinical knowledge, skills, and experience and cannot, therefore, make the right decision for their patients.similarly, the results of the study of perona et al., show that clinical knowledge and skills and clinical experience are key factors in emergency care personnel’s clinical decision-making.lack of those capabilities poses a challenge when the personnel should make clinical decisions and adversely affects the quality and safety of the care.accordingly, it is necessary for pre-hospital emergency care administrators to take measures to improve the professional capabilities of their personnel [23].emergency nurses association (ena) has always stressed that advances in technology, increase in professional knowledge, and work specialization mean that emergency care nurses’ clinical knowledge and performance need to be improved regularly.since emergency care is an unpredictable area and, occasionally, bijani et al. bmc emergency medicine (2021) 21:11 page 9 of 12 the personnel have to deal with a large number of patients with various issues, it is necessary for the emergency care personnel to possess such professional capabilities as clinical knowledge and skills to be able to make the right clinical decision and provide safe and quality care accordingly [24].the results of the study of aloyce, et al. show that the emergency nurses who lack the required clinical knowledge and skills make mistakes in their clinical decision-making and triage [25].in the present study, “teamwork skills” was found to be another important aspect of professional capabilities which affects clinical decision-making.in grover’s qualitative-exploratory study of emergency nurses’ experiences of teamwork in a hospital in australia, the participants stated that teamwork skills are among the essential clinical skills which emergency nurses must possess.the participants’ experiences showed that when the emergency department is busy and a large number of patients must be attended to, teamwork skills can significantly facilitate clinical decision-making.the personnel with poor teamwork skills get nervous and confused at such times and cannot make logical decisions for their patients [26].based on the participants’ experiences, the emergency care personnel occasionally have to provide care to patients who are very irritable and aggressive.if the personnel fail to manage their own feelings and emotions in such circumstances, they cannot make a good clinical decision for the patients and may even take actions which put the patients’ lives at risk.the results of the study of lin et al., show that emergency nurses must possess high levels of resilience and tolerance so that they can employ their capabilities in the critical conditions of emergency situations and, by making quick and correct clinical decisions, implement effective clinical interventions [27].halpern, states that pre-hospital emergency is a complicated, unpredictable, and stressful environment.at times, patients or their companions, who are under considerable stress, may behave in violent and insulting ways.therefore, it is essential for the emergency care personnel to possess such psychological qualities as resilience and emotional intelligence to be able to control their emotions and make the right clinical decision and, consequently, take effective clinical measures.by holding workshops, pre-hospital emergency care administrators can enhance the personnel’s psychological capabilities [28].. similarly, gunnarsson et al., report that emergency nurses must possess resilience and emotional stability to be able to use their professional abilities effectively and take proper clinical measures at the time of an emergency.although such factors as fatigue from work overload, shortage of personnel, and stressful work conditions in emergency departments have an adverse effect on the performance of emergency care personnel, nurses who are resilient and can manage their emotions continue to provide high-quality care [29].occupational and environmental factors constitute another category of the major challenges and barriers which affect emergency care personnel’ clinical decisionmaking.according to the study of li, et al. (2018), shortage of experienced nurses in emergency departments, increase in workload, and fatigue and burnout are the most significant occupational factors which adversely affect the personnel’ diagnoses and decisionmaking and lead to patients’ dissatisfaction and poorquality care [30].another category of the major challenges which affect emergency care personnel’ clinical decision-making is inefficient organizational management.according to the study of bijani et al., employment of inexperienced personnel who lack the required expertise, lack of facilities and equipment, inadequate support for the personnel, absence of clear instructions, conflicts in the regulations, and failure to hold workshops for the professional empowerment of emergency nurses are the most significant challenges which affect the quality of triage in emergency departments [31].these findings are consistent with the results of the present study.from the participants’ perspective, lack of an emergency doctor to consult and lack of a set of standard clinical guidelines are among the major barriers to satisfactory clinical decision-making.thus, it is recommended that the senior management at emergency care departments set standard evidence-based clinical guidelines for the personnel to follow.also, pre-hospital emergency care personnel should have access to the clinical consultation of an emergency doctor stationed at the emergency care center.likewise, bashiri et al. report that evidence-based clinical guidelines and instructions and having access to advice from a doctor in emergency care departments play a key role in improving the process of clinical decision-making by emergency care personnel and will result in higher quality care [32].another important category which was found to affect emergency care personnel’ clinical decision-making is ethical issues.showing respect for the privacy of patients and maintaining their human dignity are among the most important ethical responsibilities of caregivers [33].accordingly, all caregivers are expected to maintain their patients’ privacy and dignity [34].in the present study, too, the views of the interviewed pre-hospital emergency care personnel showed that one of the key factors which influence clinical decisionmaking is showing respect for the physical and sexual privacy of patients.the dominant religious-cultural beliefs and values in iran dictate that the bodies of all individuals, especially women, should be covered, in bijani et al. bmc emergency medicine (2021) 21:11 page 10 of 12 particular their genitalia and breast area, and that there should not be any physical contact between men and women who are not related to each other.thus, to respect the privacy of patients and accident victims, even in pre-hospital emergency care, it is necessary that patients receive care from same-gender caregivers.but the national laws of iran forbid women from working in pre-hospital emergency care.therefore, there is need for changes which will allow the employment of female personnel in emergency care.on a similar note, the results of the study of torabi et al., show that, from the viewpoint of pre-hospital emergency care personnel, maintaining the physical and sexual privacy of patients is among the key aspects of clinical and ethical decisionmaking [35].in pre-hospital emergency care, maintaining patients’ privacy according to the patient’s physical conditions, personal life, and lifestyle is essential.these rights apply to deceased victims, too [36].in the study of abelsson & lindwall, the participants state that covering the body of a victim, e.g. keeping a patient’s body covered when electrodes are being attached, is essential to maintaining his/ her privacy.keeping a victim’s body covered in line with the victim’s values and beliefs while emergency care is being given to him/her at the scene is an ethical obligation [37].maintaining patient privacy and patient dignity are closely connected.dignity is a social and cultural concept and individuals’ perceptions of maintaining privacy and dignity are influenced by their beliefs and values [38].as a result of the dominant religious and cultural beliefs in iran, women feel embarrassed when they remove their clothing to be examined by a male nurse or doctor and consider that situation to be unethical.therefore, ems male personnel in iran feel uncomfortable when they should provide services to female victims at the scene of accidents, but try to give care based on their cultural and religious values and commitment to professional ethics.strengths of the study the present study is the first qualitative attempt at exploring the challenges which affect clinical decision-making from iranian pre-hospital emergency care personnel’s point of view and can be regarded as a significant accomplishment.in addition, in the present study, a wide range of the factors which have an impact on pre-hospital emergency care personnel’s clinical decision-making were identified.these findings can be employed to improve the quality of pre-hospital emergency care services and the personnel’s clinical decision-making skills.limitations of the study one of the limitations of the study is that it only addressed the views of pre-hospital emergency care personnel and the ideas of patients, as the beneficiaries of emergency care services, were not taken into account.also, due to differences between the cultural, social, and economic conditions of iran and those of other countries, the results may not be completely transferable and similar studies should be carried out in other countries.conclusion in today’s healthcare systems, patients in critical conditions usually receive their first care from pre-hospital emergency care personnel.the more correctly, accurately, and quickly these professionals act, the lower will be the rate of fatalities and possible disabilities and the more trust people can place in the services provided by pre-hospital emergency care departments.the success of these departments depends on a variety of factors, one of the most important of them being the ability to make good clinical decision-making.therefore, it is necessary that those challenges should be identified and addressed in order for the quality of emergency services to improve.the results of the present study show that clinical knowledge, experience, and skills contribute to emergency care personnel’s professional capabilities in making clinical decisions.good teamwork skills and time management can prevent feelings of confusion when the number of the injured to be attended to is large.effective clinical decision-making skills can not only help the personnel make the right decision, but enhances their resilience and enables them to adapt to hard and unpredictable conditions.professional factors, organizational management, and ethical matters constitute the other major factors which influence the clinical decision-making of emergency care personnel at the scene of accidents and determine the quality of their clinical performance.thus, it is essential that prehospital emergency care managers improve the quality of ems personnel’s clinical decision-making ski
number of words= 2414
[{'rouge-1': {'f': 0.27781477717718167, 'p': 0.8929166666666666,'r': 0.1644976076555024}, 'rouge-2': {'f': 0.2356658515050561, 'p': 0.6971777003484321,'r': 0.14179896290386917}, 'rouge-l': {'f': 0.36157775275662457, 'p': 0.7995597484276729,'r': 0.23361071932299013}}]
-----------------------------------------------------------------------------------------------------------------------------------
p104:
Extractive Summary:
according to the burden of disease study performed in iran in 2003, mental and behavioral disorders were ranked as the second in intentional and unintentional injuries [1].therefore, the role of paramedics is critical in rapid and accurate decision making.paramedics have three major needs for a good performance: knowledge, skill for an appropriate clinical decision making, and organizational factors [3].educational courses have been shown to improve their knowledge, attitude, skill, and self-efficacy in performing the job and also will help them in better decision making on the scene [4–6].emergency paramedic may face many difficulties in making decisions for emergency patients with mental illness [7].this article is a report of the process of designing a rapid and effective guide for paramedics who take care of patients to answer increasing demands in a pre-hospital setting.mesh terms, expert opinion, and some keywords from the existing literature review were used to select these keywords.5. to what extent has patient safety been considered at this stage?7. to what extent is the safety of the patient’s relatives and those present at the scene taken into account? 8. considering the current law, how legal is any of the actions of emergency technicians?after adding the scores based on the expert’s decisions, items with a mean score 6 and higher, remained unchanged in the protocol and those with a mean score of 3 and lower were eliminated from the protocol.other items were discussed once more and were finally agreed upon with some minor changes.the above case was presented in a meeting of 60 emergency technicians (with more than 15 years of working experience) from different cities of iran.applicability, clearness and comprehensibility of items were discussed.primary actions the first step is to ensure the safety of the patient, technicians, and people on the scene [15].patient management patient management includes: a) behavioral management, b) pharmacological management, c) patient family management. have empathic and non-judgmental attitudes and behaviors  accept the patient’s hallucinations and delusions appropriately  don’t make a false promise to the patient  use short, simple sentences and repeat the sentences if necessary  listen to the patient  use patients’ words as much as possible  reassure the patient that you understand the problem  encourage the patient to provide information to those who can help  attempt to meet the patient’s spiritual needs (include general spiritual principles in the patient-therapist relationship, showing compassion and unconditional acceptance to the patient)  encourage the patient to provide information to those who can assist him/her. have empathic and non-judgmental attitudes and behaviors  accept the patient’s hallucinations and delusions appropriately  don’t make a false promise to the patient  use short, simple sentences and repeat the sentences if necessary  listen to the patient  use patients’ words as much as possible  reassure the patient that you understand the problem  encourage the patient to provide information to those who can help  attempt to meet the patient’s spiritual needs (include general spiritual principles in the patient-therapist relationship, showing compassion and unconditional acceptance to the patient)  encourage the patient to provide information to those who can assist him/her.in case of aggression, in addition to the mentioned points, keep the patient at least 2m away, tell the patient that aggression is unacceptable, offer medication and in order to prevent harm themselves or others, use physical restraint if s/he continues [19].the aim of emergency medical treatment should be to calm down the agitated patient as quickly as possible without reducing the patient’s level of consciousness.if the patient’s condition does not improve or he/she does not cooperate in the treatment, intramuscular antipsychotics such as amp haloperidol 5 mg along can be used.in addition to the above-mentioned points, consider the contraindications of restraining which include the followings [22, 23]: cases in which patients used phencyclidine (pcp) based on the family history  patients with recent surgery in the eye or central nervous system (because of an increase in intracerebral or intraocular pressure)  patients with a low level of consciousness or with delirium c) intervention in the patient family including having empathy and understanding of the critical situations and psycho-education about the conditions and places that they can attend [24].the expert panel suggests that suicide emergencies need a separate protocol.discussion a psychiatric emergency refers to any disturbances in a patient’s thinking, emotions, or behavior that requires immediate intervention.this disruption usually puts patients in critical conditions, which can put them, their family, and people around them in danger.it is essential to distinguish between the physical and psychological causes of these symptoms because it completely changes the course of treatment.in some reference books and articles, there are general protocols for managing psychotic patients [25].but in most of the previously written protocols, the management of a symptom such as aggression or agitation has been considered [24–27].gargia et al. suggested several recommendations on the assessment of agitation emphasize the importance of identifying any possible medical cause [29].in the current protocol, the first thing is the safety of technicians, their patients, and people who are in the scene.the second point in the present protocol is to consider the overlap of many symptoms of physical illness with psychiatric disorders and to consider the differential diagnosis.in the current protocol, like previous ones, nonpharmacological management is preferred over pharmaceutical methods.lorazepam can be a useful drug, given the short time in the pre-hospital emergency and the need to calm the patient down with the least side effect.injectable benzodiazepines and antipsychotics such as olanzapine (considering its interaction with lorazepam and the possibility of cardiovascular collapse), ziprasidone and haloperidol are the last lines of treatment [19, 35, 36].the use of physical restraint is proposed for patients who have not responded to primary treatments and may harm themselves and others.special attention for patients with delirium is similar to previous protocols [23].study limitations the study has several limitations.
number of words= 973
[{'rouge-1': {'f': 0.2807990356220666, 'p': 0.7566666666666666,'r': 0.17238568588469186}, 'rouge-2': {'f': 0.16774672187210285, 'p': 0.3451677852348993,'r': 0.11079601990049752}, 'rouge-l': {'f': 0.294524531095374, 'p': 0.5988461538461538,'r': 0.1952847380410023}}]
-----------------------------------------------------------------------------------------------------------------------------------
p105:
Extractive Summary:
it is possibly due to increase in rates of childhood obesity and subsequent increase in biliary tract disease [9, 11–14].studies have shown that biliary disease is usually the most common comorbidity associated with pediatric pancreatitis [2, 15–17].however, these studies include a wide age range including both young children and adolescents.another theory about the cause of increased childhood pancreatitis is a heightened awareness of pancreatitis among clinicians and increase in widespread access to lipase testing [1, 6, 7, 9].there are few studies that focus primarily on younger patients and describe the etiology, risk factors and outcome of these patients, and no studies that look specifically at preadolescent children in the emergency department [18].our objective was to determine the characteristics of this patient population in order to illuminate this population and disease.methods we performed a retrospective study of all consecutive pediatric patients under the age of 13 years between 2006 and 2016 who presented to the pediatric emergency department with a final diagnosis of pancreatitis.an icd code of acute, recurrent or chronic pancreatitis and age less than 13 years were used to query the medical records database.charts of patients with a previous history of pancreatitis but currently admitted for an unrelated problem and with no other signs or symptoms of concurrent pancreatitis were also excluded.using a standardized data collection form and trained data collectors not blinded to the study objective, the following data points were collected: gender, height, weight, ethnicity, past medical history, medications, and whether or not the patient had a family history of pancreatitis.finally, the stated etiology of pancreatitis, whether the episode was acute, chronic, or recurrent, and the number of previous pancreatitis episodes were obtained.we planned to develop etiologic categories starting with cholelithiasis, triglycerides, and medications.other categories were determined in a post hoc manner.we assumed that alcohol would not be a significant etiology in this age group due to previous research showing that alcohol use disorder is nonexistent to extremely rare in the preadolescent age range, use of alcohol among children has declined significantly over the last 10 years, and that alcohol has been found to be nearly negligible as an etiology for pediatric pancreatitis [19–25].descriptive statistics were used.results during the 10 year period from 2006 to 2016, we identified 153 visits to the emergency department for children less than 13 years with a final diagnosis of pancreatitis.of the 139 visits, 85 visits were children with a first episode of acute pancreatitis, and 54 were visits for recurrent pancreatitis.the median age for acute pancreatitis visits was 8 years (iq range 4–11).the median time from the first acute episode to a first recurrence was 6.3 months (iq range 2.5–12months).no patients had hypertriglyceridemia during the study period.there were 54 visits for recurrent pancreatitis of which 20 patients had one episode each of recurrent pancreatitis; six patients each had two episodes of recurrent pancreatitis; one patient had three recurrences; one patient had five recurrences; two patients each had seven recurrences.of the 30 patients with recurrent pancreatitis, 16 were found to have underlying genetic etiologies or structural risk factors including 11 patients with genetic variations, one patient treated for autoimmune pancreatitis, one patient with caroli disease, one patient with hennakam syndrome, one patient with celiac disease, and one patient with a pancreatic duct stricture.of the six patients, two were diagnosed with chronic pancreatitis and treated for their episodes despite normal lipase levels and no imaging findings of acute pancreatitis; three patients were diagnosed with recurrent acute pancreatitis based on imaging: one with an us showing an echogenic pancreas, one with a ct showing a pseudocyst, and one with an us and ct showing a thinned pancreas and infected pancreatic pseudocyst.other studies of pediatric pancreatitis showed similar significant proportion of idiopathic cases [26–33].this high frequency is similar to that seen in other studies of pediatric pancreatitis [19, 27, 33, 35].some of the patients with the etiology listed as unknown or uncertain had risk factors such as cholelithiasis or family history that were not listed as the etiology.since labs are less routinely required or ordered in children than in adults presenting with abdominal pain, it is possible we may be missing pancreatitis in children without known risk factors.another way that pediatric pancreatitis may differ from adult pancreatitis is in recurrence rate.
number of words= 706
[{'rouge-1': {'f': 0.5218392313064567, 'p': 0.8127652733118971,'r': 0.3842857142857143}, 'rouge-2': {'f': 0.26848089956778853, 'p': 0.3893548387096774,'r': 0.20487738419618529}, 'rouge-l': {'f': 0.4574197972446779, 'p': 0.6255555555555556,'r': 0.360519877675841}}]
-----------------------------------------------------------------------------------------------------------------------------------
p106:
Extractive Summary:
road traffic injuries (rtis) have become a major health problem in many low- and middle-income countries (lmics) [1].rti is the leading cause of injury death, worldwide, among those ages 15–29 years [2, 3].however, it is now agreed that the impact of rtis can be minimized if post-crash care is maximized [11].effective post-crash care is supposed to include prompt communication with and activation of the emergency medical system (ems), if one is available; a prompt response from the activated system; and assessment and treatment of the victim on site, followed by expeditious transport to an appropriate health facility [12].the care that rti victims receive from prehospital providers ranges from no care at all to high-quality care, depending on the knowledge and skills of the providers.because a minute can make a difference between life and death in the prehospital environment, intervention should be appropriate for the best outcome.studies show that interventions range from basic life support (bls) to advanced life support (als) [13]; although als includes more components, it has been shown to be complicated, to be an inefficient use of resources, and to be no better than bls measures at saving lives at the crash scene [14].while most high-income countries (hics) have an advanced ems [15], few lics have any system-level form of emergency medical services; further, it has been found that even where such systems exist in lics, 98% of individuals who use them are not satisfied with the care providers [16].tanzania is one of the great majorities of lics where an ems is not available.however, in most cases police become involved in post-crash care directly or indirectly by providing instructions to other responders and coordinating transport for rti victims.although the study found that 99.1% of officers had a positive attitude toward caring for rti victims, the officers reported performing many incorrect interventions during immediate post-crash care that could endanger victims’ lives.regarding prehospital care in tanzania, at least one study has focused on the prehospital experiences of injured victim [23], yet little is known about the experience of first responders in providing initial post-crash care to victims of rtis.method study design we selected a qualitative explorative study design in which we used individual interviews and focus group discussions (fgds) with traffic police officers in dar es salaam region, tanzania.however, it has been found that more than 50% of victims transported by the police die before they arrive at the hospital [21].during phase three (may–june 2018), three fgds were conducted; two fgds had 10 participants while the third had 11 participants.participants were selected from traffic police officers who had participated in a recent survey study [22].a strong inner fear of touching individuals who are injured arises, the officers said, but they reported that they find courage, and calm themselves in order to fulfil their duties of clearing the scene, maintaining safety (their primary role), and meeting others’ expectations of transporting injured victims to the hospital and providing some initial help when possible.doing work that is not easy even for professional rescuers, the officers are further challenged by limited access to training and technical equipment; this creates a difficult environment that puts their lives and those of the victims who receive care from them at increased risk.from analysis of descriptions of initial post-crash care of rti victims from the injury scene to the hospital, three themes emerged: maintaining safety while saving injured victims’ lives and facilitating access to the health facility refers to the officers’ perceived role of maintaining personal, bystander, and victim safety, as well as helping victims all the way from the scene to the hospital.improving support system and empowering frontline personnel refers to the officers’ views on actions that should be taken to support their role of providing initial post-crash care of rti victims (table 1).theme 1: maintaining safety while saving injured victims’ lives and facilitating access to the health facility the traffic police officers said that in addition to their primary role of maintaining safety, they also provided some initial management and helped injured victims reach the health facility for proper care.providing initial help to the injured victim the participants reported several initial actions they took to help rti victims.transporting rti victims to the hospital the officers reported that transporting rti victims to the hospital was a crucial responsibility.this responsibility was taken as a priority regardless of the condition of the injured person; private cars and sometime police patrol cars were mentioned as means of transport used to take victims to the hospital.this is because you might ask for help in the first car and the driver comes up with a lot of excuses.... in that case i take my own money, hire a taxi, and ask the driver to rush the victims to the hospital”.the officers reported using victims’ or bystanders’ clothes to prevent direct blood contact when helping victims.although this method was mentioned by several participants, some declared that the method was not effective and that it did little to prevent crossinfection.(participant 2) difficulty facilitating victims’access to health facilities concern was expressed that even if a traffic police officer arrived early at the scene, the injured victim would still be unable to get to the hospital quickly.(participant 7) participants further reported feeling demoralized because they received so little recognition for their efforts to help rti victims, especially from bystanders who ended up blaming the traffic police officer for not arriving at the scene on time and not delivering care at the scene the way they thought it should be done.lack of a support system at the scene and at health facilities the officers related that once a crash occurs, bystanders crowd the scene without offering any assistance to the police in helping the victims.they also suggested that once the number of police patrol cars was increased, a few could be specifically identified and reserved for transporting victims.the need to employ more police officers who would be trained in first aid and who would be specifically available for caring for injured victims was also expressed.(participant 10) training for police and drivers on victims’ first-aid care and road safety first-aid training would help both police and drivers, especially bus and taxi drivers, deliver better care to rti victims.based on the expression of the need for training, suggestions for workshops on how to initiate care of ill and injured victims were highlighted.this is because provision of post-crash care via an ems is organized, timely, and safe, and its delivery is done by trained personnel using well-equipped ambulance for transporting victims [30, 31].in the present study, traffic police officers reported that one of their responsibilities after arriving at the scene was to provide some initial injury management and other aid to victims and facilitate their transport to the hospital.although this was considered “an initial care to injured victims,” the study found that the concept of first aid is not well understood by traffic police and that for them, the provision of first aid basically involves interventions such as extricating victims from the scene and transporting them to the hospital for further management.providing initial help without applying such safety principles could expose victims to further injury or result in permanent disability or even a decreasing their survival time.describing the initial interventions without mentioning application of safety principles, as was the case in many interviews in the present study, indicates a lack of proper knowledge and skills for providing care to rti victims on the part of traffic police.poor knowledge and skills in regard to the provision of care to rti victims by first responders has been demonstrated in other studies to be the main reason for inappropriate intervention during care provision [22, 33].this situation calls for immediate remedies; otherwise, the police officers who are first at the scene will continue to be of no help to victims, except to rush them to the hospital with poor outcomes afterward [34].unavailable equipment and supplies not only limits officers’ ability to provide quality care to rti victims, but also puts them at risk of suffering biological, physical, chemical, and even psychosocial problems [36]those people who are forced to convey victims to the hospital in their private cars do not have knowledge about transporting and caring for injury victims; furthermore, their cars are not designed to carry these victims [17, 34].potholes and other forms of roadway damage not only slow transport, they also increase the likelihood of further injury because victims are not properly secured in the car.the training has been shown to be costeffective: similar training has been done in other countries, and participants have shown retention of skills and knowledge to a significant level [44], with improved patient outcomes the result [28].improvement of infrastructure will hasten the arrival of the rescuer to the crash scene and reduce the time spent getting to the hospital.lastly, medical care organizations can better satisfy their responsibility to save lives by placing care providers closer to potential crash scenes and increasing collaboration with the police officers who have been identified as, in many cases, the first responders.implications of the study we explored experience of police officers in provision of initial care to rti victims at the scene.for effective outcome of post-crash care, such care needs to be appropriate, timely and safe.to equip police officers better for that role, lifesaving skills and basic principle of postcrash care should be an inclusive topic in police training curriculum and one of the competencies to be acquired during their initial training.to ensure traffic police officers effectively fulfil this role, the government, through the ministry of internal affairs in collaboration with ministry of health, should provide police with necessary equipment that can be utilized in provision of basic post-crash care to rti victims at the scene.to facilitate easy access to formal hospital care and ensure maximum outcome to rti victims, these findings suggest the need for government efforts of establishing an organized ems that ensure safe and appropriate continuum of care from the scene to the hospital.limitations of the study we explored traffic police experiences of providing postcrash care, trying to understand officers’ role, challenges, and opinions about improvements.we acknowledge that similar insights can be acquired from taxi drivers, truck drivers, and community members, who have been found to be the main first responders in other studies [34, 44].ten interviews and three fgds were conducted for the present study, which might seem a small sample.interviews were conducted in swahili and then translated to english; we acknowledge that some of the meanings of the participants might have been lost in the translation process.however, two authors with swahili as their native language were involved in the translation process.trustworthiness when the study findings are worth believing, the study is said to be trustworthy [45].in a qualitative study, trustworthiness is assessed on the basis of four criteria: credibility, transferability, dependability, and confirmability [46].in the present study, credibility was ensured through triangulation of participants with various experiences who shed light on the research question from a variety of aspects.to enhance credibility and dependability, the data collected from individual interviews were triangulated with those from fgds during the analysis process [47–50], and selected codes, categories, and themes were shared among the co-authors, who gave critical comments and suggestions based in large part on their diverse backgrounds and degrees of familiarity with the setting.to confirm that the findings reflected the informants’ experiences rather than the researchers’ understanding of the problem, the presented findings were supported by codes and quotes.transferability was enhanced by describing the study context and the processes for data collection and analysis in order to show that the results can be used in other contexts and among other traffic police officers.however, lack of appropriate lifesaving knowledge and skills among traffic police results provision of unsafe interventions, minimizing the chances of surviving of rti victims.a lack of specified means of transport for injured victims delays timely access to health facilities, increasing the chances of death and disabilities resulting from delayed interventions.for optimal post-crash care delivery, efforts should be made to improve the post-crash care support system and empower frontline personnel, including traffic police.government and other relevant authorities should ensure that traffic police and other first responders are well trained in basic skills for delivery of post-crash care at the scene.furthermore, the government should invest in establishing a trauma prehospital care system, which would consist of accessible, available, and properly equipped ambulances and trained traffic personnel.however, more research will be needed to determine the acceptability and efficacy of such a system in the tanzanian contex
number of words= 2075
[{'rouge-1': {'f': 0.32690882511698793, 'p': 0.8815942028985508,'r': 0.2006579561362576}, 'rouge-2': {'f': 0.2091731091316092, 'p': 0.4711627906976744,'r': 0.13442577030812325}, 'rouge-l': {'f': 0.36944411294060286, 'p': 0.7232663316582915,'r': 0.24808219178082191}}]
-----------------------------------------------------------------------------------------------------------------------------------
p107:
Extractive Summary:
public awareness is of great importance, especially for life-saving information and skills.globally, many countries emphasize related topics in educational institutions and workplaces.death from sudden cardiac arrest remains very common worldwide [1, 2].approximately 50,000 people per year can survive until professional help arrives when basic life support (bls) measures are performed properly.out-of-hospital cardiac arrest (ohca) causes 350,000 deaths each year in europe [3].in the us, the mortality rate due to ohca is more than 90%, causing 276,000 deaths annually [4].when the first person to witness ohca uses an automated external defibrillator (aed), the chance of survival doubles.however, this instrument is used in only 4% of cases [5, 6].the rates also differ according to the specific area where the study was done.in a study evaluating knowledge and attitudes towards cardiopulmonary resuscitation (cpr) among university students in riyadh, saudi arabia, it was found that 31% did not have prior cpr knowledge [17].methods the purpose of the current study was to evaluate public awareness, knowledge and attitudes towards bls in jordan.the participants who met the following inclusion criteria were included: (a) jordanian nationality, (b) aged 18 years and above, (c) able to read and write arabic, (d) signed informed consent, and (e) not graduated from or studying at any medical field colleges (medicine, pharmacy, nursing, dentistry, and laboratory).the principal investigator met with all research assistants and explained the purpose of the study and the questionnaire.a chi-squared test was used to determine if there were differences between those who received training and those who did not regarding the identification of cardiac arrest, action performed when witnessing cardiac arrest, and practical application of cpr.most of those received their training at school (n = 33, 37.9%), followed by university (n = 26, 29.9%), and the media (television and internet) (n = 20, 23%).signs of cardiac arrest the response rates for the signs of cardiac arrest are presented in table 2.the highest rates were for (1) chest pain (n = 129, 43%), (2) respiratory standstill (n = 119, 39.7%), (c) loss of consciousness (n = 114, 38%), and (d) difficulty breathing (n = 107, 35.7%).recognizing cardiac arrest table 3 shows comparisons between those who had received training about cpr and those who had not regarding cardiac arrest signs, including (a) consciousness evaluation, (b) respiratory evaluation, and (c) circulation evaluation.among those, 42.8% called the ambulance, 20% told someone to call for help, 10% gave chest compression, 10% gave mouth-to-mouth breathing, 8.6% gave both chest compression and mouth-to-mouth breathing, and 8.6% just watched and left.in general, jordan has a lower percentage of trained people in comparison to other countries, indicating that more focus is needed to train the public in life-saving skills to bring jordan in line with international norms.these results highlight the importance of training programs and ensuring that necessary information is both fully understood and practiced by trainees.the response rates for the signs of cardiac arrest showed that the highest rates were for chest pain, respiratory standstill, loss of consciousness and difficulty breathing.similar data were reported in a study conducted in turkey, in which respondents indicated that the highest rates were for signs of cardiac arrest (60.7%), followed by 49.3% who answered cessation of circulation, and finally cessation of breathing (40.7%) [1].therefore, chest pain and circulation are given first priority by lay people, who then look for respiration.among those, nearly half stated that they called the ambulance, and the rest with equal or lower percentages responded that they called for help, gave chest compression, and gave mouth-to-mouth breathing.the lowest priority concern for performing cpr to both family and strangers was contamination by blood or vomit.these characteristics support the idea of a greater willingness to conduct cpr among educated and employed people.in their study, chen et al. [19] described the concerns regarding cpr applications for family members and strangers.the questionnaire included individual information, current status of bystander cpr training, and individuals’ willingness and attitudes towards performing cpr.the authors found that 25.6% of lay people received cpr training, the majority (98.6%) of whom would perform cpr on their family members, but fewer (76.3%) were willing to perform it on strangers.most respondents (53.2%) were worried about legal issues.when laws were implemented to protect bystanders who give first aid, the number of laypersons who were not willing to perform cpr on strangers dropped from 23.7 to 2.4%.although the number of people in china who knew cpr was increasing over time, cpr training was still much less common than in many developed countries.the barriers as mentioned by the authors were that laypersons were not well trained, and they feared being prosecuted for unsuccessful cpr.the authors also recommended a need for more accredited cpr training courses, and they noted that laws should be passed to protect bystanders who provide assistance.there are some centers providing cpr training for lay persons in different areas in jordan, including in the north, middle, and south.however, to date, there is no legislation on the part of the ministry of health and the ministry of interior to protect laypersons in case they perform cpr on victims and mistakes occur.for this reason, some people will hesitate to provide cpr if the victim is not one of their family members.the relevant government bodies should be informed about such results in order to improve outcomes and decrease mortality and morbidity after cardiac arrest, especially ohca.conclusion and recommendations the results of this study indicate the importance of continuing to train the population in cpr knowledge and skills; specifically, cpr education programs and evaluations of their effectiveness are needed.
number of words= 914
[{'rouge-1': {'f': 0.3373954362977305, 'p': 0.750952380952381,'r': 0.2175748194014448}, 'rouge-2': {'f': 0.178260149263334, 'p': 0.3188038277511962,'r': 0.12371900826446282}, 'rouge-l': {'f': 0.36990360001924893, 'p': 0.6768965517241379,'r': 0.25448637316561845}}]
-----------------------------------------------------------------------------------------------------------------------------------
p108:
Extractive Summary:
poor communication during patient handover is recognised internationally as a root cause of a significant proportion of preventable deaths [1] and as such is named as one the of the top five world health organisation improvement priorities [2].despite the introduction of a number of recommended strategies to reduce this harm, the handover phase of care continues to impact negatively on patient outcome [3–6].it is widely accepted that handover of patients in any clinical setting carries significant risk [7].however, handover of critically ill or injured patients from the pre-hospital to emergency department (ed) team carries additional risk due to the time critical nature of the process and multiple human factors involved in dealing with stressful clinical events [5, 8].such knowledge has led to calls for further, more rigorous research into handover, and to develop solutions to support handover in diverse environments [5, 9, 10].despite the limited evidence on pre-hospital to ed handover practice, a number of pragmatic options, informed by theory, have been recommended.these include for example, the development of shared mental models, standardisation of approach (an agreed handover system and format/mnemonic) applied at the interface between professional domains, and the introduction of technology to support and enhance the process [5, 7, 11].but unfortunately, cultivating a movement towards a shared mental model has remained a challenge with evidence of considerable inconsistencies in mnemonic usage and preference both nationally and internationally [12–15].beyond these higher level, theoretically informed systems and processes, there are other more fundamental and pragmatic challenges to consider and address.recording clinical data used during handover accurately and with ease is incredibly challenging during the delivery of prehospital care.excluding some of these essential variables during pre-alert or handover may delay subsequent life-saving interventions [18, 19].hardware placed in ambulances to support the recording of clinical data used during handover is often in tablet form.additionally, data recorded will be variable and with heavy contamination of gloves being a particular problem in these environments, glove changes can be frequent [26].as such, both gloves or paper will be easily discarded or lost.methods this project aimed to measure the feasibility and acceptability of a novel intervention designed to support clinical information recording and delivery during pre-alert and handover within the pre-hospital and ed setting.design and setting a simple pre and post-test design was used with a historical control.the service is primarily set within an anglo-american model of care [29] whereby road based paramedics and emergency medical technicians (emt) deliver the majority of care.at the time of study the service operated within 5 geographical divisions (north, east, south, west and west central).the study was undertaken in west central division, which covers the most densely populated areas in the west of scotland.our intervention hospital and ambulance station were therefore located in a busy urban setting in the centre of glasgow; population 606,000.all ed participants were invited to complete the respective pre and post-test questionnaires.a study stakeholder group was formed to synthesize the results of both along with published evidence and particularly within the context of existing challenges affecting key points in the handover process: pre-handover, arrival, handover and post-handover [7].a specific focus was retained on the technical aspects of information recording in both the prehospital and in-hospital settings.a graphic design expert designed the card based on the stakeholder group’s requirements.the prototype intervention consisted of a double-sided a6 card in high contrast colour with pre-alert and handover clinical information requirements on opposing sides.corresponding boxes for clinical variables were available for writing in with a marker pen.the variables included were known prognostic indicators (individually or collectively) for degrees of severity in injury and illness [32, 33].the intervention was piloted in a simulated setting with prehospital critical care practitioners and physicians prior to being finalised.the simulation led to a change in physical format from a card encased in a sealable plastic cover to a stand-alone, high density plastic card (additional file 1).a number of pens were tested and a type selected that ensured rapid dry and non-smudge properties.cards were to be used in time-critical cases with both the pre-alert and handover sections to be completed contemporaneously with assessment at appropriate times during care.during pre-alert and handover clinical information would be delivered in the sequence provided on the pahc.receiving staff would record this handover information in the respective sections on their af.potential participants (n = 99) received an e-mail containing a link to the online survey along with information on the project.consent was presumed by completion of the questionnaire.key clinical data were recorded on these sheets during the pre-alert and handover.the different questionnaire formats reflected the operational environments for each clinical group and were designed to improve response rates.the decision to use paper-based surveys for ambulance clinicians was informed by the predictable difficulties experienced with a highly mobile workforce, their inability to access computers and past studies undertaken in similar settings [37, 38] that used evidence informed methods [39, 40] which had previously provided improved response rates.questions consisted of likert scales and dichotomised yes/no responses.ed staff repeated the same questions used in the pre-test questionnaire.this permitted comparisons between the pre-test (historical control) and post-test measures.prior to distribution, questionnaires were assessed for ease of completion and comprehension.this led to a small number of changes in wording to aid comprehension.again participants were provided with a cover letter and information sheet with consent presumed by completion of the questionnaire.descriptive statistics were used to present the results of the questionnaire.mann whitney u analysis was used to compare the median ratings of the pre and post-test measures on opinions of handover quality.there were approximately 160 (20/week) pre-alerts made to the ed during this time period.the median length of service in years was 16 (iqr 8–30).acceptability and perceived utility of intervention ninety six percent (n = 24) of participants reported receiving the intervention.most felt both the pre-alert and handover components of the card were ‘useful’ to ‘very useful’ (92%; n = 23 for pre-alert and 72%; n = 18) for handover).however, a small proportion (12%; n = 3) rated the pen as either ‘poor’ or ‘very poor’.perceptions on the cards legibility in both bright or poor lighting were also sought.sixty four percent (n = 16) rated it as ‘easy’ (24%; n = 6) or ‘very easy’ (40%; n = 10) to read in bright lighting and 56% (n = 16) ‘easy’ (24%; n = 6) or ‘very easy’ (32%; n = 8) to read in poor lighting.forty percent (n = 10) were neutral in their response.emergency department participants thirty seven percent (n = 37/99) of ed staff responded to the pre-test questionnaire; doctors 63.2% (n = 24) and nurses 36.8% (n = 13).slightly less responded to the post-test questionnaire; 29% (n = 29/99); doctors 48% (n = 14) and nurses 52% (n = 15).the overall responses in the pre-test survey were relatively neutral but clearly demonstrated a requirement for improvement in each of the handover domains measured.results from the post-test survey demonstrated small but statistically significant improvements in perceptions of handover in 3/5 domains measured (table 1).objective measure of pre-alert information ambulance pre-alert forms were routinely used within the ed throughout both the pre-and post-test stages of the study.to provide an indication of any change in handover clinical information provision data were taken from three time points i) the month leading up to the intervention introduction, ii) the end of week 5 (after the 4 week bedding in period) and iii) at week 8.despite these similarities in demographics, low response rates and small sample size limit the results generalisability to the wider ambulance population and as such these results should be interpreted with caution.
number of words= 1256
[{'rouge-1': {'f': 0.37459260320380866, 'p': 0.808709677419355,'r': 0.24374810318664644}, 'rouge-2': {'f': 0.24096073232878037, 'p': 0.4648220064724919,'r': 0.1626347760060744}, 'rouge-l': {'f': 0.37404085347704286, 'p': 0.671063829787234,'r': 0.25927973199329984}}]
-----------------------------------------------------------------------------------------------------------------------------------
p109:
Extractive Summary:
the aim of this study was to determine the frequency of ambulance use by patients coming to the major emergency departments (eds) of pakistan.it will also compare the characteristics of patients coming by ambulances and those coming by other modes of transportation, such as public and private vehicles or walk-in patients to the eds.the aman foundation is another non-profit organization in karachi which deals with healthcare, education and skills, and nutrition for underprivileged [12].a few ems services arose due to efforts of provincial governments such as rescue 1122 [13].however, these ambulance services currently work in pakistan at the local level and are not part of an integrated emergency care system.pak-neds did not collect data from any of the above mentioned ambulance services.study procedure data collectors were specifically hired and trained for pak-neds.they worked in three shifts providing 24/7 coverage.a one-page standardized tool was developed based on an ambulatory care survey tool developed by the centers for disease control and prevention, usa and on previous surveillance work done in pakistan [14,15].the tool gathered information related to patient demographics, presenting complaints, treatment and management provided in the ed, provisional diagnosis and disposition from the ed.the mode of ed arrival was categorized into two groups; ambulance and nonambulance.all presenting complaints were categorized into two major categories, injuries and non-injury.injuries included unintentional and intentional injuries.the types of injuries recorded were falls, burns, drowning, poisoning, road traffic injuries and firearm injuries.non-injury included general presenting complaints like fever, fatigue, weakness, swelling and complaints based on the body organ involved; for example, chest pain was grouped under cardiovascular system, rectal bleeding as part of gastrointestinal system.other systems in this category were respiratory, central nervous system, musculoskeletal, head and neck, and uro-gynecology.cities were grouped together by geographical location of participating hospitals: aga khan university and jinnah post-graduate medical center in karachi; mayo hospital in lahore; benazir bhutto hospital and shifa international hospital in rawalpindi/islamabad; lady reading hospital in peshawar; and sandeman provincial hospital in quetta.we also looked at the use of ambulance as an outcome variable.univariate and multivariate logistic regression was carried out to look at the factors associated with ambulance use.a multivariate model was developed with independent variables including gender, age group, city, hospital type, presenting complaint and disposition.there are no global standards for appropriate utilization rate for ambulances.many factors are likely to play a significant role in the utilization rate including availability of ambulances, cost of the service, differences in the disease burden and severity of illnesses, age distribution of population, geographic spread and availability of alternate methods of seeking care.an even higher percentage of 67.3% was reported in a recent study from india [21].pak-neds shows that patients coming to the ed were likely to be elderly patients, those with injuries especially head injuries, and those who were likely to die in the ed.in comparison to older patients, injury was the common reason for transporting younger patients to ed via ambulance.
number of words= 485
[{'rouge-1': {'f': 0.4639989693528883, 'p': 0.6334328358208956,'r': 0.36607843137254903}, 'rouge-2': {'f': 0.20343381796763485, 'p': 0.2572659176029963,'r': 0.16823182711198428}, 'rouge-l': {'f': 0.3614114101717305, 'p': 0.48721854304635764,'r': 0.2872413793103448}}]
-----------------------------------------------------------------------------------------------------------------------------------
p110:
Extractive Summary:
background coronary artery disease (cad) is a complex, multifactorial disease driven by the cumulative and interactive modular effects of gene–gene, gene–environment and epigenetic interactions [1].notwithstanding intense investigation in the postgenomic era, the fundamental biological pathways underlying the multidecade process of atherosclerotic formation and chronic inflammation in cad have not yet been addressed [2].the need for unraveling the molecular and genetic underpinnings of cad at a deeper level is stressed nowadays, due to exceptionally high mortality rates of cad, despite the expanded arsenal of precision medicine [3].hitherto, several genome-wide association studies (gwas) have mapped more than 150 single-nucleotide polymorphisms (snps) potently implicated in cad pathogenesis [1, 4–7].these candidate variants are not yet established though and as next generation sequencing (ngs) becomes the heart of high-throughput genotyping technologies, several plausible genetic variants linked with multifactorial traits of cad might be discovered, shedding light on the road of personalized medicine [8].meanwhile, significant therapeutic implications emerge from the integration of genetic data into predictive risk scores.specifically, several studies have been envisaged, in order to correlate distinct genetic variants with modulation of the risk for cad occurrence or progression [9–11].in those studies the severity of cad has been assessed via clinical, laboratory or imaging parameters, but not with the synergy between percutaneous coronary intervention with taxus and coronary artery bypass graft surgery (syntax) score yet [12–15].despite, the syntax score relies on invasive coronary angiography findings and the discovery of risk stratification algorithms that facilitate non-invasive estimation of cad complexity could alter the prognostic plan in patients with cad.ultimately, the gess trial aspires to develop a genetic syntax score that could non-invasively enable the identification of patients with complex and severe cad after a bloodbased gene expression analysis.written informed consent will be obtained from each patient prior to study enrollment and the trial procedures conform with the declaration of helsinki [17].gess study is designed to enroll 1080 consecutive adult patients admitted to ahepa university hospital of thessaloniki, greece and undergoing coronary angiography for clinical purposes.for the purpose of this research, patients with history of prior percutaneous coronary intervention or coronary artery bypass grafting and patients unwilling to provide informed consent will be excluded from the study.accordingly, study participants will be classified into 3 main subsets, based on their clinical presentation: 1.patients with chronic coronary syndrome, and iii.moreover, all enrolled patients will undergo selective coronary angiography, which will be performed through radial or femoral artery approach in the cardiac catheterization laboratory of the hospital.according to their syntax score, patients will be categorized into the following groups: i. low syntax score (0–22) group, ii.the first participant of the study was enrolled in february 2019 and 783 patients have been recruited through november 2020.telephone follow-up will be systematically carried out for every study subject at 1 year after enrollment, in order to document the incidence of cad symptoms, major adverse cardiovascular and cerebrovascular events (macce-need for coronary revascularization, myocardial infarction, stroke/ transient ischemic attack or all-cause mortality) and bleeding complications (bleeding academic research consortium classification score [18]).statistical considerations sample size estimation and endpoints of the study the primary endpoint of the study is to discover potential correlations of the syntax score with patients’ genomic profile and create a blood-based gene expression test (genetic syntax score) which could accurately identify patients at high risk for cad of moderate or high severity.to this regard, we made use of the exact sampling distribution of the squared multiple correlation coefficient implemented in g*power assuming 250 predictors, a two-tailed test, power of 0.9, significance level of 0.05, ρ2 = 0.13 and a ratio of unexposed to exposed equal to 2 (based on a pilot study on 100 patients).secondary endpoints of the study are the development of a panel of genetic markers that, in conjunction with clinical parameters, could strongly predict the occurrence of macce or any bleeding events during follow up.to assess the fitting performance of the final model, well-known evaluation metrics for regression (e.g. mean and median squared, absolute and percentage errors) and classification tasks (accuracy, f-measure, g-mean, precision and recall) will be used, whereas for the evaluation of the prediction performance of the model, data-generating schemas (i.e. holdout and k-fold cross-validation) that split the available dataset into training and test sets will be performed.more specifically, the non-parametric kaplan–meier analysis will be conducted for graphically evaluating the survival function of patients, while log-rank tests will be conducted for investigating effects of different factors on survival distribution.discussion gess is a prospective ongoing study designed to determine the impact of the presence of several genetic variants on cad severity.the aim of this study is to further understand the pathogenesis of cad by utilizing 3 fundamental pillars: (1) invasive coronary angiography and standardized syntax score calculation; (2) revolutionary ngs technologies; and (3) systems biology-based bioinformatics.to our knowledge, hitherto, this is the first study designed to establish a prognostic blood assay for the association of the presence of a large number of snps with cad severity, as evaluated via the syntax score.the cardiogramplusc4d consortium has carried out a meta-analysis in a total sample size of over 190.000 patients and demonstrated a highly significant correlation of 36 snps with cad [6].compass and predict trials created 2 gene-expression scores outperforming clinical factors and non-invasive imaging in discriminating patients with > 50% stenosis [45, 46].first, the single-center character of the study and the enrollment of patients from a greek-based population may limit the generalizability of our findings, even if our sample will represent a broad spectrum of patients with cad.therefore, we could define cad at the deepest level and clinical cardiologists would be guided in decision-making via an absolutely personalized approach.conclusion in conclusion, genotyping of patients presenting with cad symptoms could potentially disentangle genetic risk variants implicated in cad progression.
number of words= 950
[{'rouge-1': {'f': 0.4116103648556353, 'p': 0.7037209302325582,'r': 0.290871327254306}, 'rouge-2': {'f': 0.19707922570121256, 'p': 0.2944897959183673,'r': 0.14809330628803247}, 'rouge-l': {'f': 0.32938869149015176, 'p': 0.5006220095693781,'r': 0.24543859649122807}}]
-----------------------------------------------------------------------------------------------------------------------------------
p111:
Extractive Summary:
with an estimated annual incidence of 1.7 per million [1], cushing’s disease is rare.untreated, it poses serious complications including osteoporosis, hypertension, dyslipidaemia, insulin resistance, and hypercoagulability [2] and is associated with a 4.8 fold increase in mortality rate [3–5].since the first report in 1997 [13], the selective removal of an adrenocorticotropic hormone (acth)-secreting pituitary adenoma by endoscopic transsphenoidal surgery has gained popularity as the first line treatment for cushing’s disease.the primary goal of etss treatment in cushing’s disease is to produce disease remission and to provide long-term control, while minimising complications.remission rates are dependent on tumour size, preoperative mri, cavernous sinus invasion, intraoperative visualisation of the tumour and pre- and postoperative acth and cortisol concentration [11].several studies also report pituitary neurosurgeon experience as a major factor for operative success [2, 14, 15].reported remission and recurrence rates after tss for cd vary widely according to the criteria utilised to define remission [11], and in some studies due to limited patient numbers or short follow-up periods.indeed, there is no clear consensus on how best to define postoperative remission; an early morning serum cortisol concentration < 138 nmol/l (5μg/dl) within 7 days of tss is quoted in the 2015 endocrine society clinical practice guideline as indicative of remission [16].a more strict day 3 cut-off of 50 nmol/l (1.8 μg/dl) has been reported in paediatric studies [17], and also included in the endocrine society guideline [16]; the literature suggests this cut-off is associated with remission, and a low recurrence rate of approximately 10% at 10 years [14].the main objective of this study was to assess the outcomes of endoscopic transsphenoidal surgery for cushing’s disease in a tertiary pituitary centre; remission using two widely accepted criteria [16], recurrence and postoperative complications.methods study design this is a retrospective analysis of a prospectivelymaintained database of patients operated on by a single neurosurgeon (mj), via image-guided endoscopic transsphenoidal approach for cushing’s disease.patient data was gathered over 8 years (january 2012 to february 2020) and identified from the institution’s prospective database.clinical and biochemical data during the follow-up period was reviewed.approval was granted by the hospital audit committee.as per standard guidelines, cushing’s disease was diagnosed on the basis of elevated serum acth measurements, along with confirmatory hormone responses to peripheral corticotropin releasing hormone (crh) test and inferior petrosal sinus sampling (ipss).patients with previous tss prior to the study period were excluded.surgical procedure a single neurosurgeon subspecialising in endoscopic pituitary and anterior skull base surgery, m.j, carried out all etss surgical procedures.the surgical technique has been described in detail in publications by cappabianca et al. (1998, 1999) and jho et al. (1997, 2000, 2001) [13, 18–21].in summary, the procedure consists of a binostril endoscopic transsphenoidal approach.a selective adenomectomy was performed on patients with adenomas noted on pre-operative mri.in cases of negative pre-operative mri, exploration of the pituitary gland was performed.to confirm the diagnosis of acthsecreting adenoma or hyperplasia, all specimens removed underwent histopathological and immunohistochemical staining for pituitary hormones.a blood sample for serum cortisol was drawn at 0800 h on the morning of day 3, if clinically stable, prior to administration of hydrocortisone.the endocrine society clinical practice guideline define post-operative biochemical remission as morning serum cortisol < 138 nmol/l (5μg/ dl) within 7 days postoperatively [16], ‘standard criteria’.in our institution, we also apply a biochemical cut-off of < 50 nmol/l (1.8 μg/dl) at day 3 postoperatively to allow early indication of biochemical remission, ‘strict criteria’.median (range) duration of symptoms was 24 months (6–144), 72% (28/39) had hypertension, and 28% (11/39) had type 2 diabetes.three patients had an early repeat etss for persistent disease; day 3 serum cortisol ranged from 306 to 555 nmol/l and interval to repeat etss from 10 days–3 months.eleven patients (28%) had a cortisol measurement between 50 and 138 nmol/l on day 3, seven of whom had received metyrapone therapy prior to etss.ten (91%) were glucocorticoiddependent at 3 months based on synacthen/itt; 0800 h cortisol had fallen to < 50 nmol/l in six patients.similar results were found when the four patients with macroadenoma were excluded.three patients underwent a repeat early endoscopic transsphenoidal surgery, fig. 1.over a median (range) duration of follow-up of 24 (4–79) months, one patient had recurrence of cushing’s disease.there was no statistical difference in rates of recovery of hpa axis in patients with day 3 cortisol < 50 nmol/l, and those who only passed standard criteria for remission (< 138 nmol/l) [7/20 (follow- up 25 (3–59) months) versus 2/11 (follow-up 16 (3– 79) months) respectively, p = 0.43].discussion reported remission rates following etss in patients with cushing’s disease (cd) vary widely, predominantly due to differences in criteria used to define remission [11].our remission rate, including those patients who had an early second etss, using standard guidelines, is 92%, on par with other larger studies [7, 8, 11, 25, 29].this criteria is in place in our institution so that we can safely identify patients who have early signs of remission to facilitate discharge on day 3 post-operatively; however reporting these rates in isolation lead to a misleadingly low remission rate compared to the more lenient criteria proposed by the endocrine society [16].on postoperative assessment, serum cortisol fell between the two criteria for remission and if remission was strictly defined as a day 3 cortisol < 50 nmol/l, then this patient had in fact persistent hypercortisolaemia.patients should be counselled regarding risk of post- brady et al. bmc endocrine disorders (2021) 21:36 page 6 of 8 operative endocrine deficiencies, in particular permanent diabetes insipidus.longer follow-up is required to accurately assess recurrence rat
number of words= 914
[{'rouge-1': {'f': 0.4364815959822837, 'p': 0.8033333333333332,'r': 0.29964509394572025}, 'rouge-2': {'f': 0.23728701321160722, 'p': 0.3910702341137124,'r': 0.17031347962382445}, 'rouge-l': {'f': 0.42283460734442596, 'p': 0.6732608695652174,'r': 0.30819742489270385}}]
-----------------------------------------------------------------------------------------------------------------------------------
p112:
Extractive Summary:
various risk factors, such as obesity, and blood pressure (bp) levels, lipids and glycaemic parameters predict the development of cardiovascular disease (cvd).in the pathophysiology of diabetes-related complications, the role of the accumulation of advanced glycation endproducts (ages) has been well established.binding of circulating ages to specific receptors (i.e. the receptors for ages [rage]) and subsequent uptake in arterial walls may play an important role in the development and progression of atherosclerosis [13, 14].the accumulation of ages can be evaluated by measuring skin autofluorescence (saf) [15].the study was approved by the medical ethics review committee of the university medical center groningen.all participants provided written informed consent.presence of type 2 diabetes was self-reported or based on the use of blood-glucose-lowering medication (oral agents and/or insulin), or fasting blood glucose ≥7.0 mmol/l and/or hba1c ≥ 6.5% (48 mmol/mol) at laboratory evaluation.participants with available saf measurements did not differ in sex ratio, age, glucose and hba1c from those without saf measurements.of these, 1863 completed the follow-up questionnaires and additional laboratory testing between 2014 and 2018.of the remaining 486 individuals only interim questionnaire results were available.in the 1863 participants with complete questionnaire and laboratory data, median follow-up was 4.0 years.smoking status was classified into never, former or current smoking.the use of medication was verified using the atc (anatomical therapeutic chemical) classification system by a research assistant at the baseline investigation only.we calculated saf z-scores (adjusted for age) based on the total lifelines population, separately in men and women.calculations, definitions and statistical analyses baseline as well as new events of cardiovascular disease were defined as a previous or an incident of myocardial infarction, transient ischaemic attack (tia), cerebrovascular accident (cva), intermittent claudication or therapeutic intervention including percutaneous transluminal coronary angioplasty (ptca) with or without stent, coronary artery bypass grafting (cabg) or peripheral vascular surgery.individuals with known type 2 diabetes were older than those with new type 2 diabetes (58.7 ± 10.8 years vs 55.0 ± 12.0 years, p < 0.001).participants with newly-detected type 2 diabetes had lower saf levels than those with known diabetes (saf z-score 0.34 ± 0.89 vs 0.56 ± 0.99 au, p < 0.001, table 1).levels of blood pressure, serum lipids and glycaemic parameters were not significantly different.fig. 2).they also had higher saf levels than individuals who remained alive: saf z-score was 0.81 ± 1.06 in those who died vs 0.44 ± 0.94 in those who remained alive (p < 0.001).similarly, incidence of cardiovascular disease but not mortality was higher in those with known diabetes compared to those whose diabetes was detected at the baseline screening.univariable analysis showed that saf was significantly associated with combined outcome (or 2.59, 95% ci 2.10–3.20, p = 1.3 × 10− 18).also, age, male sex, waist circumference, diastolic bp, heart rate, egfr, as well as the use of bp-lowering medication and statins, and baseline cvd showed a significant association with the combined outcome.in supplemental table 1 we report the association between saf and the individual outcomes.this association remained significant after adjusting for all other baseline variables (p = 0.013).the multivariable model showed that saf, age, systolic bp, waist circumference, current smoking, statin use and baseline cardiovascular disease were independently associated with mortality.saf was no longer significant with incident cardiovascular disease in the multivariable models, in which age, current smoking and baseline cardiovascular disease showed the strongest association.again, saf was strongly associated with the combined outcome, even when adjusted for the other variables age, systolic bp, waist circumference, statin use and current smoking (supplemental table 2).this association was independent of cardiovascular risk factors like age, sex, waist circumference, smoking, and glycaemic parameters.in the present study we show that saf is also significantly and independently associated with the combined outcome of new cvd events and mortality in people with type 2 diabetes.it has long been known that the formation of ages is increased in people with diabetes as a consequence of high glucose levels and oxidative stress [8, 27].nevertheless, saf was associated with this combined outcome independently of factors like existing cardiovascular disease, age, smoking, lipid or bp levels.saf was associated with a threefold increased mortality risk, and this association remained highly significant after adjusting for several confounding factors (supplemental table 1).in that study a high odds ratio for mortality was also obtained when modelling with the linear method based on the entire population without diabetes or cardiovascular disease.several earlier reports have described the cross-sectional relationship between saf and the presence and development of vascular complications in type 2 diabetes.saf did predict cardiac mortality in people with diabetes [29] and in patients on chronic haemodialysis [30–32], and predicted the occurrence of cardiovascular events and mortality in individuals with peripheral vascular disease [33].
number of words= 772
[{'rouge-1': {'f': 0.49769729735413637, 'p': 0.7280976863753212,'r': 0.37806257521058967}, 'rouge-2': {'f': 0.26415084245961673, 'p': 0.36381443298969074,'r': 0.20734939759036145}, 'rouge-l': {'f': 0.463381783297125, 'p': 0.5919298245614035,'r': 0.38070496083550914}}]
-----------------------------------------------------------------------------------------------------------------------------------
p113:
Extractive Summary:
background prader-willi syndrome (pws, omim 176270) is a rare, complex neurodevelopmental disorder with a reported incidence of approximately 1 in 15,000–30,000 newborns [1].pws is caused by a lack of expression of paternally inherited genes in the q11-q13 region of chromosome 15, most commonly caused by a paternal deletion or a maternal disomy [1, 2].pws in adults is characterized by muscular hypotonia, hyperphagia requiring restricted diet and regular physical activity to prevent the development of severe obesity [3].in previous studies, different tests were used to evaluate the function of the hpa-axis.the hair sample was stored on a paper in an envelope until analysis [13].methanol was used to extract cortisol and after purification, cortisol was quantified by liquid chromatography − tandem mass spectrometry (lc–ms/ms) (waters, milford, ma) [16].the stress was assessed by the question: did any stressful events occur during the last 3 months?however, due to low number of controls for males in the 18–25 age-group (9 adults with pws, 23 matched-controls) matching was made in a ratio of 1:2.5 for this particular sex-age group.the akaike information criteria (aic) test was used to confirm best fit model where by both bmi and reported stress were retained in the model, as they predicted the model well.mean (sd) for age was 33.4 years (12.7) in the patients with pws and 42.1 (11.6) years in the controls.by virtue of the matching process based on the sex-age groups, there were no significant differences between the patients with pws and controls with respect to age and sex.individual hair cortisol levels in patients with pws are displayed in table 1.six patients with pws had hair cortisol levels > 10 pg/mg and allthirteen other adults with hair cortisol of 10 pg/mg and below had not experienced any stress, and the remaining ten adults reported different kinds of stress that did not result in high levels of hair cortisol.using the same linear regression model in patients with pws, hair cortisol increased with bmi (β = 0.05; 95% ci [0.02–0.09], p = 0.002, which remained after adjustment for stress (β = 0.04; 95% ci [0.01–0.08], p = 0.012) (table 2).hair cortisol increased with bmi and reported stress in patients with pws.however, during the past decade, the measurement of hair cortisol has emerged as a non-invasive method to estimate the cortisol concentrations over months [6, 14, 17].thus, cortisol in hair retrospectively represents the average cortisol level over the corresponding period.this is an attractive procedure, especially in a population such as pws where the psychological and behavioral features might complicate advanced sampling procedures.in addition, four patients declined participation because they felt that even the small hairless spot on the scalp after cutting the hair would stigmatize them further, although the sampling spot at the back of the scalp is usually too small to be visible afterwards.cortisol levels have been found to correlate positively with waist to hip ratio, waist circumference and bmi [19, 26–30] and to be higher in obese individuals compared to normal-weight people [28, 29].to retain all patients with pws in the statistical analyses, matching for bmi between patients with pws and controls was not done since some severely obese patients with pws could not be matched with controls.it is well known that some patients with pws suffer from diseases secondarily to obesity, such as diabetes, cardiovascular diseases, respiratory diseases and arthrosis [31], and pws is associated with a decreased life expectancy, with an annual death rate reported to be as high as 3% [32, 33].however, this is a pilot study and for the purpose of our study we were interested in whether stressful situations at all would elicit an increase in cortisol.finally, patients with pws who were able and willing to participate, might represent a selected group of quite healthy and well-functioning adults with pws.increased bmi and reported stress were both associated with increased hair cortisol levels.the function of the hpa axis in pws is an ongoing issue and there is a need for further investigations.among these to study the long-term cortisol exposure and the effect of stress.the present study is the first to analyze hair cortisol in pws, and our results suggest an adequate long-term cortisol level and cortisol response to chronic stress in pws.
number of words= 696
[{'rouge-1': {'f': 0.4181395374046283, 'p': 0.7049809885931559,'r': 0.29721088435374154}, 'rouge-2': {'f': 0.2095663609917245, 'p': 0.31427480916030537,'r': 0.15719346049046323}, 'rouge-l': {'f': 0.3387724387024813, 'p': 0.5227027027027027,'r': 0.2505929919137466}}]
-----------------------------------------------------------------------------------------------------------------------------------
p114:
Extractive Summary:
in this study, we used a qualitative research approach to query participants in a randomized clinical trial regarding their experience with mtm.written informed consent was obtained from all participants for trial participation, and verbal consent for participation in the qualitative telephone interview was obtained separately.we first developed an interview guide (additional file 1: table s2) using an iterative process that incorporated the perspectives of study team members and participants, and drew from past work in this patient population [18].we continued contacting participants until content saturation was reached.the interviews queried the participants about the meals received and their perceptions of the influence of the meals on diabetes management.the interviewer (ns) used reflective probes to motivate participants to elaborate on their comments.interviews were audio-recorded with permission from participants, and recordings were transcribed verbatim.we measured height, weight, and blood pressure using calibrated instruments and a standardized measurement technique; we also performed phlebotomy for laboratory assessment of hemoglobin a1c and serum lipid levels.this approach allowed us to produce an organized framework, while giving us the flexibility to include new codes as they emerged.each transcript was coded, using nvivo 11 software (qsr international, melbourne, au), independently by two investigators: the interviewer and the principal investigator of the study.the two coders met weekly to ensure consistency and transparency in the coding.discrepancies were resolved through reflection and dialogue until consensus was reached.relevant quotations were selected to illustrate key points within each theme.results participant characteristics we reached content saturation after interviewing twenty individuals from the community servings: food as medicine randomized crossover clinical trial (nct02426138).this represented approximately half of the total number of trial participants.the mean age of the included individuals was 58 (sd: 13) years, and 60.0% were women (table 1).one participant said “i was glad that i was able to participate, and that the drivers provided the ease with the meals, and it’s exciting to get meals weekly.it kind of alleviated the headache of either buying or cooking.you know?and when you knew it was a delivery day, you had peace of mind for the rest of the week[laughter].regarding meal delivery, many participants were pleased with the convenience of home-delivered, medically-tailored meals, though a small number expressed difficulty coordinating meal deliveries due to their work schedule.most participants were satisfied with the amount of food they received over the course of the study, stating the food typically lasted for 5 days, the intended amount of time.food preferences and cultural appropriateness most participants enjoyed receiving medically-tailored meals.one said, “i thought they were very well prepared.like, “here’s everything you need for one healthy meal [laughter].most participants recognized that the meals provided were appropriate for those with diabetes.however, some did not, which underscores the importance of nutritional education in conjunction with mtm.one participant said, “some of the meals, i didn’t even know what they were.even though it gave you the name, i didn’t know really what it was.most participants reported that meals were acceptable, but some expressed interest in receiving meals representative of their country of origin or cultural background.one summarized, “i understand that it does better on this diet.but these are foods that i would not be able to afford to make for myself, or be physically capable of making for myself.the question’s a little more complicated than that.i mean, i understand diabetes and i understand what foods work and what don’t work.” participants also reported that the support of the program reinforced their efforts in diabetes selfmanagement, and may have augmented self-efficacy.participants noted several benefits they perceived from program participation, aside from diabetes management itself.regarding diabetes management, participants reported improvements in weight, and in biomarkers of diabetes control, such as hemoglobin a1c, with one saying, “so what i’ll tell you was when i started my [a1c] level was so high.participants suggested combining mtm and diabetes self-management education, or a lifestyle intervention, and providing additional financial assistance, particularly with medications.regarding the meals in particular, participants emphasized the importance of receiving food that was culturally appropriate with acceptable taste and familiar ingredients.focusing solely on clinical biomarkers or healthcare utilization likely overlooks important benefits perceived by the participants.this study only analyzed a portion of participants in one mtm program, from a circumscribed geographic area.additionally, while the experience with the intervention reported in the interviews was strongly positive, with concerns raised primarily about food preferences, those who had more negative experiences may not have wanted to be interviewed.however, we regard these findings as hypothesis generating for future studies.in this study, the majority of participants lived alone, but because food insecurity is experienced at the household level, the experience of other household members is certainly relevant.these limitations were balanced by key strengths, including in-depth qualitative examination of a participants in a novel intervention program, and direct feedback from participants to improve future mtm.though further research, particularly longer and larger-scale randomized trials, is needed to demonstrate the effectiveness of mtm interventions for health outcomes, it is also important to consider how widespread implementation may be achieved if effectiveness is established.recent innovations in healthcare financing provide a pathway to sustainability as a covered health insurance benefit [26].states are incorporating mtm into their programs [26–28], and recent changes to the medicare advantage program also offers opportunities to make mtm more available to beneficiaries [29].conclusions medically-tailored meal delivery programs are a promising approach to managing a difficult clinical problem— how to improve health in those with both diabetes and food insecurity.medically-tailored meal programs offer the potential to improve not only biomarkers, but also patient-reported outcomes that are important components of quality of life.
number of words= 915
[{'rouge-1': {'f': 0.3778103125582536, 'p': 0.7317100371747212,'r': 0.25464730290456433}, 'rouge-2': {'f': 0.16874644211481574, 'p': 0.26402985074626867,'r': 0.12399792315680166}, 'rouge-l': {'f': 0.35283546769292995, 'p': 0.610880503144654,'r': 0.24805383022774327}}]
-----------------------------------------------------------------------------------------------------------------------------------
p115:
Extractive Summary:
in ias, there are also high titers of autoantibodies against endogenous insulin, which occur without any prior exposure to exogenous insulin [2, 3].patients usually present with postprandial hypoglycemia, marked neuroglycopenic symptoms of confusion, and an altered state of consciousness in these two conditions [1].the reduced accuracy of insulin measurement is not clinically relevant in patients with ias because of the high serum insulin values.moreover, there is no cross-reactivity of the insulin assay with synthetic insulin in this detection method.this was accompanied by diabetic nephropathy as well as a history of gout for more than 30 years.at this time, his c-peptide values were 5.45/6.94/6.94/7.89/7.17 nmol/l at 0/0.5/1/2/ 3 h, respectively (normal range: 0.27–1.28 nmol/l).his insulin concentrations were > 1000.00/> 1000.00 pmol/l at 0/2 h, respectively (normal range: 13–161 pmol/l), during an oral glucose tolerance test (ogtt), his hba1c concentration was 10.2%, and an anti-insulin antibody test was positive (reference level: < 0.4).his glucose concentrations still fluctuated between 11mmol/l and 22mmol/l in the daytime, while hypoglycemia occurred at night.he was diagnosed with type 2 diabetes mellitus 8 years before admission.three months previously, the dose of insulin aspart 30 was gradually reduced to 4–10 units because of hypoglycemia.however, he had unexpected episodes of unconsciousness due to hypoglycemia 12 days before admission.moreover, his symptoms were obviously relieved after food intake.he was subsequently treated with voglibose instead of insulin injections.however, the hypoglycemic symptoms continued to appear between 02:00 am and 03:00 am, despite the absence of other treatments.2089.5 pmol/l at 0/1/ 2/3 h during the ogtt, respectively.at the time of presentation, he had been in a hypodynamic state for 15 days.one year earlier, he developed polydipsia, hyperuresis, and cutaneous pruritus, and lost weight.one month previously, he visited our hospital because of complaints of recurrent midnight hypoglycemia, particularly at approximately 02:00 am.for treatment of ias, prednisone therapy was started at 12 mg daily (4 mg per dose three time daily) then reduced to 8 mg daily after 2 weeks and to 4 mg per night 1 week later.the diabetes treatment plan was changed to human biosynthetic insulin injection (16 units per day), sitagliptin (50mg per day), and voglibose (0.3–0.2 mg twice a day).he also started sweating excessively, and at that time, his fasting plasma glucose concentration was 2.8 mmol/l.ehh is diagnosed with the presence of inappropriately high serum insulin concentrations while plasma concentrations of glucose are < 55mg/dl and ≤ 70mg/dl in individuals with and without diabetes, respectively [4, 5].the resulting hyperglycemia further promotes insulin release.most patients with ias achieve remission with nutritional management [6], and small frequent meals with low carbohydrates are preferred [7].in addition, plasmapheresis and rituximab can be used to eliminate insulin autoantibody titers in the circulation [1, 8, 12].aside from ias, another type of autoimmune hyperinsulinemia is type b insulin resistance [2, 14].type b insulin resistance syndrome is characterized by severe hyperglycemia and is less common hypoglycemia compared with ias [17, 18].apart from autoimmune-induced ehh, insulinoma and nesidioblastosis should also be considered.surgical resection is the firstline treatment for insulinoma [22].the difference between exogenous insulin-and endogenous insulin-induced hyperinsulinemia is the lowering of plasma glucose, lipoproteins, and inflammatory markers [28].as glucose concentrations eventually fall, insulin secretion also subsides, and the total insulin level decreases.this leads to an increase of released free insulin concentration that is inappropriate for the glucose concentration, resulting in postabsorptive hypoglycemia [1].this is because changing to another insulin type can make the insulin antibodies disappear and glycemic concentrations more stable.however, if islet function is good, we prefer to gradually reduce the same insulin dose and add oral drugs.first, steroids reduce the hypoglycemia phenomenon that occurs in the middle of the night in non-classical ias.
number of words= 603
[{'rouge-1': {'f': 0.2937900238861228, 'p': 0.7652380952380953,'r': 0.18179173047473202}, 'rouge-2': {'f': 0.14857268994746373, 'p': 0.27192307692307693,'r': 0.10220858895705523}, 'rouge-l': {'f': 0.2724636492641413, 'p': 0.5977777777777777,'r': 0.17644257703081234}}]
-----------------------------------------------------------------------------------------------------------------------------------
p116:
Extractive Summary:
by 2014, the number of people with diabetes worldwide reached 422 million, accounting for 8.5% of the total population [2].diabetes has become one of the main sources of global disease burden [6].participants were those who resided in the 5 monitoring areas of jilin province for 6 months or more within 12 months prior to the survey and were 18 years old or older.according to the multi-stage stratified cluster sampling method, 5 counties (cities) were selected as monitoring points in each stage, 4 towns (streets) were selected in each monitoring point, and 3 villages (residential committees) were selected in each township.questionnaire survey the questionnaire survey was conducted in accordance with the china chronic disease surveillance questionnaire survey procedure and was conducted face-to-face by investigators with unified training [19].a male waist circumference > 90 cm and a female waist circumference > 80 cm was indicative of abdominal obesity [22, 23].systolic blood pressure ≥ 140 mmhg (1 mmhg without antihypertensive drug) =0.1333 kpa and diastolic blood pressure ≥ 90 mmhg or having been diagnosed with hypertension by a township hospitals in the past 2 weeks were considered high blood pressure [24].in people who have been diagnosed with dyslipidemia in the past, we asked them to answer the supplementary question in the questionnaire, which one of the high triglycerides, high cholesterol, high ldl-c or low hdl-c is diagnosed (if there are multiple indicators of one sample diagnosed as abnormal, it would be included in multiple variables).the logistic regression model required considering the collinearity problem when incorporating variables.we used tolerance values and the variance inflation factor (vif) to examine collinearity.the results are shown in table 1.the results are shown in table 3 comparing the prevalence of type 2 diabetes in people with different health statuses, the results showed that the prevalence of type 2 diabetes was statistically significant among subjects with different tc, tg, ldl-c, and hdl-c, and the significance level was set at 0.05.among them, p is the predicted probability of the logistic regression model.substituting the prediction model into the testing data set, with a critical value of 0.5, the results show that the prediction accuracy of the model was 90.8% and the area under the roc curve was 0.711 (95% cl: 0.697–0.724).the sensitivity of the roc curve was 67.8%, and its specificity was 64.7%.the results are shown in table 5 bp neural network model the 13 variables that were significant by the chi-square test were included in the neural network model.that is, 13 units were established in the input layer.the number of hidden layers can be 1 or 2.the hidden layer activation function is a hyperbolic tangent, the input layer activation function is the softmax, and the output layer has two units.first, when the hidden layer was 1, the area under the roc curve of the model with a different number of hidden layer nodes was discussed.the results are shown in table 6.when the number of hidden layer nodes was 5, the area under the roc curve was the largest, 0.780 (95% cl: 0.767–0.792); the sensitivity was 72.94%; and the specificity was 72.42%.the results are shown in table 6 the area under the roc curve of different hidden layer nodes with two hidden layers in the bp neural network model is shown in table 7 finally, the bp neural network with 1 hidden layer and 5 hidden layer nodes was taken as the final prediction model.the bp neural network ranks the importance of predictor variables for different predictors, and the details are shown in table 8 dividing the predictor importance indicator by the maximum indicator value yields a normalized predictor importance order.the results showed that the top five variables were tg (100.0%), alcohol consumption (91.4%), age (74.9%), hypertension (58.8%), and tc (50.4%).using the bp neural network model for prediction, with 0.5 as the predicted quasi-probability boundary value, the prediction accuracy of the model was 91.3% and the area under the roc curve was 0.781 (95% cl: 0.768–0.794).it can be seen from the figure that the first layer is tg, indicating that tg had the strongest correlation with type 2 diabetes and that the risk of type 2 diabetes was higher in people with higher tg than normal.the remaining variables were hypertension, age, smoking, and abdominal obesity.the accuracy of the prediction model was 90.7%, and the area under the roc curve was 0.698 (95% ci: 0.684– 0.712).the sensitivity of the curve was 64.5%, and the specificity was 65.7%.the area under the roc curve is ranked from bp neural network model (0.780) to logistic regression model (0.711) to decision tree model (0.698).three models’ aic are 293.178, 281.895, 343.877.the results are shown in table 9 the areas under the roc of different models are shown in table 10.the results showed that the difference between the roc area values of the bp neural network model and the logistic regression model was statistically significant (p < 0.001), the difference between the roc area values of the bp neural network model and the decision tree model was statistically significant (p < 0.001), but the difference between the roc area values of the logistic regression model and the decision tree model was not statistically significant (p = 0.0711 ≥ 0.05).that is, the larger the area under the roc curve and the closer to 1, the better the diagnostic effect of the model.among them, the most influential factors associated with the onset of type 2 diabetes were tg (or = 2.233), age (or = 1.734), hypertension (or = 1.703), alcohol consumption (or = 1.674), and tc (or = 1.463).this study showed that people aged 70 years and older were more susceptible to type 2 diabetes than those below 70.the hardening and aging of blood vessels and the change in the normal function of the vascular wall caused by inflammatory reactions and adipose tissue can also affect the expression of intracellular protein kinases, regulating the expression of inflammatory genes [29–31], affecting the normal function of islet β cells and reducing insulin secretion.  hypertension often coexists with diabetes [32], which may have a common risk factor, such as obesity.this study suggests drinking is a risk factor for type 2 diabetes.studies have shown that [33] moderate drinking can improve insulin sensitivity, increase high-density lipoprotein cholesterol and adiponectin levels, and reduce inflammation, but heavy drinking increases energy intake and causes obesity, impairs liver function, and increases the pancreatic burden, which accelerate the progression of diabetes.compared with the logistic regression model, the bp neural network model is not affected by the interactions between variables and has nonlinear mapping abilities, selflearning and self-adaptive abilities, generalization abilities, and fault tolerance.li lixia et al. [34] used a logistic regression model and a bp neural network model to predict liver cancer and also concluded that the bp neural network model was superior to the logistic regression model.the area under the roc curve was the largest, and the prediction ability was the best.the results showed that the prediction was better when the number of hidden layers was 1.(3) the bp neural network cannot judge whether the variable is a protective factor or a risk factor.the research factor cannot be judged to be a risk factor or a protective factor, whereas the logistic regression model can explain the direction of the variable well.the area under the roc curve of the decision tree model in this paper was the smallest of the three prediction models, and the difference between the bp neural network model and the logistic regression model was statistically significant.li xianwen et al. [37] found that the prediction of a logistic regression model was better than that of a decision tree model in a study of health literacy in hypertensive patients, in agreement with the results of this paper.the results of this study showed that the bp neural network model was a good predictive model for type 2 diabetes, but for practical applications, the logistic regression model can explain the variables and results more intuitively.
number of words= 1314
[{'rouge-1': {'f': 0.3430694437126458, 'p': 0.7804377104377105,'r': 0.21985795454545456}, 'rouge-2': {'f': 0.21700469064928465, 'p': 0.4281081081081081,'r': 0.14533759772565744}, 'rouge-l': {'f': 0.34396130302641154, 'p': 0.5671428571428572,'r': 0.24682926829268292}}]
-----------------------------------------------------------------------------------------------------------------------------------
p117:
Extractive Summary:
in resistant hypertensive cohorts, the prevalence of pa is approximately 20% [2].furthermore, specific treatments are available that can cure hypertension and ameliorate its complications [5].therefore, early and accurate diagnosis is imperative.a previous study demonstrated that in china, the overall prevalence rate of pa in adults with resistant hypertension was 7.1% [6], which is not the highest among those previously reported in other ethnic populations outside china [7, 8].however, china has a large general population with a prevalence of hypertension of 29.6% [9].it is challenging to accurately and quickly diagnose pa in such a large population.hence, convenient diagnostic processes with a high efficiency for identifying pa are currently urgently needed in china.confirmation tests are a crucial step in the process of obtaining a functional diagnosis of pa.the endocrine society’s guidelines for pa recommend 4 confirmatory tests, including the fludrocortisone suppression test (fst), saline infusion test (sit), captopril challenge test (cct), and oral sodium loading.among these confirmatory tests [1], the cct is easy-to-conduct and is considered to have the same diagnostic efficiency as the other tests [1].however, there is no consensus on how the results of the cct should be interpreted [1, 10, 11].the endocrine society guidelines recommend that a post- cct suppression percentage < 30% confirms pa, but this threshold has not been verified in studies [1].however, other groups prefer to use the post-cct aldosterone renin activity ratio (arr) or post-cct plasma aldosterone concentration (pac) to diagnose pa [12–15].diets and genetic susceptibilities are different between chinese and western populations [16], and previous studies have suggested that chinese and western individuals perform differently on the sit [17].the present study aimed to investigate the usefulness of, optimal interpretation approach to, and cutoff value for the cct in diagnosing pa in chinese patients.furthermore, the correlation between estimated sodium intake and cct performance was also assessed.methods subjects the present study was conducted in the nanjing drum tower hospital.all subjects provided informed consent.potassium-wasting or -sparing diuretics were discontinued at least 4 weeks prior to testing.patients with severe hypertension were prescribed α-blockers, and long-acting calcium channel blockers were prescribed if necessary.biochemical measurements sodium and potassium levels in blood and urine were measured using fully automated instrumentation.plasma renin activity (pra) was measured by radioimmunoassay using a commercial kit (atomic hi-tech co., ltd, beijing, china).the intra- and inter-assay coefficients of variation (cvs) for pra were 10 and 15%, respectively.pac was measured by radioimmunoassay with commercial kits (northern technical and biological institute, beijing, china).the intra-assay and inter-assay cvs for this assay were 5.6 and 6.2%, respectively.blood samples were drawn to measure pra and pa at time 0 and at 2 h after challenge.adrenal vein sampling (avs) procedures were performed between 0800 and 1200 h by one radiologist using the bilateral simultaneous technique without cosyntropin stimulation.according to previous recommendations, successful cannulation was considered a selectivity index (si) ≥ 2, and lateralization of aldosterone secretion was defined as a lateralization index (li) ≥ 2.results clinical characteristics of patients with pa we recruited 110 consecutive patients with pa. of these patients, 82 had an apa, and 28 had iha.compared to the nc and ph groups, patients in the pa group had lower serum potassium and higher serum sodium and urine potassium levels (p < 0.01).the prevalence of hypokalemia was higher in the apa group than in the iha group (92.7% vs 75%).compared to patients with iha, those with apa had higher pac (p < 0.01).however, the mean post-cct suppression percentage of pac was only approximately 9.3%, and only 11.7% of the patients with ph showed a post-cct decrease in pac greater than 30%.in pa patients, the cct increased pra and lowered pac slightly.after cct, pac remained higher and pra was lower in the pa group than in the ph and nc groups (p < 0.01).patients were divided into zhu et al. bmc endocrine disorders (2019) 19:65 page 3 of 7 low and high sodium intake groups according to median daily urinary sodium.these findings indicate that using a suppression percentage of 30% as the approach cutoff may cause many hypertensive patients to be misdiagnosed.however, when we split our patients according to the median daily urinary sodium, we found that plasma aldosterone and pra values were not different between low- and high-sodium intake patients.our and song’s studies both detected only a small amount of post-cct suppression in pac but a dramatic increase in post-cct pra or prc in normal and ph patients.compared to the post-cct pac, the post- cct arr is a better approach when used with an optimal cutoff of 20 for interpreting the results of the cct in chinese patients.
number of words= 757
[{'rouge-1': {'f': 0.49228578487331964, 'p': 0.7964150943396227,'r': 0.3562453531598513}, 'rouge-2': {'f': 0.23646829270902153, 'p': 0.347602523659306,'r': 0.17918114143920597}, 'rouge-l': {'f': 0.4066552194201226, 'p': 0.6111764705882352,'r': 0.3046938775510204}}]
-----------------------------------------------------------------------------------------------------------------------------------
p118:
Extractive Summary:
despite a historically late start in its recognition as a disease entity, hypothyroidism has remarkably become one of the most frequently diagnosed diseases in the western world, and levothyroxine one of the most frequently used drugs worldwide [22–24].consequently, a new disease class of subclinical hypothyroidism was introduced, which is solely based on the presence of an elevated tsh while the thyroid hormones ft3 and ft4 remain within their respective reference ranges [26].a transition occurred from the era of low metabolic rate regarded as synonymous with hypothyroidism to a purely biochemically based definition [28].consequently, treatment habits changed over the last decade and were more related to laboratory records than subjective patient experience.however, for two reasons this is an area of considerable uncertainty.firstly, thyroid-related patient complaints overlap with a plethora of non-specific symptoms caused by other conditions and diseases [29–36].in these conditions, lt4 treatment may not be superior to placebo in symptom alleviation [40–42].because tsh, ft4 and ft3 are interrelated through the operation of hypothalamic-pituitary-thyroid feedback regulation, integrated pairs of tsh and ft4 values define the so-called individual set points [43, 44].unlike a population-based univariate reference interval, set points are subject to multivariate normality and narrow homeostatic ranges [43].when plotting tsh against ft4 concentrations the resulting distribution in a healthy population does not describe the familiar rectangle, but a kite-shaped area [43].as a consequence, this strategy divorces diagnostic disease definitions from treatment targets.both the non-specific nature of complaints and inherent deficiencies in the diagnostic process raise an unsettling dilemma for patients and thyroid specialists alike.the issues are exemplified and particularly pertinent to an etiological disease entity whose consequences are paralleled in similar outcomes: primary hypothyroidism due to total thyroidectomy in patients with differentiated thyroid cancer.treatment requirements and dosing of the drug lt4 changed when guidelines relaxed the need for tsh-suppressive treatment targets for these patients [46, 47].although this could not be done in a prospective study, careful retrospective analysis revealed some interesting trends [50].this sentiment was confirmed by a large online survey of 12,146 hypothyroid patients conducted by the american thyroid association [11].others have arrived at similar conclusions [57].in laboratory diagnostics, the high individuality of tsh and thyroid hormones has long been recognised since the pioneering work of andersen and colleagues [58].the analytical distinction between the averaged versus the typical outcome is clinically relevant for all thyroid drug trials, independently of evidence class and study design, because doctors are naturally more interested in the latter.demonstration of averaged equivalency cannot therefore be a satisfactory analytical goal [61].accordingly, the value of statistical evidence derived from historical meta-analyses [62–66] and rcts on the acceptability of t3/t4 combination therapy is severely weakened and requires careful reconsideration [60].many rcts were conducted with inferior quality of life instruments available at the time and relied on statistical techniques both less suited for highly individual parameters and in addition susceptible to simpson’s paradox.a thyroid-specific qol has only recently been developed and validated [51].simpson’s paradox (also known as amalgamation bias or collider stratification bias) may explain, at least in part, why otherwise well-performed studies failed to provide a convincing relationship between symptoms and thyroid function tests [27, 54, 59– 61].the paradox is a relevant factor for the relationship of patient complaints, biochemical markers and treatment response to lt4 [60].this bias - unless properly accounted for statistically - dissociates the personal treatment responses from the statistical group effect, thereby masking individual treatment success or failure in an unchanged grouped outcome.a lack of group to individual generalizability has been increasingly recognized in other fields, requiring explicit testing for equivalence of processes both at the individual and group level [61].trials purporting to relate tsh and thyroid hormone levels to the incidence of osteoporosis and atrial fibrillation fall under the same fundamental caveats [24, 27].in particular, the rotterdam study has shown that within the euthyroid range the prognostic implications of thyroid hormones and tsh differ, and, that tsh measurements therefore cannot substitute for ft4 concentrations in predicting the risk of atrial fibrillation [67].the cause of atrial fibrillation poses a complex problem, as its occurrence has been physiologically and statistically associated with both high and low ft3 concentrations [68].thyrotoxicosis due to exogenous thyroid hormone intake and endogenous hyperthyroidism have different physiological roots.this traditional distinction should be noted because the interrelationships between tsh and thyroid hormones differ on lt4 treatment from those in thyroid health [24, 56, 57, 59, 69].rather tsh by itself, unaccompanied by measurements of ft4 and ft3, is an unsuitable risk measure in lt4-treated patients, displaying considerable inherent uncertainty in an individual about the risk - benefit ratio for tsh values close to the lower reference limit [27, 69].serious correction of scientific evidence is not unprecedented in medicine.notably, some cholesterol trials have undergone re-interpretation, reversing previous conclusions, following re-analysis of recovered crude data with improved statistical methods [70].tsh and ft3 dissociate under lt4 treatment, particularly in athyreotic patients where equilibria are formed between tsh and ft4/ft3 different from the healthy state [56, 57, 59].this is of little benefit to patients who will continue to complain, and with some justification, that the medical profession is not listening, thereby abandoning one of its primary functions in the doctor-patient relationsh
number of words= 853
[{'rouge-1': {'f': 0.4073529722058679, 'p': 0.6471428571428572,'r': 0.2972215973003375}, 'rouge-2': {'f': 0.16201119135826314, 'p': 0.21899713467048712,'r': 0.12855855855855858}, 'rouge-l': {'f': 0.3379131764685201, 'p': 0.47789473684210526,'r': 0.26135802469135805}}]
-----------------------------------------------------------------------------------------------------------------------------------
p119:
Extractive Summary:
outcomes we considered clinical outcomes, process, and structure metrics when assessing the quality of diabetes care at wdh.the primary clinical outcome of interest was change in hba1c from baseline to 18months.secondary clinical outcomes included the change in hba1c, blood glucose, bmi, blood pressure, and the frequency of diabetes complications at quarterly intervals.diabetes complications specifically tracked within routine clinical care included symptomatic hypoglycemia/ hyperglycemia, presence of peripheral neuropathy, and ocular complications.secondary process metrics included patterns of medication use and frequency of documentation of recommended diabetes care services.in addition, we compared service delivery at wdh against 19 different domains in the minimal, standard, and comprehensive care recommendations from the idf global guideline for diabetes care [11].statistical analysis results were presented descriptively as percentages, means, and medians, with corresponding standard deviation and range, where appropriate.comparison between baseline clinical data and subsequent data was made using the paired t-test, with statistical significance being set to a p value < 0.05.stata 13.1(college station, tx, usa) was used for all analyses.the median diabetes duration prior to enrolment was 4 years.this reduction was statistically significant for each interval comparison for the smbg group, the non-smbg group (usual clinic care), and for the overall group.participants in the smbg group had the highest baseline hba1c at 12.4% and showed the largest drop after 3 months of care to 9.6% (table 3).this trend, however, was only statistically significant when comparing months 3, 6, and 12 to the baseline rbs (table 4).statistically significant reductions were not seen with systolic or diastolic blood pressure from the baseline mean blood pressure of 134/83 mmhg.minimal and non-significant changes in the mean bmi were seen throughout the periods of evaluation.in terms of diabetes complications, participants in both the smbg and regular care groups demonstrated high initial frequencies at enrolment of peripheral neuropathy manifested primarily through foot symptoms, ocular complications, hypoglycemia, and hyperglycemia, which declined over time as illustrated in table 6.in terms of documentation of recommended diabetes care procedures, assessment for hypoglycemia and hyperglycemia, blood pressure, ophthalmological symptoms, and symptoms of foot disorders were documented for more than 90% of the clinic visits.85.1% of the evaluable population had an hba1c result documented twice per year, 2.7% documented having their serum lipids checked at any time point, 1.5% had a creatinine result documented, and nobody had a documented microalbumin test.low dose aspirin (75–100 mg) was prescribed for 30% of the clinic population for primary and secondary prevention of cardiovascular events.in comparing the services available at wdh to the idf global guideline recommendations, the wdh diabetes clinic provided care that was below the minimal recommendations for care in 5 domains, met the minimal care recommendations in 7 domains, met the standard care recommendations in 7 domains, and didn’t meet the comprehensive care recommendations in any of the domains (table 7).discussion this multifaceted evaluation of the quality of care within the wdh rural diabetes clinic provides insight into the unique aspects of implementing diabetes care in rural ssa.in addition, we noted a high adherence to recommended diabetes care procedures in this resource constrained setting.this may have been a result of their preferential enrolment into the intensive smbg program.our investigation has several limitations which are important to highlight.first, multiple points of documentation were required during clinic visits in a somewhat fragmented process that involved documentation in a moh diary and a moh prescription form, in addition to the standardized encounter form.these cumbersome and time-consuming requirements anecdotally led to documentation fatigue among providers and potentially incomplete documentation in the electronic data collection tool.while this approach had multiple challenges with missing data, this approach gives readers a realistic view of the initial observations seen when establishing a specialty diabetes clinic in a rural setting.third, the wdh clinic received philanthropic support from several sources which was used to offset the costs of vital laboratory tests and expensive medications such as hba1c and insulin.nevertheless, our findings suggest that logistical considerations can be effectively addressed if many of the financial barriers are overcome or partially subsidized.this analysis strategy was utilized to ensure that the process measures analysed in this study reflected the entire population receiving care in the clinic and were representative of the patient population typically seen in ssa.this decision to analyse patients in a combined fashion was influenced by several of the limitations we have observed within this population and the limited infrastructure in this setting.while this represents a limitation of this study, it is worth noting that only 3% of the population was < 20 years of age at the time of enrollment.despite these limitations, our comprehensive analysis provides an in-depth assessment of the unique aspects of providing care in rural resource-limited settings in ssa.the improvements in the glucose control of the population, highlights how barriers to care can be overcome through collaborative partnerships including local providers, international funders, and academic institutions.this analysis has also provided our overarching diabetes clinic operations team with considerable guidance on how to promote continuous quality improvement within our service delivery.in order to address the domains in which the wdh clinic failed to meet the recommended idf criteria standards for quality care, we have specifically addressed many of the limitations of the clinic by including programs which have studied the ideal methods for assessing gestational diabetes [29], assessed the reliability of point of care lipid testing [30], incorporated point of care testing for creatinine and potassium, tested various screening approaches [31], incorporated additional medications such as statins within the clinic pharmacy [13, 32, 33], implemented community based diabetes care delivery alongside microfinance services [34], and included a mobile point of care electronic medical record system eliminating the need for duplicative data entry [35–37].
number of words= 937
[{'rouge-1': {'f': 0.4044944126242602, 'p': 0.778185053380783,'r': 0.2732686414708887}, 'rouge-2': {'f': 0.18634738358395822, 'p': 0.2985714285714286,'r': 0.135439672801636}, 'rouge-l': {'f': 0.3113392430858274, 'p': 0.5280645161290323,'r': 0.22074309978768578}}]
-----------------------------------------------------------------------------------------------------------------------------------
p120:
Extractive Summary:
canadian consensus guidelines similarly do not recommend asymptomatic screening, but instead suggest use of validated screening tools if there is clinical concern for a cognitive disorder [4].however, it is not clear that these are the best screening tools for use in primary care.the time allocated for a typical office visit makes it challenging to perform a cognitive assessment [5].this highlights the current need to better optimize dementia care within primary care.specifically, we seek to understand what practices fps can undertake to ensure accurate and timely testing and management.org/ 10.17605/ osf.all data generated or analysed during this study are included in this published article in additional file 1: appendixes 1 and 2.a systematic review of systematic reviews was determined to be the past method to further summarize and tailor the current body of literature on this topic into a format that would address the existing evidence to practice gap.within each cluster, terms were combined with or, and between the clusters with and.the reference list of a previous relevant systematic review of systematic reviews published in 2014 was also searched [8].study selection systematic reviews were considered if they met the following inclusion criteria.• population: primary care or family practice settings seeing persons with dementia.articles were also selected for inclusion if they were english-language articles, included relevant descriptions of the interventions used, and outcome data was available.all full-text articles were assessed independently for inclusion by b.f and j.h.-l.the mmse was used as a reference standard in the majority of the included studies.the amts was validated for use in general practice [12].diagnostic accuracy and physician education the diagnosis of dementia by fps varies but is generally low, as reported in 3 different systematic reviews [11, 16, 19].a cm in particular, such as a nurse specialized in care of older adults, can be an asset to a primary care team with the collective goal of collaborating towards meeting the needs of the patient-caregiver dyad [30].in the case management intervention group of a randomized controlled trial, neuropsychiatric symptoms of dementia decreased (mean effect size (mes) = 0.88), as well as the numbers of hospital (mes = 0.66) and emergency department admissions (mes = 0.17) [26].however, it was found that there was a lack of successful implementation of a cm into care teams within primary care because of the absence of cms within the primary care setting, and 52 % of cms reported ineffective communication between the cm and fps [26].there was no clinically important difference observed on neuropsychiatric symptoms between patients with mild to moderate alzheimer’s disease taking cholinesterase inhibitors versus placebo [11].a meta-analysis showed that 58 % (95 % ci: 43-72 %) of family caregivers were in favor of early dementia diagnosis, 50 % (95 % ci: 35-65 %) needed education on dementia, and 23 % (95 % ci: 17-31 %) needed in-home support [33].the mmse, which is copyrighted, may not be the best test for use in general practice.instead, the amts appears to be the most suitable tool for use in a busy primary care office, as it has good diagnostic accuracy, does not appear to be copyright protected and takes less time to administer than the mmse [12, 14, 15].unfortunately, many pharmacologic studies do not focus on primary care or fps, making it difficult to draw conclusions about the approach to take regarding the use of medications in this context.one systematic review found no clinically important differences between groups receiving cholinesterase inhibitors and those receiving a placebo in the development of behavioral and neuropsychiatric symptoms of alzheimer’s disease [11].similarly, cholinesterase inhibitor use was found to have uncertain clinical benefit in a recent systematic review that explored the benefits and harms of prescription drugs for the treatment of alzheimer disease, regardless of care setting [36].to improve dementia identification, fps should participate in educational interventions.
number of words= 630
[{'rouge-1': {'f': 0.3807698680489206, 'p': 0.6048837209302325,'r': 0.2778313253012048}, 'rouge-2': {'f': 0.15391797018414502, 'p': 0.20618677042801556,'r': 0.12279034690799398}, 'rouge-l': {'f': 0.29825891743576954, 'p': 0.3977777777777778,'r': 0.23857142857142857}}]
-----------------------------------------------------------------------------------------------------------------------------------
p121:
Extractive Summary:
job satisfaction research into job satisfaction is heavily influenced by herzberg’s two factor theory.herzberg and colleagues hypothesized that factors intrinsic to the content of the job, such as received recognition, autonomy, and personal growth have a positive influence on job satisfaction.by contrast, a deficiency in factors extrinsic to the actual work, such as interpersonal relationships, working conditions, salary, organization policy, and administration are assumed to decrease job satisfaction [33, 34].several studies into job satisfaction of pcps confirmed these hypotheses [4, 35–38].the divide between intrinsic and extrinsic factors can also be seen in the literature on physician well-being interventions.examples of extrinsic interventions are workflow improvements, individual resilience courses, or communication training [39–41].interventions focusing on intrinsic factors aim to create joy through meaning-making, defining joy as a “feeling of success and fulfilment that results from meaningful work” ([8] p608).to engage professionals, authors propose helping them reflect on their professional identity via questions such as: ‘why did you choose this profession?’ or: ‘what calls you to practice?’ [35, 42–44].consequently, part of ph’s appeal is that it explicitly includes meaning-making as one of its six dimensions for both patients and professionals.several healthcare providers reported that ph helped them find meaning in their work [15, 42, 45], however the impact of ph on meaning-making by professionals has not yet been empirically studied.methods research aim, design, and setting this article is based on an ethnographic case study into how the adoption of positive health affected primary care professionals’ job satisfaction.the present study focused on one general practice that was implementing ph.all twelve staff members were asked to participate in the study to explore how ph affects the various professions, interprofessional relationships, and practice as a whole.in order to gain an in-depth understanding of pcps’ experiences, the first author collected archival material and conducted qualitative online interviews with pcps.data collection archival materials collected included documents associated with the implementation of ph in the general practice, the general practice itself, and the history and development of ph.these documents included, among others: minutes from team meetings, internal management documents, communication between the practice and the health insurer, the website of the general practice, and newsletters.documents selected for inclusion were written either in dutch or english.interviews took place in april and may 2020 and the researchers employed a semi-structured approach.this allowed for flexibility in pursuing an interviewee’s experiences, while ensuring that all topics were addressed [43].to this end, the researchers constructed an interview guide to cover the implementation of ph, job satisfaction, and professionalism (additional file 1) [43].before the start of the interviews, the first and second author met with the practice manager and gps to ensure transparency and align mutual expectations.interviews were carried out via a licensed version of zoom video communications, inc.and once by phone, considering safety measures taken in response to the covid-19 pandemic.all interviews were audio recorded.to increase the reliability and validity of the study, the researchers made use of member checks by offering the participants a summary of their interview [44].written informed consent for publication was obtained from all participants.the interviews were carried out by the first author.at the time of the study she held a bachelor of science degree and was a master’s student at maastricht university, as well as a medical student at utrecht university.she had limited experience with respect to interviewing.to ensure the quality of the project, the entire process was supervised and guided by the second author, a senior researcher at the department of health services research of maastricht university.she is an experienced qualitative researcher with expertise in healthcare humanities and science & technology studies.in total, approximately 270 pages of material on this general practice, and 250 pages on the history and development of positive health were analyzed.eleven out of the total of twelve primary care professionals were able to participate in the interviews (table 1).one employee declined because of personal circumstances and upon request the personal data of one respondent will not be disclosed.the interviews lasted on average 58 min.each participant was interviewed once.ten participants were present in the workplace during the interviews, one participated from home.the interviewer was based at her home.no non-participants were present during the interviews.the interviewer took notes during and after the interview.interviews ended when all participants were included.data analysis analysis was conducted iteratively by the first, second and third author.in analyzing interviews and documents, the researchers followed an inductive approach using the thematic analysis guide of braun and clarke [46].nvivo 12, a qualitative analysis software program, was used to keep track of the analysis.thematic analysis consisted of an iteration of three phases of coding, namely open, axial, and selective coding [46].during the coding process, the first and second author regularly discussed the proposed codes and themes to reach consensus.the third author aided the first and second author in establishing themes and subthemes.the coreq checklist was used when writing this report (additional file 2) [47].the results were presented to the participants, who recognized themselves in the analysis.results: positive health as an adaptable frame for change thematic analysis identified three themes regarding the adoption of positive health and primary care professionals’ job satisfaction, namely [1] adopting and adapting positive health, [2] giving substance to positive health in practice and [3] changing financial and organizational structures.theme 1 describes how the adoption of ph in this practice was the result of ideology, strategy, and favorable circumstances.it also discusses the multi-interpretability and malleability of ph.adopting ph enabled pcp’s to better express, legitimize, and promote their values of holism and autonomy.these values were not only prominent in patient care, but also in the attitude of professionals towards each other, which is addressed in theme 2.this supported them to align everyday conduct with how they envisioned their profession, which increased their job satisfaction.next, theme 3 depicts how the practice changed its financial and organizational structures upon adopting ph.consequently, pcps could invest more time in their patients and in their own wellbeing.they, were, thus, able to further their vision in practice.these changes contributed to their job satisfaction.in this way, ph functioned as an adaptable frame for change in the general practice.the following pages will unpack this conclusion.an additional data table with quotes is provided in additional file 3.adopting and adapting positive health most respondents considered the adoption of positive health to be a minor change from former ways of working, since ph fit the vision of the practice well.however, several respondents explained that precisely this match made the adoption of the concept possible.in addition, ph provided the practice with a new frame to express and legitimize its comprehensive approach to healthcare.at the same time, the respondents offered different interpretations on what the concept entailed and the practice adapted the concept to its needs in several ways.this section will further unpack how the practice adapted itself to adopt ph, and adapted the concept while adopting it.it’s a match!document and interview analysis showed that the introduction of ph in this practice was the result of ideology, strategic considerations, and fortunate circumstances.the current practice owners dissociated from another general practitioner in 2015 due to a personal conflict, different ideas about the role of the gp, and other styles of practicing.the newly emerged practice aimed to interact with patients on a more personal level, to approach them holistically, stimulate their self-reliance, and to be part of the community.one gp described the role the practice aspires to fulfill in the community: i would also like it if [the villagers] just say: ‘it’s a place where i like to go (…) that general practice, you can do everything there (…) you should go there to talk about your stairlift or about your cleaning service, or depressed spouse’, and that we don’t immediately say: (…) ‘that’s not the job of the gp’, but that we say: ‘come let’s look at it together’ and of course immediately involving (…) [our] network.(r11, gp) in pursuing this vision, the practice made several changes.for instance, they created a volunteer-run vegetable garden and raised the minimal consultation time from 10 to 15 min to create more time for the patients.with this, the practice owners aimed to create more joy1 in their work, as this nurse specialist explained: they made the choice a couple of years ago themselves, also because of their job enjoyment, to do things differently.so they conscientiously chose to spend more gp hours than they formally could according to the guidelines, in order to have more job enjoyment and more time for the patient.(r4, nurse specialist) the practice’s alternative way of working was met with interest from a health insurer that was looking to experiment with positive health as a new health concept.conversations between the practice and health insurer started in 2017, followed by the practice adopting the concept of ph.this helped the practice to express and legitimize its new approach to care work: [y]ou can say ‘we’re working in conformity with positive health.’ ‘oh right, i have quite a good picture of what that entails at yours then.’ plus, it can be an argument to partners and financiers to get done what you want.(r10, nurse specialist) in 2019 a pilot project under the flag of ph started between the practice, the insurer, and the municipality.one of the goals of the pilot project was to improve job satisfaction, which prompted this study.theme 2 and 3 will provide further detail on how the practice implemented the concept.interestingly, some participants pointed out that this match between a general practice and ph is not self-evident.one of the gps explained why the concept is not a fit for everyone: there are also a lot of people who would not like this at all.they say, that’s just drivel, that’s not at all family medicine.but well, i have noticed that i do like it.i don’t feel like i’m being less of a doctor.instead i get more to the core.(r11, gp) explaining this, he said, that some gps would consider this approach not medical enough; talking about the person rather than the complaints.positive health as a malleable concept despite the small size of this practice, the pcps offered different interpretations of ph during the interviews.some respondents struggled to describe what the concept meant: i still find it hard to give it something tangible, like what is it really?in fact it is a little bit of everything in your work.(r2, practice nurse) two new employees reported that they had not received information on what ph entailed or how it should be implemented when they started working in the practice.however, professional values that hang together with ph (e.g. collegiality, the importance of self-care), were discussed more elaborately.interestingly, the practice’s policy documents lack a clear-cut description of how the practice owners interpreted ph or envisioned the concept’s implementation.
number of words= 1775
[{'rouge-1': {'f': 0.3461208191666008, 'p': 0.8795238095238096,'r': 0.21545454545454545}, 'rouge-2': {'f': 0.21202371702696907, 'p': 0.452089552238806,'r': 0.13848582129481007}, 'rouge-l': {'f': 0.3474487234988534, 'p': 0.7294594594594594,'r': 0.22803108808290157}}]
-----------------------------------------------------------------------------------------------------------------------------------
p122:
Extractive Summary:
the extra workload and time needed both to set up and to recruit within a normal consultation are major barriers to participation by gps and practices [1–3].participation can be increased where there is perceived clinical value and / or benefit to patients, adequate remuneration for time and streamlined recruitment processes that minimize workload [1–4].one approach to minimizing the workload and cost is to make use of data routinely collected with electronic health records (ehr) to identify eligible patients and collect outcome data [1].‘transform’ (translational research and patient safety in europe) is an electronic trial data collection platform that integrates with the ehr to: perform automatic eligibility checking of entered read/snomed codes of patients upon presentation; capture electronic case report form data part-filled from the ehr at pre-defined points in the study workflow; and use mobile and web portals to collect patient reported outcome measures (proms) [5].transform was developed as part of a 5-year eu fp7 programme.following a successful pilot study in poland [6], transform was used in the uk-based rest study, a 3-arm pragmatic trial of treatment for acute otitis media with discharge in children comparing topical, ‘immediate’ oral and ‘delayed’ antibiotic treatment options [7].otitis media with discharge (aomd) is a painful and distressing condition and most children are treated in primary care with ‘immediate’ oral antibiotics [8, 9].however, the use of systemic antibiotics risks side effects and antibiotic resistance [10, 11].while responsible for a significant proportion of antibiotic prescribing, aomd is not as common as infections such as tonsillitis and acute bronchitis, meaning individual gp practices would expect to recruit relatively small numbers of children.the infrequency of recruitment opportunities makes it more likely that potentially eligible children would be missed because clinicians will find it difficult to keep it in mind and to remember the process for recruitment.transform provided an automatic alert for potentially eligible children, guided and recorded recruitment procedure and auto-populated patient data.the runny ear study (rest) randomised controlled trial therefore aimed to recruit 175 gp practices across the united kingdom and use transform to support efficient trial processes [7].the results of the rest study will be reported elsewhere [12].we conducted a nested qualitative study to describe the experience of primary care practice staff of rest trial processes.this paper reports the views and experiences of primary care staff deploying and/or using the transform trial software, within the context of uk primary care.methods purposive sampling was used to select participants in order to capture maximum variation in views and experiences [13].primary care staff involved in trial processes were purposively sampled in relation to site, role and whether the practice was able to successfully recruit patients.views were sought from recruiting clinicians, practice research staff, and those primary care staff involved in installing and supporting the transform software, which included staff in management and it support roles.in-depth interviews with primary care staff were conducted using a flexible topic guide to ensure that the primary issues are covered across all interviews, but enabled participants to introduce unanticipated issues [14].the topic guide was revised to include new topic areas identified from earlier interviews, particularly around the barriers to implementing transform (supplementary file 1).the researcher used open-ended questioning techniques to elicit participants’ experiences and views of key events and participants were asked to provide examples.the interviews were conducted over the phone, lasted 20–45 min and were recorded using a digital voice recorder.audio recordings were transcribed and anonymised to protect confidentiality.interview transcripts were imported into nvivo 12 qualitative data analysis software.analysis began shortly after data collection and was ongoing and iterative, informing further data collection and identifying changes needed to the topic guide.thematic analysis [15], utilising a data-driven inductive approach, was used to identify and analyse patterns and themes of particular salience for participants and across the dataset using constant comparison techniques [16, 17].a subset of transcripts was independently double coded by members of the team (cc and jh) and discussed to achieve coding consensus and maximal rigour.sample size was informed by the concept of ‘information power’ [18], with analysis and sampling conducted in parallel and continuous assessment of the suitability of the information within the sample with regard to study objectives.results sixteen primary care staff were interviewed: 9 gps and 7 other staff, from recruiting and non-recruiting practices (including 1 practice that withdrew from the study) (table 1).some of the gps had experience of recruiting to the trial and some had experience of getting the transform software working.staff from recruiting practices had each recruited at least 1 child.all of the gps were partners and their practice’s research leads, with years in practice ranging from 4 to 33.the non-clinical staff included practice managers, practice it leads, a research coordinator and a research nurse (with no clinical role) who had experience of installing the transform software and the processes involved in getting it to work.the findings are organised below into three thematic areas: views of automated data capture software; experiences of implementing new software; and the challenges to software implementation from the limited, varied and changing it context in uk primary care.“it puts the details in which is time saving”: views of automated data capture participants were keen on the idea of a system that would automatically capture data on recruited patients.they faced considerable workload pressure, and this means there is little time for additional work, which sometimes was a barrier to participation in research studies.at the time of this study there was no national guidance for ccgs from nhs x or nhs digital as to what assessment was required and national assurance process for new software.during the rest trial, many practices updated their operating systems and administrative rights over software installation transferred from individual practices to ccgs,.these changes were driven by nhs initiatives to improve it service provision and align gp it operating arrangements, which include adherence to gdpr and measures to protect from ransomware attacks [24].however, these changes had the unintended consequence of forming additional barriers to the implementation of software that had to be installed at a practice level and was designed to extract and export (anonymised) patient data from ehr and to a secure server outside the nhs.primary care it systems are not solely under the control of individual practices or ccgs or the nhs.rather they are part of a complex adaptive system that spans all three and includes other stakeholders, such as the ehr software providers, all of which may drive different types of change.any new software needs to be able to operate within this complex adaptive system and successful implementation requires engagement with all the key stakeholders.conclusions pragmatic trials, which need to recruit within consultations, are essential for the production of high-quality evidence about what works under normal clinical care [1].primary care is a busy and time pressured environment [2, 3].software that links to ehr and automates some of the trial processes and data collection could support greater participation and more efficient trial designs [1, 5].in order for that to become a reality, there is need for: 1) higher priority placed on research it by all the stakeholders who influence primary care it provision; 2) provision of substantial technical support to gp practices to get any new software functioning smoothly without adding to practice workload; and 3) development and testing of software like transform as a platform service to ensure that it is deployed and running smoothly before it can be used with live clinical trials.the uk nhs recently announced the gp it futures framework [27], from the new digital care services model, to supply it systems and services to gp practices and this may offer better opportunities for engagement by developers of health research software.
number of words= 1267
[{'rouge-1': {'f': 0.3938809434093799, 'p': 0.8570967741935485,'r': 0.2556925418569254}, 'rouge-2': {'f': 0.232359915765553, 'p': 0.44216828478964404,'r': 0.15758568164508757}, 'rouge-l': {'f': 0.40873583139395026, 'p': 0.7158333333333333,'r': 0.28602787456445994}}]
-----------------------------------------------------------------------------------------------------------------------------------
p123:
Extractive Summary:
background epistaxis describes different forms of nasal blood loss and is a common symptom that occurs in medical practice.in 2016, 2.5 million persons were insured with the aok lower saxony, representing around 36% of all those insured by the shi in the german federal state of lower saxony [11].consequently, all outpatient cases of epistaxis (r04.0 according to icd-10, german modification, diagnostic certainty “secured”) were included along with the specialty of the diagnosing physician (gp, ent, pediatrician or another specialist group).since outpatient diagnoses are reimbursed on a quarterly basis in germany (i.e. four three-month periods per year), each case was allocated to a diagnosis quarter.consultations with the same case number (referring to a specific patient, case, physician and quarter) were considered only once.lastly, we assessed predefined fee positions according to the uniform fee position regulation (ebm) coded with an epistaxis case (i.e. with the same case number).cases were further displayed by the years of the epistaxis diagnoses.second, analyses were conducted on the patient level stratifying patients by the age of their first epistaxis diagnosis during the study period.we further assessed whether a patient consulted a gp and an ent specialist (i) at least once during the study period or (ii) during one quarter or two consecutive quarters.using the date of the abolition of the practice fee, we last displayed the number of patients consulting the respective physician groups at least once before and since 2013.ethics as the article used anonymized secondary data, patient-informed consent was not required by german regulations.the ent specialist did not record any other disease besides the epistaxis in around 30% of the cases.it was found that a tamponade was only invoiced in 4.5% of all cases treated by the ent specialist, while the additional fee position for the “treatment of acute, difficult to stop nose bleeding” was charged in 19.7% and the lupe laryngoscopy in 26.5% of all ent-consultations due to epistaxis.minor surgical interventions were invoiced in 12.1% (09360) and 13.7% (09361), respectively.the fee position of “minor surgical intervention in infants, toddlers and children” was invoiced in 1.2% of all epistaxis cases.patients with at least one epistaxis diagnosis during the study period: number of diagnoses, involved medical specialists and impact of practice fee on the patient level, 98,351 of the 160,963 persons (61.1%), had only one epistaxis diagnosis during the study period.the older patient population (61–80 years) consulted the ent specialist most frequently (see table 3).the ranged between 6 and 12%.as illustrated by table 4, during the practice fee remuneration (10 euro per quarter in the years before 2013) there were only minor differences in the use of specialist groups compared to the years following its abolition.hypertension, atrial fibrillation/flutter and an antithrombotic therapy were slightly more common among cases consulting a gp.the gp recorded more co-diagnoses than the ent.twenty-three thousand one hundred eighteen patients (14.4%) had been diagnosed by both ent and gp during a relatively short time period.the practice fee remuneration had no impact on the consultation of the physician groups.most studies in literature were either limited to a hospital setting or to specific populations like infants [13–17].next to differences in health care systems, this hampers comparisons in an international context.this observation is confirmed by the analysis of the invoiced fee positions.the allocation of a patient to a gp, ent or emergency department based on the severity of disease is not always reasonable.we assume that a deductible has a controlling effect if medical services are demanded more often than actually necessary.in case of epistaxis the patient can rarely estimate the actual amount and seriousness of blood loss and the controllability by the patient’s own co-payment is limited.in addition, the waiting times for an appointment with an ent specialist in germany are comparatively short [3, 21].the proportion of ent consultations remained relatively stable during 2016 (see fig. 1).consideration regarding patient care with regard to the care epistaxis patients received in the present study, it is striking that from the age of 20 years onwards a similar number of patients were treated by gp and by ent specialists.we assume that most patients directly consulted the ent specialist without visiting the gp first or at all.the german society for general and family medicine (degam) defines the responsibility of the gp as the first medical contact and basic care provider for all patients with physical and mental health disorders in emergency, acute and long-term care as well as areas of prevention and rehabilitation [23].however, these were rather rare diagnoses.the practice fee did not lead to better patient allocation between primary and secondary care.by means of targeted performance management and controlled allocation of (expensive) diagnostics, a managed care model allows holistic care in the sense of “disease management” [30].strengths and limitations of the study the major strengths of this study are the large database and the long time period of 10 years.however, the present study is limited by the nature of the data of a single statutory health insurance.it is known that insurances differ with respect e.g. to demographics, socio-economic status and morbidity, which limits the generalizability of the results [32].conclusion the outpatient treatment of epistaxis, based on the results of this study, constitutes a considerable medical and economic burden in germany.
number of words= 861
[{'rouge-1': {'f': 0.4262888682199625, 'p': 0.7966187050359712,'r': 0.2910065645514223}, 'rouge-2': {'f': 0.24832662649680642, 'p': 0.42018050541516244,'r': 0.17624315443592553}, 'rouge-l': {'f': 0.4043053149254543, 'p': 0.661194968553459,'r': 0.2911764705882353}}]
-----------------------------------------------------------------------------------------------------------------------------------
p124:
Extractive Summary:
consequently, factors related to adherence to the planned implementation strategy, dose received, i.e., the extent to which the recipient was exposed to the implementation strategy, participant responsiveness and actual involvement, as well as modifications made and the role of context, become central issues for understanding the impact of implementation initiatives to improve real-world clinical practice [3, 5].evaluating the degree to which implementation strategies are operated as designed within implementation trials are key in order to determine the internal and external validity of implementation studies [1, 3–5].additionally, they help in the interpretation of outcome results of interventions translated to real practice and inform the optimization of both the clinical intervention and/or implementation strategy to favor adoption of the intervention and implementation and future scale-up in other contexts and settings [8–11].however, despite the importance of implementation fidelity evaluation, first, there is currently no framework explicitly establishing either a set of procedures or specific requirements to guide the evaluation of the fidelity of an implementation strategy [4, 14].among general existing frameworks to guide fidelity evaluations [2, 3, 5, 7, 8], the framework stated by dane et al. [8] and its adaptation by dusenbury et al. [5], has been successfully used by others for rating the fidelity of implementation strategies [4].briefly, the pvs-prediaps implementation strategy consist in conducting externally facilitated collaborative modeling process through a set of planned implementation actions with phc professionals in order to adapt and integrate an evidence-based healthy lifestyle promotion intervention to prevent type-2 diabetes within the routine primary care services.this fidelity evaluation will help to gain understanding about the quality of the implementation of the pvs-prediaps strategy and specifically regarding the two procedures compared for engaging professionals and deploying the implementation actions.this in turn will help to explain and interpret future results of the prediaps trial (ie., to reject a possible type iii implementation failure error), will identify what has been changed from the original implementation plan and how changes may impact outcomes, and will inform how future dissemination and scale-up can be improved.briefly, the prediaps is a randomized cluster implementation trial conducted in nine basque health service (osakidetza) pc centers that aims to assess the effectiveness and feasibility of different engagement procedures to perform a facilitated interprofessional collaborative process - the pvs-prediaps implementation strategy- to optimize type-2 diabetes prevention in routine pc.headed by a local leader and an external facilitator, centers were expected to perform a collaborative structured process to adapt an evidence-based intervention to promote healthy lifestyles and its implementation to their specific context.pc users: patients aged 30 years or more who sought medical attention at least once through the participating centers between 2 march 2017 and 2 march 2018, and had been classified as at high risk of developing t2d and/or prediabetes (abnormal fasting glucose level or glucose intolerance plus an additional known cardiovascular risk factor) but did not have a documented diagnosis of t2d in their health record were eligible to participate in the t2d prevention program at the centers.then, patients identified as at high risk should be invited, and if they agree, participate in an intensive structured intervention program focused on the prescribing of personalized plans for lifestyle change (low-energy low-fat diet or mediterranean-type healthy diet; and at least 150 min of moderate physical activity a week).lastly, patients should be followed up, initially with frequent contact and then annual check-ups.the 5 a’s (ask, advise, agree, assist, and arrange follow-up) intervention framework was used to standardize the provision of the evidence-based behavior modification techniques used to promote changes in physical activity (150 min of moderate physical activity a week) and diet (mediterranean- type healthy diet) to prevent type-2 diabetes in high-risk patients (see appendix table 4).implementation strategy for facilitating the adoption of the recommended intervention in routine clinical practice the pvs-prediaps strategy is based on the creation of an inter-professional community of practice that undertakes a process of modelling, adapting and integrating the recommended clinical intervention into the local context, led by the clinicians themselves, a local leader and an external facilitator [15].the specific discrete implementation strategies used in the prediaps trial, as cataloged by the expert recommendations for implementing change (eric) taxonomy [17], are described elsewhere [15].random allocation to the procedures compared as described elsewhere [15], with the aim of isolating the effect of two different procedures to engage professionals and perform the pvs-prediaps facilitated interprofessional collaborative implementation strategy to optimize t2d prevention in routine pc, centers were randomly assigned to: a global strategy, seeking involvement and cooperation between physicians and nurses from the outset; or a sequential strategy, first led by nurses, and then seeking the pragmatic involvement and cooperation of physicians later in the process.dose the following indicators related to the process of conducting the prediaps trial and the actions embedded to deploy the implementation strategy are specified for reporting on the dose: a. percentage of centers included out of all those approached b. percentage of healthcare professionals who initially collaborated out of the total number of professionals at each center c. actions carried out over time (training, work sessions, etc.).d. participation of collaborating healthcare professionals in each action and actual exposure to the implementation strategy actions compared to that originally planned (% of hours/action received out of the total number of hours that would be implied by participation in all the actions of the strategy, i.e., 20 h or 4.5 h).quality of program delivery/participant responsiveness a structured group interview was carried out with the local leaders from the centers involved (n = 9) in order to assess the perceived usefulness of the implementation strategy among healthcare professionals.six open-ended questions focused on the implementation strategies perceived to be part of the prediaps trial, the perceived value of these strategies, and recommendations for their optimization.specifically, the following questions were used: a) personal rating of the implementation process and associated strategies (through questions such as, “specifically, which aspects of the process for optimizing practice do you consider the most important or useful for doing your job?additionally, not all centers managed to organize and run one of the planned ongoing supportive training sessions.table 2 displays the original implementation plan involving 11 eric discrete implementation strategies and the intervention mapping, which consisted of a combination of 3 eric discrete strategies, yielding a total of 14 discrete strategies.in the group surveys/interviews regarding the perceived usefulness of the implementation strategies, the healthcare professionals recognized half of these 14 planned strategies.the prediaps project seeks to generate scientific evidence concerning the optimization of healthcare practice in primary prevention of t2d in osakidetza pc centers through the application of implementation science as a way to achieve feasible, sustainable and effective translation of the recommended evidencebased clinical intervention to clinical practice [15].despite some differential exposure to overall strategy within the nursing staff of compared groups, professionals involved have been notably exposed to the implementation strategy and the planned program differentiation related to engagement of professionals and deployment of the implementation strategy has been attained.part of the present evaluation of pvs-prediaps implementation strategy’s fidelity involves examining the implementation strategy dosage, that is, the degree of passive exposure to the planned implementation strategies and actions.as described in the literature, commonly reported obstacles faced by physicians to fully engaging in implementation actions included heavy workload, staff turnover, difficulties in investing time and effort improvement initiatives beyond providing care, and existing practice priorities [21–23].although all healthcare professionals participating in the prediaps trial were exposed to the 14 eric discrete implementation strategies with minor modifications, our qualitative evaluation of their experience indicates that they only recognized having received half of them.they also failed to identify several “ongoing” implementation activities, such as ongoing training and ongoing support, and local discussion and consensus sessions (collaborative modeling sessions).it is possible that these activities were seen to be typical, or standard, implementation tools that were too obvious to mention in the evaluation session.in any case, differential participation or exposure to the strategy could compromise the future implementation of the clinical intervention that we are seeking to promote [24].nevertheless, an adequate fit between “fidelity” of the strategy and the necessary “adaptability” to the local context of centers remains a great challenge in implementation trials [25].with respect to the planned implementation strategy [15], it proved unfeasible to carry out some activities, for example, the total number of planned monitoring and ongoing facilitation sessions.there was demand among professionals for additional actions related to specific core strategies within the overall implementation strategy like training actions regarding the clinical intervention.contextual factors, for example, site characteristics, needs and priorities are considered to be among the main drivers for tailoring implementation strategies [27], and one of the approaches used is to permit flexibility in order to enhance alignment and involvement while offering support and guidance towards change [26, 27].given the emphasis of the implementation strategy on facilitation to create a learning collaborative to develop and adapt the implementation strategy to an individual center’s context, it is not surprising that these two strategies were identified by at least half of the centers.in the prediaps trial, the perception of these additional discrete implementation strategies provides further evidence of an appropriate dose having been received and suggests further ecological validity of the overall implementation plan.despite these limitations, a major strength of this study is the nature of the results obtained regarding fidelity, as they demonstrate that professionals involved were capable of identifying and rating the implementation actions conducted.these components include adherence to the planned implementation strategies, dose/exposure to the strategies, quality of delivery, participant responsiveness to the strategies received, modifications made and program differentiation.given the lack of operational definitions and existing frameworks to evaluate the fidelity of implementation strategies, this paper helps advance scientific research on fidelity.in this sense, it seems to confirm the high quality of the implementation of the pvs-prediaps strategy and of the two procedures for its deployment.further, it will help to explain and interpret future results of the prediaps trial, by rejecting the possibility of an implementation failure and by informing about potential confounding factors due to differences observed between comparison groups, these being potentially associated with both exposure and results.lastly, it points out to some core elements of the implementation strategy that should be improved for future dissemination and scale-up, as for example the training component.despite some differential exposure to overall strategy in comparison groups, mainly in the nursing staff, professionals involved in both comparison groups have been notably exposed to the implementation strategy and the planned program differentiation related to engagement of professionals and deployment of the implementation strategy has been attained.the frame framework [18] may be a useful complement in order to identify and report modifications made within the planned implementation strate
number of words= 1768
[{'rouge-1': {'f': 0.35027899915941924, 'p': 0.8506267806267807,'r': 0.22054945054945055}, 'rouge-2': {'f': 0.21716687676108556, 'p': 0.45,'r': 0.1431170973062122}, 'rouge-l': {'f': 0.35259332550038136, 'p': 0.6829032258064516,'r': 0.2376470588235294}}]
-----------------------------------------------------------------------------------------------------------------------------------
p125:
Extractive Summary:
background uncomplicated urinary tract infection (uuti), or cystitis, is among the leading reasons for infectious disease consultation in primary care [1].nearly half of women report at least one episode of uuti during their lifetime [2].there is no standard definition of uuti internationally accepted.in france, uuti in women is defined as an acute urinary tract infection in adult under 75 years-old, without signs of pyelonephritis or risk factor for more severe disease (pregnancy, urogenital abnormalities, immunodeficiency, frailty).acute cystitis affects several facets of women’s life: social and intimate relationships, self-esteem, and ability to work.depending on the situation, the impact can vary from a woman to another: from small discomfort to severe disability impacting personal and/or professional life [3, 4].nonetheless, cystitis can resolve spontaneously [5] with a rare risk of pyelonephritis [6].in this context, some patients prefer either to discuss the advantages and disadvantages of antibiotics with their physicians [7] or even to avoid antibiotic uutis treatment [8].this lead several authors to suggest a shared decision-making approach in uuti [9, 10].currently, the systematic and immediate prescription of empiric antibiotic treatment is often recommended to reduce symptoms [11], even though physicians tend to overestimate patients’ desire to take antibiotics [8].first line antibiotic treatment often differs from one country to another [12].in france, the french-language infectious pathology society (société de pathologie infectieuse de langue française) recommends fosfomycin [13], but it is important to note that, frequently, the antibiotic prescribed empirically does not match pathogen sensitivity on the microbiological results [14].the effectiveness of a systematic antibiotic strategy on the infection must be put into perspective with potential adverse effects like mycosis, headache, dizziness and digestive disorders, and with the risk of resistance emergence [15].in france, fluoroquinolones resistant e. coli are found in 3 to 7% of uti in primary care [16, 17].the resistance rate of bacteria responsible for uuti is higher during the year following the prescription of an antibiotic for urinary tract infection in the primary care setting [18, 19].additionally, research indicates that compliance with prescribed treatment is variable [4].few countries already recommend other strategies such as delayed treatment.thus, in the united kingdom and the netherlands, guidelines for uuti suggest that physicians discuss the options with the patient to delay antibiotic treatment [20, 21].some elements collected from the focus groups corresponded with the existing literature, such as representations of uuti and its risk factors [8], representations and opinions on antibiotics [8] [7], and the impact on social or professional life [3].patients mentioned numerous representations relating to the gynaecological sphere, intimacy, sexuality, and fertility.such representations are rarely found in articles regarding uuti.their evocation was facilitated during focus groups composed solely of women, including the observer and the investigator.the importance of representations around femininity was integrated into the ptda using a pink/purple colour.some members of the focus group expressed the desire to have a more gender-neutral representation.the pink colour was widely validated by the patients during the alpha-testing phase and was consequently retained.some of these representations correspond to known risk factors (i.e. sexual intercourse), however, the steering group decided not to mention risk factors because they were not directly involved in the decision-making process.other representations were beliefs (risk of infertility) that were absent in the literature.some patients wanted to be able to discuss them.this information could be added in a leaflet handed over to the patient.most of the patients’ values expressed in the focus groups could be integrated into the ptda, particularly in the slider.an empty slider allows the patient to express additional values, like her expectations regarding treatment (reduction of recurrence, rapid symptom relief, etc.).one concern expressed among the interviewed patients and doctors was the risk of pyelonephritis.there is little data on the natural course of uuti without antibiotics, and the meta-analysis comparing antibiotics to placebo did not show a significant increase in the risk of pyelonephritis between the two [6].however, patients treated with an anti-inflammatory drug had a higher risk of pyelonephritis compared to those treated with an antibiotic [9] [10] [40].this increased risk could be explained by the harmful role that anti-inflammatory drugs can play in infectious diseases [40].some of the interviewed physicians were concerned that the use of the ptda would lengthen the consultation.there is little evidence on the impact of shareddecision making on the length of consultations [52], but a study comparing a standard approach to the use of ptdas in the management of depression in primary care did not show a significant difference in the length of the visit [53].a noticeable limitation of this ptda is the lack of medical perspectives exterior to the study.the number of interviews with physicians during the alpha-testing phase was low.as already reported in a cochrane review, during ptdas development, patients’ views were more often collected than those of physicians [24].the steering group chose to present the probability of symptoms after three days.this time frame made possible to present data in the ptdas regarding the firstline antibiotic in france (fosfomycin) and placebo.this short delay is in line with the french guidelines which recommend another consultation after three days in case of treatment failure, but also with the british guidelines proposing a 48-h delay for the delayed prescription of antibiotics.indeed, delayed prescribing could reduce antibiotic use [54].this option is close to immediate non-prescription and re-evaluation in case of persistent symptoms.delaying the prescription can therefore fit in the use of the ptda, which is not currently recommended in france [13], contrary to the united kingdom and the netherlands [20] [21].hence, our ptda proposes not to prescribe antibiotics immediately.ptdas have a better clinical impact when they are developed simultaneously to national guidelines [25], as nice has been able to do [27].our ptda is similar to nice’s ptda with regards to the information on the options, their benefits, risks, and consequences, and is based on the same literature references.on the other hand, nice’s ptda does not include a diagram to facilitate an appropriate understanding of numerical probabilities; besides, it does not allow patients to clarify their values nor foster deliberation, as recommended by the ipdas [22].our ptda was created in line with international standards and will soon be beta-tested in a larger study [28].helped by users’ feedbacks, the beta-test phase will allow to improve our tool and the global acceptance of the concept from both patient and physician perspectives.the possibility to implement this approach in france will also be studied during the beta-test.indeed, there may be some difficulties to implement the ptda in clinical practice.the ptda leads to a discussion with the patient about the choice of using antiobiotic or not, whereas actual french guidelines advise a systematic antibiotic prescription.furthermore, systematic antibiotic prescription requires less investment from the practitioner.nevertheless, practitioner investment can lead to better outcomes: if the practitioner is optimistic about diagnosis and prognosis, symptoms could resolve faster [43].thus, if our ptda shows good implementation during the beta-test, it would lead to a large-scale study with a validated ptda to evaluate its impact on antibiotic prescription and patients’ satisfaction.this could influence new national guidelines on cystitis for general practitioners and then facilitate the ptdas implementation.conclusion uuti is no longer considered as a pathology for which the symptomatology is experienced similarly for all patients and for which a single treatment should be offered.the literature review suggests that uuti is an equipoise situation, hence, shared-decision making seems to be the best way to broach this subject.developing a tool to help for shared-decision making in primary care consultation is necessary to move forward and to assess its efficiency compared to a systematic antibiotic treatment approach.to do so, we developed one of the first ptda for uuti treatment in primary care, in line with international standards.the impact of our ptda on patients’ satisfaction and antibiotic prescription remains to be evaluated, but its conception has already brought a lot of information on patients’ and practitioners’ perceptions of uu
number of words= 1294
[{'rouge-1': {'f': 0.35891123389791835, 'p': 0.7852103559870551,'r': 0.2326195732155997}, 'rouge-2': {'f': 0.19171189491012905, 'p': 0.345974025974026,'r': 0.1325920471281296}, 'rouge-l': {'f': 0.3344774433662165, 'p': 0.6197076023391812,'r': 0.22905245346869713}}]
-----------------------------------------------------------------------------------------------------------------------------------
p126:
Extractive Summary:
background prostate cancer is the second most frequent cancer and the fifth leading cause of cancer death in men worldwide [1, 2].therefore, the net benefit of prostate cancer screening remains unclear, resulting in ambiguity that is reflected by different psa testing recommendations for physicians [18–20].in terms of psa testing, it has been shown that the personal beliefs and the specialization of the physician may also be relevant to the usage of psa testing [21–23].the guideline recommends against active offering of psa testing to men without clinical symptoms of pca and is actually (in 2020) under revision.apart from being free available on the internet, the guideline is, among others, published in a dutch scientific journal for gps, and is part of trainings and education for gps.in the present study, we aimed to survey gps in the netherlands to assess their approaches, attitudes, and knowledge regarding the use of psa screening for early prostate cancer detection, focusing on the prostate cancer recommendations set out in the nhg guideline.additional file 1 shows the dutch questionnaire, additional file 2 the english translation of this questionnaire.the final iteration comprised 31 questions in five sections, addressing issues such as how and when to initiate psa screening, the implications of results, awareness of the recommendations from national guidelines, and awareness of the results of relevant studies concerning psa screening.based on this, the questionnaire was revised, according to the comments of the physicians.questionnaires were completed once.the nhg guideline the 2013 nhg guideline on lower urinary tract symptoms in men provides strategies for early prostate cancer detection without advocating general psa screening for prostate cancer [20].instead, guidance for psa testing is given in two scenarios, and even then, depends on clinical assessments: [1] when an asymptomatic patient makes a request; and [2] when there is clinical suspicion.physicians are required to provide the following details related to prostate cancer: the risk in older men (incidence), the rarity of symptoms, and the risk of death.if the patient decides to undergo early detection, a dre should be performed first, and a suspicious outcome should trigger direct referral to a urologist.statistical analysis response proportions were calculated separately for the gps and the gp trainers.by contrast, all 23 gps at the training day completed the questionnaires (100%).almost none would recommend testing to relatives and most of the male gps (71%; n = 44) had not undergone psa testing themselves and did not plan to do so in the future.more than 40% of the gps (43%; n = 37) were not worried about missing a diagnosis of prostate cancer in patients, and in most cases, considered screening for other cancers to be more important.for symptomatic patients, gps tended to order psa tests rarely for lower urinary tract symptoms compared with sometimes for other unclear discomfort.in asymptomatic patients actively requesting psa testing, 39 gps (46%) said that they would agree to the request within the same session after providing information on the benefits and risks of the test.less commonly, gps reported they would first discuss the test but would require a separate appointment before deciding whether to perform the test (35%; n = 30).discussion we have presented the results of a survey conducted in the north-east of the netherlands to assess the approaches, attitudes, and knowledge of gps concerning the application of psa screening for the early detection of prostate cancer.however, before ordering a psa test, gps in that study performed dre less frequently than in ours, which is consistent with the results of a survey among 303 physicians in south africa [26].this finding was notable because most gps in our survey were male, and research has indicated that male gps are more likely to order a psa test than female gps [28, 29].decisions about screening and psa testing made by gps in australia were mostly at the discretion of individual clinicians, resulting in significant variations in practice.however, the replies of gps in the uk reflected a clear, consistent, organizationally embedded approach based on evidenced recommendations to discourage screening [30].we agree that this suggests that health care systems, organizational structures, and guidelines collectively affect how physician’s view and handle psa testing for early cancer detection, which is also supported by others [30].although most gps in our study reported using the nhg guideline in daily practice, only a few followed the advice to refer a patient to a urologist without performing a psa test if the dre raised suspicion.the reason for this remains unclear, as do the reports by some of our respondents that they adopt their own (unsuitable) criteria and psa cut-off values for when not to refer a patient to a urologist.a systematic review looking at the state of psa testing policies worldwide revealed significant variation in follow-up policies after a normal or raised psa level, and that this is often discordant with the available practice guidelines [33].the conflicting advice in current guidelines on prostate cancer could lead to the variations seen in daily practice [34].there are several limitations to our survey, primarily related to the small sample and the restricted catchment area of a single university hospital, which may not have been representative of the national population.although other postal surveys among gps show comparable or even lower response proportions, this could be improved by relying on on-site surveys [35, 36].
number of words= 876
[{'rouge-1': {'f': 0.39554809946685515, 'p': 0.7707874015748031,'r': 0.26603524229074893}, 'rouge-2': {'f': 0.19587488062438033, 'p': 0.32296442687747035,'r': 0.14056229327453143}, 'rouge-l': {'f': 0.33557244525284025, 'p': 0.5564864864864865,'r': 0.2402127659574468}}]
-----------------------------------------------------------------------------------------------------------------------------------
p127:
Extractive Summary:
the final simplified chinese option5 scale and user manual are available from the corresponding author upon request.table 1 shows the basic information about the key actors of the translation process.inclusion criteria for gps were as follows: (1) working at chsc for over a year; (2) signing informed consent (including consent and audio-recording); (3) willing to participate in the study.a total of 209 audio-recordings were rated with the simplified chinese version of option5 scale.sample size with an estimated dropout of 20% consultations, an enough sample size was aimed to detect correlations above 0.5 with a power of 80% for the planned analyses.rating process a total of 209 audio-recordings were separately evaluated by both raters (zy and bx) in order to assess interrater reliability of the simplified chinese version of observer option5 from july to september 2019.one of the raters (bx) rated them a second time within one month from the first rating to assess intra-rater reliability.data analysis the data were analyzed using spss statistics 22 (spss inc., chicago, il).all item scores and total scores of option5 were statistically described.in general, intra-class correlation coefficient (icc) can be used for quantitative or classified data, while the kappa coefficient method is suitable for classified data [19].in the present study, inter- and intra-rater reliabilities were calculated using weighted cohen’s kappa value on single item testing, and using icc on the total score testing.cronbach’s α was applied to evaluate the internal consistency of the scale.strength of agreement of weighted cohen’s kappa statistics between 0.81–1.00 was classified as almost perfect, 0.61–0.80 as substantial, 0.41–0.60 as moderate, 0.21– 0.40 as fair, 0.00–0.20 as slight, and < 0.00 as poor [20].the results of icc between 0.75–1.0 were classified as excellent, 0.6–0.74 as good, 0.40–0.59 as moderate, and 0–0.39 as poor [21].because there is a violation of normality, criterion validity was calculated using spearman’s rank correlation to compare option5 and option12.of all consultations, 99 focused on the decisions regarding hypertension, 39 concerning type 2 diabetes, 34 concerning hyperlipidemia, 20 concerning another chronic disease, and 17 concerning multiple other chronic diseases (see table 3).the results of rating item of simplified chinese version of option5 the rating results from rater 1 and rater 2 showed that the scores of the items were mainly concentrated on 1 and 2; the frequencies of the scores are reported in tables 4 and 5.reliability of the simplified chinese version of option5 inter-rater reliability on the total score level, inter-rater icc of the simplified chinese version of option5 was 0.859; 95% confidence interval was 0.724–0.917 (p < 0.001).weighted cohen’s kappa value of option5 ranged from 0.376 (item 5) to 0.649 (item 2) on the single item level (see table 6).intra-rater reliability on the total score level, the intra-rater icc of the simplified chinese version of option5 was 0.945; 95% confidence interval was 0.904–0.965 (p < 0.001).internal consistency cronbach’s α value of 5 items of the simplified chinese version of option5 was 0.746, indicating that there was a good internal consistency in these 5 items in the scale.the validity of the chinese version of option5 spearman’s rank correlation coefficient between option5 and option12 for the chinese versions was 0.660 (p < 0.001), indicating that there was a moderate positive correlation between the two scales (fig. 1).discussion in the present study, the original option5 was translated into chinese and cross-culturally adapted for further testing of its reliability and validity.the validation of the simplified chinese version of option5 the results showed that the simplified chinese version of option5 has satisfactory psychometric properties.the scale was found to be reliable on the total score level and to have a good internal consistency among 5 items of the chinese version.on the single item level, inter-rater reliability coefficient ranged from fair to substantial (kappa = 0.376–0.649), and intra-rater reliability coefficient ranged from medium to excellent (kappa = 0.469–0.883).furthermore, a moderate positive correlation between observer option5 and option12 also showed a good concurrent validity of the simplified chinese option5.these results provide convincing evidence about the validity and reliability of the scale and are consistent with the results of previous studies [13, 14].on the single item level, item 5 was the only item that had the lowest levels of inter-rater agreement (kappa = 0.376).this suggests a poor consistency of two raters on item 5.consultations of longer duration may lead to reduced inter-rater agreement [13].in further research, improving item specificity, training raters extensively, and providing more guidance may help to improve inter-rater consistency [13].on the total score level, compared to inter-rater reliability in the first testing of the english version (icc = 0.67) [13] and the psychometric testing in the dutch version [15] (k = 0.68), our result on inter-rater reliability was relatively higher (icc = 0.859).this outcome may be related to the single clinical decision making involved in the consultation.in the present study, most doctor-patient interactions focused on only one problem in treatment.this may have led to a relatively low consistency of studies that used the english version of the scale.
number of words= 825
[{'rouge-1': {'f': 0.4370279397939612, 'p': 0.7906896551724139,'r': 0.3019644839067703}, 'rouge-2': {'f': 0.2489280483339436, 'p': 0.40910034602076123,'r': 0.17888888888888888}, 'rouge-l': {'f': 0.42808073367463445, 'p': 0.6325000000000001,'r': 0.3235211267605634}}]
-----------------------------------------------------------------------------------------------------------------------------------
p128:
Extractive Summary:
the integration of nurse practitioners (nps) into primary care has been viewed as a solution to shortages of doctors [1], and a tool for improving patient access to care and lowering costs [2].it has also been suggested that the integration of nps into communitybased care delivery is critical to accomplishing the transformation of primary care into primary health care (phc) [3, 4] - a transformation that has itself been linked to improved care, improved outcomes, and lowered costs [5–8].the shift to phc is one towards prevention, health, wellness and the successful management of chronic disease [9–14], accomplished through team-based care [15].the integration of nps into phc-focused teams, and their integration into healthcare systems more broadly has been advocated for in a range of policy environments, including both canada and the united states [16–19].despite this alignment with transformation principles and much policy enthusiasm, the utilization of nps in canadian primary care has been inconsistent [17]. to better understand the factors behind the inconsistent utilization of nps, this paper presents one canadian province’s recent and ongoing efforts to increase the integration of nps into its primary care system.we describe the nurse practitioner support program (npsp1) in alberta, canada, and present qualitative data from interviews with stakeholders.these interviews highlight the challenges to achieving the goals outlined in the npsp and to np integration in the province’s primary care environment.for them, the npsp supports an inappropriate form of governance and dysfunctional form of team-based care.for one participant, there was a key difference between a team that was working together collegially, and a team that was built around doing work for physicians.this participant described the difference as being one between: people who are willing to work actually in a team – [and people who] want a team to work for them.completely different (participant i).when team was defined collegially, and so governance hierarchies were flattened, not just np integration, but reported job and patient satisfaction improved.under these conditions the employee-np model was viable from a governance standpoint as much as it still suffered from financial challenges.illustrating this, an np participant described a period of collegial physician-np teamwork at a pcn where they worked: [w] e called it the dream team … the patients were really happy …it [was] the best job of my whole life … and we co-referred, we shared, we had hallway consults –it was incredibly dynamic.and we all got sort of dispersed, and we weren’t allowed to eat lunch together … and almost everybody either quit or was let go (participant i).where anxieties about overlapping scopes of practice and expertise, as well as financial viability, had briefly been set aside, they returned with the medical director who had the authority to re-impose old hierarchies.with these hierarchies came a revised definition of team.as a concept it shifted away from a collegial levelling and towards treating employee nps as tools for greater physician productivity.as the social distinctions between the professions were reasserted, and the governance of nps by physicians became the reality, the two groups no longer ate lunch in the same physical spaces and morale suffered.under what this np saw as an inappropriate governance regime within their pcn, nps became mere “helpers to physicians” expected to “fill in where doctors have left holes in care,” (participant i).another np described how their role as a pcn employee had been to fill in for physicians when physicians were unavailable or during times when the physicians preferred not to work.they described how: physicians [in the pcn where i worked] would not let me practice.[they] refused to allow me to practice … i didn’t get it.but then, something [would need] to be assessed right away and [they would say], “oh, you could go do that!” after hours, friday nights and weekends.[then it was] no problem, but during the week i could not have clinic space (participant xiv).from the perspective of a participant working for both a pcn and the provincial medical association, this employer-employee relationship along with its governance implications, was appropriate.[i] f we don’t have doctors that will work until 8:00 pm and nps are willing to fill in to meet those primary care needs … [that is] totally [acceptable].absolutely use [the nps] in that capacity.but for the day to day, like the eight-tofive work of the physician when we have so many physicians, it wouldn’t make sense to me that you use the np in the same way (participant xiii).a pcn administrator participant noted how deploying nps after hours and to fill in when and where doctors were unavailable or uninterested in working was ultimately at odds with the npsps goal for comprehensive primary care: [if an np is] just providing access in terms of evenings and weekends, you can’t necessarily be there to provide that comprehensive care (participant ix).in this way the hierarchical rather than collegial governance enabled by the npsp’s choice of the pcns was seen, by some participants, as working against the policy’s central goals.discussion alberta’s npsp faces a number of critical challenges that impede its ability to achieve its stated objective of integrating nps into the province’s primary care system.these challenges include governance issues that distribute authority and funding options unequally; financial disincentives for nps, physicians, and pcns; and a small number of highly delimited job opportunities.each of these represents an opportunity to adjust the policy to be better calibrated to accomplish its main goal of np primary care integration.from the majority of participants’ perspectives, perhaps the most problematic aspect of the npsp’s use of the pcns as mechanisms of integration was that this gave physicians the final say on job availability, remuneration, and termination, as well as how key ideas like ‘care team’ are operationalized.in this sense, our interviews highlighted governance impediments to np integration similar to those identified elsewhere [54], with other policy and legislative arrangements described as major barriers to effective integration [55].whether nps are to be employees or independent providers, for primary care integration to succeed, governance arrangements that see them collaborating with - rather than subordinate to physicians - are likely a pre-requisite.here we are drawing on the observations of others who have noted the ways in which funding and care delivery models that support medical dominance tend to impede collaboration [56].our data confirm that nps are not encouraged to integrate when physicians are granted the authority to resolve territorial conflicts over scope of practice in their own favour, or to define whether a team will be collegial or hierarchical.as d’amour et al. [57] have noted, successful collaboration in healthcare teams is the result of careful work at interpersonal, system, and governance levels, not imbalanced relationships.in this sense, the npsp in its present form, embedded as it is in the billing and governance structures of the province, is not able to reach its full potential as a means to increase access to quality primary care through np participation.beyond governance as an issue of professional autonomy, the npsp fails to address longstanding financial disincentives that affect nps themselves, physicians, and the pcns.it is imperative to consider physician compensation structures in place where nps are attempting to integrate.family physicians in alberta are able to directly access public funding by billing the government ffs [58], or if they choose through an annualized, sessional or blended capitation agreements with the government [41].in contrast, the npsp affords none of these options to nps and instead requires them to be employees of “physician-controlled” pcns.as such, nps find themselves both unable to open practices of their own and find it challenging to generate enough revenue to cover the overhead they incur as employees.for their part, physicians operating on a ffs basis – which is to say the majority of primary care practices in the province – find that nps, along with other members of the care team working alongside them, are unable to bill for services that physicians would normally provide.the inability to bill the government for services rendered by non-physician care team members has been consistently identified as an impediment to integrating non-physicians into ffs practices [26, 59].indeed, it has been identified as restricting np integration specifically [54, 56, 60–63].if these are the disincentives for individual nps and physicians, at the pcn level the npsp proposes that pcns use the per capita revenue generated by the patients on an np’s panel to cover, or partially cover, that np’s salary.the challenge here is that this per capita funding is at the center of the pcns’ financial model.redirecting per capita payments – the pcns’ only source of revenue – towards np salaries is a redirection away from other care initiatives and practice support work at the heart of these organizations’ mandate.in this sense, deploying a physician who can bill the government ffs, and not an np who draws down the local budget, is a more sensible option as a physician frees up more pcn money.the npsp does not adequately address the fact pcns are financially incentivized to utilize physicians over nps – a critical point since the decision to utilize nps, as the policy is presently written, remains a choice for the pcn to make.if the goal is for longterm np utilization, rather than a “fill in” for the shortterm, these financial disincentives need to be addressed.the sense among participants was that options beyond pcns, and options beyond panel driven general practice were central to achieving greater np integration in the province’s primary care system.of the npsp was in its failure to conceptualize the operational and practical role of nps as they integrated into primary care.the program does not provide clarity regarding the roles and positioning of nps in the primary care system: is the objective to add np jobs, where the np acts as a supplement to physicians in certain geographical areas, with certain patient population, or after hours? or is the role of the np to partner with a physician and together manage patients?perhaps the goal was to enable nps to operate as independent practitioners.these goals do not have to be mutually exclusive, but each require different operational and funding barriers to be addressed for the stated goal to be met.any policy that impacts the role of nps should offer clear definitions of goals and roles of the program, particularly from the perspective of the finer operational details.in addition, the concerns of existing stakeholders need to be anticipated and addressed in a comprehensive manner – meaning they must be developed with broad clinician and stakeholder consultation.in other words, nps should, as one participant put it, “have a seat at the table” in providing input to how their role is utilized.conclusion three major factors are impeding alberta’s npsp from realizing its own objective of increasing np integration into primary care: 1) financial viability issues in which nps, physicians, and pcns are all adversely affected; 2) policy issues in which pcns with competing priorities act as np employers, and nps are expected to panel patients in competition with pcn physicians; and 3) governance issues in which nps are not afforded sufficient authority over their role or how the key concept of ‘care team’ is defined and operationalized.in its current iteration, the npsp does not appear to be a long-term solution for increasing np integration into the province’s primary care environment.increased np integration in primary care likely requires increased funding flexibility that will allow nps to access funding directly from the government, outside pcns, with funding options to fit their individual practice setting.in addition, future np policy development should: 1) ensure a clear goal for the np role is established through clinician and stakeholder consultation including nps themselves; and 2) ensure funding, policy, and governance structures are aligned with this envisioned goal for successful np integration into various primary care practice settin
number of words= 1946
[{'rouge-1': {'f': 0.2914611538631744, 'p': 0.8610447761194029,'r': 0.17542018896071607}, 'rouge-2': {'f': 0.19722579755219796, 'p': 0.47823970037453184,'r': 0.12422885572139304}, 'rouge-l': {'f': 0.30796545629195116, 'p': 0.6688023952095807,'r': 0.20003901170351107}}]
-----------------------------------------------------------------------------------------------------------------------------------
p129:
Extractive Summary:
digital consultation with primary care physicians via mobile telephone apps has spread rapidly in sweden since 2014.three of the private companies (kry, min doktor and doktor.se) have about 90% of the digital consultation market in sweden [1].in 2019, approximately 1 million digital consultations with physicians took place in sweden, representing 5% of all consultation sessions with physicians in primary care.the number of digital consultation sessions increased by 67% in 2019 from the previous year, suggesting a fast expansion in the popularity of this service [1].since the start of the coronavirus pandemic, digital consultations with physicians in sweden have increased by approximately 60% between february and march 2020 for the largest private health companies.the publicly funded health care system also experienced a rapid increase in digital consultations during the same period [2, 3].the swedish government has set an ambitious ehealth goal of becoming the best in the world at using the opportunities offered by digitalization to make it easier for people to achieve good and equal health and welfare, and to develop and strengthen their own resources for increased independence and participation in the life of society [4, 5].the european commission also aims to achieve more efficient health care by means of ehealth, emphasizing the importance of user-friendly, accessible, and safe care for patients throughout the european union [6].the new digital consultation allows remote working because primary care physicians can work from home, outside their traditional primary care environment [9, 10].research on remote working has established that there can be problems concerning social isolation, lack of progress in career opportunities and not being visible in the organization as well as physical disadvantages in the work environment [11, 12].however, despite the spread of digital consultations in primary care, there is a lack of knowledge concerning how the new service affects physicians’ psychosocial work environment, including their perceived work demands (e.g., perceived time pressure and role conflict between work and family), control over working processes (e.g., decisional latitude and skill discretion), and social support (e.g., interaction with co-workers and supervisors), which has been investigated among remote workers in other populations [13–15].we have not been able to identify any studies in sweden or internationally that have investigated how digital consultation is perceived with regard to psychological well-being and psychosocial work environment.such knowledge is important considering the research demonstrating many problems associated with physicians’ mental health and working conditions [16–19].studies in many countries in europe and the united states have shown high and increasing perceived stress and burnout among physicians [16–20].furthermore, primary care physicians’ workload seems to have increased due to an increase in demand for health care among citizens as well as an increase in bureaucratic paperwork and tasks viewed as illegitimate by physicians [7, 20–22].previous research on digital consultation with physicians has primarily focused on the patients’ point of view and experiences of such meetings [23–27].studies have also investigated the cost-effectiveness of digital consultation [28, 29].hence, there is a paucity of studies concerning digital patient consultation conducted from the perspective of physicians and focusing on their psychosocial work environment.addressing an important knowledge gap, the aim of this study was to investigate primary care physicians’ perceived work demands, control over working processes, and social support when providing digital consultation to primary care patients.methods study design and setting the study has a qualitative design, using semi-structured, individual interviews.a qualitative approach was chosen because little is known about the psychosocial working conditions of primary care physicians in digital consultations with patients.the swedish health care system consists of 21 regions (previously known as county councils), which provide health care for the population, mainly funded by taxes.all residents are insured by the state with equal access to health care for the whole population.fees are low and regulated by law.primary care (both traditional and digital consultation) is provided by both public and private actors, with the private providers representing 43% of the total number of primary care units in sweden 2018 [28].private health care companies dominate the market with regard to digital health care services (90%), but regions are increasingly launching their own digital consultation services [1, 30, 31].the private health care companies included in our study are contracted to regions; the out-of-pocket fees for their patients are equal to that of publicly funded health care [32].the objective of this sampling strategy was to recruit physicians who represented a broad spectrum of experiences and perceptions.we sent an e-mail to this person, briefly informing them about our study and asking for physicians from the region to participate.we did not receive any response from 8 regions, whereas 4 regions agreed to participate and provided contact information to key persons in the organization to enable us to establish contact with physicians who had worked with digital consultation.the remaining 9 regions responded that they did not offer digital consultation and/or declined participation.we approached 29 primary care physicians from the 4 regions, of which 17 agreed to participate.of these, 5 agreed to participate in our study.we approached 12 physicians from these companies, of which 11 agreed to participate.thus, in total we recruited 28 physicians for the study (table 1).to the other researchers, the participant was known only by initials and other demographic, non-identifying data.no participant withdrew participation during or after the interviews.the study was approved by the ethics review board in region östergötland (2019–01910).transcripts are stored in the authors’ password-protected computers with no access for anyone other than the authors.data collection the authors developed a semi-structured interview guide to capture the physicians’ perceptions and experiences concerning the psychosocial work environment of digital consultation.the interview guide was assembled by the research team behind the study, based on the existing literature on psychosocial work environment [14].we pilot tested the interview guide in 2 interviews, which indicated that further questions regarding the digital work environment needed to be incorporated into the interview guide.despite this, the first 2 interviews included relevant information and were therefore included in the analysis.the interviews were conducted by all authors except pn and js.each interview lasted between 24 and 84 min and was digitally audio recorded.no field notes were taken during or after the interviews.the interviews were conducted in by video meeting, telephone, or a personal meeting, depending on what suited the participant best.during the interviews, only the participant and interviewer were present to allow the participant to speak freely.the first 3 interviews were transcribed verbatim by hf and the remaining interviews were transcribed by a professional transcription agency.all transcripts were carefully examined by hf to ensure accuracy.the interviews took place between april and october 2019.saturation of data was discussed by the research team, and according to earlier research, the major themes and codes are derived from data after 12 interviews.theoretical framework the interview questions were informed by the job demand-control-support (jdcs) model [14, 34].the model was also used as a framework to analyze the data by means of categorizing the physicians’ perceptions and experiences concerning digital consultation into the 3 categories of the model, i.e., demand, control, and support, in the deductive analysis of the data.the job-demands model was originally developed by karasek [35].job demands may involve time pressures and role conflicts, whereas job control refers to employees’ ability to control their work situation [13, 14].combining the 2 dimensions of job demands and job control, karasek [14] stated that jobs high on demands and low on control (“high strain jobs”) carry a high risk of development of adverse psychological symptoms such as anxiety and depression, but also cardiovascular disease [34].by contrast, in jobs that are low on demands and high on control (“lowstrain jobs”), adverse reactions are unlikely.the job-demands model was later extended by johnson and hall [34] who added the dimension of support at the workplace as a third dimension.the public regions offered digital consultations as a minor part of their regular primary care.the publicly employed physicians either performed digital consultations as part of their regular employment or as an additional, reimbursed task.they explained this in terms of non-existing production demands, i.e. number of consultations with patients per hour, in combination with the type of medical cases they handled in digital care.you managed that without any problems.they noted that digital consultation had some shortcomings that could have a negative impact on patient safety, e.g., when patients sought medical advice for conditions that were not suitable for digital consultation or when there were technical problems so that the contact with the patient was lost and the patient could not be reached again.for i think, for example, sometimes when your patients’ swedish [language] is very poor, i feel, and i actually think many agrees with me, that it is better with a traditional consultation where you really can sort things out and talk, face to face.it was viewed as risky to prescribe medication without knowing what other medications the patients were prescribed since a majority of patients were unknown to the physicians when working with digital consultations.they noted that the digital consultation predominantly reached young, presumably healthy individuals, possibly at the expense of the elderly population with a higher burden of disease.the question about who decides what is, and what is not, an important medical condition was also brought up.in most regions and private companies represented in this study, digital consultation means that patients first get in touch with highest medical level (i.e., a primary care physician) instead of, e.g., discussing their problems with a nurse as a first step.in my opinion, health care today is very, very available for quite young, healthy individuals.but for those who are really the most ill, it can be a complicated system and difficult to access.[#15] moreover, it [digital consultation] may be a waste of resources, like i told you before, you immediately get the highest level of competence, in a very convenient and available way.and that’s really the greatest risk, that you pay a lot for a small measure from the health care system.social support can also influence job stressors in a positive direction, acting as a “buffering” mechanism.we have this community, it’s a fantastic source of support among colleagues concerning both work environment issues and also practical concerns, but also concerning knowledge about patients.you feel ashamed, you know how others react to this “so you’re after the easy money, how bad you are, taking it from taxes”.they mean the cheapest health care goes to traditional primary care and that we (the digital physicians) take the easy patients from traditional primary care, or “unnecessary” patients.many people think like this, which isn’t true.most of the patients haven’t been able to get help from traditional primary care.from patients i usually get acknowledged but from my employer it has become worse.[#6] however, some of the participants also observed that digital consultation situations could create a more anonymous form of physician-patient communication, making some patients lose their inhibitions so that they were occasionally quite rude and sometimes said inappropriate things they probably would not have done in a face-to-face consultation.furthermore, mesko and győrffy [48] propose a change in the patient-doctor relationship to more of a partnership.the findings in our study are also in line with earlier research concerning communication and the relationship between caregiver and patient; patients may experience the digital way less intimidating and find it easier to say what is on their mind than in traditional consultations [49].
number of words= 1870
[{'rouge-1': {'f': 0.35768275122384596, 'p': 0.9483382789317507,'r': 0.22040650406504067}, 'rouge-2': {'f': 0.2773843408365801, 'p': 0.6801190476190477,'r': 0.17421962379257755}, 'rouge-l': {'f': 0.4029067473448373, 'p': 0.84720207253886,'r': 0.264300518134715}}]
-----------------------------------------------------------------------------------------------------------------------------------
p130:
Extractive Summary:
however, the decision-making process is difficult owing to the complexity of the options, which have very different risk profiles and impact on quality of life.it therefore has been suggested that this decision is well-suited to shared decision-making [2].shared decision-making describes a collaborative process between patients and healthcare providers that results in a decision reflecting best medical evidence and the patient’s values and preferences [3].shared decisionmaking has the potential to improve healthcare outcomes by increasing patient knowledge and aligning care with patient values [4].further, shared decision-making is considered an important pathway to achieving patientcentered care and improving health care quality [5, 6].among inflammatory bowel disease patients, shared decision-making regarding medical management improves health care outcomes, including patient adherence, anxiety, satisfaction, and costs of care [7–9].however, preferences for participating in decision-making are dynamic and for a single patient may change with time or with different decisions [11, 12].following total proctocolectomy, the decision between ipaa and ei is a high stakes decision with permanent implications.however, very little is known about the extent to which patients with ulcerative colitis participate in making a decision about surgery or their needs during the decision- making process.methods participants we enrolled adult patients age 18–70 years with ulcerative colitis who had surgery at the university of california, san francisco resulting in an ipaa or a permanent ei in any number of stages.eligible patients completed surgical therapy 4–24 months prior to the interview.the age limit of 70 was chosen to optimize the group where ipaa would be a surgical option [14].in each of these age groups we planned to recruit at least one man, one woman, one patient undergoing permanent end ileostomy and one undergoing ipaa.interviews semi-structured interviews were conducted by a female member of the team with prior experience conducting qualitative research (rh).the university of california, san francisco committee on human research approved all study procedures.results we conducted a total of 17 interviews, 16 were included in the analysis.participant characteristics are detailed in table 1.participation in decision making when it came to deciding between ipaa and ei, most ulcerative colitis patients expressed active participation in decision-making.eleven patients reported that they experienced “patient-led” decision-making (where the patient felt independently responsible), 3 shared decision-making with the surgeon, and 2 experienced surgeon-led decision-making (please see table 2 for representative quotes).many patients felt that the decision between ipaa and ei was well-suited to patient-led decision-making: the surgery is going to change your life.in this group, 6 patients had a passive tone in decision making regarding the choice to have surgery, and in many instances, didn’t see that there was a choice to be made.this sometimes was due to the severity of the illness: there was really no [choice] - i didn’t care.in contrast, these same patients showed increased engagement in the decision-making process when facing a decision between ipaa and ei.these included patients who felt that they did not have enough time with their surgeon during decision-making, the overwhelming complexity of the decision, and lack of support from family members who did not agree with the patient’s decision (table 3).first, patients had variable experiences with their family members, some having supportive and helpful relationships: there were no difficulties.and patients found that specific people were helpful, while others were not: my dad’s just kind of there, literally, he’s just kind of there.ulcerative colitis patients vary in their desire for social support and in the amount of support available when making a decision between permanent ei and ipaa, highlighting the role for an individual assessment of patient needs during decision making.improving decision making when discussing ways to improve decision-making, three themes emerged.still others wished to have more information about what life would be like in the long-term: i mean, most of the stuff i heard sounded as close to a regular life as you could get.so i don’t know.to me, there’s lots of realities that i don’t feel like are really introduced or just explained.only three patients felt content with the information they used to decide about surgery.discussion in this study, we used semi-structured interviews to characterize the process of decision-making between ipaa and ei among surgical patients with ulcerative colitis.we found that patients largely experienced active participation in decision-making between ipaa and ei, but struggled with not having enough time with their surgeon, dealing with unsupportive family members, and feeling overwhelmed by the general complexity of the decision.patients felt that decision-making could be improved through increased information, access to peer education, and earlier consultation with a surgeon.our results demonstrate high levels of patient participation in decision-making about surgery.this finding is not surprising when considering other studies in inflammatory bowel disease patients, which demonstrate high levels of patient interested in shared decision-making about medical management.oh, that would be the last thing.i don’t know what i would do if i had that.that’s horrible’…so there have been some people in my life who, again, have this stigma of what this is like, having an ileostomy.ulcerative colitis patients vary in their desire for social support and in the amount of support available when making a decision between permanent ei and ipaa, highlighting the role for an individual assessment of patient needs during decision making.patients felt that decision-making could be improved through increased information, access to peer education, and earlier consultation with a surgeon.by focusing on ulcerative colitis patients who have made the difficult decision between ipaa and ei, our study addresses a critical gap in the existing literature, which includes studies on general perceptions of shared decision making [10] and educational needs [17, 18] of non-surgical inflammatory bowel disease patients, or patients who are making a decision between surgery and medical management [19].it is important to address these barriers so that patients can participate to their desired level and in a meaningful way.patient decision aids have promise for supporting shared decision making and three have been developed for inflammatory bowel disease patients [21–23] with further trials underway [24, 25].in the present study, we found that patients undergoing subtotal colectomy as a first step in their surgical treatment were likely to report a passive role in decision-making regarding the decision to undergo subtotal colectomy, but then later played an active role in making a decision about ipaa vs ei.the reasons for the decreased participation at the time of subtotal colectomy were not purposefully explored in the present study, however, other studies demonstrate that patients making medical decisions tend to defer decision making to physicians when they have more severe disease [20] or disease likely to result in mortality [13].this finding underscores the importance of reassessing patient desire to participate in decision-making at each step of treatment.the majority of patients in the present study desired increased information prior to making a decision about surgery.the need for improving patient education is particularly relevant in ulcerative colitis patients, because more than 56–62% feel insufficiently informed about their disease [17, 18, 27].however, patients feel that existing educational materials are not tailored to their needs, including a lack of coverage of important topics, such as long-term recovery, and practical matters, such as returning to exercise, dietary restrictions, and management of stomas [19].our study demonstrated that some patients struggled with inadequate support from family members and there was a desire for increased peer support as a mechanism to improve the decision-making process.this support gap has been challenging in inflammatory bowel disease patients because, although other studies show that patients wish to have peer support when they are making decisions [10, 19], structured, regular peer support groups for inflammatory bowel disease often fail [31].one reason for this finding is that patients find peer support helpful during flares of disease or when they need help with a particular problem as opposed to when they are feeling well [31].this finding is in agreement with prior work that shows that more than half of uc patients would have preferred to have an earlier operation [32].in order to create the generalizability, we performed purposeful enrollment of patients across age groups, sex, and procedure type and ensured saturation prior to completing enrollment.
number of words= 1332
[{'rouge-1': {'f': 0.3184389162530641, 'p': 0.7842857142857143,'r': 0.19977649603460707}, 'rouge-2': {'f': 0.19096257961711882, 'p': 0.3847410358565737,'r': 0.126998556998557}, 'rouge-l': {'f': 0.34341065292096223, 'p': 0.6255555555555556,'r': 0.23666666666666666}}]
-----------------------------------------------------------------------------------------------------------------------------------
p131:
Extractive Summary:
background primary sclerosing cholangitis (psc) is a fibroinflammatory disease of the bile ducts resulting in chronic cholestasis often progressing to end-stage liver disease and/or the development of hepatobiliary neoplasia [1].no effective medical treatments to slow disease progression exist for psc outside of liver transplantation, which is not always curative as disease recurs in about 25% of transplanted patients [2].clinical presentation of psc is heterogeneous, it can affect both small and large intrahepatic and/or extrahepatic bile ducts and is often, but not always, associated with concurrent inflammatory bowel disease, which occurs in ~ 70% of patients [4].psc has a substantial socioeconomic burden.the risk of developing hepatobiliary cancer has been estimated to be significantly higher in psc patients compared to the general population [7].unfortunately, diagnosis of early-stage cca is often extremely difficult due to the lack of reliable diagnostic biomarkers, and thus, many patients with cca are not eligible for curative surgery due to advance disease stage at diagnosis [10].lack of prognostic tools and treatment options for psc is driven in part by our poor understanding of its pathogenesis, which is thought to be complex, the interaction of homeostatic perturbations driven by genetic variants, environmental influences and biological response throughout the course of disease [11].moreover, while some of the current animal models do demonstrate individual features of psc, none display the constellation of characteristics commonly observed in patients [21].thus, use of these models is largely limited to focused studies, the findings of which may not be truly reflective of human disease.this resource will be made available to the broader community of psc-interested researchers, bringing together investigators with a wide range of expertise to accelerate the pace of psc research.the ultimate goals are to individualizing care of patients with psc care and improving their outcomes.however, additional sample types including plasma, serum and peripheral blood mononuclear cells (pbmc) were collected and are available for use in psc research.this protocol collects additional sample types from blood, including rna and circulating cell-free dna, as well as stool and urine specimens.both protocols collect(ed) specimens and data from psc patients and controls and the current protocol is open to individuals enrolled in the earlier one.where necessary to discriminate between the two protocols in the remainder of the manuscript we will refer to them as the “current” (mayo clinic irb #16-005892) or “previous” (mayo clinic irb #670-02) protocols.we also engage in collaboration with clinicians and researchers from other non-mayo medical centers to increase patient enrollment while balancing the need to obtain sufficient medical data to perform informed studies with privacy concerns of patients and the collaborating centers.control subjects individuals without liver disease who receive their health care at one of the three primary mayo clinic sites (minnesota, florida, and arizona) or in the broader mayo clinic health system are identified by chart review and invited in-person or by mail to participate in our studies.the resource includes patients with variant forms of psc such as small-duct disease and psc overlapping with autoimmune hepatitis; as well as patients who have previously received a liver transplant.patients with other concurrent liver disease (besides for autoimmune hepatitis) or who are unable to provide informed consent are not eligible for enrollment.consent process participant consent is obtained at the beginning of the recruitment process by mail-in, electronic or face-toface protocols.patients are encouraged to contact the study staff with any questions and are informed that their response will not impact their medical care.patients enrolling in the current protocol are also asked to fill out a 24-hour food frequency questionnaire in conjunction with submission of their stool sample, which is available online via the automated self-administered 24-hour dietary assessment tool (https:// www.the previous protocol primarily collected blood, which was processed and stored as aliquots of dna, buffy coat, pbmc, plasma and serum.participants in the previous protocol were offered renumeration in the form of a choice of healthrelated books.in the current protocol, we ask participants to provide blood, urine, and stool samples at time of enrollment.while we encourage individuals to provide all sample types, this is not required.for psc patients, we plan to request additional blood, urine, and stool samples at 2-to-4-year intervals, depending on level of interest.however, agreement to provide future samples is not a requirement for enrollment.finally, fresh tissue specimens (liver parenchyma and bile duct) are collected through the mayo clinic tissue request acquisition group from patients with psc and controls who undergo liver transplantation, partial hepatectomy, or are a liver donor at mayo clinic.all biospecimens are stored de-identified, labeled with a study-id traceable only by the team of the principle investigator (pi) or by certain members of the bap lab staff.in contrast, most of the biospecimens collected under the current protocol are stored in the bap lab following sample preparation.this storage service provides improved back-up and monitoring service and facilitates transfer to core laboratories responsible for aliquoting samples and performing omics-scale experiments.additionally, all study samples, whether stored in the pi lab or the bap lab, are tracked by study staff using a customized database that coordinates sample location, sample usage, prospective lab results and mapping to phenotypic and clinical data.this approach will help to ensure specific research questions can be adequately addressed, ethics considerations undergo proper review and that data and sample usage is agreed upon and documented.this difference is due to this control population being shared with a similar biobank of pbc patients that we maintain, of which 90% of the patients are female and they tend to be older than psc patients.genomic dna and/or buffy coat, plasma and pbmc samples are available for the majority (> 90%) of psc patients and controls while serum is more limited as it was collected as a residual of prospective liver function tests in the previous protocol.as with the previous protocol, the demographics of the patient group are consistent with other reports with 56.0% being male, median age of diagnosis of 41.5 years and 78.0% having concurrent ibd.thus, while 87 of the patients had advanced disease at time of study enrollment only 9 of them had already received a liver transplant.so far, an additional 20 patients developed advanced disease and 24 patients received a liver transplant during follow-up.a total of 22 patients were diagnosed with hepatobiliary cancer prior to study entry, with an additional 7 patients developing hepatobiliary cancer during follow-up.first, the primary goal of the previous protocol was to perform genome wide association studies, which only require a confirmed psc diagnosis.second, mayo clinic is a major referral center for patients who develop psc-related complications (advanced disease, abnormal biliary cytology, suspicious strictures, and biopsy-proven cca).
number of words= 1087
[{'rouge-1': {'f': 0.42318262185634675, 'p': 0.7814285714285714,'r': 0.29015915119363395}, 'rouge-2': {'f': 0.24992232541678375, 'p': 0.41957020057306593,'r': 0.17796460176991152}, 'rouge-l': {'f': 0.42018607606426434, 'p': 0.6414285714285715,'r': 0.31242424242424244}}]
-----------------------------------------------------------------------------------------------------------------------------------
p132:
Extractive Summary:
since the first documented case of covid-19 in the united states (us) on january 21, 2020, many hospitals have had to enact strict measures to prevent further spread of the virus among the population.the pandemic has led to widespread disruptions in the performance of elective and outpatient procedures.endoscopic procedures were especially affected by these disruptions due to high rates of aerosolization of the virus by upper endoscopy, viral shedding in the stool, and potential contamination of endoscopy suite surfaces with viable virus up to 72 h after initial contact [1].as endoscopic procedures also carry a high risk of viral spread to healthcare providers, it became necessary for hospitals to set up an organized model for re-opening endoscopy suites to elective procedures through a combination of testing, triage, and provision of appropriate personal protective equipment to patients and staff.little is currently known regarding the efficacy of these measures in preventing transmission of the novel virus to staff and patients in this context.since patient perception of procedure associated infection has been identified as a significant concern when restarting outpatient elective procedures, it is crucial to assess the safety of our procedures to ensure ongoing appropriate utilization of outpatient endoscopy [2].as of the date of this manuscript, there have been over 33 million confirmed cases of covid-19 in the united states out of 173 million cases worldwide with over 3.5 million covid-19 related deaths worldwide [3].the infectious transmission of the virus is thought to be via respiratory droplets with potential for airborne spread during aerosol-producing procedures, such as endoscopy [4].the insidious nature of the epidemic stems from its high rate of asymptomatic spread, with 80% of cases being mild or asymptomatic [4].as the prevalence of disease became more clearly established by widespread testing within our institution and community, the division of gastroenterology and hepatology developed a tiered strategy in an effort to resume timely scheduling of patients awaiting diagnostic procedures.despite the absence of procedural covid-19 transmission at our institution, concerns voiced by patients provided important insight into challenges the gastroenterology community will have to address in the immediate future and beyond.patients who had completed their respective endoscopic procedures were contacted by telephone to inquire if they underwent covid-19 testing within 14 days postprocedure (table 3).7.2% of patients in the same cohort identified new financial limitations as reasons for not scheduling.the average number of days patients were tested prior to their procedures was 2 days.of the patients who were symptomatic, fever, cough and gastrointestinal symptoms were the most reported.no significant interim upper or lower gastrointestinal bleeds were identified.discussion in an attempt to preserve hospital beds, resources, ppe and to protect staff and patients during the covid-19 pandemic, non-emergent endoscopies were delayed or canceled leading to a substantial reduction in procedures performed in endoscopy suites.use of our model for reopening endoscopy suites by a tier-based system allowed recognition of barriers for scheduling and facilitated appropriate utilization of the excess capacity to accommodate most patients across all tiers within 4 weeks.thus, minimizing delays in patient care.this narrowed the utility of our tier system, as our highest priority patients identified covid-19 related unemployment and financial limitations besides peri-procedure infection concerns as reasons for postponing their procedures.while covid-19 related concerns accounted for the largest proportion of patients in all groups, a financial disparity was noted in comparison to our tier 4 patients (most often average risk colon cancer screening colonoscopies), none of whom reported insurance or cost concerns for reasons for postponement.this may explain this disparity, particularly at the beginning of the pandemic when infection burden in our area was relatively low compared to other parts of the country.additionally, there could be a delay in the diagnosis of nearly 22,000 high grade adenomatous polyps with malignant potential and 3000 colorectal cancers, resulting in a 6.5% increase in the 6-month mortality rate for patients who would eventually be diagnosed with colorectal cancer [17, 21, 22].it is therefore important to methodically recommence non-emergent endoscopy to minimize the economic and health impacts of its suspension.to conclude, our tiered approach for rescheduling while following strict risk mitigation strategies allowed for successfully resuming outpatient endoscopic procedures keeping patients as well as endoscopy staff safe and protected from covid-19 transmission.conclusion by implementing a tiered approach to reschedule patients undergoing elective endoscopy during the height of the covid-19 pandemic with strict risk mitigation strategies, our institution was able to safely restart outpatient endoscopy to minimize delays in endoscopic diagnosis and patient care.
number of words= 737
[{'rouge-1': {'f': 0.4055124164339383, 'p': 0.7599563318777294,'r': 0.2765359477124183}, 'rouge-2': {'f': 0.1929141780476091, 'p': 0.3068421052631579,'r': 0.1406806282722513}, 'rouge-l': {'f': 0.37177554890072545, 'p': 0.5941379310344828,'r': 0.27052770448548813}}]
-----------------------------------------------------------------------------------------------------------------------------------
p133:
Extractive Summary:
it occurs between the seventh and eighth decade of life and rarely before the age of 40 [5].it occurs with aging specifically due to changes in fibrosis and muscle necrosis of the upper esophageal sphincter [6].symptoms may present for weeks to years before presentation and diagnosis.most patients present with a complaint of dysphagia [7].two mechanisms have been proposed by which the diverticulum can cause dysphagia: incomplete opening of the upper esophageal sphincter and extrinsic compression of the cervical esophagus by the diverticulum [8].as the diverticulum enlarges, the symptoms become more severe with resultant weight loss.a sudden increase in severity of hematemesis may signal the development of ulcer [9].esophagography is necessary to confirm the diagnosis of zd [1].several surgical options are available for the management of zd [10].herein, a typical case of zd diagnosed through barium swallow examination is reported in an elderly man.case presentation an 85-year-old male was referred to our center with a history of pain in swallowing and later regurgitation of undigested contents to his mouth 3-4 h after meals.the odynophagia and dysphagia was progressive in the last one-year before his presentation.initially his dysphagia was to solid food alone and later to both solid foods and fluids.his appetite was affected by his odynophagia and he lost weight in 6-month period, in which he or his family members failed to quantify.there was no history of halitosis.one month prior to his presentation, the patient developed intermittent cough during meals.he was relatively healthy prior to onset of these symptoms.there was no history of hypertension or diabetes mellitus.on physical examination, he was visibly wasted however his vitals were normal and there were no neck swelling or enlarged neck lymph nodes.his abdomen was soft and non-tender and no organomegaly was detected.his complete blood count showed hemoglobin concentration of 10.9 gm/dl, total white blood cell count of 7000/ml and platelets of 250,000/μl.esophagogram was done with oral suspension of barium sulphate demonstrated a smooth posterior outpouching of the proximal esophagus which was 2 cm from the epiglottis.the outpouching demonstrated an air-contrast level and measured 4.2 × 3.4 × 3 cm.
number of words= 345
[{'rouge-1': {'f': 0.39089148583988353, 'p': 0.6731746031746031,'r': 0.2754054054054054}, 'rouge-2': {'f': 0.1908183678108661, 'p': 0.28600000000000003,'r': 0.1431707317073171}, 'rouge-l': {'f': 0.35224264882557416, 'p': 0.5467441860465116,'r': 0.25981481481481483}}]
-----------------------------------------------------------------------------------------------------------------------------------
p134:
Extractive Summary:
these indicators are simple to measure and provide excellent convenience to prevent and manage many diseases [12, 13].however, in recent years, in-depth studies have found that the waist-to-height ratio (whtr) can better assess the risk of central obesity, diabetes, hypertension, and other metabolic diseases [14–17].however, few studies have investigated the association between whtr and nafld, which contain deeper relationships, such as non-linear relationships, and whether a special population exists among different subgroups.additionally, the sample sizes of several existing studies on the association between whtr and nafld are relatively small (n = 250–6143).therefore, this study aimed to further explore and analyse the association between whtr and nafld in adults using a large sample size.methods subject population and design our study population was from a large-scale health examination programme called ‘human dock’ in japan that aims to promote public health and assesses common chronic diseases and their risk factors through physical examination.the research data have been uploaded to dryad public database by okamura et al. according to the terms of service of drayad database, we can use this data for secondary data analysis based on different assumptions [20].in previous studies, research ethics was approved by the murakami memorial hospital ethics committee and informed consent was obtained from all subjects; additionally, in order to protect the privacy of patients, the effective identification id of all subjects in the study was replaced by health codes, and the whole study process followed the declaration of helsinki.information such as height, weight and wc came from the patient’s self-report.the habit of exercise was defined as participating in any exercise more than once a week; smoking status was divided into nonsmokers, former smokers, and current smokers by asking about smoking history when baseline data were collected.drinking status was divided into non-drinking or small drinking (< 40 g/w), light drinking (40–139 g/w) and moderate drinking (140–209 g/w) depending on the amount of alcohol consumed.bmi was calculated as weight/height2, and whtr as wc/height.haematological indicators were tested by sampling venous blood after a night of fasting and included gamma-glutamyl transferase (ggt), alanine aminotransferase (alt), high-density lipoprotein cholesterol (hdl-c), aspartate aminotransferase (ast), total cholesterol (tc), haemoglobin a1c (hba1c), triglyceride (tg), and fpg.according to the results of four types of ultrasound, such as liver brightness, hepatorenal echo contrast, vascular blurring, and deep attenuation, the evaluation was made and a final diagnosis was made [22].additionally, we used a multivariate logical regression model to examine the nafld risk corresponding to each whtr quintile.as a sensitivity analysis, we treated the whtr quintile as a continuous variable and examined the whtr quintile and nafld risk trend.in the nafld group, the general clinical indexes (age, height, weight, bmi, wc, whtr, sbp, dbp, smoking status, and drinking status) of the participants were higher than those in the non-nafld group (p < 0.001).additionally, among the quintile groups of whtr, participants in quintile 2, quintile 3, quintile 4, and quintile 5 showed 3.62-fold, 5.98-fold, 9.55-fold, and 11.08-fold increased risks of nafld, respectively, compared with that in quintile 1 (p trend < 0.0001) (table 3).the regression spline curve indicated that the relationship between whtr and nafld was non-linear, in which a whtr of approximately 0.4 might be the threshold effect of nafld risk, 0.6 might be the saturation effect of nafld risk, and a value between 0.4 and 0.6 might demonstrate linear relationship with the risk of nafld (fig. 1).subgroup analysis to further reveal the deep association between whtr and nafld, interaction tests were performed in the predefined subgroups (table 4).a whtr value between 0.4 and 0.6 likely indicated a linear relationship between whtr and nafld risk.ir causes an increase in free fatty acids in hepatocytes, and these fat molecules make the liver more vulnerable to a second blow, increasing the liver’s susceptibility to other damage factors [35, 36].additionally, this study is the first to confirm that the association between whtr and nafld is non-linear and provides a reference range of whtr associated with nafld risk.this study’s limitations are mainly due to the following aspects: (a) a cross-sectional design was adopted; thus, the causal association between whtr and nafld could not be determined.however, due to the retrospective nature of the study, the issue of observational bias was avoided.however, abdominal colour ultrasound has reduced the economic burden and physical damage of physical examination in healthy individuals; with the improvement in ultrasonic detection technology, the sensitivity and specificity of abdominal colour ultrasound in detecting nafld have reached a high level [39]; (d) although we have adjusted a large range of known risk factors, many risk factors may exist that we have not yet discovered or cannot measure, causing inevitable residual confusion; (e) since this study is based on a secondary analysis of previous research data [20], diabetes patients are not included in the data package, so it may cause a certain selection bias.
number of words= 799
[{'rouge-1': {'f': 0.47289000009132237, 'p': 0.8367844522968197,'r': 0.3295693779904306}, 'rouge-2': {'f': 0.2968182662120649, 'p': 0.4919858156028369,'r': 0.21251497005988024}, 'rouge-l': {'f': 0.4169029391013153, 'p': 0.7046153846153846,'r': 0.29602739726027394}}]
-----------------------------------------------------------------------------------------------------------------------------------
p135:
Extractive Summary:
the institutional review board of the ehime university graduate school of medicine approved the study protocol (#1505011).each participant completed a selfreported questionnaire that collected data on eating habits.current smoking was defined as positive if a subject reported smoking at least one cigarette per day.current drinking was defined as positive if a subject reported any drinking, regardless of frequency or amount.blood samples were taken in the morning after overnight fasting.” (the 5 response options were very slow, slow, medium, fast, and very fast).self-reported eating speed showed a high level of agreement with eating speed reported by a friend; the percentages of exact and adjunct categories of answers (e.g., very fast and fast were regarded as agreeing) were 46% and 47%, respectively [20].eating until full was defined on the basis of whether or not the respondent answered “yes” to the question, “do you tend to eat too much?” skipping breakfast was defined as missing breakfast at least three times per week: “how often do you eat breakfast?” (the 6 response options were less than once a month, 1–3 times per month, 1–2 times per week, 3–4 times per week, 5–6 times per week, and every day).definition of mucosal healing the mayo endoscopic subscore (mes) contains four categories [21].partial mh and mh were defined as category 0 and 0–1 in this study, respectively.a single endoscopic specialist was responsible for evaluating mes and mh, and was blinded to eating habits, crp, and clinical remission.statistical analysis (1) eating rate was divided into three categories: slow (reference), moderate and quick.(2) eating until full was divided into two categories: no (reference) and yes.(3) skipping breakfast was divided into two categories: no (reference) and yes.estimations of crude odds ratios (ors) and their 95% confidence intervals (cis) for clinical remission, partial mh, and mh in relation to eating habits were performed using logistic regression analysis.age, sex, bmi, current smoking, current drinking, prednisolone use, and anti-tnfα monoclonal antibody use were selected a priori as potential confounding factors.statistical analyses were performed using sas software package version 9.4 (sas institute inc., cary, nc, usa).all probability values for statistical tests were two-tailed, and p < 0.05 was considered statistically significant.results table 1 shows the clinical characteristics of study participants.the percentages of men, current smoking, and current drinking were 59.2%, 7.5%, and 40.8%, respectively.mean age, bmi, clinical remission, partial mh, mh, quick eating, eating until full, and skipping breakfast were 51.1 years, 22.75, 58.8%, 63.2%, 26.1%, 53.1%, 62.2%, and 13.6%, respectively.in the crude analysis, eating rate was not associated with clinical remission or partial mh.after adjustment for sex, age, bmi, current smoking, current drinking, and prednisolone and tnf-α monoclonal antibody use, eating moderately and eating quickly were independently inversely associated with mh; the adjusted ors were 0.38 (95% ci 0.16–0.85) and 0.38 (95% ci 0.17–0.81), respectively (p for trend = 0.033).table 3 shows the association between eating until full and clinical outcome.the percentage of mh in patients who skip breakfast was marginally lower than in those who did not skip breakfast (p = 0.06).this is the first study to report the association between eating habits and clinical outcome in patients with uc.eating quickly was positively associated with gerd in a case–control study of 1518 chinese  subjects [14].second, most of the patients were receiving treatment.fifth, this cohort consisted only of japanese patients with uc.finally, the patients of the present study were not likely representative of patients with uc in japan.
number of words= 565
[{'rouge-1': {'f': 0.49335105580359123, 'p': 0.8094957983193278,'r': 0.354789644012945}, 'rouge-2': {'f': 0.33663030347647716, 'p': 0.529915611814346,'r': 0.2466612641815235}, 'rouge-l': {'f': 0.4767904140146942, 'p': 0.7246762589928057,'r': 0.3552664576802508}}]
-----------------------------------------------------------------------------------------------------------------------------------
p136:
Extractive Summary:
it is a spectrum of diseases comprising two distinct conditions: non-alcoholic fatty [2].a study on the epidemiology and disease burden of non-alcoholic steatohepatitis suggested that nash can affect 3% to 5% of the global population, with minor variations at the country-specific level [13].in thailand, several studies have calculated the incidence and prevalence of nafld.certainly, an understanding of the economic burden of nash in thailand might be useful in helping policy makers with the development of strategies to manage this disease, which is likely to become a significant health issue in the near future.methods description of model a markov model, which was built in microsoft excel, was adapted from a study conducted by chongmelaxme et al. [20] to estimate the health care costs and prevalence of nash with significant fibrosis (fibrosis stage 2 and higher) in thailand.the size of the population of each age group was based on thailand official statistic registration systems, and the age-specific mortality rate (asmr) was based on global health observatory data from the world health organization [21, 22] (additional file 1: appendix 1 and 2).all costs were discounted at the rate of 3%, in accordance with the recommendations of the thai health technology assessment (hta) guidelines [23].the transitional probabilities were mainly based on the study of chongmelaxme et al. [20], which was the previous, model-based, economic evaluation study in thailand.the treatment costs only related to nafld and nash.also, it can be assumed that the costs of management of dc, hcc, lt, and post-lt would be the same in patients with advanced liver disease, regardless of the disease etiologies [15].analysis the outcome measures were the first-year cost, the fifthyear cost and the lifetime cost for each age group.the sensitivity analyses were conducted for all age groups.additionally, about 50% of the nash with significant fibrosis population in 2019 would comprise adults aged under 39 (table 2; additional file 1: appendix 5).in 2019, the gdp of thailand was $543 billion which this burden account for approximately 3% of the 2019 gdp.however, these rates are slightly higher than the global prevalence, which ranges between 3 and 5% [13].however, these contributing factors need to be explored further.moreover, younger people today will face a higher burden of metabolic syndrome as compared to earlier generations [53], this could contribute to higher nafld and nash prevalence in the lower-aged cohort.focusing on each age group, we found that the lifetime cost per case for the younger age group was lower than that for the older group.this means that the economic burden of this disease is likely to have been markedly underestimated.this is a critical issue that must be considered.there is an even lower ratio for the current number of hepatologists in thailand (1:6,000 for nash with significant fibrosis, and 1:130,000 for the general population, of whom about 6% may be diagnosed with nash at any time).these ratios indicate that if a strategy is not developed to prevent nash and its consequences, there will be insufficient numbers of medical professionals to provide adequate care to all patients and to effectively monitor the disease.screening is one possible strategy.it could enable early disease detection and promote early treatment, which would require relatively low funding.for example, while cirrhosis and hcc need highcost treatments, these diseases typically have poor outcomes.our results are consistent with other studies that claimed that the burden of nafld and nash are large and are ever increasing [14, 15, 46, 59].the findings of the present study highlight that the prevention and treatment of nash should be addressed by every lmic in the world.a key strength of this study was that its findings are likely to be valid as most of the input data used for the model were drawn from local sources.nevertheless, the probability of liver transplantation was based on local data [33–35].fifth, regardless of clinical manifestation, entire patients in this model who had been diagnosed nash were assumed to receive standard treatment and applied in the cost calculation.in the real-world practice, those without clinical abnormality might not receive the treatment.lastly, our study did not consider other non-liverrelated events which are known to associated with nash such as cardiovascular diseases, obesity, etc.thus, the total costs will increase if the treatment of those comorbidities are included.this should prompt clinicians and policy makers to pay more attention in developing and implementing the effective strategies for the prevention and management of na
number of words= 723
[{'rouge-1': {'f': 0.4583267349603752, 'p': 0.7469759450171822,'r': 0.3305820105820106}, 'rouge-2': {'f': 0.23519930961508434, 'p': 0.3493103448275862,'r': 0.17728476821192052}, 'rouge-l': {'f': 0.3889200443148653, 'p': 0.5640476190476191,'r': 0.296775956284153}}]
-----------------------------------------------------------------------------------------------------------------------------------
p137:
Extractive Summary:
background nutritional support for critically ill is now recognized as an integral part of patient care [1, 2].many of the conditions associated with admission to the intensive care unit (icu) cause delayed gastric emptying and result in gi dysfunction such as multi trauma, hyperglycemia, burns, mechanical ventilation, cardiac surgery, renal dysfunction, respiratory failure, medications, or the disease process itself.efi, defined as the failure to provide sufficient en to critically ill patients due to delay of gastric emptying with the absence of mechanical blocking, is a common problem in critically ill patients with a stated prevalence of 30–46% and is accompanied by cumulative energy deficit, prolonged icu stays, decreased ventilator-free days, and increased mortality [5, 6].prokinetic agents such as metoclopramide, erythromycin, cisapride, and domeperidone have been used to enhance gastric emptying and are commonly used in the icus [8].the safety profile of available prokinetic agents is a major concern when selecting therapies for efi treatment.moreover, the effects of the drug decrease rapidly with time where after a few days of treatment with metoclopramide, tachyphylaxis occurs such that success of feeding is less than 20% by day 3 of therapy in patients with high grvs [13].itopride hydrochloride, a prokinetic drug that has been reported to enhance gi motility through a dual mode of action; by preventing the effect of dopamine on the d2 receptors of the cholinergic nerves in the post-synaptic region.therefore, we conducted a clinical study to determine the efficacy and safety of itopride in critically ill patients with efi in comparison with metoclopramide.for whatever reason, the physician could discontinue the participation of any patient, including inability to comply with the protocol.if a patient withdrew from the study or was withdrawn from it, this was noted on the case report form along with the reason for withdrawal.patients were assigned to one of the two groups as follows: 1 itopride group these patients received 50 mg itopride (ganaton ® kahira pharmaceuticals co., cairo egypt under license from abbott laboratories) enterally t.i.d.2 metoclopramide group these patients received 10 mg metoclopramide (primperan ® sanofi aventis) intravenously every 6–8 h. the treatment duration was 7 days for both groups.enteral feeding protocol: continuous feeding with 1.5 kcal/ml fresubin® (fresenius kabi, egypt) in the form of 18 gm carbohydrates, 5.8 gm fat, and 5.6 gm protein per 100 ml, was administered through nasogastric tube, starting with a rate of 20 ml/h, the feeding rate was increased gradually till the target energy requirement was reached.study procedures all patients were subjected to the following: patient data collection – baseline characteristics: demographic data of the participants; age, gender, height, weight, body mass index (bmi), organ function as assessed by the sequential organ failure assessment (sofa), severity of illness as assessed by the acute physiology and chronic health evaluation ii (apache ii).—icu admission date and diagnosis.– nutritional risk assessment: modified nutrition risk in critically ill (mnutric) score was measured for each patient at baseline and at the end of the study.– cardiovascular assessment: a 12-lead ecg was done on each patient at the screening visit to exclude qt prolongation, and at the end of the study to detect any effect of itopride or metoclopramide on the qt interval.secondary outcomes 1. determining the adequacy of enteral nutrition & compliance with enteral nutrition orders was registered daily; en volume ratio (vr) % considered as an index of efficacy of nutrient delivery, was calculated as follows: en vr (%) = (administered volume of en/ prescribed volume) × 100. 2.4. occurrence of ades: if adverse events occurred, the time of onset, duration, severity, relationship to itopride or metoclopramide and the requirement of treatment were evaluated.additionally, at day 7 of the study, there were no differences between the itopride and metoclopramide groups regarding apache ii (p = 0.44), sofa (p = 0.65) or mnutric (p = 0.06).mean prescribed feed volume was 1310.66 ml (± 67.089) in itopride group, and 1333.03 ml (± 63.461) in the metoclopramide group, (p = 0.156).mean protein requirement in the itopride group was 90.14 g/d (± 8.357) and 93 g/d (± 6.207) in the metoclopramide group, p = 0.109.in table 2, at day 1, there was no statistically significant difference between the 2 groups regarding grv with mean grv of 359.5 (± 82.6) for the itopride group and 344 (± 99.3) for the metoclopramide group, (p = 0.47).moreover, there was a statistically significant difference between the 2 groups regarding the grv percentage of change between day 1 and day 7 with more decrease in grv in the itopride group 75.7 (± 18.8) than the metoclopramide group 57.3 (± 18.9) (p = 0.001) as shown in fig. 3.secondary outcomes difference in feed volume, energy and protein delivered between itopride and metoclopramide groups.regarding the mean prescribed energy delivered, for the itopride group 1551.43 (± 216.417), it was significantly higher than the metoclopramide group 1390.14 (± 126.098), p = 0.001 and similarly, the percentage of energy ratio was higher in the itopride group 89.158 (± 14.63) than the metoclopramide group 79.7 (± 9.397), p = 0.002.for the mean grams of protein reaching the patients of itopride group 85 (± 8.135), it was significantly higher than the metoclopramide group 78 (± 8.419), p = 0.001.icu los no statistically significant difference was found between both study groups concerning the icu los (p = 0.71) as shown in table 3.adverse events were summarized in fig. 5.itopride was well tolerated and only minimal adverse events were documented; one patient suffered from diarrhea and one patient complained of abdominal pain.for the metoclopramide group, 1 patient suffered from headache and 1 patient suffered from drowsiness.all the reported adverse events from both groups were mild and subsided without interfering with continuation of treatment.on the other hand, two patients of the metoclopramide group recorded qt interval prolongation.neither vomiting nor requirement of post-pyloric feeding tube insertion due to feed intolerance or infectious complications were reported.high grv is a reliable surrogate marker of delayed gastric emptying, and grv can still be useful for the early detection of delayed gastric emptying and the commencement of pharmacological treatment.[3] however, a grv of 250 ml is defined as the threshold for the early detection of feeding intolerance and prompt initiation of therapy.desensitization, downregulation and endocytosis of neurohumoral receptors have been proposed as mechanisms underlying the occurrence of tachyphylaxis [24].this came in accordance with the results of another study that defined successful feeding for achieving ≥ 80% of energy target, [24]and this was different from other studies [6, 37].in the same concern, the current trial showed high feeding success rates that came in accordance with the results of the promote trial [38], and compared with other icu studies, the rates of aspiration, vomiting or regurgitation, and pulmonary infection were low [39, 40].likewise, another post marketing surveillance study reported that the most reported ades were diarrhea, headache, giddiness, constipation, and itching/ rash and most of them were mild and not related to itopride therapy [33].therefore, cns adverse effects and rises in serum levels of prolactin induced by itopride’s antidopaminergic activity are less frequent and less severe than those associated with other dopamine receptor antagonists.likewise, previous study has reported cns adverse events attributed to the use of metoclopramide [43].however, since we have not included any patient with qt abnormality and with concomitant drug ingestion, further studies with itopride in high-risk groups would be needed.the current study showed that usg, which is a simple, non-invasive, widely available, inexpensive valid diagnostic test with a good inter-observer agreement, and which provides real-time structural and functional information regarding most parameters of gastric motility, can be easily used to measure grv, potentially leading to improvement of patient management.it involves no radiation and can be performed at the bed side and the test does not involve radiation exposure and allows repeated measurements when the effects of drugs or therapeutic procedures are to be evaluated.similarly, previous studies have reported the usefulness of two-dimensional us in assessing grv and suggest that ultrasound accurately determines grv [21, 48, 49].additionally, it was demonstrated to be comparable in sensitivity to scintigraphy which is considered the “gold standard” in evaluating ge [50].this study has several strengths.first, the treatments were blinded and randomized.in addition, the short duration of treatment and follow-up period to observe possible drug-related complications; thus, the frequency of complications may have been underestimated.in addition, our study had high severity index and nutrition risk; so, the impact of therapy in patients with lower disease severity and nutrition risk could not be determined.also, future studies using higher grv thresholds are warranted.therefore, in order to optimize interpretation and generalizability, large, multicenter randomized controlled trials must be designed to further validate the safety and efficacy of itopride in clinical settings and to confirm our results as itopride is a promising prokinetic with high potency and safety compared to other prokinetics particularly metoclopramide.our experience with usg suggests that further studies combining clinical assessment of efi with grv measurements may establish usg as a simple objective tool to guide individual prokinetic therapy.conclusion in summary, the findings of this study revealed that in the treatment of efi, itopride, a dopamine d2 antagonist with anti-acetylcholinesterase effects, is superior to metoclopramide.the precise mechanisms by which itopride improves symptoms have yet to be determined, and more clinical studies are required to determine the effectiveness and optimal length of treatment in different populations.
number of words= 1541
[{'rouge-1': {'f': 0.3447588888354179, 'p': 0.8536065573770493,'r': 0.21599877825290165}, 'rouge-2': {'f': 0.19956464580370184, 'p': 0.4055263157894737,'r': 0.1323471882640587}, 'rouge-l': {'f': 0.32847953956907416, 'p': 0.6769364161849711,'r': 0.21685314685314686}}]
-----------------------------------------------------------------------------------------------------------------------------------
p138:
Extractive Summary:
other than serving as a digestive conduit, the gi tract also plays an important role in immunomodulation, hormone control, fluid and electrolyte balance, and physical protection from ingested environmental threats [1–3].along this line, gi dysfunction has been proposed to be the motor of multiple organ dysfunction in critical illness although the pathophysiology involved (e.g., bacterial translocation, altered intestinal tight junctions, cytokine production and interaction with the gut microbiome) remains incompletely understood [3, 7–9].moreover, by this way, we could include a more homogeneous patient population for comparisons of clinical presentation and disease severity between different groups of study subjects.materials and methods study settings and population this retrospective observational study was conducted at a university-affiliated hospital in taiwan.the protocol has been approved by the research ethics committee of the national taiwan university hospital (201902005rind) and written informed consent was waived because of the retrospective and non-interventional design of the study.comorbidities of interest were chronic kidney disease, chronic obstructive pulmonary disease, coronary artery disease, diabetes mellitus, heart failure and malignancy [11, 12].the confusion, urea, respiratory rate, blood pressure and age ≥ 65 (curb-65) score was calculated to assess the severity of pneumonia [13].gi complications were evaluated during the first 3 days of hospitalization and their definitions were described as follows: (a) bowel dilatation: radiologically confirmed bowel dilatation in any bowel segment; (b) diarrhea: loose or liquid stool three or more times per day; (c) gi bleeding: appearance of blood in vomited fluids, nasogastric aspirate or stool; (4) ileus: absence of stool for three or more days [2, 5].in case that patients were rehospitalized after the index admission, the main reasons for rehospitalization were also obtained and categorized as infectious or non-infectious etiologies.the primary outcome was the survival status at hospital discharge.the logistic regression model was built to identify factors independently associated with hospital mortality in the multivariate analysis.the statistical analyses were conducted by using the spss version 15.0 (spss inc., chicago, il) software package.the mean age of the study population was 73.7 ± 15.2 years and 598 (59%) of them were males (table 1).on average, the curb-65 score was 1.6 ± 1.0.the leading comorbidities were diabetes mellitus (28%), malignancy (19%) and chronic obstructive pulmonary disease (12%).diarrhea (5.2%) was the most common complication and approximately 1 out of 9 patients (11%) experienced one or more gi complications.there were 862 (86%) survivors and 139 (14%) nonsurvivors on hospital discharge (table 1).the non-survivors were older (77.2 vs. 73.2 years; p = 0.003) and had a higher curb-65 score (2.1 vs. 1.6; p < 0.001) than the survivors.independent risk factors of mortality included an increase in curb- 65 scores (or 1.952 per point increase; 95% confidence interval [ci] 1.516–2.514), comorbid malignancy (or 1.943; 95% ci 1.209–3.123), development of septic shock (or 25.896; 95% ci 8.970–74.765), and the presence of any gi complication (or 1.753; 95% ci 1.003–3.065).in addition to an association with increased hospital mortality, the presence of gi complications in patients with pneumonia was also associated with a longer hospital stay (table 4).the observation that the majority of readmissions in this study could be ascribed to infectious diseases may partly support this hypothesis.therefore, the findings altogether illustrate the important prognostic role of gi complications in patients with non-gi diseases under a non-critical care setti
number of words= 538
[{'rouge-1': {'f': 0.5170516656983604, 'p': 0.7449116607773851,'r': 0.39593856655290105}, 'rouge-2': {'f': 0.33063133832717967, 'p': 0.4600709219858156,'r': 0.25803418803418804}, 'rouge-l': {'f': 0.4631425444491378, 'p': 0.6366666666666667,'r': 0.36394812680115274}}]
-----------------------------------------------------------------------------------------------------------------------------------
p139:
Extractive Summary:
the prevalence of both h. pylori ulcer and gastrotoxic drug peptic ulcer has declined due to improved eradication therapy for h. pylori infection and reduced use of nsaids, respectively [9–11], while there have been several reports on the recurrence of peptic ulcers after the eradication of h. pylori in nonusers of nsaids [6, 12, 13].thus, h. pylori-negative and/or gastrotoxic drug-negative peptic ulcer (hngnpu) has emerged as a “new” disease entity [14, 15].eosinophilic gastroenteritis (eoge) is an inflammatory disorder characterized by eosinophilic infiltration of the stomach and/or duodenum.in some cases, inflammation of the esophagus, distal intestine, and colon may also be present.eoge occurs without any other known cause of tissue eosinophilia.vomiting, abdominal pain, and growth retardation are the most common symptoms [17], and approximately 40% of patients with eoge have a history of allergic disease including asthma, eczema, or rhinitis [17–19].the study subjects were categorized into 5 groups according to the etiology of the ulcer: 1) h. pylori infection (n = 51); 2) gastrotoxic drugs (n = 18); 3) idiopathic peptic ulcers (n = 144); 4) systemic diseases such as henoch-schönlein purpura (hsp) and crohn’s disease (n = 23); and 5) eoge (n = 19).anastomosis site ulcer after gastrojejunostomy was excluded from the study.patients referred from other hospitals after upper endoscopy and those transferred after ulcer treatment were also excluded.medical records of participants including demographic data, clinical features, allergy history, drug history, endoscopic findings, histopathologic findings, and laboratory tests were reviewed and analyzed retrospectively.medication history was defined as drug intake during the 4- week period before diagnosis.this retrospective study was approved by the institutional review board (irb) of seoul national university bundang hospital (irb no. b-1707/409–105).endoscopic evaluation esophagogastroduodenoscopy with mucosal biopsies was performed in all 255 study participants using a gifxp260 or gif-q260 scope (olympus, tokyo, japan).diagnosis of peptic ulcers was made based on endoscopic findings, which included the location, size, and number of ulcers and ulcer recurrence.colonoscopy with mucosal biopsies was performed additionally in 23 patients, to rule out gastric or duodenal ulcers due to systemic diseases.histopathologic evaluation endoscopic mucosal biopsies were obtained from the esophagus, gastric antrum and body, and duodenum, during the upper gastrointestinal endoscopy, in all participants.results patient characteristics of children with peptic ulcers totally 255 (15%) children out of 1694 patients who had undergone upper endoscopy for investigation of upper gastrointestinal symptoms, during the study period, were identified to have gastric or duodenal ulcers, on endoscopy, among whom one patient with anastomosis site ulcer was excluded from the study.of the 255 pediatric patients with peptic ulcers, 159 (62.4%) and 69 (27.1%) patients had only duodenal ulcers and gastric ulcers, respectively.changes in the diagnosis of causes of peptic ulcers in children during the study period figure 1 reveals the distribution of the etiology of peptic ulcers in children during the study period from 2003 to 2017.comparison of endoscopic and histopathologic findings among the 5 groups according to the etiology of peptic ulcers in children regarding endoscopic findings, gastric ulcers (p = 0.005), duodenal ulcers (p < 0.001), multiple ulcers (p = 0.023), and gastric mucosal nodularity (p < 0.001) differed significantly among the 5 peptic ulcer groups (table 1).regarding histopathologic findings of upper gastrointestinal tracts, tissue eosinophil counts were significantly different in the esophagus, stomach, and duodenum among the 5 ulcer groups (all p < 0.001) (table 1).when comparing the laboratory, endoscopic, and histopathologic findings of the eoge group and the non- eoge ulcer groups, only blood eosinophil counts and tissue eosinophil counts of the esophagus, stomach, and duodenum were significantly higher in the eoge group (p = 0.001 & p < 0.001, respectively) (supplemental digital content 1 & table 3).discussion this is the first study that investigated the etiology of peptic ulcers in pediatric patients and its change over time, especially emphasizing on hngn-pu as an emerging etiology of peptic ulcers and eoge as a significant cause of hngn-pu in children.furthermore, according to laine et al., about 20% of duodenal ulcers recurred 6 months after successful eradication of h. pylori, in a meta-analysis of seven well-designed trials [13].charpignon et al. demonstrated that there were significant differences in the age of onset and the comorbidity of idiopathic peptic ulcer diseases (e.g. hngn-pu), compared with both h. pylori and nsaids/aspirin-associated peptic ulcers [14].however, the prevalence and clinical features of hngn-pu in children has not been evaluated yet.in the present study conducted for 14 years from july 2003 to april 2017, we recruited children with peptic ulcers and categorized them into 5 groups to investigate the differences in clinical features and the laboratory, endoscopic, and histopathologic findings of peptic ulcers in children, according to the etiology.in our study, 19 of 255 (7.5%) patients with peptic ulcers were recently diagnosed with eoge on the basis of clinical features and histopathologic findings of significant tissue eosinophil infiltrations, without any other organic diseases associated with gastrointestinal eosinophilia.this study has a few limitations.in children with hngn-pu, ulcer recurrence, peripheral eosinophilia, and history of atopic dermatitis might provide high levels of clinical suspicion for eoge, requiring thorough histopathologic investigation of tissue eosinophils counts, on the basis of endoscopic biop
number of words= 845
[{'rouge-1': {'f': 0.42270577775532386, 'p': 0.8131906614785993,'r': 0.2855756207674944}, 'rouge-2': {'f': 0.30295178349124213, 'p': 0.5504687500000001,'r': 0.20898305084745764}, 'rouge-l': {'f': 0.4675924661341295, 'p': 0.7366666666666666,'r': 0.34249357326478147}}]
-----------------------------------------------------------------------------------------------------------------------------------
p140:
Extractive Summary:
rapidly ageing populations means that many people now die in advanced age, and the number of deaths in advanced age is projected to increase substantially over the coming decades in most countries [1].firstly, there is evidence that the inverse care law applies at the end of life, meaning that people most in need of care often miss out [2].crucially, this pattern of care is determined in no small part by social determinants such as cultural and ethnic affiliation, gender, and socioeconomic status [2].therefore, understanding cost patterns is an important factor in addressing established inequities in healthcare [3–6].however, there is debate as to the extent to which this pattern of healthcare utilisation results in real benefit to the patient [9], and if it reflects the end of life aspirations of people of advanced age [10].finally, it is essential that governments are prepared for the level of investment required to support current patterns of end of life healthcare utilisation.although a high level of healthcare utilisation is common in the last year of life, this variation has not been sufficiently described in new zealand.in light of population projections estimating a 88% increase in the number of older māori (the indigenous people of nz, with older people being defined as those aged 80+), and a 55% increase in the number of older non-māori in the next 10 years in nz, as well an increase in the number of deaths per year in nz by 30% in the next 15 years [11, 12], this paper aims to establish public hospital (acute) and longterm aged residential care home costs in the 12 months before death in māori and non-māori of advanced age in nz.methods population and data sources this paper draws on data from an existing longitudinal study in new zealand (lilacs nz), as well as data from matching participants national health index (nhi) numbers to the hospitalisation national minimum dataset (nmds) held by the ministry of health (moh).when lilacs nz was initiated in 2010, 937 older new zealanders were enrolled from a population-based sample, including 421 māori aged between 80 and 90, and 516 non-māori all aged 85 [13].in this cohort, the non-māori group was overwhelmingly made up of europeans, with 89% being nz european, and 10% other european [14].all participants were followed up and nhi matching enabled notification of deaths.at the time of this study, 454 individuals in the cohort had died, 213 of whom were māori and 241 non-māori.covariates on study participants were gathered in the first wave of data collection, including gender, age, region, district health board (regional funding body) of residence (dhb, bay of plenty or lakes), and living arrangement (alone, with spouse only, or other).deprivation (10 deciles regrouped into 3 categories) was based on the nzdep, an area level index ascertained by geocoding participants addresses at the time of enrolment [15].health conditions (asthma, cancer, cerebrovascular accident, congestive heart failure, and diabetes) were ascertained from a combination of self-report, physical examination, hospital and primary care medical record review, and blood analyses [16].the nmds contains admission data on acute public hospital admissions and long-term aged residential care admissions (henceforth termed care homes).care homes in new zealand consist of both high-level dependency (similar to nursing home care in the united kingdom and australia and skilled nursing facility care in the united states of america) and low-level dependency (similar to residential homes in the uk, hostels in australia, and assisted living care in the usa).care home costs reported are those relating to high-level dependency (i.e. hospital level aged residential care) only.these wies are calculated based on length of stay and diagnostic-related groups [17], with individual level data available on request from the moh.statistical analysis māori and non-māori are reported separately to examine potential inequities and because care practices differ between cultural groups.we tested for differences between different levels of variables within the same ethnic group.the small number of non-māori men with high costs and long lengths of stay may have skewed the data, largely caused by the few non-māori men residing in care homes for long periods in their last year of life.costs in the last year of life were much higher in comparison to costs for individuals who did not die in the same time period (table 3).in new zealand, ethnic disparities in health care utilisation and outcomes throughout life are well documented [4, 5].we were unable to illustrate a strong trend toward older decedents costing less than their younger counterparts.we suggest that the differences found in mean costs in table 1 may be related to skew in the data, and indeed, the ‘last year cost’ and ‘length of stay’ variables were both highly skewed (partly shown in table 2, see supplementary figures 1 and 2, additional file 1), especially in non-māori men.as different groups may have different needs requiring different types of care and support at a system level, it is important that a holistic approach to patient and family healthcare is considered, while simultaneously respecting ethnic and gender specific care patterns.internationally, some patient surveys suggest that around half of persons with a serious chronic illness would prefer to die at home rather than in hospital [45].other literature highlights more preventative strategies, such as the need to implement and evaluate interventions that are known to reduce hospitalisations [7, 46].the primary strengths of our study are that we enrolled a relatively large number of people of advanced age using a population-based strategy, had an acceptable response rate at the studies outset, and were able to access most participant’s administrative health records.as mentioned however, inpatient costs generally dominate expenditure at the end of life, so it is likely we captured the majority (or much more) of healthcare expenditure among our study population.furthermore, it must be acknowledged that there is considerable variability in hospitalisation cost data and that results need to be interpreted with caution.a further analysis of this administrative data would allow for a greater sample size and stronger regional lens as care access, utilisation, and underlying health structures may differ across the country.there were ethnic and gender disparities highlighted.this raises questions as to whether money at the end of life is being spent appropriately, and how it could potentially be more equitably targeted to meet the diverse needs of older people and their famili
number of words= 1041
[{'rouge-1': {'f': 0.4154977817768056, 'p': 0.9158498023715416,'r': 0.26870009285051066}, 'rouge-2': {'f': 0.3231290680870884, 'p': 0.677142857142857,'r': 0.2121933085501859}, 'rouge-l': {'f': 0.41384689504823613, 'p': 0.8182517482517482,'r': 0.276963249516441}}]
-----------------------------------------------------------------------------------------------------------------------------------
p141:
Extractive Summary:
one of the problems associated with a longer lifetime is falling.falling in elderly individuals is affected by diverse factors, some of which can be balanced, while some cannot.some of the benefits of using smart computers for improving balance entail the possibility of easy application at home, lower costs compared to other therapeutic methods, and better acceptance of the intervention by individuals due to being interesting [25].xbox kinect is a smart computer that can simulate balance exercises.moreover, this smart computer allows people to move freely and have diverse positions [27].for instance, bieryla (2016) conducted a pilot study on healthy elderly individuals from living communities and an intervention group trained with the kinect for xbox 360 for 3 weeks.another systematic review also showed the positive effects of kinect systems on rehabilitation for elderly people suffering from stroke and falling risk [33].elderly people living in nursing homes, on the other hand, may suffer from balance disorders and fear of falling for reasons other than stroke.according to neil et al., there are significant differences in the physical interaction and therapeutic emphasis of diverse games [35].nurses have an important role in ensuring the safety of their clients and face many challenges in this regard [35].given that lack of balances and falls are threatening factors for the safety of the elderly, it is necessary for nurses to investigate the impact of various interventions on reducing these threats.the exclusion criteria included a history of acute and chronic physical cognitive, and mental diseases that might hinder exercising, participation in other exercises similar to the intervention, having problems leading to problematic exercises, unmodified hearing and seeing problems, and balance disorders due to the problems of the vestibular system and cerebellum diagnosed by a doctor.ethical consideration the present study was approved by theethics committee of shiraz university of medical sciences (ir.sums.- rec.1398.573).it was registered in iranian registry of clinical trials (irct) with registration number of irct20190727044347n1 on 2019-08-17.all necessary permissions for conducting the research were obtained from the relevant administrators and all methods were performed in accordance with the relevant guidelines and regulations.sample size according to the research by park et al. (2017), using the equation for the difference of bbs means (mean of 50 ± 6.27 for the intervention group and 44.7 ± 7.47 for the control group), and considering type 1 error of 0.05 and power of 80%, the sample size was calculated as 27 for each group.randomization the participants were divided into an intervention group (i.e., exercise by xbox kinect) and a control group (i.e., routine programs of the nursing homes) through random allocation using double randomized permutation block with foursome blocks.the random list was generated by a statistician using the “random allocation” software, and 15 blocks were selected for forming the two study groups.the list was then provided to the researchers, and one of them performed the randomization accordingly.for simulated balance exercises, xbox kinect was applied, which is a game console simulating balance exercise in the game environment and allows a person to move freely and have diverse positions.in this study, suitable games for improving the balance of elderly people were selected in a meeting with the professors of the rehabilitation department.all selected exercises required the application of upper and lower organs while standing (table 1).exercises were stopped in case of fatigue, pain, or dyspnea.in the control group, the elderly individuals received the routine programs of nursing homes, including jogging in the nursing home, table tennis, and some artistic activities, for 6 weeks.outcome measures and follow-up data collection tools in the present study included a demographic questionnaire, fes, berg balance scale (bbs), and the tug test.the validity and reliability of the english and persian versions of this tool have been evaluated and confirmed in the previous studies.the psychometric characteristics of this instrument were investigated and confirmed in the previous studies in iran [38].the validity and reliability of this tool have been assessed and confirmed in various studies [39, 40].the tug test was conducted for these individuals, 65 of whom had a test time of over 14 s and could be enrolled into the study.afterwards, these people were evaluated in terms of other inclusion criteria and finally, 60 (16 females and 44 males) were selected and divided into a control and an intervention group through random allocation (fig. 1).the demographic characteristics of the participants have been presented in table 2. accordingly, no significant difference was found between the two groups in terms of age, gender, marital status, education level, occupation, duration of stay at the nursing home, and number of falls before the intervention (p > 0.05).however, the mean score of this factor decreased significantly following the exercises (p < 0.001), but did not change in the control group (p = 0.217).however, ki-hun-cho et al. demonstrated that although video exercises enhanced dynamic balance, no significant impact was noted on the static balance of the individuals under investigation [42].furthermore, receiving visual feedbacks in simulated exercises could lead to the elevation of the participants’ awareness of their balance control and enhancement of their selfefficacy [44].in the present study, the tug test that consisted of three stages and was another index of balance was over 14 s in both groups at the beginning, demonstrating balance disorder in both groups.these results were in line with those of the research by htut et al. [44] concerning the effect of virtual exercises on the physical, cognitive, and functional status of elderly people.yang et al. [45] also stated that vr exercises had impacts similar to those of real exercises, and might enhance balance.however, in the kwok et al. study, these exercises did not diminish the fear of falling in elderly individuals in 12 weeks, but reduced the fear after 24 weeks [47].the lack of sports activities in these people might cause impaired balance and augmented fear of falling, leading to a higher rate of falling [30].the impact of video exercises, as an interesting tool [14, 50], could provide a guide for the treatment team for the enhancement of these variables.as a result, the most is made of small spaces in institutes, which do not have sufficient space for other exercises or when elderly individuals are not interested in other sports.strong points and limitations the tendency of elderly individuals for participating in the exercises in the current study was among the strong points of the research.nevertheless, a limitation for this investigation was a disturbance in the sensor of the xbox kinect in some situations, which caused the researcher to be mistaken by a participant in some situations and made him keep the distance to solve this problem.the difficulty of exercises for some participants at the beginning was another study limitation.additionally, kinect systems are not suitable for patients with severe disabilities [29].this might be taken into consideration in future studies.of course, games were not used as a rehabilitation strategy in the present study.
number of words= 1137
[{'rouge-1': {'f': 0.37163526059659674, 'p': 0.852101167315175,'r': 0.2376396997497915}, 'rouge-2': {'f': 0.21591908505312427, 'p': 0.4215625,'r': 0.14512520868113524}, 'rouge-l': {'f': 0.3365052464611213, 'p': 0.6871875000000001,'r': 0.2228046421663443}}]
-----------------------------------------------------------------------------------------------------------------------------------
p142:
Extractive Summary:
frailty is a medical condition of increased vulnerability and poor resolution of homeostasis after a stressor event as a consequence of cumulative decline in many physiological systems during a lifetime [1].around 40 % of the hospitalized older patients are frail which is associated with poor health outcomes, such as functional decline, hospital re-admission, institutionalization, and mortality [2, 3].identifying (pre) frail older adults, and those at risk for progression of frailty is important.some older adults may benefit from interventions targeted at prevention of frailty progression to lower the risk of poor health outcomes like functional decline [4, 5].reliable and valid assessment of frailty and how to measure relevant changes in frailty over time is therefore crucial.several frailty instruments exist for the purpose of diagnosing, risk stratification, and evaluating frailty over time [6].comprehensive geriatric assessment is currently the gold standard for diagnosing the frailty status in clinical practice [1], but the cumulative deficits model or frailty index (fi) and the frailty phenotype (fp) are the most widely used instruments used to establish frailty status in research [7, 8].construct validity and predictive validity of negative health outcomes of the fi and fp have been extensively evaluated and are proven to be satisfactory in both community-dwelling and hospitalized older adults [9–11].reproducibility and responsiveness of change scores of frailty instruments are poorly studied especially after hospitalization and it is still unclear which frailty instrument is most appropriate for measuring change scores over time or the effectiveness of interventions [9, 11, 12].twelve months’ follow-up measurements continued to the end of april 2019.inclusion criteria were age ≥ 70 years and an expected hospital stay of at least two days.exclusion criteria were no understanding of the dutch language, any (temporary) cognitive condition that influenced decision making capacity, and no written informed consent.the research ethics committee of the university medical center groningen ruled that no formal ethics approval was required (file number: 201,600,268).all participants provided written informed consent before participation.data collection baseline assessment took place within four days after admission.telephonic assessments were performed at three and twelve months’ post-discharge, in which baseline questions were repeated and two anchor questions were added [13].data were collected by trained research staff.questionnaires frailty instruments the fi score was calculated using 34 deficits associated with health status [14].the fp was assessed by five selfreporting criteria including strength, walking ability, weight loss, physical activity, and exhaustion [15, 16].a detailed description of the included items of the fi and fp are presented in additional file 1, tables a1 and a2.patient-reported anchor questions anchor questions were used as an external criterion for measuring responsiveness [17].two different anchor questions were used: (1) ‘in general, how is your health state now, compared to three months/twelve months ago before hospitalization?’ (2) ‘in general how is your daily functioning now, compared to three months/twelve months ago before hospitalization?’ response options were a five point likert scale.improved was scored if a patient answered ‘slightly better’ or ‘much better’.thresholds for floor and ceiling effects were if ≥ 15 % of the patients achieved the lowest or highest possible score, respectively [19].reproducibility test-retest reproducibility was assessed among patients who reported to be unchanged according to the anchor question three months post discharge.srms were separately calculated for improved, unchanged, and deteriorated patients according to the anchor question for both the fi and fp using continuous scores.cut-off values were: ≤0.2 small, > 0.2 and ≤ 0.5 doubtful, > 0.5 and ≤ 0.8 good, > 0.8 very good internal responsiveness for the improved and deteriorated patients; <0.50 good, ≥ 0.50 small for the unchanged patients [22].these patients were older and had higher frailty and comorbidity index scores at baseline compared to patients with complete data for all assessments (n = 118) and patients lost to follow-up after twelve months (n = 56) (additional file 2, table s1).only the fi showed sufficient responsiveness (> 0.70) to detect deterioration in frailty over three months (fig. 1).responsive frailty measures can therefore act as an important intermediate outcome for the evaluation of interventions aiming at preventing adverse health outcomes in older patients.this may have led to an underestimation of the ability to detect improvements, but it may have overestimated the results based on the unchanged patients.our participants could have been adapted to their changed health state after hospitalization [30] which may have resulted in a larger change score that is regarded meaningful to patients themselves.however, patients who perceived no change in health and functional status after three and twelve months post discharge, also had the smallest change scores on the frailty instruments.another explanation may be that the change in frailty status according to a clinician which was investigated by theou and colleagues [29] is different from the patients perception of change in frailty status as investigated in the current study.more than twice the recommended number of patients for evaluating psychometric properties were included [19].on the one hand, the time interval between the test and the retest, three months, is a long period to assume that the patients have remained stable.however, if some unstable patients were inadvertently considered stable in the current study, the found reproducibility scores are expected to be an underestimation of the “real” reliability.
number of words= 855
[{'rouge-1': {'f': 0.3687556468483764, 'p': 0.5919941348973607,'r': 0.2677777777777778}, 'rouge-2': {'f': 0.17923180592991914, 'p': 0.25235294117647056,'r': 0.13896551724137932}, 'rouge-l': {'f': 0.32150817206912136, 'p': 0.4944186046511628,'r': 0.23820276497695853}}]
-----------------------------------------------------------------------------------------------------------------------------------
p143:
Extractive Summary:
dementia places a tremendous burden on society worldwide.the total number of people with dementia in the world was estimated to be 35.6 million in 2010, and this number is projected to increase to 115.4 million in 2050 [1].the total cost of dementia is also enormous, estimated at us$ 604 billion in 2010 [1].under these circumstances, the prevention of dementia and dementia-related disorders is of high priority.the role of sleep in cognitive function and dementia has drawn attention, although evidence is still insufficient [2].according to recent reviews and metaanalyses, sleep duration and sleep disturbance are determinants of cognitive decline and dementia [3–7].moreover, daytime napping is reportedly associated with cognitive function in older adults [8–11].however, findings from previous studies have been somewhat inconsistent; some reported possible adverse effects of napping, especially long napping, on cognitive function [9, 11], whereas others reported possible beneficial effects of napping, especially short napping [8, 10].furthermore, except for one longitudinal study [8], only cross-sectional studies [9–11] have been conducted.we previously conducted an epidemiologic study to investigate associations between cognitive impairment and lifestyle factors, including sleep characteristics and daytime napping, in community-dwelling older adults [12].the present study aimed to clarify longitudinal associations between cognitive decline and sleep characteristics, in particular whether daytime napping is beneficial or harmful for cognitive function, based on 5-year follow-up data from participants of the study mentioned above.methods design and participants this study was a 5-year follow-up cohort study.participants of the baseline study were included in the study, and those with cognitive impairment diagnosed by the revised hasegawa’s dementia scale (hds-r) at baseline were excluded.participants at baseline were communitydwelling older adults living in the following three areas of ojiya city, niigata, which were set by the city government as model areas: heiseicho (an urban area), matto (a rural, farming area), and central katakai (an urban area) [12].among all 592 residents aged ≥65 years who were not receiving long-term care insurance services and who were invited to participate in the study, 535 (90.4%) underwent the baseline examination.a high participation rate (90.4%) was obtained due to efforts of public health nurses in charge of each area.of these 535 residents, 509 (95.1%) who were considered cognitively normal were invited to participate in the present 5-year follow-up study, and 371 (72.9%) underwent the followup examination.we also included 18 individuals who had normal cognitive function at baseline and did not participate in the follow-up examination, but were diagnosed with dementia at medical facilities during the follow-up period, because these individuals met our diagnostic criteria of cognitive decline described below.the final study cohort thus comprised 389 individuals.figure 1 shows the flow of participant enrollment.informed consent was obtained from all participants who underwent the follow-up examination.the consent was verbal because, according to the ethical guidelines for medical and health research involving human subjects in japan [13], investigators are not required to obtain informed consent in writing for human studies which are not invasive or do not involve interventions.the protocol of this study was approved by the ethics committee of niigata university.second, this study had a high participation rate at baseline (90.4%) and an acceptable follow-up rate (72.9%).finally, information regarding participant lifestyle was obtained and confirmed through interviews during home visits by experienced nurses.this study also has some limitations.first, we obtained sleep-related information through interviews by trained nurses, but the information was based on selfreport.therefore, there is a possibility of misclassification bias, which might have led to an underestimation of associations between predictors and outcomes.second, we did not collect information on naptime, although the timing of naps is an important factor related to cognitive function [24].we did not systematically obtain information on the number of naps either.these two aspects, as well as nap duration, could influence sleep quality at night, which in turn could affect cognitive function.finally, we did not evaluate conditions which could affect sleep, such as depression and sleep apnea.conclusion short daytime napping (< 30 min) reduces the risk of cognitive decline over 5 years for community-dwelling older adults.further studies will be needed to determine if short naps decrease the risk of clinically-diagnosed dement
number of words= 674
[{'rouge-1': {'f': 0.40021450686061627, 'p': 0.6939669421487604,'r': 0.2811888111888112}, 'rouge-2': {'f': 0.22064331690089437, 'p': 0.34385892116182576,'r': 0.16243697478991598}, 'rouge-l': {'f': 0.38185838378520665, 'p': 0.5635897435897437,'r': 0.28875}}]
-----------------------------------------------------------------------------------------------------------------------------------
p144:
Extractive Summary:
because patients are prone to have severe wound infection, defective skin necrosis and long-term wound healing after open fracture, it is very easy for patients to develop infectious bone nonunion in clinical practice, and open tibial fracture is one of the most common cause of infectious bone nonunion [2, 3].meanwhile, open tibial fracture is often accompanied by delayed and poor healing, and infectious bone nonunion is a more difficult problem in clinical treatment [5].clinical doctors urgently need to find an effective treatment with obvious efficacy in older patients with infectious bone nonunion after open tibial fracture [8].operative surgery is the main clinical treatment of infectious bone nonunion.its principle is to promote bone healing by clearing infective lesions and killing necrotic tissues.however, traditional treatment is to resect injured tissues, prevent active infection and repair bone defect by free bone transplantation [9].however, although several studies showed high successful rates with surgical treatment of infectious bone nonunion after open tibial fracture, the application of bone transport technology is still in the exploratory stage in the treatment of older patients with infectious bone nonunion after open tibial fracture [12–14].general speaking, young patients are more likely to choose operative reconstruction, while older patients are more likely to choose operative amputation.clinical efficacy of bone transport technology needs to be further studied in older patients with infectious bone nonunion after open tibial fracture.therefore, the current study was designed for the first time to analyze clinical efficacy of bone transport technology in chinese older patients with infectious bone nonunion after open tibial fracture.methods study participants the current study retrospectively analyzed 220 older patients with infectious bone nonunion after open tibial fracture.all patients received operative treatment in orthopedics department, harbin no. 5 hospital, between july 2012 and november 2019.surgical procedures patients avoided all surgical treatment during the acute attack period and received surgical treatment after infection was controlled with antibiotics.bone transport technology was conducted in patients (group a) based on the following surgical procedures: 1) to resect infected, necrotic and scar tissues, protect significant nervous and vascular structures, peel off hardened and necrotic bones, and debride wound surface and necrotic space; 2) to fix ring external fixator in the metaphyseal of proximal and distal tibia, parallel to the upper and lower tibia rings and across osteotomy plane; 3) osteotomy was conducted at the proximal tibia with its plane horizontal with tibial nodule surface.after soft tissue was adequately covered (3 months later), these patients were treated with membrane induction technique with antibiotic bone cement.patients without soft tissue defect were directly treated with membrane induction technique with antibiotic bone cement: 1) to resect infected, necrotic, inflammatory granulation and fibrous scar tissues, protect significant nervous and vascular structures, peel off hardened and necrotic bones, and debride wound surface and necrotic space; 2) to fill necrotic space using antibiotic bone cement with appropriate size and additional vancomycin; 3) to fix external fixator and make fracture stable; 4) bone cement was removed and bone graft was implanted after membrane induction was formed (6 to 8 weeks later); and 5) external fixator was removed after bone healing.these experts were from orthopedics department, anesthesiology department, geriatric department, endocrinology department and cardiology department.they were not masked to surgical procedures.results there were 164 male patients and 56 female patients, with an age range of 65 to 71 years and an average age of 67 ± 1.3 years.as shown in table 2, linear and positional alignment (70.9 vs. 57.3), american knee society knee function score (167.7 ± 14.9 vs. 123.8 ± 15.7), baird-jackson ankle function score (89.9 ± 3.5 vs. 78.4 ± 4.9), bone healing index (43.0 ± 2.0 vs. 44.3 ± 3.0) and clinical recovery (8.2 vs. 4.5) of patients in the group a were significantly better than those in the group b (p < 0.05 for all).infectious bone nonunion caused by open tibial fracture has significantly affected physical and psychological health of patients, and obviously increased the burden of family and society [17].the treatment of infectious bone nonunion is especially challenging in older patients with open tibial fracture.in the early stage after surgical operation, the affected limb can not bear certain weight, thus aggravating its osteoporosis in older patients.general speaking, young patients are more likely to choose operative reconstruction, while older patients are more likely to choose operative amputation.however, amputation significantly reduces physical and psychological well-being of older patients, and successful operative reconstruction could be a better alternative option [7].clinical efficacy of bone transport technology needs to be further studied in older patients with infectious bone nonunion after open tibial fracture.the current study demonstrated that bone transport technique not only takes less time, but also promote patients’ recovery than membrane induction technique with antibiotic bone cement.complete resection and debridement of necrotic tissue, long course of antimicrobial and supportive therapy and adequate reconstruction of bone and soft tissues, are of immense significance to effective achievement of bone transport technique in treating infectious bone nonunion after open tibial fracture [21, 22].bone transport direction can be appropriately adjusted based on bone healing, and older patients should be encouraged to bear proper weight after operation.conclusion the current study demonstrated that bone transport technology achieved better knee and ankle joint function recovery and superior bone healing and clinical efficacy than membrane induction technique with antibiotic bone cement, suggesting that bone transport technique is worthy of extensive promotion to improve clinical condition of older patients with infectious bone nonunion after open tibial fractu
number of words= 900
[{'rouge-1': {'f': 0.4825914187213076, 'p': 0.8420364741641337,'r': 0.3382154171066526}, 'rouge-2': {'f': 0.4138410507777469, 'p': 0.7102439024390244,'r': 0.29198731501057085}, 'rouge-l': {'f': 0.5162336517855355, 'p': 0.7972727272727274,'r': 0.3816883116883117}}]
-----------------------------------------------------------------------------------------------------------------------------------
p145:
Extractive Summary:
family carers are the main source of support for older patients living at home, making them germany’s largest “nursing service”the term spouse in this paper thus encompasses husbands and wives as well as unmarried partners living together in a long-term partnership.various studies have examined the burden of caregivers in terms of disease, age, gender, relationship, housing, work, social support and re-hospitalization [5, 7, 9–11].those with negative experiences reported a perceived lack of choice in the matter of caregiving [18].to date, these coping dimensions by hessel et al. [23] have not been applied to the context of family caregivers, specifically spouses.caregivers are confronted with individual situational demands and use different available coping strategies.even less is known about the relationship between burden and coping by spousal caregivers [17], especially for those of geriatric patients without dementia.previous typological studies with caring spouses described the experiences across different groups of neurodegenerative diseases, particularly dementia and parkinson disease [26–29].aim the aim of this study was to explore the burden and coping strategies of caregiving spouses of geriatric patients without dementia and a hospitalization within the last year.sample caregiving relatives were recruited from selected participants in the transsectoral intervention program for improvement of geriatric care in regensburg study (tiger) (clinicaltrials.gov id: nct03513159).exclusion criteria were a palliative status, and a planned hospital readmission within the next 4 weeks [32].a total of 59 relatives participated in the tiger study.for the present study, we selected those who met the following inclusion criteria: (a) being a spouse or partner in a long-term relationship and sharing a common household (b) being the main caregiver and (c) the care recipients had been classified as a minimum degree two regarding long-term care need (‘pflegegrad’).data collection and data analysis figure 2 illustrates the evaluation process from the collection of quantitative and qualitative data, to single case analysis, and finally the creation of a caregiver typology.quantitative data caregiver data were collected using demographic information and a validated self-questionnaire from the tiger study.total scores range from 0 to 88, with higher scores indicating a higher perceived burden [8]. expressed emotions (ee): the family questionnaire (fq) assessed two scores: criticism (cc) and emotional over-involvement (eoi).the interviews lasted 42–112 min, were audio-recorded, transcribed verbatim and pseudonymized [38].the interviews were analysed following the structured content analysis method by mayring [39].both quantitative and qualitative data were summarized in a matrix to obtain an overview of the nine individual cases.the nine individual cases were then clustered into groups according to the predefined attributes of the zbi and the dimensions of the befo.results description of the sample the final sample (table 1) consisted of nine spousal caregivers, four women and five men, with a mean age of 78.9 years.all couples lived in an apartment or house without any other supporting family members in the close vicinity.the coping strategies for most of the spouses could be attributed to all three dimensions of the befo, one participant from each of one or two dimensions.the subjectively experienced stress measured by the psq showed a broad range (score = .12–.92; mean = .53).furthermore, two participants receiving high cc and high eoi scores had expressed the suspicion that their spouses had dementia.type a can be characterized by the motivation to care due to feelings of affection.type c can be characterized by the motivation to care due to feelings of obligation.j were assigned to this type.for the last 2 years, mrs.when his wife “was not there, it was gloomy.then i look where she always sits or lies, […], she is not here” (mr.a, para.and then you see that everything turned out well.a, para.94).mr.his father had also cared his wife: “but he never complained.5-6 years at least.yes, yes.mother needed a lot of care” (mr.a, para.98).mr.a considered supporting his wife “a matter of course and we have said yes, in good times and bad times” (mr.a, para.38).a, para.19).during the interview, mr.a repeatedly emphasised how important being together is to him.a, para.and i am glad that i have her.a, para.74).mr.that makes you not want to look so far ahead.that comes by itself” (mr.a, para.46).in mr.a, para.162).they lived in a rented, non-barrier-free apartment.mr.mrs.because after that he is done.and me too sometimes” (mrs.e, para.29).recently she purchased a medical alert system: “i always have my cell phone with me, but when i know he has the medical alert button i just feel safer” (mrs.e, para.25).e, para.17).e, para.53).e, para.95).stress” (mrs.e, para.37).“today it is so and tomorrow it can be […].e, para.33).mrs.e, para.45).e, para.59).“such a situation comes and then it is over again” (mrs.e, para. 63).e, para.95).another active way of dealing with stress was “chasing the problems away by walking.you then see things a little differently” (mrs.e, para.71).e, para.67).[…] and that we are there for each other.that’s why we got married” (mrs.e, para.39).mrs.e, para.45).e, para.47).in mrs.and mrs.his last hospitalization was for clarification of syncope.mrs.“j, para.43).j, para.29).j, para.37).j, para.71).what do i have to take with me?what do i have to think about?j, para. 65).the lack of time was a key issue for mrs.j. mrs.j, para.”126).additionally, the needs and the well-being of the partner to be cared for were the center of attention.the positive view of the relationship was based on the shared past.caregivers fitting this type tried to increase their self-confidence especially through coping by ‘seeking attention and care’.similarities can be found in the “case manager” type by davis et al., which described spouses who managed the situation as a task to be done and organised it primarily on their own, but without involving their partner [27].with type c,however, withdrawal was more to focus on oneself, trying to protect oneself from being overburdened.in regular increments, the positive aspects of care were less frequently mentioned moving from type a to type c. furthermore, no association between type and attributes of the care recipient could be found.for example, there were no couples living with another relative in the same household or in the immediate neighbourhood in the current sample.larger samples in further studies are necessary for sophisticating this typology.the analysis of qualitative data is open to various interpretations.an attempt was made to limit this weakness through a structured approach and a high level of transparency to data collection, analysis and interpretation.type b should be empowered to accept support, especially in the organisation and planning of care.
number of words= 1037
[{'rouge-1': {'f': 0.3893770830576924, 'p': 0.7861290322580645,'r': 0.2587755102040816}, 'rouge-2': {'f': 0.2141560686504507, 'p': 0.3742071197411003,'r': 0.15000000000000002}, 'rouge-l': {'f': 0.3560538353516956, 'p': 0.6389655172413793,'r': 0.2467857142857143}}]
-----------------------------------------------------------------------------------------------------------------------------------
p146:
Extractive Summary:
the prevalence of inadequate vitamin status was greater for those who were non-users of vitamin supplements compared to users.the group difference was significant for vitamin b12; among non-users 29/47 (61.7%) had b12 ≤221 pmol/l compared to 2/28 (7.1%) among users, p < 0.0001 (see table 4).a multiple linear regression model showed creatinine (p < 0.05) and vitamin b12 (p < 0.0001) as significant predictors of the criterion homocysteine.among the other nonsignificant predictors were creatinine clearance, vitamin b6, erythrocyte folate, and age (see table 5).the amount of variance of the criterion explained by the combined predictors was 41.2%.discussion the purpose of this study was to examine the frequency of b vitamin deficiencies in a long-term care population in ontario.secondary objectives were to examine the relationship, if any, between specific markers of vitamin adequacy and: 1) the use of vitamin supplements, 2) the use of drugs that alter gastric acid secretion, 3) cognitive function as measured by performance on the standardized mini-mental state examination (smmse).in this study, the prevalence of folate, vitamin b6 and niacin deficiency based on established reference values were 1.3%, 5.3% and 26.7%, respectively.among 177 institutionalized elderly (65–89 years) residing in mexico, ortega et al [23] found 6.6% to have erythrocyte folate <360 nmol/l.these authors also found a positive correlation between folate intake and erythrocyte folate (r = 0.28, p < 0.05).the fortification of many foods with folic acid in north america may explain the more optimal folate status in our subjects.in addition to dietary intake differences, geographical differences related to the prevalence of atrophic gastritis could also be considered.atrophic gastritis creates a low acid environment, which is conducive to bacterial colonization and subsequent bacterial synthesis of absorbable folic acid [25].the prevalence of vitamin b6 ≤ 30 nmol/l at 5.3% was low when compared to joosten et al [26] who reported 51% of 286 hospitalized patients (61–97 years) living in belgium, germany or the netherlands, to have vitamin b6 <28.7 nmol/l.various factors could be considered to explain the difference in prevalence between the two studies including differences in sample size and variable consumption of nutrient rich foods.with respect to the latter, residents of ontario may be better nourished because of habitual nutrient-rich food intake before and during hospitalization.niacin status has not been routinely examined in populations of younger or older adults.thus there are few reports with which to compare our data.we report here a relatively high prevalence of low niacin status compared to the vitamins b6 and folate.the protective effect of niacin nutriture in carcinogenesis [27] prompted researchers towards developing a biomarker for niacin status.with the finding of nad content in erythrocytes as a sensitive marker for niacin status in humans [28], niacin number (nad/nadp*100) became a convenient way to evaluate the status of this micronutrient.a mean niacin number of 175 with a 95% confidence interval of 127 to 223 were found among free-living healthy adults and metabolic ward patients [24].fu et al [28] suggested a niacin number below 100 (or ratio of nad/nadp of 1.0) may identify subjects at risk of niacin deficiency.additionally, analysis of niacin status in different populations revealed 15–20% are niacin deficient [29,30].our study found 26.7% at risk of niacin deficiency; a prevalence worthy of greater attention.vitamin b12 deficiency is estimated to affect 10% to 15% of people over the age of 60 [12].serum cobalamin is most commonly used to diagnose deficiency although some have argued it has poor diagnostic efficiency [31].pennypacker et al [18] argued that serum vitamin b12 was insensitive for screening since similar numbers of geriatric outpatients with low (<148 pmol/l) or low normal (148– 221 pmol/l) serum cobalamin had elevated metabolites (>21.3 μmol/l hcy and >376 nmol/l mma), 62% and 56% respectively, which fell with cobalamin treatment.using identical low and low normal cobalamin references ranges, yao et al [32] reported elevated metabolites (>16 μmol/l hcy and >270 nmol/l mma) in 80% and 33% of geriatric outpatients, respectively.for individuals 65 and older with serum cobalamin below 221 pmol/l, the authors recommended analysis of hcy and mma, especially in those patients with unexplained hematological and neuropsychiatric disorders [32].using serum b12 as an estimate of b12 vitamin status, we found that fewer than 7% of the hospitalized patients had levels that fell below 148 pmol/l while 34.7% had levels between 148 and 221 pmol/l.hcy elevations (>13.3 μmol/l) within low (<148 pmol/l) and low normal (148–221 pmol/l) cobalamin ranges were 60% and 46.2%, respectively.homocysteine and methylmalonic acid (mma) – both products of reactions requiring cobalamin – carry greater diagnostic utility [31], although the cost of associated assays may hamper their widespread use [31].among older people, increased medication use and gastrointestinal changes are factors that affect micronutrient absorption.the contributing factors to these gastrointestinal changes are pernicious anemia and helicobacter pylori infection, associated with type a and type b atrophic gastritis, respectively [12].atrophic gastritis, bacterial overgrowth and medication use (proton pump inhibitors or h2-blockers) are possible explanations.however, the mean b12 (304.6 and 300.5 pmol/l) and hcy (15.2 and 12.8 μmol/l) levels did not significantly vary between users and non-users of drug therapy, respectively.it is thus unlikely these medications would have induced low cobalamin and elevated homocysteine states.two factors should however be acknowledged in evaluating the potential effect of gastric acid lowering medications on vitamin b12 absorption: 1) type of drug (ppi or h2-blocker) and 2) duration of medication use.in a study comparing omeprazole (a ppi) with h2-blocker therapy in zollinger-ellison syndrome patients [33], those treated with omeprazole (n = 111; mean duration of 4.5 years) had significantly lower serum b12 levels than those treated with h2-blockers (n = 20; mean duration of 10 years).omeprazole treatment reduced acid secretion to significantly lower levels than h2-blockers.sustained hypochlorhydria or achlorhydria,primarily occurring in those treated with omeprazole, was the only factor associated with reduced serum b12 levels.to explain this difference researchers have noted that ppis cause prolonged, profound hypochlorhydria [34] whereas the effect of h2 blockers is short lived – a transient hypochlorhydria [35].among users of drug therapy (n = 22) in our study population, the majority used h2-blockers (16/22 or 73%) versus ppis (6/22 or 27%).duration of medication use was not considered in our study.studies have shown that prolonged treatment with ppis (~3.5 years) [36] or h2-blockers [37] is needed to develop low cobalamin levels.the potential for acute medication use and the preferred administration of h2-blockers over ppis may thus be contributing to the lack of effect of these medications on vitamin b12 and hcy levels.alternatively, it is possible that the small number of subjects, particularly in the drug-user category, may have obscured differences between the two groups in our study.in future work it would be valuable to have a larger number of drug using subjects for comparison.joosten et al [25] estimated 51% of 286 hospitalized elderly to have hcy elevated >13.9 μmol/l.the only exclusion criteria for this group of subjects were the presence of life-threatening disease.otherwise, subjects with common geriatric diseases (vascular disease, dementia, diabetes mellitus, osteoporosis, osteoarthritis) were included.among the factors that significantly correlated with homocysteine were vitamin b12 (r = -0.27, p = 0.0001), serum folate (r = -0.38, p = 0.0001), and creatinine clearance (r = -0.44, p < 0.001) [24].within our sample population, a 41.3% prevalence of hcy >13.3 μmol/l was found, the only significant predictors being vitamin b12 (p < 0.0001) and creatinine (p < 0.05).if a relationship exists between renal insufficiency and hcy, it would be more meaningful and reliable to use creatinine clearance over creatinine as a marker of kidney function in older people.this is because body mass declines with age and creatinine clearance is determined irrespective of muscle mass.creatinine clearance was not a significant predictor of hcy in our study, implying a lack of effect of renal function on hcy levels.the significant relationship we found between creatinine and hcy is in line with what others have observed [38-40].the dependence of hcy on vitamin b12, vitamin b6 and folate as coenzymes in its metabolism explains the significance of b vitamins in cases of elevated hcy.
number of words= 1323
[{'rouge-1': {'f': 0.36282492083278967, 'p': 0.7303260869565218,'r': 0.24136812411847675}, 'rouge-2': {'f': 0.17574342353798947, 'p': 0.28798365122615804,'r': 0.1264573041637262}, 'rouge-l': {'f': 0.3596160828705681, 'p': 0.5766666666666667,'r': 0.2612751677852349}}]
-----------------------------------------------------------------------------------------------------------------------------------
p147:
Extractive Summary:
however, by manipulating stimulus parameters a second channel the npi channel, may also be responsive to vibrotactile stimuli in this range.the npii channel, associated with meissner's corpuscles in glabrous skin and hair follicle receptors in the hairy skin, and their rapidly adapting (ra) afferents fibres, appear to best respond to low frequency (2–100 hz) vibration.the hairy skin of the forearm has a predominance of slowly adapting afferent units, with rapidly adapting units being only sparsely distributed [7].the hairy skin surfaces examined in this study were chosen for their proximity on the somatosensory homunculus to the area for the glabrous hand [8].alterations to the somatosensory input to one area on the homunculus are known to affect the sensitivity of surrounding areas [9].vibrotactile sensibility has also been reported to show age related changes [12–20].however, most of these quantitative studies also had limitations.methods subjects testing was performed on 22 young adults (11 females and 11 males; 17–27 years old, μ ± δ, 20.2 years ± 2.2) and 22 elderly adults (11 females and 11 males; 55–90 years old, μ ± δ, 68.6 years ± 10.6).the study was approved by the human ethics committee of the university of sydney and conforms to the guidelines for human experimentation of the national health and medical research council of australia.apparatus sinusoidal vibration was delivered perpendicular to the skin surface at each test site via a 2 mm diameter perspex© probe attached to the shaft of a mechanical vibrator (gwv4).the vibrator was mounted on an isolated rigid trunnion (gearing and watson, t4), in turn attached to a series of adjustable levers allowing the vibrator to be positioned at a range of heights and orientations.sinusoidal waveforms were generated by a computer equipped with the labview® application, passed to a linear power amplifier (gearing and watson, pa30) and then delivered to the mechanical vibrator.the use of the labview® application allowed instantaneous alterations in the frequency and amplitude of the sinusoids.although online measurements of the dampening effect of applied loads on displacement of the vibrator probe were not made, offline measurements using a linear displacement transducer (od4, schlumberger industries) indicated that there was no reduction in the peak-to-peak amplitude of the sinusoid under loads equivalent to those experienced during data collection.as the mechanical vibrator did not emit any audible sound at the frequencies and amplitudes used in this series of experiments auditory masking was not required.the tip projected 2 mm through a 6 mm diameter hole in a rigid perspex plate (300 mm2) suspended from the rigid trunnion.the probe and the rigid surround were separated by a gap of 2 mm on each side.sinusoidal vibration amplitude in both the ascending and descending modes was incremented and decremented respectively, in steps of 0.17 μm for 200 hz vibration and 1.05 μm for 30 hz vibration.this method of limits procedure has been shown to be as reliable as, and more time efficient than the two-alternative forced-choice procedure in a group of adult subjects aged 22 to 56 years [22].furthermore, in four elderly subjects we studied the vibration detection thresholds for 30 hz and 200 hz vibration on the tip of the index finger and on the thenar eminence using both methods.variable a was defined as age, firstly young vs. elderly, and then as old (55–64 years, n = 9) vs. older (65–74 years, n = 6) vs. oldest (over 75 years, n = 7), variable b was defined as frequency (30 hz vs. 200 hz) and variable c was defined as test site.at the group level, the threshold of vibration detection at the fingertip at both 30 and 200 hz was significantly (p < 0.001) lower than detection threshold at each of the other sites.pooled results for all subjects at each site are shown in table 1.these results conform with previous observations on frequency tuning curves obtained from both psychophysical human and electrophysiological animal studies (for review see [23]).accordingly, data obtained from the ascending and the descending modes were combined for further analysis.the differences in sensitivity between elderly fingertips and other sites were significantly greater, (f 1,42 = 15.65, p < 0.001), than the differences between young fingertips and other sites.for this frequency at the fingertip, although the elderly group had higher detection threshold values overall, the statistical analysis did not reveal any significant differences between the two groups.analysis of data from the elderly group figure 3 shows the group means and standard errors for vibration detection thresholds at each site for the three subgroups of elderly subjects.at the fingertip, although overall there was an increase in the detection threshold values with increasing age, there were no statistically significant differences between those aged under 65 and those in the older (aged 65–74) and oldest (aged 75–90) groups (p = 0.06 - 0.24).at the remaining sites (forearm, shoulder and cheek) there was greater deterioration for the detection of 30 hz vibration compared to detection of 200 hz with progressive aging.this effect was more pronounced at the shoulder and at the cheek, than at the forearm.secondly, the gap between the contactor and the rigid surround used here was larger than most other studies (2 mm vs 1 mm).the relatively small contactor and larger contactor-rigid surround gap used here may account for the generally higher detection thresholds seen in this study in comparison to others.for the hairy skin locations, the detection thresholds for 30 hz vibration are higher than those previously reported for the forearm [24,32], but considerably lower than those reported for the cheek [25].comparison of detection thresholds between young and elderly subjects the overall results of this study are in agreement with previous studies [13–21], that aging has a significant deteriorating effect on vibration sensibility, and that greater deterioration is observed on the proximal sites tested.the reasons for the differential effect of aging on detection threshold at various body regions are not clear.some or all of these factors may be responsible for the age related changes observed in vibrotactile sensibility at various body regions.if aging affects a proportion of afferent units and their associated receptors in a given area, it is likely that the densely populated regions, such as the fingertips, may retain their sensitivity compared to less densely populated areas, such as the thenar eminence.alteration of inputs to the central nervous system may occur not only as a result of peripheral or central lesions, but also following increased stimulation of a particular body region.the functional implications of a larger cortical representation can be assessed by means of psychophysical methods.previously, stevens, foulke & patterson [40] assessed the tactile acuity in blind and age-matched sighted subjects, and reported that blind subjects showed better performance at the fingertips than sighted subjects.it may well be the case that the afferent channels conveying different aspects of mechanoreception are differentially affected during aging.such central changes may occur through various mechanisms, such as expansion of representations or synaptic strengthening of neural connections.the preservation of detection thresholds at the fingertips reflects the functional importance of the skin of the fingertip throughout life in exploration and manipulation of the environment, and is consistent with central mechanisms of plasticity likely to result from such use.
number of words= 1182
[{'rouge-1': {'f': 0.3666581647325758, 'p': 0.8040425531914894,'r': 0.23747572815533982}, 'rouge-2': {'f': 0.185569386805515, 'p': 0.3297864768683274,'r': 0.1291093117408907}, 'rouge-l': {'f': 0.34395907202917636, 'p': 0.6292105263157894,'r': 0.23666666666666666}}]
-----------------------------------------------------------------------------------------------------------------------------------
p148:
Extractive Summary:
these interventions include lowering blood pressure and cholesterol levels and the use of hormone replacement, vitamins, non-steroidal anti-inflammatory drugs to prevent the onset of ad and the use of cholinesterase inhibitors (cheis) and cognitive behavioural interventions to slow the cognitive decline.of the potential preventive measures the best evidence, from randomised control trials, is for treating hypertension [2,3], although there is debate about this [4].we have developed new measures of the population impact of interventions, which estimate the numbers of people in a defined population who will benefit from an intervention [13-18], and are population extensions of the clinically relevant number needed to treat (nnt).the measures require an estimate of the risk of the health outcome we are examining, the benefits of the proposed intervention derived from the literature, how common is the condition and the current use of the intervention in the population.these measures of health improvement can be set against the cost of each intervention and appropriate priorities could be established that will maximise gain for the population.as examples of this, we have shown that interventions for schizophrenia, going from current to best practice in a population of 100,000, would prevent between 6 and 40 hospitalizations and between 6 and 44 relapses in a year.similarly, depression interventions would lead to between 100 and 325 relapses prevented [18,19].oldham was selected as a typical urban borough with availability of the required data from local surveys.step 1 – literature review to examine key interventions with established or potential benefit for ad, among the general population, among those with mild cognitive decline and those with ad.the main outcome measure was taken as cognitive decline, as this is an important outcome in ad and one for which measures of relative risk reduction are available.step 2 – we examined the current practice of the use of preventive and treatment agents, where possible from the relevant population (oldham).this included a retrospective audit on prescription of cheis, analysing the case notes and pharmacy prescriptions from 2002 to 2005 conducted in oldham and other health areas by the university of manchester psychiatry division co-ordinated by one of us (np) [21].the measure we used in this study was the number of events prevented in a population (nepp) which describes the impact of the interventions and is defined as "the number of events prevented by the intervention in your population over a defined time period [15,19]." the number of events prevented in the population (nepp) is calculated as follows: nepp = n*pd*pe*ru*rrr where n = population size pd = the prevalence of the disease in the population pe = the proportion eligible for treatment ru = the risk of event of interest in the untreated group or base line risk rrr = the relative risk reduction associated with the treatment.in order to reflect the incremental effect of changing from current to 'best practice' and to adjust for levels of compliance, the proportion eligible for treatment, pe, is (pb - pt)pc, where pt is the proportion currently treated, pb is the proportion that would be treated if best practice was adopted and pc is the proportion of the population who are compliant with (adherent to) their medication.disease severity was based on the mini mental state examination (mmse) score which classifies mild 21–26, moderate 10–20, moderately severe 10–14 and severe ad with a score less than 10.about 50–64% of the patients suffer from mild to moderate degree [5].a similar figure of 82% adherence was seen elsewhere [27].the relative risk reduction of cognitive decline by treatment with chei's and the risk of event of interest in the untreated (placebo) group or the base line risk was taken from livingston and katona [28], and re-calculated from the original trial [29].baseline risk of the development of ad was taken from the control group of the randomised controlled trial [3].results cholinesterase inhibitors table 1 shows the estimates and the results of the calculations.to assess the benefit of going from current to 'best practice', we used unpublished results from the local audit data [21] showing that 46% of the eligible patients were treated with chei's (although this may well be an overestimate [30]).in addition, there would be costs associated with the detection of these extra patients and their recruitment to the clinic and their on-going care.table 1 shows that increasing the current treatment rates for hypertension by 20%, would lead to the prevention of 8.2 (95% ci 2.3, 16.8) incident cases of ad in the next year.as well as the numbers of outcome events prevented, we present the numbers of people who would need to be treated to achieve these outcomes.the combination of different interventions could also be investigated using this methodology, although it would depend on the existence of original trial data to provide estimates of the benefit.in a cost-effectiveness analysis, such as that used by nice [5,6], value weights are assigned to each outcome, whereas we suggest that the prioritisation in 'competition' with other demands on resources is performed by the policymaker on the basis of the impact on the population – we have termed this 'population cost-impact analysis' [17].we have used cognitive decline, not nursing home admission.the calculation of population impact is a relatively new and developing area of research [31].studies do show that the cost of care is more with the worsening of cognitive disability [32], including a uk study which assessed the cost of care for non-institutionalised patients with ad over a 3 month period showing the significance of cost variation according to the severity of cognitive disability [33].our findings could be further supplemented by a local qualitative study analysing the views of patients, carers and clinicians regarding the benefits of interventions in ad.we have used long-term follow up from the systolic hypertension in europe study [3], which builds on an earlier report of benefit [2], despite the fact that a later meta-analysis [4] showed a much smaller and nonsignificant effect.
number of words= 979
[{'rouge-1': {'f': 0.43906321853351005, 'p': 0.7201305483028722,'r': 0.3158045409674235}, 'rouge-2': {'f': 0.22084293415072404, 'p': 0.32654450261780105,'r': 0.16683794466403162}, 'rouge-l': {'f': 0.3890080308989426, 'p': 0.5922222222222222,'r': 0.2896261682242991}}]
-----------------------------------------------------------------------------------------------------------------------------------
p149:
Extractive Summary:
residents were selected by the rns according to the following criteria: (a) being at least 65 years old, (b) having a diagnosis of dementia documented by the medical file (all had a diagnosis of dat), and (c) having been living in the same facility for at least three months.residents suffering from delirium or any form of psychosis were excluded.fifty-five residents were initially selected but data could not be collected for six of them because a respondent was not available during the data collection period.the final sample therefore included 49 residents, which is near the sample size estimate of 50 indicated by a power analysis for pearson correlation coefficient (r) conducted prior to the study.this sample size estimate was obtained for a power of 0.80, an alpha level of 0.05, and an effect size of 0.40 determined from the correlation obtained between discomfort and overall agitation by buffum et al. [20].an effect size of 0.40 is halfway between medium (0.30) and large (0.50) effect size conventions for r proposed by cohen [24] measures descriptive variables the following information was drawn from the resident's medical file and care plan: date of birth, sex, date of admittance to the facility, medical conditions, as well as diagnosis and type of dementia.analgesics taken on a daily basis were also recorded for control reasons because these can influence the experience of discomfort.rns, who filled out questionnaires for the residents, were also asked to provide descriptive information (gender, years of education, and years of experience working with older adults).severity of dementia the level of cognitive impairment was measured using the functional assessment staging (fast) developed by reisberg and colleagues [25,26].the scale comprises seven stages measuring function loss associated with cognitive deterioration.each resident was therefore given a score between 1 and 7 where 1 refers to a normal cognitive function and 7 to severe dementia with very severe cognitive impairment.intraclass correlation coefficients indicate good test-retest reliability (0.86) and interrater agreement (0.87) [27].previous studies have indicated strong concurrent validity between fast and various psychometric and mental status assessments [26,27].for example, sclan and reisberg [27], found that increasing functional disability assessed by fast is significantly related to decreasing cognitive functioning (r = -0.79).disability the level of disability in performing activities of daily living (adl) was measured using a subscale of the functional autonomy measurement system (smaf) [28,29].this adl subscale helps measure the level of impairment for five activities (feeding, washing, dressing, personal hygiene and using the bathroom), as well as urinary and fecal incontinence.each item is evaluated using a fourpoint likert-type scale where 0 indicates that the person is autonomous while -3 denotes dependency.hébert and colleagues have found that the smaf has acceptable interrater reliability (mean cohen's weighted kappa of 0.75) and that the scale can be reliably used by both nurses and social workers in either community or institutional settings [28,29].results obtained by desrosiers et al. show an intraclass correlation coefficient of 0.95 for test-retest and 0.96 for interrater reliability [30].concurrent validity is supported by a strong correlation between smaf scores and the amount of nursing time required for care (r = 0.88) [28].agitation agitation was evaluated using the french version of the cohen-mansfield agitation inventory (cmai) [31,32].the frequency of all 29 items of the cmai is evaluated for the previous two weeks using a likert-type scale and scores given for each item varies from 1 ("never") to 7 ("several times per hour").for each resident, an overall agitation (oa) score as well as a score for each one of the three factors of the cmai (ab, napb, and vab) were calculated by summing across items.these factors have been identified for the french version of the cmai [31] and the corresponding items are very similar to those of the factors of the original version [9]: ab (6 items: hitting, grabbing, kicking, scratching, pushing, and spitting), napb (6 items: pacing, trying to get to a different place, handling things inappropriately, hoarding, hiding, general restlessness), and vab (6 items: complaining, repetitious sentences or questions, negativism, constant requests for attention, cursing or verbal aggression, screaming).deslauriers et al. [30] have demonstrated the interrater reliability (r = 0.72), test-retest reliability (r = 0.72), internal consistency (cronbach's alpha from 0.75 to 0.77), concurrent validity (r = 0.74) and construct validity of the french version of the cmai completed by nurses.discomfort the discomfort scale for patients with dementia of the alzheimer type (ds-dat) [17] is composed of nine behavioral indicators of discomfort determined following interviews with caregivers working with persons suffering from dementia.thus, the authors of the scale gave particular attention to content validity by identifying the kinds of behavior most frequently associated with a sign of discomfort in this population.the nine indicators are (a) noisy breathing, (b) negative vocalization, (c) content facial expression, (d) sad facial expression, (e) frightened facial expression, (f) frown, (g) relaxed body language, (h), tense body language, and (i) fidgeting.each item comes with a list of observable forms of behavior which helps evaluators observe and record the signs of discomfort as objectively as possible.this tool makes it possible to evaluate the frequency (from 0 to ≥3), intensity (high or low) and duration (long or short) of the nine indicators associated with discomfort, as perceived by the observer, in the course of an observation period usually lasting five minutes.the level of discomfort is then derived from the value attributed to these three components.each of the nine items is evaluated independently on a scale from 0 ("no observed discomfort") to 3 ("high level of observed discomfort").content validity was insured by initially generating twenty-six items from interviews with nursing staff from inpatient alzheimer units and then retaining eighteen items rated by independent nurses holding advanced nursing degrees as relevant with an operational definition of discomfort (the number of items was later reduced to nine).the internal consistency of the ds-dat has been evaluated in two studies [17,22] with results showing cronbach's alpha varying from 0.74 to 0.89.the interrater reliability (r varying between 0.61 and 0.98) and test-retest reliability (r = 0.6) have also been verified [17,22,33].nurses acted as raters in studies reporting the psychometric properties of the ds-dat [22,33].procedure rns working on the residents' units were asked to collaborate in this study.a group meeting was called with the principal investigator (icp) to explain the study to the rns, obtain their consent, and provide them with information so that they would be able to identify residents matching the study criteria based on each resident's file.the rns were also given a presentation to demonstrate how to complete each of the scales used in the study.because the administration and scoring of the ds-dat is complex, a large portion of the meeting was devoted to demonstrating how to use this tool.each item of the scale was explained in detail, case examples were presented, and staff members had the opportunity to practice scoring.these instructions included hurley et al.'s [17] recommendations regarding the timing of the assessment.the ds-dat, the cmai, the adl subscale of the smaf and the fast were completed by the rn most familiar with each resident.they were given two weeks to complete and return all scales (i.e., the cmai was completed at the end of the two weeks and all other scales were answered during this period).the principal investigator could be contacted during this period to answer questions or provide additional information regarding the procedure.staff members were the participants in this study.indeed, the main variables reflect their perception of selected residents.the residents themselves did not participate directly in the data collection.staff members did provide the researchers with factual information obtained from the residents' file to which they had authorized access as part of their work.prior to the study, the researchers were granted permission to obtain this information by the professional services director of each participating facility.
number of words= 1287
[{'rouge-1': {'f': 0.3239484519025786, 'p': 0.6974509803921569,'r': 0.21096916299559473}, 'rouge-2': {'f': 0.18562284640065793, 'p': 0.33229508196721314,'r': 0.12878030859662015}, 'rouge-l': {'f': 0.3247511209019532, 'p': 0.5754945054945055,'r': 0.22619694397283532}}]
-----------------------------------------------------------------------------------------------------------------------------------
p150:
Extractive Summary:
you are saying that we are over burdened with this clinical work and managerial work, so unburden us recruit more, this does not mean that you will start giving our functions to anybody else.” idi06 (doctors’ association).respondents supportive of ayush task shifting also included the ayush doctors’ association and rural medical practitioners.these respondents mentioned the allopathic or integrated care due to the lack of doctors, and noted the limitations faced by this cadre, including the power asymmetry between allopathic and ayush providers and the need for better training.policy 7(b) explore further task shifting options from mbbs to nurses (community health officers).all respondents who discussed this policy option were supportive, but some noted that for it to be successful, proper education and training mechanisms would need to be put in place.respondents also mentioned the often-poor quality and training of private nursing colleges and additional support systems would be required to address these competencies gaps.“what is happening - our nursing colleges in last 10 years were very poor and lot of private nursing colleges have come up who are not training at all, they are just giving the degree.so we do not know which nurses or which type, where they have been trained.” idi07 (medical college leadership).the nursing association was supportive of this policy under the condition that proper allowances were provided, and that sufficient guidelines and policies were developed for nurses in these positions.respondents also mentioned absence of policies that protect nurses who are already filling in the gaps during doctor shortages, further highlighting existing structural and power dynamics between the nursing and medical doctors.some limitations included existing restrictions to prescribe medications and continued opposition from the medical association.“doctor leaves the office at 2 o clock, but we continue to do the dialysis until 7 o clock, there is no doctor there, until dc is not filled, doctors don’t come there, that is the condition of [facility name] and forget about other places … these kids have the full power to give any emergency drug, they have joined us 7 months ago, they have the full power to give injections … they can give everything, they decide whether you have to give oxygen to him or not, you have to use the drip or not.” fgd02 (nurses’ association).national level respondents who were supportive of this policy measure mentioned examples of past successes of mbbs to nurse task shifting and highlighted examples from other countries.reiterating similar concerns mentioned by nurses’ associations, national level respondents mentioned legal barriers and existing acts which prohibit nurses from prescribing medications.“… one option is instead of interfering with each and every legacy act just have one new act and new set of rules … so, then that is a much simpler solution.rather than trying to amend each and every act, because we have so many different acts which kind of affect this issue.” id17 (national-level health agency).cross-cutting challenges in developing and implementing a coordinated approach to strengthening rural distribution and retention of doctors fragmented governance structure and the absence of an hrh unit respondents noted extensive fragmentation in governance structures within the state, and nationally, that made coordinated approaches to rural retention and distribution, and hrh policymaking more broadly, challenging.“uttar pradesh is the only state where in the medical health department there is medical health, family welfare, medical education, ayurved unani are made into 4 parts and all 4 have ministers, there are 4 principal secretaries (pramukh sachiv) … there is nothing like this anywhere else in india.” fgd2, health worker association.addressing perceived corruption and weak accountability in the health sector respondents spoke openly about their perceptions of the lack of strong accountability mechanisms and the existence of corruption in postings, transfers and other hr processes such as promotion, benefits, leave, etc.such corruption in their view lead to major difficulties in both attracting candidates and also ensuring that candidates are deployed and retained in rural areas.fully addressing the types of corruption noted by respondents would involve accountability or other policy interventions that would likely face considerable opposition from those cadres negatively impacted by such measures.for example, respondents noted the need for regular transfers of clerical officers at frontline facilities, district and subdistrict offices in order to curb corruption within hr processes.in response to a question about the impact of perceived corruption on hrh policy, a district administrator noted, “if these kinds of things [corruption] happen then the interest to work also starts to diminish...in my mind i am thinking that my children are living far away from me and i should be able to meet them at least once in a week.but i am unable to get any leave for district-level administrators.power dynamics and health worker associations: the power of health worker associations to facilitate or impede certain policies was as an important factor in developing and implementing a comprehensive strategy to rural retention and distribution.doctors’ associations were reported to be particularly powerful in influencing the process.“if [government doctors’ association] opposes this then it cannot go further.the government doesn’t have so much power to go against [the government doctors’ association] and implement a policy.” idi1 (health systems expert).a few respondents noted that other health worker associations also wielded power – albeit to varying degrees – and could significantly impact policy processes.for example, one respondent discussed how the association representing clerical officers could stymie the initiative to regularly transfer the cadre in order to reduce corruption within hr processes.phasing and timing of policies within the bundle some respondents noted that certain policies need to be implemented jointly with others in order to meet the goal of rural distribution and retention.for example, a representative of a doctors’ association noted that a policy around rural postings should ideally be complemented by investments in staff housing and good quality options for educating school-aged children.conversely, a few respondents noted the potential for adverse outcomes when bundling certain policies together, such as home district posting and permitting dual practice.[respondent explaining opposition to home district posting] “because one, they will get to be near their relatives and friends, and going home will be on their mind, they will think about going home.their efficiency will reduce; more focus will be given on private practice ….” idi14 (rural medical practitioners’ association).opportunities for reform despite the many challenges, some respondents also noted that there were several opportunities to strengthen hrh policy in the state.successes in improving rural distribution and retention from other indian states and other countries with similar health systems provided roadmaps for decision-makers to adapt.the growing focus on primary care through recent policy changes at the national and state level signaled an important opportunity for strengthen availability of services at these facilities, provided that various departments could coordinate with one another effectively.discussion ensuring the availability of health workers in rural areas is a persistent challenge in india and other lmics.recognizing the complex, interlinked nature of the problem, normative guidance increasingly focuses on ‘bundled’ interventions which utilize a range of policy measures to more effectively address the issue [10, 40, 41].adopting this type of coordinated approach to health policy is challenging in any context, and arguably, more so with hrh policy.hrh policy is often complex, due in part to governance bottlenecks, power hierarchies amongst and between health workers and administrative cadres, and ineffective or weak systems of accountability and transparency [42–44].however, few studies have explored stakeholder perspectives across a range of hrh policies [17], and even fewer specifically explored issues of rural distribution and retention [14].our analysis reveals several barriers to developing and implementing holistic policies to strengthen rural distribution and retention in up, india.we discuss three key findings here -.the lack of coordinated and effective governance in the health sector is a major barrier to improving rural retention and development in up, an issue that has emerged in the development of hrh policy in other lmics, such as bangladesh [14] and south africa [17].our study builds on this literature by describing the disconnects between various units within the health sector, and between health and other sectors.rural areas in uttar pradesh continue to face considerable inequities in access to quality, affordable health care, education and other social services, as well as limitations in coverage of transportation, utilities and commerce.these challenges have long been identified as major barriers to both supporting students from rural areas to gain admission to medical colleges, and also attracting health workers to live and work in these regions [9].the multisectoral action needed to improve these conditions was found to be lacking in this context, as it has in other settings and other policy issues [9, 45].the impact of power asymmetries amongst stakeholders in shaping rural retention policy has been previously discussed [14, 46].our findings add to this literature by highlighting the policy role of health worker associations, particularly various doctors’ associations.those policies that were contested by doctors’ associations (i.e., ayush task shifting) would require considerable political capital to be adopted and implemented.given the mix of policies required for improving rural retention and distribution of doctors, decision makers would have to carefully plan the selection and timing for formulating and adopting policies, particularly those that could result in opposition or blocking from health worker associations.finally, stakeholders discussed major challenges in implementation due to inefficient mechanisms for accountability and perceptions of widespread corruption across the health system.similar issues have been identified in hrh policy in other regions of india [47], as well as other lmics [42, 48–50].addressing these accountability concerns may require additional policies – for example, regularly transferring clerical cadres responding for hr processes – that would likely result in resistance from their representative associations.in other instances, the development of complex policies such as the regulation of private practice would require careful end-to-end planning, particularly in terms of enforcement and accountability, in order to avoid the emergence of new problems brought about directly as a result of the policy.the growing literature documenting anti-corruption, accountability and transparency policies in lmic health sectors would offer important insights for stakeholders working to develop and implement hrh policy [51–53].nevertheless, up is at a unique moment in the development of hrh policy.at the national and state level, there appears to be strong commitment to expanding rural distribution and retention of doctors, and considerable resources invested in expanding supply of health workers, including doctors.several important policy decisions have already been taken with regards to the policy challenges noted in this paper.for example, the state has transitioned from the system of having chief medical officers post medical officers, to the state managing these postings centrally.similarly, the state has taken steps to improve the quality of nursing in the public sector by increasing the level of qualifications for new recruits.these developments suggest a window of opportunity in the near term to develop and implement policy solutions.policy implications our analysis has several implications for policy.we modified the initial policy proposals and also identified new approaches that addressed concerns raised during the analysis (see table 5).at the time of writing, plans for further consultative processes to formalize these policy options have been disrupted by the covid-19 pandemic; we hope to re-initiate them as circumstances allow.here, we present approaches for addressing the issues laid out in the analysis.it is apparent that opposition to specific policies (in the case of up these included compulsory rural postings and ayush task shifting) implies the need for careful negotiation and/or high-level political support in order to pass these policies in the face of opposition.policymakers should also consider bundling interventions strategically, pairing a policy that will likely meet with resistance, with one that will be favorably received (home district posting and private practice, for example, in the up case).while goup may have particularly acute challenges in terms of fragmented governance for health this is a common problem across other states and countries [45].for hrh issues in particular, it will be important to examine existing health and multisectoral governance structures and identify areas for improvement.for example, a unit focused specifically on hrh policy that coordinates across various ministries, departments and agencies would be a first step in identifying and addressing gaps and bottlenecks in the policy process.such a unit may also be tasked with looking within and beyond the health sector for particular policy levers that can address various challenges with rural deployment and distribution.
number of words= 2035
[{'rouge-1': {'f': 0.30459780653199714, 'p': 0.8110714285714287,'r': 0.1875082586125531}, 'rouge-2': {'f': 0.1670769503125335, 'p': 0.3326865671641791,'r': 0.11154863078375826}, 'rouge-l': {'f': 0.28893888589214645, 'p': 0.6229411764705883,'r': 0.18809045226130655}}]
-----------------------------------------------------------------------------------------------------------------------------------
p151:
Extractive Summary:
a potential reason for this “research to practice gap” is the challenge of spreading information about relational coordination to those who could benefit from its application.the product should be designed for and by end users who know their local context, be they an individual provider, a clinical team, a community, or an organization.the objective was to introduce relational coordination concepts and research to a broad audience.active dissemination occurred by the study lead (hg) who scheduled time to speak on monthly cyberseminars hosted by va hsr&d and operational programs (e.g., va office of nursing services) and weekly distribution of informational materials via email or phone to va change agents [17].active and passive dissemination of rc in va content and membership in the relational coordination research collaborative was paid by the rc in va program funds and delivered through existing va communication channels.dissemination and engagement data from the dissemination phase were captured as invitations to present rc in va content, number of email and phone contacts by study leads, social media impressions and engagements, and new va members to the relational coordination research collaborative.these metrics were feasible to collect and are common measures to assess the impact of social marketing-based efforts [11].implementation phase this phase involved partnering with a small group (e.g., 10–40) va researchers and operational staff to establish processes to study rc in va.trainings were planned for va staff to learn about relational coordination assessment and interventions and apply them to current projects.the rc survey is a proprietary instrument administered through rc analytics [18].the survey was available to five va teams.administrative and copyright costs were paid by rc in va program funds.an rc in va application was placed on the relational coordination website and shared via email with interested parties.promotion of the rc in va application occurred using the methods described in the dissemination phase.adoption data from the implementation phase included the number of applications requested, submitted, and funded, as well as training attendance.an end of project survey was sent to the five rc in va research project leads 1 year after program implementation.the text items were included to allow participants to expand or offer new information on the rc in va program.analysis two researchers (bc and hg) collected the dissemination, engagement and adoption data from va researchers and operational staff during both phases of the project.quantitative data were calculated using descriptive statistics.the text responses in the survey were analyzed using deductive content analysis in microsoft excel 16.3.only content that fit the matrix of analysis were summarized and reported.sociodemographic variables of participants were not collected.results dissemination - phase one during the initial dissemination phase, 51 instances of email and phone communication were exchanged between the study lead (hg) and va researchers and operational staff.content was shared via twitter 47 times, receiving 62,139 impressions and 398 instances of engagement.four articles were written for va newsletters.implementation - phase two the implementation phase resulted in 13 requests for rc in va applications; six (46%) applications were submitted, and five (38%) submissions were selected.information on the rc in va programs were shared at five conference podium presentations and in two publications [20, 21].five projects submitted additional grants to continue their work.survey findings from the five rc in va projects (n = 9) indicated most participants (n = 5; 56%) learned about the rc in va project through word of mouth.one participant read about it on twitter while three found the program through a va presentation.three participants heard about the project through other means.they added it would have depended on the price and if they could have written the cost into existing grants.the majority (89%; n = 8) indicated they would not continue with the relational coordination research collaborative partnership for the va does not reimburse staff for memberships, meaning individuals would be required to pay for membership in the relational coordination research collaborative themselves.discussion this paper described dissemination of the rc in va program to researchers and operational staff across the united states.using the social marketing elements of product, price, place, promotion, we highlighted lessons and strategies that can be applied to the broad dissemination of a theoretically driven team communication and relationship theory and assessment tool through a large healthcare organization.the products created for the rc in va program were designed for and by end users who knew the va context [22, 23].these included informational content that could be delivered during phone calls, in-person meetings, over cyberseminars, and via social media.the success of this approach is supported by the results of our project.literature on healthcare interventions suggests that tailoring evidence-based practices or interventions to an individual’s preference or need is likely to improve professional practice or patient outcomes [24].this is in contrast to the “one size fits all” approach, which has been found to be inconsistent with goals of maximizing patient outcomes, quality of care, and intervention adherence [25].this was a successful dissemination strategy, as noted by respondents’ hesitancy to continue as a partner and use the rc survey in future work due to cost.the push marketing approach was effective for it targeted new customers who hadn’t heard about the rc in va product [30].this dissemination approach focused on piquing researchers’ interest in a manner that took very little effort on their part.push marketing as a dissemination strategy can apply in any setting and should utilize multiple platforms (e.g., social media, newsletters, presentations) to ensure communication saturation is achieved and the target audience is reached.the promotional efforts and results of rc in the va suggested that, while low-effort promotion via social media and newsletters was effective in reminding the target audience of the program, the most impactful promotional effort was active outreach through personal contact via email, phone calls, presentations, and discussion with change agents.our findings, which align with social marketing theory, suggest dissemination within healthcare settings should consider product design and price, in addition to placement and promotion.
number of words= 976
[{'rouge-1': {'f': 0.41663963991631453, 'p': 0.7604024767801858,'r': 0.2869260700389105}, 'rouge-2': {'f': 0.1804282265813727, 'p': 0.2749689440993789,'r': 0.13426484907497566}, 'rouge-l': {'f': 0.39995780624333754, 'p': 0.5933160621761657,'r': 0.301651376146789}}]
-----------------------------------------------------------------------------------------------------------------------------------
p152:
Extractive Summary:
the first is to invite a group of representative citizens, e.g. citizens’ juries, to voice their opinions, which may then be used to deliver recommendations applicable to policy and practice decision-making [9].the second is to involve organized civil society groups, e.g. patient organizations or community groups.research indicates great diversity among european countries as to the extent to which cancer patient organizations partake in policy decision-making and with what impact [10, 11].the third is to invite a specific group of citizens to participate, the ones that are affected by a decision, e.g. people with cancer (which may not belong to an organization or similar).today, there is great variation in the methods and terminology used in different healthcare systems when it comes to involving particular groups, but there is a growing body of literature on patient and family advisory councils (pfacs).pfacs are groups of patients, family members and caregivers who meet regularly and serve as advisors in diverse healthcare settings, from individual clinics and hospitals to healthcare systems [6, 12].although some researchers conclude that pfacs can provide new insights that can assist healthcare providers in optimizing current methods [13] and change staff ‘culture’ or awareness [14], the impact of pfacs is uncertain, in particular regarding healthcare decisions at the population level [6, 14, 15].similarly, for pfacs in cancer care, it has been suggested that they seem to play an important role in the drive to make healthcare more patient-centred, but there is very little information about their efficacy [16].in general, there is a lack of robust evidence for the impact of ppi in healthcare policy and service improvement [17–20].as pfacs most often operate at the hospital level and include healthcare staff, the vast majority of studies on pfacs concern hospitals, i.e. the operational level within a healthcare organization.however, although unusual, this type of council may also operate at the managerial level, i.e. in parts of the healthcare organization that do not deliver healthcare but rather work to develop care and research.one such example is pfacs attached to the six regional cancer centres (rccs) in sweden.the rccs have been part of sweden’s national cancer strategy from 2009.the overarching goal of the rccs is to work towards more equitable, accessible and patient-focused cancer care of high quality throughout the entire chain of care [21].although an overall assessment of the development of the rccs concluded that one of the greatest merits is strengthened patient participation and patient-centring [22], it is uncertain what the outcomes of the rcc pfacs are, and if this is a type of involvement that leads to changes in cancer services or policy.in the literature, the outcomes or impacts of involvement are sometimes referred to as the ‘effectiveness’ of participatory activities [23].the aim of the study was therefore to investigate the effectiveness of one of the swedish rcc pfacs operating at the managerial level, with the aim to contributing to the development of cancer care.evaluation has been suggested one of nine essential principles to optimize patient and public involvement [24].the rcc organization and patient representation the responsibility for the funding and provision of healthcare in sweden is placed with 21 self-governing regions, which work together in six larger clusters (cooperative regions).each of the six cooperative regions, which are divided geographically, has a regional cancer centre (rcc) that works in cooperation with the regions within its cluster to develop and improve the quality of cancer care.organizationally, the rccs are linked to the regions in different ways because the regional forms of collaboration and decision-making differ, but they are all anchored in the regions’ leadership.the rccs, which differ in size from about 25 to 60 employees, are managed by an operations manager that leads the work in a governing board with representatives from the regions within its cluster (health professionals and public officials).the rccs also work nationally to develop and improve the quality of cancer care.for example, the rccs collaborate through a collaboration group where the rcc managers meet regularly with the national cancer care coordinator at the swedish association of local authorities and regions (salar).the collaboration group advises the salar, the ministry of health and social affairs and the national board of health and welfare on cancer care issues.for the period of 2020–2022, the rccs focus their work on ten common development areas, three of which are prevention and early detection, staff competence, and patients and next of kin.the rccs have the fundamental task of supporting the patient’s position in healthcare.in practice, the involvement of patients and their next of kin takes place at several levels relating to the rcc’s work (table 1).with the rcc as a main coordinating body in cancer care improvement, with a clear vision of involving patients and their next of kin, it becomes relevant to explore the effectiveness of involvement within the rcc setting.this article studies the councils for patients and their next of kin, which target the macro and meso levels within the healthcare organization (table 1).methods design the method used for this study was a qualitative, singlecase study [25].the study was conducted in one of the rcc pfacs, which have worked toward improving patient and next-of-kin representation to increase their influence.data collection two types of data sources were used to address the purpose of the study: interviews with pfac participants and meeting minutes from the pfac meetings.those who did not answer received two reminders.the document study was conducted by analysing meeting minutes accessible from the rcc web page.the minutes were written by the rcc coordinator and confirmed by the pfac participants.analysis framework based on ideas about deliberation, abelson et al. [26] have described four general principles that can be used to guide the design and evaluation of collective involvement processes in the healthcare sector: (i) representation; (ii) the structure of the process or procedures; (iii) the information used in the process and (iv) the outcomes and decisions arising from the process [26].the framework is an adaption of abelson et al., 2003 [26].young people and people born in another country were poorly represented.some of the pfac participants were recruited by the rcc as members of local cancer patient organizations, while others were recruited by physicians or friends or without belonging to an organization.a few mentioned that they hesitated before taking on the task, questioning, for instance, what they could contribute and described being encouraged and reassured by rcc representatives.the meeting minutes show that some changes had been made during the last couple of years to improve representation in the pfac.for example, one participant said: ‘you cannot represent yourself when you sit there, because then you do not do much good’ (interview #1).they say thank you and that is that’ (interview #8).in improvement of the pfacs activities if local cancer organizations were more involved.that is a prerequisite for understanding how it is’ (interview # 4).one such example of information was when the manager had informed about how the ongoing pandemic had affected cancer care in their geographical area.a few participants explained that it was a good strategy to hold discussions in smaller groups, as they used to do during the meetings, which helped some to speak up.someone mentioned that the pfac did not come up with any suggestions that could support the rcc board, while another mentioned that the pfac should be used more as a referral group and obtain different questions from the board on which to express their views.’ (interview #6).it was repeatedly mentioned that the rcc manager and the coordinator partaking in the meetings, who were described as perceptive and professional, listened to the pfac participants, ‘took things further’ and did not dismiss any ideas, not even very difficult ones.one participant explained: ‘it is often they say, ‘we will take this to the board, we will present it there’ ‘(interview #6).(s)he also said that ‘the further up in the organization we have representatives, the more likely it is that we get through something we suggest’ (interview #2).yet some could, and one example was contact nurses, which was described as an issue pursued intensely by the pfac.another example was digital care plans, which a participant said was originally discussed in the pfac.one gave an example of when the lack of knowledge stopped a pfac activity.however, this was not reflected in the interviews; rather the pfac participants saw the rcc representatives as allies against the regions or hospitals when cancer care did not function properly and the rcc representatives could thus be thought of as external facilitators [30] in the relation between patients/next of kin and cancer care services and policy-makers.the pfac participants were generally familiar with how local cancer care provision worked but were less familiar with how the system works at the managerial level (mesoand macro-levels), e.g. where different types of decisions are being made and which channels should be used to push for different types of changes.
number of words= 1463
[{'rouge-1': {'f': 0.39730491764655607, 'p': 0.8568852459016394,'r': 0.2586051080550098}, 'rouge-2': {'f': 0.21655495302372657, 'p': 0.3987671232876712,'r': 0.14863695937090432}, 'rouge-l': {'f': 0.4012413155990125, 'p': 0.7081909547738694,'r': 0.2799173553719008}}]
-----------------------------------------------------------------------------------------------------------------------------------
p153:
Extractive Summary:
the relationships are mostly categorised into individual (consumer, family caregivers and mental health professionals) and organisational attributes (environmental attributes) [3, 11].the individual attributes align with factors in consumer and family caregivers as well as mhp factors that enable or hinder the establishment of the therapeutic relationship.this might include consumers’ insights, knowledge, humour and communication challenges (verbal and non-verbal), and work fatigue, skills, competency and attitudes of mhps [3, 11, 12].conversely, the organisational attributes constitute factors influenced by the treatment setting or environment.these attributes may include the philosophy of care (e.g. paternalistic and medical model traditions, organisational policies and practices, increasing workload, staff shortages and a large number of patients [3, 11, 13, 14].research has highlighted that the therapeutic relationship could have a positive effect on the quality of mental health services [1, 8, 15].for example, for consumers, the therapeutic relationship promotes personal recovery, increased satisfaction with services, quality of life, reduced symptoms and functionality [4, 15, 16].given these significances, mental health stakeholders are increasingly advocating for an evidence-based process to enhance quality mental health services.although some studies have suggested the need to understand the evidence-based process in mental health nursing, there is little evidence to support the development of effective therapeutic relationship building, particularly in clinical practice.research on therapeutic relationship building has been limited to general health services, particularly in developed countries, eg.australia [8, 12] uk [17] usa, and germany) and middle eastern countries [3, 18], eg.iran and saudi arabia).in developing countries including ghana, there is limited evidence on issues related to ebp, which is integral to the recovery of consumers.although there has been increasing efforts to improve the living conditions of consumers with mental illness and, thus has resulted in increasing empirical studies on mental health services, to date, the evidence has focused on weaknesses in the health system [19], challenges in policy implementation, enablers and barriers in accessing services [20], and treatment pathways using conventional middle-range theories [21, 22].none of these studies has attempted to understand the perspectives of mhps regarding the evidence-based therapeutic process.to address this gap, a qualitative study, using realistic evaluation as part of a concurrent mixed-methods approach, was undertaken to explore mental health professional’s views on the evidence-based therapeutic process in psychiatric facilities in ghana.middle-range theories supporting evidence-based therapeutic processes several theories have been proposed to inform ebp in health service delivery, which applies to mental health.examples of these middle-range theories include the donabedian theory of quality of care [16] and hildegard peplau’s middle-range theory of interpersonal relationships [23–25].the process component of the donabedian middlerange theory describes ebp as the actual treatment stage and is comprised of consumer-based interpersonal relationships and the technical skills of the mental health professionals.according to the donabedian theory, consumer-based interpersonal relationships highlight the therapeutic relationship between consumers and providers.the technical skills describe clinicians’ knowledge about appropriate interventions and best practices, as well as their ability to accurately assess consumer problems [16].hildegard peplau’s middle-range theory states that consumer-based interpersonal relationships must go through three phases, including orientation, working (identification and exploitation) and termination (resolution) [23, 25].the therapeutic interaction between consumers and service providers is central in each of the three phases.for example, at the orientation phase, providers meet consumers and gain essential information.service providers make assessments about consumers through a collaborative and interdisciplinary plan of care at the working phase, to determine the best evidencebased interventions.finally, service providers create discharge plans, which include symptom management and recovery planning at the termination phase [24, 25].methods study setting the study was conducted between june 2019 and november 2019 at three psychiatric facilities in ghana.two of the facilities are specialised psychiatric hospitals, while the third was a unit within a general hospital.the two psychiatric facilities are located in the greater accra region and the psychiatric unit is located at the komfo anokye teaching hospital in the ashanti region.all three facilities operated based on a shared goal and focus of treatment.research design and approach (phase 1 – initial theory and assumption) this paper focuses on the qualitative component of a larger, concurrent mixed-method study design, which draws on the principles of realist evaluation to explore mental health professionals’ perspectives on evidencebased processes in the provision of mental health services.this qualitative component explores the subjective views of mental health professionals in the process of delivering mental health services.in particular, the qualitative data was used as secondary to the quantitative objective measure of the outcome of mental health services.it enabled the researchers to interact with mental health professionals and to listen to their subjective experiences in the process of providing services to consumers.it also helps to generate a rich and in-depth exploration of the therapeutic process in mental health service delivery [26].the naturalistic, holistic view of qualitative methods is relevant to better understand the therapeutic process, rather than simply the quantitative measure of outcome.specifically, the realistic evaluation cycle was conceptualised into phases and started with initial theory development [27, 28].data were captured through semi-structured interviews, surveys, and a review of the literature.the literature review [19, 21], concepts [16, 26] and quantitative components [29, 30] informing this realistic evaluation have previously been published.the realistic evaluation provides a unique perspective on services and is grounded in theory [27, 31].the central tenet of this realist methodology is that the services may work differently in different contexts [28, 32].this study offers ways to address what works (or not), when, why, for whom and under what circumstances to promote evidence-based therapeutic interaction in service provision [33, 34].the therapeutic processes in mental health services have traditionally been tested and evaluated through middle-range theory (e.g. donabedian theory or hildegard peplau’s theory).however, such theories do not offer ways to identify contextual factors, nor mechanisms that could promote the therapeutic interaction between consumers and providers.this realistic evaluation involves the development and refinement of a program theory, which is explained according to pawson and tilley’s (1997) formula: context + mechanism = outcome.the donabedian middle-range theory and hildegard peplau’s theory on interpersonal relations were used to explain the therapeutic processes of providing mental health services [35].. the qualitative data were empirically tested to develop a final program theory that explains contextual factors and mechanisms that enhance the evidence-based therapeutic processes in delivering mental health services (tables 2 and 3).the experiences of mhps based on their therapeutic interactions with consumers were used to refine the program theory.the next step involved developing the underlying assumptions, to articulate the program theory [27, 31, 36].data collection (phase 2 – recruitment and fieldwork) qualitative methods including field notes and in-depth interviews were used to collect data from mhps.the clinical coordinators in each psychiatric facility facilitated the recruitment of participants.the researchers reviewed the list of mhps in each of the selected facilities and selected those who met the eligibility criteria.mhps were included if they had worked for at least three years in the respective psychiatric facility and provided daily routine mental health services.based on the inclusion criteria, we invited 38 mhps through email, face-to-face or telephone call.the invitation contained a letter of introduction, participant information sheets, and consent forms.similarly, the head of each psychiatric facility sent a memo detailing the research and advertised it on the notice boards in each department.a total of 30 mhps agreed and were purposively recruited to participate in an in-depth interview.the data were collected until saturation, when no new information emerged.the interviews were conducted using a structured interview guide (additional file 1) and captured information on contextual factors and mechanisms that could improve therapeutic processes in mental health services.the interview guide was developed using relevant information according to the donabedian middle-range theory [16] and papau theory on interpersonal relations.specifically, the content of the interview guide covered questions on the evidence-based therapeutic process in providing mental health services.for example, the questions focused on the technical skills of providers, the training and professional development plan and consumer-based interpersonal relationships [23–25].in particular, the questions also covered clinicians’ knowledge of appropriate interventions and best practices, as well as their ability to accurately assess the problems of consumers [16].in the interview sessions, mhps were briefed about the research objectives, procedures and the consent process.the right of participants to safeguard their anonymity and integrity was respected.each mhp was informed of the study’s aims, methods, and consent to participation, potential risk/benefits and privacy/confidentiality.the mhps signed an informed consent form prior to participation.the lead author conducted the interviews and therefore read the questions and recorded (with permission) responses, using an audio recorder.the interviewer has several years of experience in conducting qualitative interviews, particularly in the health facility setting.although the interviewer understands the mental health systems, thus based on previous research, he did not have any work, social and emotional links with the participants.the interviewer also monitored all the interview process, including gestures and memos.we assigned unique identifiers to the audio recordings, to maintain the confidentiality of the participants.all interviews were conducted in english, which is the primary language of conversation in formal settings in ghana.the interviews were conducted at the psychiatric facilities, in the clinical staff common rooms, privately and separately; that is, no interview was witnessed by a clinical coordinator or service provider.each interview took approximately 45 to 60 min.data analysis (phase 3 – analysis and configuration of cmo) the data from field notes and in-depth interviews were analysed using thematic analysis, conducted according to braun and clarke [37] approach to categorising and connecting data, to develop a realist theory (tables 2 and 3).this process involved transcribing, reading, familiarising with the data, generating initial codes, searching for themes, reviewing themes, and rigorous interpretation of data.an independent transcriber transcribed the 30 deidentified audio recordings into an ms word document.the interviewer listened to the audio recordings and reviewed the transcripts and field notes.the transcribed data was then entered into nvivo 12 for analysis.the lead author, working closely with all co-authors, performed the coding process.as recommended by saldaña [38], initial coding were conducted to develop inductive codes.the authors revised the initial codes to better understand which ones to include in new inductive codes.the authors then performed focused coding, thus successive coding, of all the remaining data to identify additional codes.in the focused coding, the most significant initial codes were used as provisional categories for checking across all the transcribed data.the focused coding continued until saturation was reached – no new ideas emerged from successive coding.the analysis identified a total of 49 inductive codes.the codes were categorised into two major themes that explain the evidence-based therapeutic process in mental health service delivery (table 2).
number of words= 1735
[{'rouge-1': {'f': 0.2708648941999793, 'p': 0.807991266375546,'r': 0.16270433351618213}, 'rouge-2': {'f': 0.16176257658499574, 'p': 0.3507017543859649,'r': 0.10512623490669595}, 'rouge-l': {'f': 0.2941888487346484, 'p': 0.6657446808510639,'r': 0.1888118811881188}}]
-----------------------------------------------------------------------------------------------------------------------------------
p154:
Extractive Summary:
as an example of the problems that trials can face, one rct of aromatherapy in advanced cancer found many patients were too ill to approach, a higher number than anticipated declined, and referrers acted as gatekeepers due to scepticism about treatment and the wish to reduce patient burden [3].failing to reach recruitment targets results in underpowered trials, with poor generalisability of findings [4].data from an observational palliative care study suggests 24 % of palliative patients were excluded from research because of gatekeeping [7].however, this may be discordant with the wishes of palliative patients.a review of studies looking at the views of patients or carers towards participating in palliative care trials or research studies suggests palliative care patients are generally happy to be invited to participate in research, experience direct and indirect benefits (such as feelings of altruism) from participation, and are averse to others saying “no” on their behalf [9].despite successfully recruiting 230 participants out of a target of 240, individual participating sites struggled with achieving recruitment targets.only patients living within the catchment areas for these iapt centres were eligible to participate.recruitment was conducted by researchers from university college london, employed on the cantalk trial.topic guide the experience of researchers employed on the cantalk trial to recruit in oncology clinics suggested that some staff were reluctant to refer patients to the trial.with a view to exploring possible reasons for this, and attitudes of staff towards psychosocial research trials in general, a qualitative topic guide was developed [13], asking about (1) role in the oncology clinic; (2) previous research involvement; (3) views on trials other than clinical trials of investigational medicinal products (ctimps); (4) views about the cantalk trial; (5) factors influencing recruitment; (6) feedback about the cantalk trial; and (7) future research involvement.however, as all topics arising appeared to fit well within the existing structure of the guide, no alterations were made.interviews interviews took place between december 2015 and march 2016, after recruitment closure but within three months of recruitment ending.interviews were conducted by researchers employed on the cantalk study and trained in qualitative techniques, in a private room at the participants’ place of work, and lasted between 16 and 66 min.the analysis throughout, including the selection of text for coding and the search for themes, was guided by the study’s research questions.reflexive statement the chief investigator [ms] and the trial manager [ta] of the cantalk trial set up this qualitative study to understand the reasons for the difficulties with recruitment.it is therefore possible the researchers had preconceived ideas about why recruitment was challenging, and pre-existing biases and assumptions of researchers may have influenced the interview process.however, researchers had experience in exploring a topic in-depth through interviews, and every effort was made to develop an open and unbiased interview schedule.all participants worked within either hospital oncology clinics or a hospice and all were involved in participant recruitment for the cantalk study.five main themes emerged from the data: (1) factors limiting recruitment (2) attitudes to research and randomisation; (3) attitudes towards trials not of medicinal products; (4) views on study team vs. in house researchers; and (5) factors facilitating recruitment.some healthcare professionals displayed an awareness of their tendencies to gatekeep: … if you could be a bit more objective about it and think ‘they meet the criteria i can go and speak to them’, i think maybe sometimes it, it stopped us from approaching people … (p10).they’re [the oncology clinic] understaffed at the momentthis theme largely focused on the catchment area, with a number of healthcare professionals noting that the catchment area for treatment centres limited the number of eligible patients, restricted what clinics we screened (p09) and constituted a postcode lottery (p03).… we had a spreadsheet of nearly, i think it got to point where we had about eighty or ninety patients we had on that list, but then as these, like the barriers kept coming in we had to keep cutting out people ….patients do not want or need support this theme comprised the concept that many patients did not need (p01) or want (p08) support, including the rejection of formalised psychological support: …we had plenty of patients who both myself and the clinicians, like the consultants, really thought needed talking therapy because they would literally take up an hour of the medical oncologist’s time(p01) some patients were noted to choose church over study (p01).so it’s like i’m being part of the future research evidence base that would be used probably five, ten years to come… (p05).several participants commented that research helps to build up the evidence base (p12), is the way forward (p03), and that randomised controlled trials are the gold standard (p01).views on randomisation several healthcare professionals expressed that randomisation is necessary to answer the question, that it is the only way that we can compare things (p10), and provides a scientifically robust test (p08).however, along with this acknowledgement of the necessity and benefits of research there was also a view expressed that randomisation is a short term sacrifice [to patients allocated to usual care] for long term gain (p01).this suggests that while health care professionals understand the benefits of randomisation, it may be difficult for them to maintain a sense of equipoise: … though i’ve got patients at the moment who didn’t get the active arm, perhaps i, i say to myself ‘well in two years all of my patients will be getting this as standard treatment’ … (p01).it wasn’t like if they were,… i think if were looking at trying to, uh improve patient care from a kind of a holistic perspectives … these kind of studies and other similar ones are vital… (p08).some health care professionals also expressed the notion that both ctimp and non-ctimp trials hold equal importance, and recruitment centres need a balance (p13): … we need to do both, ‘cos(yeah)so you weren’t involving any drugs or treatment (p03):as well as noting that trials not of medicinal products could be more straightforward, healthcare professionals also noted that psychosocial research studies, which commonly involve completing outcome questionnaires with participants, can allow staff to spend more time with, chat with (p07), and better understand patients: … i like the fact that the questionnaire studies in particular enable me to spend time, more time with the patients actually understanding them as people (mmm) so they are much more people than they are drug trial subject … (p01).bias towards trials of medicinal products several healthcare professionals reported a bias towards recruiting for trials of medicinal products.consultants were noted to be involved more with ctimp trials (p02), to be just interested in medical oncology really (p11), and to favour their own studies (p01), … you only had one haematologist who’d think of cantalk but the other would think about all the other ctimp trials and then, you say, oh what about can- talk, they’re like, ‘um, no, no, no’ as in they push it to the back, that would be like the last resort … (p02).…so it’s allyes we should take part, this is something we can do cause at the time there were very limited breast studies …but yes the time was um, perhaps a little frowned upon… (p03).they know us well enough to know that we are not going to offer them something that they don’t think might help… (p14).factors facilitating recruitment engagement it was deemed important for successful trial recruitment to maximise engagement with sites: … just really more engagement to stakeholders so that they are aware (yea) of what the study is what it involves … (p12).regular presence of researcher the regular presence of a researcher in clinics and in multi-disciplinary team meetings was noted as a key factor in facilitating recruitment: …i find our mdt’s every week a really useful time for people to make their presence remembered even if it can’t be every-, it doesn’t have to be every weekdiscussion given the problems recruiting into rcts, particularly within palliative care and trials of psychological interventions, it is important to explore why trials may underrecruit.our findings suggest that whilst health care professionals felt that research was important, they did not have the time to engage in recruitment, and that trials of medicinal products were often prioritised over trials of psychosocial interventions.as site resources are finite, one way to facilitate recruitment is for university researchers to recruit in clinics.this is noted to increase recruitment in the views on study team vs. in house researchers theme, and our experience in running the cantalk trial suggested that deploying university researchers to recruit in oncology clinics was essential in meeting the trials recruitment targets.this finding is consistent with previous research showing that patients declining to participate in trials of psychological interventions often feel they do not require a psychological intervention [3].additionally, there was the notion that consultants prioritise trials of medicinal products.this bias towards trials of medicinal products, which are perceived to “count more” and provide more reward to the recruitment sites, may partly explain why trials of psychosocial interventions can be particularly problematic to recruit to [23].although the uk clinical research network does not prioritise drug trials, the present research suggests that consultants’ preference for trials which may affect physical outcomes may encourage recruitment efforts to be focused on drug trials.several trials have documented attempts to maximise engagement (e.g. [24]).recommendations based on the present research, a number of recommendations can be made which should be considered during the design, set-up and running of clinical trials of psychosocial interventions, particularly those conducted within advanced cancer populations.this is particularly important given the competitive nature of recruitment, with multiple studies competing for staff time, and especially given that psychosocial research may be “pushed out” in favour of trials of medicinal products.this could be done initially in setup meetings.thirdly, as recruitment is notoriously difficult within palliative care trials and trials of psychosocial interventions, it will be important during trial design to make the eligibility criteria as broad as possible.as this is a relatively low uptake, there may be selection bias, and individuals who participated may have more positive attitudes towards psychosocial research.the interviewers were researchers employed directly on the cantalk trial, and in some instances would have had previous interactions with the interviewees during the course of recruitment.we did not seek the views of patients in the present study, which would provide valuable viewpoints on the topics explored.conclusions the views of those recruiting to randomised controlled trials provide insight into what may block or facilitate recruitment.our findings suggest that whilst health care professionals felt that research was important, they did not have the time to engage in recruitment.
number of words= 1759
[{'rouge-1': {'f': 0.2997938403204139, 'p': 0.8740000000000001,'r': 0.1809271523178808}, 'rouge-2': {'f': 0.1681331602061296, 'p': 0.3591566265060241,'r': 0.10975704030922143}, 'rouge-l': {'f': 0.30409200789660285, 'p': 0.6880555555555556,'r': 0.19517580872011253}}]
-----------------------------------------------------------------------------------------------------------------------------------
p155:
Extractive Summary:
background in the u.s., staff in 15,600 nursing homes (nh) care for about 1.3 million older adults each day [1].in addition to providing housing, three meals a day, and personal care, nhs also provide skilled nursing care, 24-h supervision, and rehabilitation services, such as physical therapy [2].frailty and serious illnesses are common in nhs, where 50% of older adults have dementia and more than 90% require assistance with bathing and other activities of daily living [1, 3].ensuring high quality care for nh residents continues to be a major challenge [4].factors contributing to this challenge include high nh staff turnover, fragmented communication internal and external to nhs, limited resources to pay for clinical staff and technology tools, and the training and education of staff.government regulations and alternative payment models have been important drivers of improved quality in nhs [8].in 1987 the nursing home reform act mandated resident-level care planning in nhs and comprehensive inspection of nhs every 15 months [9].in the early 2000s, market-based reforms, such as publicreporting of nh quality, were implemented to generate demand for nhs with higher publicly-reported quality indicators [10, 11].synthesizing evidence from qi studies is difficult due to variations in terminology, outcomes measurement, and how findings are reported across methodologies [21].the framework for implementation research describes the pathway from clinical interventions, to implementation strategies, and then to service (e.g., safety and equity) and client outcomes [22].often referred to as tools, interventions, or methods, examples of qi strategies include root cause analysis, plan-do-study- act (pdsa) cycles, and others [25].in most qi models (e.g., the improvement model), qi strategies are designed to engage local providers and staff and walk them through a systematic, multi-step approach to developing “fit-for-purpose solutions.”“service outcomes” assess the quality of services, with quality encompassing efficiency, safety, effectiveness, equity, patient-centeredness, and timeliness [28].the adapted framework culminates in changes in “resident outcomes” [22]; in other words, changes in the health and wellbeing of nh residents.applying this adapted framework, the purpose of this study was to conduct a scoping review of published literature on qi in nhs.the intent of the review was to map-out how studies were using, evaluating, and reporting qi strategies and outcomes.we followed the prisma-scr (preferred reporting items for systematic reviews and meta-analysis extension for scoping reviews) [29].data sources and searches we collaborated with a health sciences librarian and conducted a systematic literature search to identify articles relating to qi in nhs.the inclusion criteria were (1) peer-reviewed articles published in the english language between july 2003 and february 2019 (2), used the term “quality improvement” to describe their methods or reported using a quality improvement model (e.g., six sigma) or strategy (e.g., process mapping, pdsa) and (3) reported findings related to impact on either service and/or resident outcomes.data were extracted on study design, study setting and population, problem targeted, solution selected to address problem, qi strategies used, and outcomes (implementation, service, and resident).we developed an initial coding strategy, derived from existing taxonomies and lists of qi and implementation strategies [22, 31, 32] as well as implementation outcomes [22, 33].we then applied and iteratively revised the coding strategy to fully capture data identified in our review.synthesis of results data were entered into a matrix and organized so that publications reporting on a single study were grouped together.we used vote counting to identify the frequency that studies reported each type of qi strategy, implementation outcome, and statistically significant service and/or resident outcomes.in 42 studies (71%), authors reported using a bundle of three qi strategies that included tools/toolkits, in-person training, and technical assistance.other more common implementation outcomes were reach to residents (n = 32), setting adoption (n = 24), and reach to staff [20].comparison of these outcomes across studies was limited by variability in how outcomes were measured.for example, a common pattern of reporting reach to staff and reach to residents was the number of staff trained or residents who received new services, as opposed to the rate that eligible staff were trained or eligible residents received new services.across these 31 studies, 4 studies used randomized and controlled designs and 27 studies (87%) used nonrandomized and controlled designs or non-randomized and non-controlled designs.of these, 33 of 34 studies included tests of statistical significance; 20 of 33 studies (61%) indicated significant improvement in at least one resident outcome.among the 20 studies demonstrating significant improvement, the more commonly improved resident outcomes were pressure ulcers (n = 5), hospital transfers (n = 3), and resident falls (n = 2).discussion in this scoping review of peer-reviewed articles of qi in nhs, we identified patterns in the types of quality problems addressed in nhs, solutions selected to target those problems, qi strategies used to implement solutions, and the impact that solutions and qi strategies had on implementation, service, and client outcomes.another limitation is that terminology is inconsistently applied in the qi literature and this limits efforts to extract data and synthesize findings across studies.further, authors often presented evidence of multiple service and/or resident outcomes; we coded outcomes as effective if evidence that at least one outcome indicated improvement; thus, our findings may over-state study outcomes.the lack of information on solutions limits the ability of others to replicate or compare solutions across studies.this was reflected in our finding that descriptions of clinical solutions and qi strategies frequently were reported together.qi strategies used to implement solutions authors described using an average of 6–7 qi strategies to implement solutions and address clinical problems.the disproportionate focus on qi strategies used by those external to nhs, as compared to those used by staff in nhs, may be an area for improvement.for changes in nhs to be sustained over time, nh staff must be able to engage in qi strategies and continue monitoring a problem and its solution and overcoming barriers over time [43].greater attention to nh internal strategies also has the potential to build capacity of nh staff to apply qi when new problems arise [44].for example, to what extent do nh staff who participate in a qi collaborative complete the recommended internal qi strategies (e.g., conduct pdsa cycles to iteratively develop and test potential solutions)?these findings accord with evidence in reviews of qi studies in other settings [15, 21]; for example, fidelity was described in fewer than half of reports on randomized trials of qi initiatives to improve management of chronic kidney disease [47].for example, how many and what types of nh staff must be reached for qi strategies to improve service and resident outcomes?in this review, exemplars of the practical utility of measuring implementation outcomes included a study of zimmerman and colleagues, who reported a successful qi program in 6 nhs to reduce antibiotic prescribing [37].consistent with prior literature [18], rigorous measurement of implementation outcomes provided essential data to explain the impact of qi strategies on service and resident outcomes.service and resident outcomes a major challenge for studying qi is that the observational design of most studies may not account for factors outside of investigator control that influence the impact of solutions on outcomes; moreover, few are designed with sufficient power to avoid a type i or type ii error [49].thus, findings in this review, which suggest that half of qi studies significantly improved service or resident outcomes, likely include substantial risk of bias.guidelines for reporting implementation strategies could also be applied to qi strategies, including recommendations to report the actor (who enacted the strategy), action (specific activities involved), and action target (the specific barrier or facilitator that the action is intended to change) [51].studies also are needed to characterize the context of care in nhs and describe contextual factors that interact with qi programs and influence outcomes [57], for example, nh administration, organizational structure, health records systems, and coordination with medical staff.
number of words= 1278
[{'rouge-1': {'f': 0.4217682965201585, 'p': 0.8527298050139276,'r': 0.28017202692595367}, 'rouge-2': {'f': 0.21458927034889677, 'p': 0.3716759776536313,'r': 0.15083832335329342}, 'rouge-l': {'f': 0.37661843754065694, 'p': 0.6637500000000001,'r': 0.2628934010152284}}]
-----------------------------------------------------------------------------------------------------------------------------------
p156:
Extractive Summary:
at the system-wide level, scientific associations advocated provision of legal support for the healthcare staff, to incorporate details for monitoring infected healthcare workers into the official records and launched an app for the surveillance of attacks on medical personnel.in summary, the physical and mental health of healthcare workers were pivotal topics that emerged in the interviews with managers and scientific associations that faced covid-19.workforce knowledge, training, and availability participants stressed that the colombian health system had a shortage of specialists, general practitioners, and nurses, among other healthcare workers.in addition to this shortage of professionals, the covid-19 pandemic caused a high inflow of patients due to the fact that the covid-19 virus has a rapid speed of contagion, generating a new challenge: the need to have more human health resources available to care for these patients.in response to this need, hospitals, governmental agencies, and universities collaborated and developed agreements to expand the availability of general practitioners and specialists in the country by graduating both general practitioners and residents early, increasing workforce capacity: “last week in my program six but the country graduated about 25 in total, even so, there are very few intensivists, each intensive care unit should have an intensivist, that is, there are not 7,000 intensivists in colombia.the availability increased to around 1,500 residents” (sh b 013, president).scientific associations, hospitals and academic entities came together with the aim of improving knowledge and skills for the management of patients in intensive care units (icu).system-wide collaboration produced online training, continuing education programs (e.g. managing complex patients in icu, mechanical ventilation, respiratory therapy) to systematise clinical knowledge on managing covid-19 patients: “we would not have been able to do all this without the participation and commitment of scientific associations, universities and providers, who joined an initiative of continuous training […].” (sh-a-023, manager).other academic institutions allowed open access to online training libraries through a national repository (500 to 600 free teaching activities from 55 medical schools in the country), facilitating access to training resources: “from the point of view of human resource training in health, so one of the first decisions we took was to ask all the medical faculties in the country to collect their virtual courses for us to make a national repository.” (sh-b-013, president).in summary, inter-organisational cooperation across the health system underpinned human resource planning, with the aim of enhancing existing health care workers’ knowledge and skills and increasing the availability of human resources to respond to the pandemic.discussion and conclusion since covid-19 emerged, qualitative research on its impact on the health workforce has focused predominantly on health professionals’ experiences on the frontline [3, 9, 11].research has focused attention on the individual experiences of healthcare staff treating covid-19 patients in the early stages of the pandemic.coping and self-care strategies, feelings of growth under pressure, and the simultaneous occurrence of positive and negative emotions of the health care workforce have been documented [21].psychological and/or psychiatric care provided to professionals in hospitals or other healthcare settings are highlighted as priorities for managers [5, 22].authors such as halcomb et al. [23] have reported job security as a concern of the health personnel during the pandemic.little is known about how the organisational context, including managerial decision-making, affects the human resources available to respond to covid-19.for instance, the types of actions taken by hospital managers to retain the workforce [24] in the early stage of the pandemic, how those decisions were made, and their implications for service provision considering the financial restrictions faced by hospitals [21].this paper adds to this area of research by focusing on how organisational level responses to covid-19 affected human resource planning in hospitals in bogotá from the perspective of managers within individual hospitals and representatives of nursing and medical associations at the health system level.the narratives of these actors and how they describe their experiences developing actions to protect the health workforce is scarce in the literature at the timing of writing.our study makes three contributions to an organisational perspective on covid-19.first, it describes processes of responsive planning of health services by hospital managers to meet growing and new types of demand for services.shifting roles and responsibilities of healthcare workers have been exposed in the literature from the perspective of the impact on frontline personnel [25, 26].however, this study highlights that covid-19 also affected the roles of hospital managers as they become more engaged in responsive planning.organisational interventions that emerged from such planning had implications for human resources by shaping the retention, protection, and continuing training of healthcare workers.hospital managers’ emergent roles included allocating staff to new roles and responsibilities (e.g. transfer from elective to emergency care), mitigating uncertainty among staff about the risks associated with an unknown virus, the implementation of new protocols and technologies, making changes in contractual working conditions, securing the availability of ppe, and participating in changes to health workers’ education [2, 3, 27, 28].second, it highlights the challenges faced, and actions taken, by hospital managers to mitigate the effects of responding to covid-19 on the physical and mental health of healthcare workers.hospital managers needed to address not only the physical and psychological safety of the workplace, but also the potential risks to staff outside the workplace linked to negative attitudes towards healthcare professionals.third, the important role of inter-organisational relationships at the local health system level for responding to covid-19 was identified.for instance, scientific associations played a key role in the defence and surveillance of healthcare staff wellbeing, and the types of adaptations the managers undertook to respond.staff training within provider organisations in bogotá was supported by the development of partnerships between hospitals, universities, and scientific associations which produced system-wide online courses, continuing education programs, and shared repositories of training material.with regard to existing research on health system change, the findings show a pace and approach to change not typically associated with the health care sector, which is often characterised as slow to change and subject to challenges concerning collaboration across professional and organisational boundaries [29–31].by contrast, our findings suggested the importance of system-level coordination in response to covid-19 to develop the capacity and training of the workforce and, at the provider level, concern among hospital managers to introduce organisational interventions to improve staff wellbeing.our findings support the concept that covid- 19 gave “permission” to introduce innovations [32]; implementation of workforce change was underpinned by the coordinating role of system-wide actors and managerial focus on working conditions on the front-line of care delivery.the latter has been documented in qualitative research worldwide [33].however, given that covid-19 has represented a pressing and severe threat to life, and health system responses have been subject to intense public and media scrutiny, it is not clear whether these aspects of the motivating context of change associated with covid-19 will be sustained over time or generalisable to other contexts of care.policy and practice implications the novel nature and widespread impact of covid-19 has presented challenges for human resource planning and staff training.presentation of the disease was uncertain and management guidelines were not available which made it necessary to develop training courses on topics related to pathophysiology of the disease and critical management of patients [28, 34].in europe [26], asia [25] and north america [27], health systems have faced common challenges with regard to the process of training undergraduate and graduate students in nursing and medicine.our findings in relation to bogotá point to the importance of facilitating inter-organisational collaborations across the health system to strengthen human resources.clinical training in response to the pandemic in colombia was coordinated through joint planning between hospitals, universities and professional associations.to expand clinical capacity, a favourable measure was the early graduation of trainee doctors which was enabled by the ministry of education and implemented by hospitals and universities.these interventions correlate with similar measures that have been implemented in other countries to continue health workers’ academic training [27, 28].hospital managers played an important role in attending to the psychological impact of covid-19 on healthcare workers, within and beyond the workplace.existing research by tomlin and collaborators propose a model about how health organisations and managers can respond to mitigate the psychological impacts on healthcare workers across the different stages of the pandemic [35].
number of words= 1353
[{'rouge-1': {'f': 0.32440962377475085, 'p': 0.8454237288135593,'r': 0.20071428571428573}, 'rouge-2': {'f': 0.18498302271320818, 'p': 0.38063829787234044,'r': 0.12218012866333096}, 'rouge-l': {'f': 0.2836008426438995, 'p': 0.6017460317460317,'r': 0.18551724137931036}}]
-----------------------------------------------------------------------------------------------------------------------------------
p157:
Extractive Summary:
background knee osteoarthritis is a chronic and age-related condition associated with pain and disability, and around 10 % of men and 13 % of women over the age of 60 suffer its typical symptoms [1].the disease imposes significant physical limitations on the patients and causes a loss of 19 and 34 % of health-related quality of life, respectively, in moderate and severe cases, on average [2].the prevalence of knee osteoarthritis disease in rural and urban areas of iran is 19.3 and 15.3 %, respectively [3, 4], which is more prevalent among women [5].starting in the 1970 s, knee replacement surgery (krs) is an effective and expensive approach for endstage knee arthritis [6].the demand for this treatment method is increasing substantially.the rate of personyear in the united states has more than seven times over the past four decades [7].its population-adjusted rate in iran has doubled in the last five years.increasing population age and obesity are introduced as the most important reasons for performing krs [12].however, given that both the factors do not change instantly at the population level but show significant changes gradually over time, the increased demand for this service over shorter periods could have other structural reasons; technology advances, increasing the level of community health knowledge, increasing the level of community income, changing lifestyles, and increasing access to services due to the entry of more orthopedic surgeons into the health market [13–15].due to the considerable information asymmetry between the surgeon and the patient, the surgeons simultaneously act as suppliers (as service providers) and demanders (as patient agents).this means that surgeons are entirely free to prescribe the type of treatment, providing conditions for an imperfect agency problem.in such cases, the decision to prescribe the service is influenced by the economic motivations of the providers, and the surgeon prioritizes her/his preferences over the patient [16].as such, the question raised by health policymakers is whether the krs prescribed by the surgeons were based on the patient’s needs or not?this question becomes especially acute when the physician to population ratio has increased over time and the physician’s share of patient decreases and naturally reduces his/her income.physician induced demand (pid) is seen to compensate for lost income due to fewer patients.however, this solution is more suitable for diagnostic and elective surgery services [17].the existence of pid in the iranian health system is very likely for structural reasons.physicians have a very high degree of freedom of action in the iranian health system, and regulatory bodies do not effectively monitor their performance.reimbursement to the providers is in the form of fee-for-service and dramatically increases the motivation of providers to deliver more services.also, the physician-population ratio has increased significantly over the past decade [18].various types of health insurance cover more than 90 % of iran’s population.social security organization, health insurance organization, and armed forces insurance, with the coverage rates of 42.87 %, 42.80 %, and 5.00 %, respectively, are the largest iran’s health insurance organizations.the first two insurances are committed to providing services to iranian workers, the underprivileged, government employees, and rural residents, covering 90 % of inpatient services and 70 % of outpatient services in public health facilities.also, the armed forces insurance is committed to providing outpatient and inpatient services to the armed forces and their families, most of whose services are free of charge in government and military centers and offset between 65 and 90 % of privatesector costs [18–20].using unbalanced panel data covering krs services provided by the armed forces insurance organization at the provincial level for 60 months, this study has designed econometric modeling to answer the above question and fill this knowledge gap.methods dataset we use microeconomic data for the monthly average number of krs activities by each orthopedic specialist over 2014 to 2018, compiled from the iranian armed forces insurance organization at the provincial level.the unbalanced individual panel data covers the steadystate 15,729 surgeries performed by 995 surgeons.these suppliers satisfy this condition that began their activity before 2014 and had not been retired during the study time.population data, including population size of people older than 50 years and average monthly income for each (province) and each (month) are extracted from the census results for 2011 and 2016 years.to estimate the population size of the middle years, the constant annual multiplier of 0.021 was used as the average annual growth.the economic theory of supply and demand explains the presence of pid for krs when an increased number of the surgeons decreases the patient’s quota for each the economic theory of supply and demand explains the presence of pid for krs when an increased number of surgeons decreases the patient’s quota for each surgeon, thereby lead to reducing the number of surgeries and his/her income.therefore, within the context of fixed prices, the increase in supply has led to imbalance, and in order to achieve equilibrium, doctors are trying to regain their level of income in two ways; first, increase the number of surgeries, and second, increase the price of each surgery by changing the type of surgery and making it more expensive.ultimately, an increase in supply leads to an increase in demand [17, 21].part of this elevated demand has been increased access to the services by increasing the household’s income, improving the level of health insurance coverage, the emergence of new advanced technologies, raising the level of public health awareness, or increasing the elderly population.however, according to the theory, part of this increase in demand could be due to pid.so, the induced ksr should be measured as the increment in the activities that would not have performed whiteout training and employing more orthopedic surgeons.therefore, in the current study, the changes in the supply of krs services are measured by the density variable (dit ) of orthopedic surgeons, which is equal to the ratio of surgeons to every 100,000 population over 50 years of age.variables following delattre and dormont [22], to estimating the potential pid for krs, we require measuring the activities of the surgeons in the two following ways: nit:the number of performed krs by each physician at province i at month t. this variable can show changes in the number of surgeries over time, but has four main drawbacks; first, it is not able to identify the access effect for cases that had an unmet need for any reason, including the inability to purchase the services, lack of geographical access to the surgeon, lack of knowledge about the existence of treatment and the like.second, it does not show the content of the service.as mentioned earlier, the physician can manipulate the overall cost of the service by changing the type and quality of service, but nit variable cannot measure it.this finding strongly supports the existence of pid for aggressive, costly, and high-risk services such as krs.in other words, a significant part of the increased demand for ksr services was for pid.therefore, health policymakers can minimize this induced demand by setting stricter criteria for krs licensing and requiring physicians to adhere to the relevant clinical guideli
number of words= 1162
[{'rouge-1': {'f': 0.33162758484761345, 'p': 0.8155357142857143,'r': 0.2081306865177833}, 'rouge-2': {'f': 0.20232582442443167, 'p': 0.4152914798206278,'r': 0.13374172185430466}, 'rouge-l': {'f': 0.3406819206936003, 'p': 0.6612408759124089,'r': 0.2294488188976378}}]
-----------------------------------------------------------------------------------------------------------------------------------
p158:
Extractive Summary:
these factors did not seem to influence future health care behavior while in transit, suggesting phenomena subjected to change with time and context.also, we find an increase in gp and ec use after resettlement and a decrease in outpatient/specialist care while hospitalization rates do not change pre- and post-resettlement, probably mirroring the health care system in the country of stay at each period.use of gp services more than doubled pre- and postresettlement.this rate (85 %) is slightly higher than that of the resettlement country population in norway, where 75 % reported use of gp in the last 12 months in population-based data [24].comparing numbers between surveys is encumbered with uncertainties, but we believe some of the differences in gp utilization between our sample and the resettlement country population can be explained by the fact that in some norwegian municipalities, the general health assessment upon arrival is performed by a gp.despite having a separate question for the general health assessment, we assume some participants might have had difficulties distinguishing between the two alternatives as both entails contact with a primary care doctor.another possibility is that some refugees were derived to a second visit by the gp at the first encounter for their general health assessment.in any case, it is important to acknowledge the key opportunity gps have in responding to the need of the refugee patient in early resettlement as the first point of contact.furthermore, we found an increase in ec use from 10 % before arrival to 16 % after resettlement, which is similar to the utilization rates of the resettlement country population [27].on the other hand, the use of outpatient/specialist care dropped from 32 % in lebanon to 16 % in norway.this decrease might be explained at the system level, since outpatient/specialist care services in norway require a referral, usually from a gp, while other routes are available to access such care in lebanon given a highly privatized health sector.when comparing with population-based data from the norwegian population, 36 % reported having had contact with outpatient/specialist care the last 12 months [24].a number twice as high as that of our population, but not adjusted for morbidity, so potential under-or overuse is not possible to determine with certainty.furthermore, some of our respondents might have been referred by their gps to secondary care, but still waiting for their appointments with a specialist at the time of the follow-up survey.however, the doctor-patient interaction is key in identifying patients needing a referral [28].previous research has shown that not speaking the same language is associated with decreased symptom reporting, fewer referrals to specialist care [29] and shorter consultation time [30], which also could explain our results.an inverse socioeconomic gradient in terms of utilization of outpatient/ specialist care has also been documented in norway [31] that confirms privileged groups are those that avail most of services [32].however, utilization of gp and hospital admissions, which is easier to access, was found to be equitable [33].similarly, a systematic review across europe showed that outpatient visits for specialized care were generally used less often by migrants [34].in our sample, hospital admissions did not change pre- and post-resettlement, which could point to hospital admissions having similar access thresholds across countries.finding pre-migration predictors for use of health care in norway can be of key importance to adequately prepare health services to the new migrant population.one novel finding in this study is that not having a residence permit in the transit country and having poor social relationships in transit was associated with higher use of emergency care the first year after resettlement.generally, the lack of recognized documentation in a country complicates the availability of healthcare and one can assume that acute and/or chronic diseases left uncared for contribute to higher use of care post-resettlement.hence, securing minimum acceptable living conditions for refugees in transit countries should be a priority concern globally.the strongest correlation we found was the one between poor srh and health care utilization, signifying the concordance between need for care and use of care.perceived poor health status seems to be a stable factor as it holds for both pre-arrival health status and after resettlement, even though the association after resettlement is stronger.while the association between health need and health care utilization is well-known [35], our study highlights the stability of this association along the migration trajectory.post-migration, we found associations between use of services and higher health literacy, higher education, higher social support (essi), and low levels of qol.high health literacy drops from 56 % in lebanon to only 7 % in norway, pointing to challenges with a new language and a different health care system, while high social support (essi) somewhat unexpectedly increases from 35 % in lebanon to 60 % in norway.this increase might be explained by the fact that most quota refugees are resettled as families and some are re-united with extended family members preceding them to the resettlement country.easier access to online communication and established support networks upon arrival can also explain this increase.why persons with high health literacy, higher education and high social support have increased probability of use while in norway but not in transit is difficult to answer but we assume these factors become more important in a context where there is universal health coverage, and no economic barriers to health care.for the concept of social support and social relationships, we found associations pointing in opposite directions.poor social relationships measured with whoqol-bref while in lebanon was associated with use of ec after arrival.when in norway, high social support (essi) was associated with use of ec, outpatient/ specialist care and hospitalizations.likewise, poor social relationships (whoqol-bref) were associated with outpatient/specialist care and hospitalizations.we believe some of this can be explained by measurement differences in social support instruments, not capturing the exact same phenomenon.the social relationships domain in whoqol-bref as part of qol only consists of three questions (satisfaction with relationships, satisfaction with support from friends and satisfaction with sexual relationships) and has the concept of satisfaction in it while essi consist of 7 questions and asks directly if you have someone available to talk to, receive advice, emotional support, receive help with daily chores etc. without assessing satisfaction.the environmental domain of qol describes feeling of safety, satisfaction of living place, enough money to meet needs, and satisfaction with transportation.interestingly, we found that higher scores in this domain were associated with use of a gp.this also confirms the inverse care law [32].strengths and limitations working with a cohort with similar background arriving at the same time minimizing influence of contextual factors as well as a high response rate and the use of validated instruments add to the strengths of this study.however, certain limitations need to be considered when interpreting our data.we did not assess frequencies of contact with the healthcare services, only yes/no for use at least once.because of this we are not able to separate between frequent users and persons who have only used the service once.this study has an explorative nature with a high number of statistical tests, which increases the risk of type 1 error.
number of words= 1171
[{'rouge-1': {'f': 0.34769413703509733, 'p': 0.7597810218978103,'r': 0.22542763157894738}, 'rouge-2': {'f': 0.1763194122828525, 'p': 0.3080952380952381,'r': 0.12349794238683129}, 'rouge-l': {'f': 0.33262171265155666, 'p': 0.5796774193548386,'r': 0.2332231404958678}}]
-----------------------------------------------------------------------------------------------------------------------------------
p159:
Extractive Summary:
nevertheless, our study showed that a large group of patients felt that their treating physician did not know what mattered most to them at that moment.these stories show the person behind the patient.– the questions “what matters most to you?and especially “why does this matters most?introduction effective patient-doctor communication and patient involvement can lead to increased patient satisfaction, better health outcomes, and is essential to the delivery of patient-centred care [1, 2].as a result, they may not have adequate psychological and emotional insights into the patients’ priorities [5, 6].research shows that many clinicians’ conversations are about patients and not with them [7], and that patients are seen as their disease(s) rather than as individuals [6].the goal of patient-centred care is to customize care to the individual patient, taking into consideration their preferences, needs and values.to achieve this, barry and edgman-levitan (2012) proposed asking the patient “what matters to you?”, in addition to “what is the matter?”[8].this topic has received increasing attention over the years, and an annual international “what matters to you?the institute for healthcare improvement (ihi) states that the “what matters to you?”question is a quick, simple, but yet profound way to start deep and personal conversations with patients [10].it encompasses discussing the patients’ priorities and values alongside potentially revealing unanswered questions, which could provide input for a personalized care plan [11].however, little is known about what is most important to the heterogeneous group of patients (with regards to morbidity, basic characteristics, culture, health and socio-economic status) during the acute phase of a hospital admission.the first 24 h of an acute admission will often determine the course of the hospital stay.in this phase many diagnostic tests are carried out, care plans are created, and key decisions made.it is crucial that during this time-period the priorities of the patient are clear to the healthcare team [12].as such, the secondary objective of this study was to assess the patient perspective on whether they felt their doctor knew what mattered most to them.patients in 66 hospitals were recruited simultaneously in the netherlands, united kingdom, ireland, denmark, switzerland, hong kong and singapore.questionnaire in the questionnaire we used the classic ‘what matters to you?the questionnaire was complemented by questions concerning basic characteristics, living conditions, social and work situation.to find out how patients interpreted all questions, we used a cognitive interviewing style [31] during the pilot (e.g. by asking their opinion about the content and relevance of questions).all questionnaires were available in each country’s local language.each interview took approximately 5 min.data were collected at the bedside, and either entered directly into the digital database or transcribed from a paper datasheet, without the use of audio or visual recordings.patients’ responses were not recorded verbatim, but paraphrased by the interviewer.all interviewers had their own personal castor edc account for data input and were trained by both video tutorials and written instructions.measures and warnings were built into the database to minimize the potential for errors.interviewers transcribed the patient’s answers into the castor edc database.the key list with record numbers could only be accessed by the local coordinating researcher.no directly identifiable data were entered into the database.to analyse the large number of open-text answers, a framework needed to be developed that could be used for coding both the answers to the ‘what matters most?’ and ‘why?an inductive approach of content analysis was used to identify categories and sub-categories in the data, leading to the development of a conceptual framework on what matters most to acutely admitted patients and why [32].coding and data analysis all 3700 answers (100% of data) to the ‘what matters most?’ and ‘why?’ questions to identify patterns.’ question (for example; patients often wanted to go home because they missed family members).descriptive statistics were performed with spss for windows, version 24 (spss inc).results during the inclusion period, 2798 patients had been admitted to the participating units for 24 h or less, and were therefore eligible for inclusion.figure 1 provides an overview of the inclusion process and numbers of included patients per country.the coding framework included twelve categories (health, getting home, symptom relief, functioning, medical issues, hospital experience, patient values, reassurance, possessions, emotions, urgency, and other).” (m, 61- 65y, united kingdom) other patients wanted to get better to get back to their normal life: “that i will be able to do everything i feel like again.getting home most patients mentioned the familiarity of the home situation, their role as an informal caregiver or relationships as the main reason to strive for a return to home.patients wanted reassurance and felt having a diagnosis would make them function better psychologically.more than half of all patients (51.9%) felt their treating doctor did not know what mattered to them most.in asian countries we found a relatively high percentage of patients mentioning getting better/ good health as being most important (47.8–65.8% in asian countries, 18.3–39.0% in western countries).patients in singapore mentioned their work as the reason why things mattered more often than patients in other countries (17.1% and ≤ 7.2% respectively) (supplementary material: table s5).this is in line with the main function of an acute hospital admission and the motivation and focus of clinicians: diagnosing, treating and timely discharge [33].however, when asked why they answered the way they did, patients provided more personal answers, often mentioning relationships and psychological well-being.this probably reflects the heterogeneity of acutely admitted patients with regards to morbidity, baseline characteristics, culture, health, socioeconomic status and phases of their lives.and why? were more common than others, and some categories were mentioned more frequently within certain subgroups of patients, individual priorities are not predictable.a large group of patients felt the treating doctor was unaware of what mattered most to them, partly because it did not come up during the consultation.doctorpatient communication is crucial to the doctor-patient relationship [37], and essential in delivering high quality care, since the priorities of doctors and patients can differ [38].however, since the data represents the perception of patients, it is also possible that doctors do know what matters most, without the patient consciously realizing this.as the feeling of being heard and understood is essential in the process of patient-centred decision-making [39], it is recommended to have explicit conversations about what matters most and why, even if the doctor believes they already know this.strengths and limitations the flash mob research design enabled us to include many patients within a short timeframe in seven different countries and 66 hospitals, across cities, towns and rural areas.firstly, answers from patients might have been paraphrased, which may have simplified patient answers.when asking for the patient perspective, a large group of patients felt the treating doctor did not know what mattered to them.
number of words= 1101
[{'rouge-1': {'f': 0.4008395070938762, 'p': 0.8208305647840533,'r': 0.26516407599309155}, 'rouge-2': {'f': 0.22684716808598682, 'p': 0.4066666666666667,'r': 0.15729472774416595}, 'rouge-l': {'f': 0.3480032140275905, 'p': 0.632874251497006,'r': 0.23998191681735986}}]
-----------------------------------------------------------------------------------------------------------------------------------
p160:
Extractive Summary:
[3, 4], is associated with high case fatality rate and leads to substantial increase in the demand for hospital beds and shortage of medical equipment.to the best of our knowledge, no prognostic models have been developed taking into account a homogeneous time point across patients, such as the onset of symptoms, for the assessment of disease progression.the hosmer–lemeshow test was applied to assess the goodness-of-fit.all statistical analyses were carried out using international business machines (ibm) statistical package for the social sciences (spss) 21 (ibm corporation: armonk, ny, 10504).role of the funding source the funders of the study did not have any role in data collection and in study design, data analysis, data interpretation, or writing of the report.the corresponding author had full access to all data in the study and had final responsibility for the decision to submit for publication.one hundred and ninety patients (81%) suffered from at least one underlying illness, most frequently cardiovascular disease (47%), neurological/psychiatric disorders (35%), and diabetes (21%).cough and dyspnoea were reported in 106 (45%) and 87 (37%) cases, respectively.the mean time from onset of symptoms and admission to hospital was 3.6 days (sd 3.2), while mean length of hospital stay was 13.2 days (sd 11.5).in 26 (11%) cases non-invasive ventilation was required, while 30 (13%) patients underwent invasive mechanical ventilation.eighty-eight (37%) patients experienced poor outcome, e.g. pao2/ fio2 ratio < 200 (37 patients, 42%), transfer to icu (30 patients, 34%), and death (47 patients, 53%) by day-11 after symptoms onset.blood test performed by day-5 of symptoms onset showed that mean values of crp, nlr, ldh and ck were significantly higher compared with patients not experiencing progression of covid-19 (fig. 1).after applying the cut-offs set at the best point of sensitivity and specificity identified through the roc curve, an association between disease progression and the above mentioned variables showing values above the cut-offs was observed (crp: or 7.38; 95% ci 3.83–14.23, p < 0.000; nlr: or 3.43; 95% ci 1.77–6.64, p < 0.000; ldh: or 6.70; 95% ci 2.14–21.04, p = 0.001; and ck: or 3.83; 95% ci 1.56–9.39, p = 0.003) (table 2).the roc curve for covid-19 progression is displayed in fig. 2.the model including age > 65 years, male sex, cardiovascular disease, dyspnoea and at least three abnormal blood parameters among crp (> 80 u/l), alt (> 40 u/l), nlr (> 4.5), ldh (> 250 u/l), and ck (> 80 u/l) shows fair discrimination ability (auc 0.73).to our knowledge, this is the first study assessing predictors of disease progression at specific time-points starting from the onset of covid- 19 symptoms.a systematic review of ten prognostic models revealed that the most reported predictors of disease progression and mortality were age, sex, crp, ldh and lymphocyte count [9].recently, the predictive value of nlr measured at hospital admission has been assessed in a prospective cohort, showing a high value in predicting disease deterioration, shock and death (all the areas under the curve > 0.80) [13].in our cohort, more than one-third (37%) of patients experienced disease progression by day-11 of symptoms onset.descriptive studies on clinical course of covid- 19 revealed that the median time from onset of illness to acute respiratory distress syndrome and to icu admission was 8–12 days and 9.5–12 days, respectively [14].older age and the presence of cardiovascular comorbidities were associated with a higher risk of unfavourable outcome, as highlighted by other studies [7, 15, 16].comorbidities, secondary bacterial infections, and altered cellular and humoral immune functions are more common in the elderly, thus increasing the risk of developing severe illness.a meta-analysis of 51 studies including a total of over 48,000 patients with confirmed covid-19 infection showed that fatal outcomes with covid-19 infection were strongly associated with diabetes, hypertension and cardiovascular diseases across all age groups, thus suggesting that the risk of poor outcome associated with cardiovascular diseases may be not affected by age [17].sex-related differences in immune system responses to pathogens has been observed, with female eliciting higher immune responses.in particular, poor outcome was associated with higher nlr, crp, ldh, and ck, according with several studies [12, 19].lymphopenia and elevated neutrophil count suggest an alteration of lymphocyte function and are associated with elevated secretion of il-6 and tnfalpha, since these inflammatory markers contribute to lymphocyte apoptosis as well as decreased proliferation of lymphocytes [20].several mechanisms are responsible of this alteration, including direct infection of the lymphocytes with sars-cov-2 virus, causing lymphocyte death or dysfunction.among a list of criteria, including age, body mass index, comorbidities, findings at chest x-ray, crp, and duration of symptoms among others, the highest weights were attributed to pao2 and peripheral oxygen saturation, denoting the well-known central role of respiratory findings in the assessment of the risk of rapid deterioration of covid- 19 patients.
number of words= 783
[{'rouge-1': {'f': 0.4408577824238845, 'p': 0.7189028213166144,'r': 0.3179041916167665}, 'rouge-2': {'f': 0.23835698196865523, 'p': 0.3561635220125786,'r': 0.1791127098321343}, 'rouge-l': {'f': 0.39845156278786137, 'p': 0.6019148936170213,'r': 0.2977904328018223}}]
-----------------------------------------------------------------------------------------------------------------------------------
p161:
Extractive Summary:
background dengue fever (df) is the most prevalent arboviral disease worldwide [1].dengue virus (denv) is transmitted to human through the bite of infected mosquito vectors of aedes genus [2].more than the third of the world’s population is at risk of denv infection [1].over the past 50 years, denv incidence has increased 30-fold, associated with continued geographic expansion [3].approximately 3.6 billion people are estimated to be at risk of dengue infections worldwide [4], apparent cases overally range from 50 to 100 million per year [5]; fatality is estimated in 10,000 deaths per year [6].the most affected areas are america, south-east asia and regions of western pacific [2].in africa denv is known to circulate since the nineteenth century but due to the lack of diagnostic tools and effective surveillance, the real burden is likely to be underestimated [3, 7].denv infection can cause a variety of clinical manifestations ranging from self-limited form referred as df to life threatening disease known as severe dengue [2].the causative agent of dengue fever, is an enveloped, positive, single stranded rna virus belonging to the flaviviridae family, flavivirus genus [8].its 11 kb genome contains a single open reading frame encodes three structural proteins [capsid (c), membrane (m) and envelope (e)] and seven non-structural (ns) proteins (ns1, ns2a, ns2b, ns3, ns4a, ns4b and ns5) [8].denv is classified into four antigenically and genetically distinct serotypes sharing around 65% of genome similarity namely denv-1, denv-2, denv-3 and denv-4 circulating worldwide [9].denv-1 presents five genotypes (i, ii, iii, iv, and v); denv-2 is divided in six genotypes (asian i, asian ii, cosmopolitan, american, american/asian and sylvatic); four genotypes were identified for denv-3 (i, ii, iii, and v) and denv-4 (i, ii, iii, sylvatic) [10].genotypes are defined as strains having up to 6% divergence at a nucleotide level [10, 11].for denv serotyping and genotyping, several genomic regions such as the envelope (e), e-ns1 junction and capsid pre-membrane (c-prm) have been widely used [12].genotyping using a single primer set, for both amplification and sequencing, targeting the cprm gene is preferred as it is faster and cost effective [12, 13].however, due to the absence of effective antiviral therapy, a safe vaccine that can induce efficient and balanced immune response against all different denv serotypes/ genotypes is urgently needed [14].genetic diversity among serotypes and genotypes can hamper vaccine development [11].it is crucial to perform the surveillance of circulating genotypes in a given area to guide the choice of appropriate prophylactic measures before the implementation of any vaccination trial.despite the fact that denv and its vector are known to be present in subsaharan africa, only few studies have described circulating serotypes and/or genotypes [17–20].several studies focusing on the genetic diversity of circulating denv were undertaken in colombia [21], nigeria [19] and india [22].in senegal, the first dengue infection was reported in 1970 [23].since then, many outbreaks and sporadic cases were reported [24–26].between 2017 and 2018, an unprecedented number of cases associated to this viruses (denv 1–3) occurred in senegal [27, 28].despite this growing incidence, the circulating serotypes and genotypes, and their distribution across the country are still unknown.to address this gap on knowledge, we carried out a retrospective study to identify circulating denv serotypes and genotypes among strains collected between 2017 and 2018 across the senegal using qrt-pcr, sequencing and phylogenetic analysis.we also determined the spatial and temporal patterns of serotypes/genotypes around the country.material and methods study design, settings: presentation of syndromic sentinel surveillance network in senegal (4s network) senegal, a sub-saharan africa country, has long standing febrile illnesses surveillance system.through a partnership between the senegalese ministry of health, the who country office and the institut pasteur de dakar (ipd) which hosts the who collaborating center for arboviruses whocc for arboviruses and the national influenza center [29, 30] a febrile illnesses surveillance network was established.this system, initially limited to virological surveillance of influenza like illnesses (ili), was reviewed through the establishment of a new surveillance network, based on a syndromic approach based on fever, called senegalese syndromic sentinel surveillance network or 4s network.the 4s network is responsible of the surveillance of febrile illnesses with 20 sentinel sites distributed in the 14 senegal’s administrative regions [30].these sentinel sites conduct population-based surveillance for influenza-like illnesses and other public health priority syndromes (malaria, dengue-like syndromes and diarrheal syndromes).outpatient visits are distributed geographically in diverse areas across the country (fig. 1).denv isolates were primarily derived from human samples collected between 2017 and 2018 in community healthcare centers that are part of the 4s network.sera samples and data collection from january 2017 to december 2018, in collaboration with the healthcare professionals at different sentinel sites in senegal, patients with history of fever lasting 2 to 7 days along with one or more of the following symptoms: headache, myalgia, arthralgia, ocular pain, generalized fatigue, cough, nausea, vomiting, sore throat, rhinorrhea, difficulty breathing, diarrhea, or bleeding were offered the opportunity to enrol in the study.after informed consent was obtained, 5 ml of venous blood was collected.for each patient, standardized interview form was fulfilled containing both clinical and demographic data.the collected samples were stored at 4 °c before shipment to the whocc located at ipd.samples were shipped on a weekly basis.finally, processed isolates during this study were derived from sera samples collected in a single site at a given area, our result may not be representative of the real dengue burden and serotype distribution profile within the region.early management of dengue positive cases is key to reduce fatal outcomes, timely serotyping can provide early warning of dengue epidemics to improve management of patients and outbreaks [53].our aim is to adapt and evaluate the tib-molbiol, into flexible and portable qrt-pcr devices, as useful tool for denv detection/ serotyping near to the points of needs.this work will constitute a reference for future studies on denv disease, dynamics of circulating strains and their impact on the virus epidemiology in senegal.conclusion in summary, the present study describes the genetic diversity of dengue virus in senegal between 2017 and 2018 at the serotype and genotype level using isolates available at the whocc and sequences from senegal covering the study period available on genbank.overall, our results show a circulation of three dengue virus serotypes belonging to three genotypes during the study period and show a spatial distribution pattern of different serotypes marked by localization of serotype 2 isolates mainly in the north of the country, denv-1 in louga, fatick and thiès and a widespread of denv-3 around the country.however, the iceberg effect for denv infection is well-known; cases reported to the surveillance system only represent a small proportion of total infections.to assess the true incidence and evaluate the level of herd immunity, a nationwide seroprevalence study is urgently need
number of words= 1108
[{'rouge-1': {'f': 0.32884494646621126, 'p': 0.7639655172413793,'r': 0.20951473136915078}, 'rouge-2': {'f': 0.14961933436334907, 'p': 0.25181818181818183,'r': 0.10642671292281006}, 'rouge-l': {'f': 0.3028881281951628, 'p': 0.6154545454545455,'r': 0.20087248322147652}}]
-----------------------------------------------------------------------------------------------------------------------------------
p162:
Extractive Summary:
the differential diagnosis between neuritis and neuropathic pain is critical because these involve distinct pathological processes requiring different treatments [4].misdiagnosis tends to result in prescription errors, as has been frequently identified by specialists of reference centers receiving patients forwarded from primary health care units.neuropathic pain, a nonexclusive condition of leprosy, begins because of abnormal functioning of the peripheral and central nervous systems.neuritis and neuropathic pain are commonly confused with each other.in leprosy, the frequency of neuropathic pain ranges from 11.3 to 70.3% [12, 17, 20, 27].besides this, there are no laboratory or neurophysiological tools able to identify neuritis [2].therefore, the purpose of this work was to define a simple tool to diagnose neuritis in leprosy patients and identify important clinical predictors to assist this.the present study evaluated clinical predictors used to differentiate neuritis from neuropathic pain in leprosy neuropathy patients via a standardized form that included subjective questions and physical examination data.few health units employ specialists in neurology or have resources for electrophysiological evaluations that could facilitate diagnosing patients with chronic neuropathic pain.eligibility criteria included leprosy patients with neural pain in any limb, except for cranial nerves, at least 18 years of age, fluency in the portuguese language, and ability to understand questions during anamnesis.exclusion criteria were the presence of other neuropathic pain etiologies such as human immunodeficiency virus infection and chronic diseases; pain syndromes like complex regional syndromes, fibromyalgia, painful ulcerations, radiculopathy, joint pain, and tendonitis; and patients that had been undergoing corticosteroid therapy (more than 10 mg of prednisone or equivalence).clinical history and examination after obtaining the written informed consent of the participants, information regarding sex, age, educational level, and the world health organization (who) leprosy classification were recorded.neurological evaluations focusing on the peripheral nerves were performed and patients with more than one affected limb were asked to answer separately to each of them.thermal sensation was determined by the use of cold (15 °c) metal objects; and a safety pin was utilized to ascertain pain perception in the trigeminal, ulnar, median, radial, sural, superficial fibular, and plantar bilateral nerves.lastly, the medical research council (mrc) scale was adopted to determine individual muscle strength in the upper and lower limbs; and tendon reflexes were tested using taylor’s hammer [13].standardized form a standardized form was developed (table 1) consisting of the following items: date of onset of pain, sensory and motor symptoms, worsening factors, irradiation signs, and pain triggered by nerve palpation, whether declared spontaneously or after questioning [6, 9, 27].electrophysiological examination nerve conduction was verified in the painful limbs of all participants.parameters were measured by way of the neuropack μ meb 9100 ep/emg measuring system (nihon kohden corp., tokyo, japan).skin temperatures were taken at the wrists and ankles and maintained above 33 °c in room temperatures ranging from 29 to 32 °c. standard methods were performed according to delisa, 1994 [5].demyelinating signs are defined as such when there is a latency prolongation of the compound muscle action potential (cmap) and/or sensory nerve action potential (snap); reduction of the motor or muscular conduction velocity to below 85% of the lower limit of normality circumventing the cmap; or a snap amplitude drop by up to 30% of the lower limit [1, 7, 13].despite the paucity of data in the medical literature regarding a gold standard to define neuritis, a satisfactory response of up to one month of corticoid therapy was used for diagnostic confirmation.statistical analysis the data were analyzed via spss, version 16 [26].the sample demographic and clinical features were described using median and interquartile interval for both nonparametric quantitative variables and frequencies as well as proportions of the categorical variables.pearson chi- square and fisher’s exact test were adopted to assess the association between demographic and clinical predictors.the minimum estimated sample size was 57 neural pain evaluations to obtain 80% of sensitivity and 34 for 90% of specificity, considering an acceptable difference of 10% and a 95% confidence level in an estimated population of 750 patients with neural pain.for each algorithm, indicators of test accuracy were estimated with 95% confidence intervals.fifty-one were in already in treatment for neuropathic pain, 15 for neuritis, and 22 were in corticosteroid therapy.significant differences were found between the diagnostic groups (“neuropathic pain” and “neuritis”) with regards to when the pain began; if the pain had lasted up to 90 days (p = 0.031); if movement was a worsening pain factor (p = 0.042); and if the pain was triggered by nerve palpation (p = 0.039).during case definition, defining the diagnostic criteria for neuritis was more difficult than for neuropathic pain [11].when a negative result is obtained, individuals without neuritis are correctly identified, enabling the appropriate medications to be prescribed.
number of words= 772
[{'rouge-1': {'f': 0.34448508444898207, 'p': 0.6872248803827752,'r': 0.22985130111524163}, 'rouge-2': {'f': 0.14724354205532603, 'p': 0.22384615384615386,'r': 0.10970223325062035}, 'rouge-l': {'f': 0.2951220283035432, 'p': 0.5128571428571429,'r': 0.20716814159292035}}]
-----------------------------------------------------------------------------------------------------------------------------------
p163:
Extractive Summary:
one of the most important mechanisms of antibiotic resistance in a. baumannii is the production of β-lactamase enzymes, the genes of which are usually carried on mobile genetic elements, including integrons [7].beta-lactamases are grouped into four classes based on the amino acid sequence, including: a, b, c, and d [8].oxa β-lactamases or oxa-type carbapenemases, include distinct subgroups from which oxa-23-like, oxa-24-like, oxa-40-like, oxa-51-like, oxa-58-like and oxa-143-like have been found in a. baumannii strains [9, 10].[15].acinetobacter has a high potential for the acquisition of resistance genes through mobile genetic elements, including integrons [15].methods ethical consideration informed consent was obtained from all subjects, and all methods were carried out in accordance with the relevant guidelines and regulations of ethics clearance committee of the alborz university of medical sciences.standard biochemical tests were used to identify the gram-negative bacilli isolates as a. baumannii strains these tests included catalase, oxidase, o/f test, motility, citrate utilization test and growth on tsi agar (merck, germany) [18].the a. baumannii atcc 19606 was used as a control strain.antibiotic susceptibility testing susceptibility of a. baumannii to imipenem (10 μg), gentamicin (10 μg), ciprofloxacin (5 μg), ampicillinsulbactam (20 μg), trimethoprim/sulfamethaxazole (1.25/23.75 μg), ceftazidime (30 μg), doxycycline (30 μg), and minocycline (30 μg) was performed by disk diffusion method accordant with clsi standard guidelines [20].the mics of imipenem [breakpoints (μg/ml): susceptible: ≤ 2; intermediate: 4; resistant: ≥ 8], and colistin [breakpoints (μg/ml): susceptible: ≤ 2; intermediate: -; resistant: ≥ 4] were determined by the broth microdilution method according to the guidelines of the clsi [20].the quality control strain was escherichia coli atcc 25922.determination of integrons and associated gene cassettes extraction of genomic dna of a. baumannii strains was performed using boiling method [22].amplified gene cassettes were sent for sequencing (macrogen research, seoul, korea).the sequences obtained were compared with those deposited in the ncbi database with using blast program (http:// blast.ncbi.interpretation of the results of statistical analysis showed there was no significant association between the presence of class 1 integrons and age groups (p = 0.55), and burn size (p = 0.52).according to the literature, a wide distribution of vim type metallo-β-lactamase has been reported at middle east crab [22].however, our findings showed that the blaimp and blavim genes were not located on the class 1 integrons and no association was found between mbl genes and the presence of class 1 integrons among studied isolates.considering that strains with empty integrons have the potential to capture cassettes carrying resistance genes, this result could be remarkable.the most common integron cassettes identified among integron-positive a. baumannii strains were arr2, cmla5, qace1, which encode rifampin adp -ribosyltransferase, chloramphenicol transporter and quaternary ammonium resistance protein leading to resistance to rifampicin, chloramphenicol and quaternary ammonium compounds, respectively.rifampin resistance due to the arr-2 gene carried by class 1 integrons has been documented in a. baumannii strains [28].in researches conducted in other parts of the world, including taiwan, mainland china, and france, different variants of the aada gene such as aada1, aada2 and aada6 have been identified in multidrug-resistant a. baumannii strains [23, 24, 29].despite the high resistance to antibiotics such as ceftazidime, ciprofloxacin, and trimethoprim/sulfamethaxazole, no gene cassette encoding resistance to these antibiotics was found in the present study.more information about resistant bacteria helps us to respond to how bacteria spread throughout clinical settings.part of the limitations of the current study is also due to limited resources.in addition to class 1, detection of gene cassette arrays associated with other integron classes could level up the study.seemingly, the data obtained in the present study may provide a basis for future studies and assess the trend of infection generated by a. baumannii in burn patients in our region.in addition, most of the vim type mbl-positive strains carried class 1 integrons.
number of words= 618
[{'rouge-1': {'f': 0.4849994416960787, 'p': 0.7378445229681978,'r': 0.36121725731895227}, 'rouge-2': {'f': 0.22832067385478033, 'p': 0.3182269503546099,'r': 0.17802469135802468}, 'rouge-l': {'f': 0.38402265787534084, 'p': 0.5836986301369864,'r': 0.28613832853025933}}]
-----------------------------------------------------------------------------------------------------------------------------------
p164:
Extractive Summary:
background bartonella henselae is a bacterium responsible for cat scratch disease (csd), a zoonotic infectious disease usually transmitted to human by bites or scratches of domestic cat, its natural reservoir.atypical clinical course in csd occurs in a minority of cases (5–14% of csd) [1].among these atypical csd, neurological manifestations associated with b. henselae are scarce, consisting mostly in neuroretinitis, meningoencephalitis and myelitis [2].to our knowledge, cerebral epidural empyema has never been described in the literature.we report here the case of a cerebral epidural empyema due to b. henselae diagnosed incidentally by 16s rdna gene sequencing on the abscess fluid.case presentation an adult patient visited his general practitioner for a five-day history of high fever, diffuse abdominal pain and myalgia.symptoms first disappeared and relapsed a few days later with the onset of gradual worsening headaches.a corticosteroid therapy was initiated considering sinusitis as a possible cause of the headaches, resulting in a complete resolution of all symptoms.after one week of treatment interruption, the headaches resumed and prompted the patient’s admission to the hospital.he had no past medical history except for severe acne, tonsillectomy and surgery for a sinus polyp.the initial prescription of corticosteroids without antibiotics could also contribute to the intracranial extension of the infection.the development of the granulomatous lesion in the epidural space was thus associated with the multiplication of b. henselae.because b. henselae is a fastidious bacterium that requires specific laboratory conditions, conventional growth techniques performed on cerebral samples may lack sensitivity for this pathogen.overall, 7 to 53% of cultures of cerebral suppuration remain negative with conventional techniques.because streptococci and anaerobes are the most commonly isolated microorganisms from empyema material and because most negative cultures are related to previous antibiotic treatment [14], the use of complex additional techniques may have been considered as futile.recently, the causative bacteria involved in central nervous system (cns) infections have been studied through a systematic comparison between 16s-rdna-based next-generation sequencing and conventional techniques [15].molecular assays were able to identify a larger number of bacterial taxa compared to culture with the identification of several species infrequently recorded in these infections.therefore, the absence of identification of infective microorganisms in a cns infection requires an extensive investigation and the use of additional molecular-based techniques.most patients with csd lymphadenitis experience a spontaneous resolution of their symptoms without specific antibiotic therapy.the fact that the antibiotic treatment may shorten the duration of symptoms or decrease the risk of systemic disease is debated in the literature [16, 17].however, in a large cohort of 268 patients with csd lymphadenitis, antibiotic treatment was associated with a median duration of symptoms of 2.8 weeks compared with 14.5 weeks in patients without antibiotherapy [17].limited data are available for more severe forms of the disease but a combination of active agents against b. henselae is commonly used with a favorable outcome after a 4-week regimen [18].a combination of doxycycline and rifampicin [2, 7, 9, 19] or azithromycin and rifampicin has been proposed for the management of cns infections associated with b. henselae, in particular for encephalitis [7].because of their specific pathophysiology, intracranial epidural abscesses may be easier to treat than cerebral abscesses or encephalitis with the specific impact of surgical drainage.in our case, the evolution was favorable with surgical drainage and a one-month course of doxycycline.this case report describes a first case of b. henselae cerebral epidural empyema.this emphasizes the potential unusual clinical presentation of b. henselae infection, which has been treated here by surgical drainage and appropriate antibiotic treatment.furthermore, in cns infections with negative microbiological culture results, the use of additional molecular techniques seems essential to the microbiological diagnos
number of words= 593
[{'rouge-1': {'f': 0.35661224194263685, 'p': 0.7842857142857143,'r': 0.23077170418006432}, 'rouge-2': {'f': 0.20922539346729563, 'p': 0.3937410071942446,'r': 0.14246376811594202}, 'rouge-l': {'f': 0.40202910266774455, 'p': 0.7176190476190476,'r': 0.2792307692307692}}]
-----------------------------------------------------------------------------------------------------------------------------------
p165:
Extractive Summary:
gh is contagious both in the symptomatic and in the asymptomatic phase of the disease and causes painful genital ulcers.published data show that 45 million persons aged 12 years or older have hsv-2 an- tibodies [3], that up to 70% of patients attending sexual transmitted disease clinics have hsv-2 infection [4] and that the majority of patients with initially symptomatic gh will develop recurrent disease [5].cost of illness studies represent one of the applications of economic science to medicine.the aim of these studies is to assess the economic burden of a disease and to help decision-makers in targeting preventive efforts and allocating resources [6,7].despite its widespread and increasing transmission, there is still poor understanding of the economic impact of gh in the usa, which makes it difficult to evaluate societal costs and the cost-effectiveness of preventive efforts [8].in order to give a better estimate of the disease burden, we retrieved economic information using two different approaches: in the first instance, direct interviews were conducted with a random sample of office-based physicians using a structured, comprehensive questionnaire.secondly, information was retrieved from a large, longitudinal administrative database.for this purpose, data from diversified pharmaceutical services (dps), which is a pharmaceutical benefit management firm, were used.for both approaches, the time span was one year (1996).costs were referred to on a yearly basis and computed in 1996 us dollars.data were collected and analyzed separately.approach using expert interviews a questionnaire was administered to 30 randomly selected primary and secondary care physicians practicing in the north-east of the usa.physicians were asked about the annual number of patients with gh and the total number of episodes of symptomatic gh, seeking medical care and treatment.the perspective of this analysis was societal; thus, both direct and indirect medical costs were based on the costs born by society.the us population in 1996 (279 million) served as a reference.regarding medical costs, the categories considered were the consultations and medical procedures performed, laboratory tests, pharmacological treatment and hospitalizations.unit costs are illustrated in table 1.hourly wage of $14 [12], and were valued according to estimates of time consumed by hospitalization, time lost from work due to illness, and travel and waiting-room time resulting from physician visits.for the present study, all patients diagnosed with gh who have been enrolled in the plans of four hmos at any time during 1996 were included.the annual costs attributable to gh-infection in the united states were estimated as the product of the number of incident and prevalent infections and the average present value of the costs attributable to a single gh-infection.specialists see 56 gh patients per year, including 12 (21%) incident cases.from the data available, the incidence of clinically manifest gh can be estimated to be 423,000 cases and the number of recurrent cases at about 698,000 patients in 1996.the estimated total burden of medical care for gh in the us in 1996 represents $984 million, based on an estimated occurrence rate of 3.1 mio symptomatic episodes.of these costs, 49.7% were caused by drug expenditures, 47.7% by medical care and 2.6% by hospital costs.further $214 million represent the indirect costs to society.table 4 summarizes the main epidemiologic findings from this analysis.this produces a crude prevalence of 2.89 cases per 1,000 in 1996.using this approach, the amount of medical resources absorbed by the care of gh patients in the us can thus be estimated at $244 million for 1996.based on previously published data [1, 13,14,15,16], we also estimated the direct costs of gh-associated complications.discussion this report presents crude estimates rather than precise measures of the economic costs of gh in the usa.moreover, this study showed that the average gh patient seeking treatment is likely to be younger than 40, to develop about 2 recurrent episodes, to undergo several laboratory exams, and to be treated with antiviral drugs.relapses tended to be shorter than first episodes though having a higher cost per episode.as recently shown, the great majority of people with serologic evidence of hsv-2 infection have no history of recognized gh [3].from an economic point of view, it must be borne in mind that gh and other sexually transmitted infectious diseases have negative externalities in the sense that consequences of the disease are not only limited to people who have the disease but also to other people that can be potentially infected.a lower level of compliance means probably lower shortterm direct costs, but probably higher indirect and longterm medical costs.thus, the decision to diagnose gh by individual clinicians with different levels of expertise cannot be controlled within the boundaries of claims data.
number of words= 751
[{'rouge-1': {'f': 0.4561716276733472, 'p': 0.824863813229572,'r': 0.31525916561314793}, 'rouge-2': {'f': 0.2483122628300087, 'p': 0.4059375,'r': 0.1788607594936709}, 'rouge-l': {'f': 0.391636600298185, 'p': 0.6455395683453238,'r': 0.2810817941952507}}]
-----------------------------------------------------------------------------------------------------------------------------------
p166:
Extractive Summary:
genital warts are the commonest sexually transmitted infection, affect mainly younger people, and usually caused by human papillomavirus genotypes 6 or 11.points were awarded to studies according to whether they were randomised and double blind and mentioned withdrawals or drop-outs from the study.ram extracted the data into tables, and these were then read and checked by other authors.prior definitions of outcomes of interest included those describing treatment efficacy (wart clearance) and those describing adverse events.three main efficacy outcomes and three harm outcomes were therefore sought from the trials, using the denominator of the number of patients randomised so that results were on an intention- to-treat basis.the main outcomes sought were the number of patients with: • complete clearance of warts present at the start of treatment.• at least 50% reduction in wart area.• complete clearance of warts and no recurrence thereafter.• patients withdrawing from the study because of (reported) treatment-related adverse effects.• patients withdrawing from the study because of lack of efficacy.there were additional minor outcomes of interest.one was the number of new warts that appeared after treatment started, and the clearance of these warts during treatment; clearly this could not be done on an intentionto- treat basis.recurrence was also reported as the rate of recurrence in those patients with initial complete clearance.again, this could not be analysed on an intentionto- treat basis.confidence intervals (95%) for single samples were calculated for proportions [14].relative benefit and relative risk estimates were calculated with 95% confidence intervals using a fixed effects model [15].heterogeneity tests were not used as they have previously been shown to be unhelpful [16].publication bias was not assessed using funnel plots as these tests have been shown to be unhelpful [17, 18].the number needed to treat (nnt) and number needed to harm (nnh) with confidence intervals were calculated by the method of cook and sackett [19].relative benefit or risk was considered to be statistically significant when the 95% confidence interval did not include 1.nnt or nnh values were only calculated when the relative risk or benefit was statistically significant, and are reported with the 95% confidence interval.statistical significance of any difference between numbers needed to treat for different doses or between men and women was assumed if there was no overlap of the confidence intervals, and additionally tested using the z statistic [20].calculations were performed using microsoft excel 98 on a power macintosh g4.of these, 10 were excluded (supplementary file 2) because they did not meet the inclusion criteria.a number of these were review or other articles with clinical information duplicated in other publications, but always with attribution.where information on the same patients was available in duplicate reports, we used studies with the fullest amount of clinical information.details of the six included studies are given in supplementary file 3.all six studies were conducted in the setting of home administration after initial professional examination and advice.five of the studies were explicit that not other treatment was allowed within at least four weeks of the start of the trial.wart location was predominantly vulvar or perianal in women, and penile or perianal in men.five studies were conducted in north america or the uk using 5% or 1% imiquimod cream (aldara, 3m pharmaceuticals) [21,22,23,24,25], and one [26] was conducted in pakistan using a 2% cream manufactured locally.quality scores were therefore 3 in four and 4 in two studies (supplementary file 3) out of a maximum possible score of 5 and a minimum possible score of 1.all the studies described the diagnostic procedures to diagnose genital warts.this was usually (four of five studies) a combination of clinical examination supplemented by biopsy and histology.one study used genetic techniques to identify hpv 6 and 11 [26].the study populations were all adults.five included men and women, though one had more than 90% men [22].one study examined only women [26].complete clearance of warts was reported in all five trials of hiv negative patients (figure 1).this was achieved in 51% of patients (95% confidence interval 45% to 56%) treated with the highest concentration of imiquimod (2% in one trial, 5% in four), but in only 6% (3% to 8%) of patients treated with placebo cream.this means that two patients have to be treated with 2% or 5% imiquimod for 8 to 16 weeks for one of them to have warts completely cleared (table 1).this means that two patients have to be treated with 5% imiquimod for 8 to 16 weeks for one of them to have wart area reduced by at least 50% (table 1).substantially fewer patients were cured with 1% imiquimod in two trials, and for this concentration the nnt was 8.1 (4.7 to 30).consequently the number of patients fulfilling this outcome could be calculated, with the number of patients randomised as the intention to treat denominator.this means that three patients have to be treated with 5% imiquimod for 8 to 16 weeks for one of them to have warts completely cleared, and for them not to recur (table 1).substantially fewer patients were cured with 1% imiquimod in two trials, and for this concentration the nnt was 10 (6.4 to 26).the proportion of new warts appearing since the study started and which had completely cleared by the end was 39% (24% to 54%; 41 patients) with 5% imiquimod, was 21% (12% to 30%; 78 patients) with placebo, and the nnt was 5.4 (2.8 to 91) (table 1).this means that for every five patients with new warts appearing after the trial started, one more had the new warts cleared by the end of treatment than with placebo.recurrence occurred in 18/112 patients (16%; 95% confidence interval 9% to 23%) of those treated with imiquimod 5%, 2/30 (7%; 2% to 16%) of those treated with imiquimod 1%, and 1/13 (8%; -7% to 22%) of those treated with placebo.for the three large trials [21,22,23] 121/254 patients had warts cleared at the end of treatment with 5% imiquimod, and only 27/121 (22%) of these had a recurrence or reinfection.numbers needed to harm could not be calculated, nor could an overall weighted percentage of patients with moderate or severe reactions.withdrawal because of lack of efficacy was described in five studies, and pooling information from the highest concentration of imiquimod in each trial (2% or 5%) showed that 1.7% (0.3% to 3.1%) of patients withdrew because of lack of effect with imiquimod, compared with 7.4% (4.3% to 11%) with placebo.for instance, the five studies used for data pooling all excluded patients with hiv infection.one outcome not reported, but one that could be inferred, was that of patients with warts completely cleared by the end of treatment, and with no recurrence of warts during the 10-16 weeks of follow up.this occurred despite very different absolute percentages of patients achieving the outcome, because placebo rates were high (20%) with the easiest outcome (at least 50% reduction in wart area) and as low as 4% with the hardest (warts completely cleared by the end of treatment, and with no recurrence).that analysis could only be done using detailed information on individual patients.they demonstrate imiquimod to be effective in home application, though not in patients with hiv infection with the evidence presently available [24].
number of words= 1183
[{'rouge-1': {'f': 0.4209335587965899, 'p': 0.8352173913043479,'r': 0.281369095276221}, 'rouge-2': {'f': 0.23275734582782084, 'p': 0.4072093023255814,'r': 0.16294871794871796}, 'rouge-l': {'f': 0.40107811225122314, 'p': 0.67625,'r': 0.2850776053215078}}]
-----------------------------------------------------------------------------------------------------------------------------------
p167:
Extractive Summary:
background campylobacter infection is a zoonotic disease, observed in most parts of the world.it is estimated to cause 5–14% of diarrhoea, worldwide [1], and also in the western world campylobacter infection has emerged to be the most important bacterial cause of gastrointestinal infection.unlike salmonellosis with well-known routes of transmission, the epidemiology of campylobacteriosis is still largely unclear [3].however, all these factors could at most explain 50% of the sporadic cases [3].the houseflies are important mechanical vectors in the transmission of many infectious diseases with low infective dose, such as shigellosis, typhoid fever and e. coli infection [11,12].already in 1983, rosef and kapperud postulated that flies might play a linking role by transmitting campylobacter from animals to human food [16].although less tolerant to desiccation than some other food-borne pathogens [22], campylobacter can survive on dry surfaces for at least seven days [23], thus enabling the bacteria to survive for several days both on the body of the fly and in desiccated fly faeces.from these results the author drew the conclusion that the health hazard from the transmission of campylobacter from animals to human food is small.on the contrary, giving the numerous contacts between flies and human food, we find it highly likely that if one out of every 40 flies carries campylobacter the health hazard would be significant.stanley and jones have previously shown the importance of cattle and sheep farms as reservoirs of campylobacter [26].campylobacter spp have been isolated from sewage contaminated water [29], contaminated soil [30] and aquatic sediments [31], and in sand from bathing beaches [32].there are therefore likely considerably more campylobacter than shigella in the close vicinity of humans.direct transmission from the soil could probably account for some of the cases in children, but less likely for adult cases.4. seasonality of the disease the distinct seasonality in the temperate regions [3,5,7,8,33] fits well with the fly hypothesis.a recent study from the uk has shown a close temporal association between the incidence of campylobacteriosis and fly density [34].therefore, if our hypothesis holds true, there should not be the same distinct seasonal peaks in the tropics.lack of detailed data on seasonal fly density and quite large geographical regions for our risk estimates of campylobacteriosis, prevented us from making any correlations between risk of campylobacteriosis and the presence of flies in the tropical regions.5. age profile small children are less able to protect themselves from flies than older children and adults, and are more likely to have their hands on fly-soiled surfaces.6. dominance of solitary cases if intake of chicken and undercooked meat (or cross-contamination from these food items) was a major route of transmission, clusters of cases within the same family should be common.such studies could only be done in high incidence areas, and would require good laboratory support.
number of words= 459
[{'rouge-1': {'f': 0.4748048437716494, 'p': 0.7101869158878504,'r': 0.35661087866108787}, 'rouge-2': {'f': 0.20746257797497644, 'p': 0.2812676056338028,'r': 0.16433962264150945}, 'rouge-l': {'f': 0.3630768190869911, 'p': 0.49335766423357663,'r': 0.2872284644194757}}]
-----------------------------------------------------------------------------------------------------------------------------------
p168:
Extractive Summary:
background non-typhoidal salmonella spp.is a substantive cause of human gastroenteritis in many parts of the world [1].in germany, non-typhoidal salmonellosis remains the most frequently reported infectious disease.for example, in 2001, the 77,185 salmonella reports (incidence:94/ 100,000) received at the federal level by the robert koch- institut (rki) accounted for 31% of all notifications for the 54 notifiable conditions [2].salmonella enterica subspecies enterica serotype enteritidis (s. enteritidis) is the predominating serotype followed by s. typhimurium.they represented 65% and 23% of the reported cases of non-typhoidal salmonelloses with known serotype in 2001.thus, the remaining ~250 serotypes reported to the rki in that year, including s. oranienburg, accounted for only 12%.in mid-october 2001, the national reference center for salmonella and other enteric pathogens (nrc) in hamburg noted an unusual increase in the number of s. oranienburg isolates received in october.at that time, no increase was noticeable in the national database for statutorily reportable infectious diseases; 50 s. oranienburg notifications (median: 1 per week) had been registered for 2001.on november 19, the nrc in wernigerode informed the rki that it had received a s. oranienburg isolate in september.the isolate was submitted by a private laboratory for serotyping and had come with the additional source information "confectionery sample".upon inquiry, a large german chocolate manufacturer (company a), which produced a broad variety of chocolates and products made thereof, called the rki on november 27, and confirmed that it had sent in the confectionery sample.according to company a, the positive sample originated from an in-house control of a chocolate product and the pertaining batch, due to be exported to the united states, was completely destroyed and not distributed.notwithstanding, the number of statutory s. oranienburg notifications had sharply increased and continued to rise.this report describes the epidemiologic, food safety, and microbiological investigations of this outbreak.methods epidemiologic investigation descriptive epidemiology a standard exploratory questionnaire was distributed on 20 november 2001 via state health departments to all local health departments to aid the collection of data on food and environmental exposure from cases.international case-finding a request was distributed to participants of the enter-net surveillance network [6] on december 10, to see if other countries were affected or had relevant information.case-control study on december 3, while exploration of patients were ongoing and results inconclusive, s. oranienburg isolates from patients and from the in-house chocolate control were found to be indistinguishable by pulsed-field gel electrophoresis (pfge).on the same day, a multistate case-control study was initiated and coordinated by rki to test the hypothesis that at least one product from company a was associated with s. oranienburg-infections.as we were denied a product list from company a, we resorted to the company's web-site and included in our food history evaluation all the products listed there.some products from company a, e.g., bars of chocolate (brand a), were exclusively sold at a large chain of discount grocery stores (chain x).we found that the majority of chocolates sold at chain x were produced by company a. therefore, for the analysis we constructed a variable for chocolate(s) purchased at chain x ("chain-x-chocolate") as a proxy for chocolate-products from company a because most patients could remember the flavor of the purchased chocolate, but seldom the brand name.all 16 states of germany reported s. oranienburg cases during the outbreak period, with the highest incidence in the state of schleswig-holstein (1.78/100,000) bordering on denmark.in total, 206 of the 440 german counties were affected with a median of one report and a maximum of 16 from the city of hamburg during the outbreak period.on 18 december 2001, two months after the initial outbreak alert, a leftover consumed reportedly by this child in the seven days before symptom onset tested positive for s. oranienburg.at this point in time the investigators in denmark, without knowledge of the german s. oranienburg problem, independently suspected german chocolate bought in chain x as the source of the danish outbreak.all 15 chocolate isolates showed pfge profiles indistinguishable from human isolates of the outbreak period.s. oranienburg, a rare serotype in food as well as in humans in germany, was isolated from retail-sampled chocolates of two brands produced by company a, from chocolate leftovers that had been consumed by patients before symptom onset, and from an in-house sample of company a obtained prior to the outbreak.estimates of the number of s. oranienburg cells per gram in this outbreak ranged from 1.1–2.8.consequently, the observation that a higher proportion of cases reported eating chocolate on a daily basis added to the evidence that chocolate was the vehicle in this outbreak.furthermore, the danish data provided powerful supplementary evidence because consumption of german chocolate was particularly common in germany but unusual in denmark.therefore, in multinational outbreaks, international collaboration provides an important means for disclosing the common source of infections, particularly when the contaminated food is very popular in one (likely the source) country (e.g., [23,24]).this included an extra heating of the milled cocoa beans by a special heat-steam treatment with 125– 130°c as an additional safeguard.however, no environmental samples and very few samples of raw ingredients (n = 10) were obtained.an enter-net urgent inquiry was sent after the first results of molecular subtyping suggested a link between human cases and chocolate from company a. until then, investigators in germany and denmark had worked independently unaware that the outbreak extended outside of their respective countries.an earlier inquiry, ideally as early as an outbreak was suspected by the investigating countries (or as an increase was noted in the enter-net database), may have speeded up hypothesis generating, and thus, may have helped in earlier identification of the vehicle, thereby preventing illnesses.finally, a public warning or recall of company a products did not occur before a brand a leftover tested positive although the confluence of information – the results of the case-control study, the danish investigations, and the subtyping comparison between human isolates and the in-house sample – had already pointed to company a products as the source of the outbreak.therefore, it has been argued that public health action should be based on well-performed epidemiological investigations encompassing clear statistical associations with a specific exposure [25-27].despite the use of improved production technologies, the chocolate industry continues to carry a small risk of manufacturing salmonella- containing products.for the future, awareness among german food safety authorities must be heightened for the need to base public health action not exclusively on laboratory confirmation in food, and to conduct timely and comprehensive source investigations to enhance food safety in the long-run.the international scale of this outbreak shows how easy it is to distribute a contaminated product across many countries.this underlines the necessity of mechanisms for international surveillance and information dissemination such as enternet to ensure that international outbreaks can be dealt with rapidly and in an appropriate manner.similar networks should be set up or, if existing, should be connected (possibly overseen by who), to allow rapid communications to other parts of the world when it is clear that a contaminated product is distributed internationally.competing interests the author(s) declare that they have no competing interests.authors' contributions dw was the principal investigator of the german part of this outbreak; he carried out the statistical analysis of the case-control study, and drafted the manuscript.se conducted the danish part of this outbreak investigation.pr detected the outbreak and conducted microbiological investigations.rp and ht conducted the pfge-analysis.ew coordinated food safety investigations in this outbreak.ya conducted the swedish part of this outbreak investigation.
number of words= 1223
[{'rouge-1': {'f': 0.3418462713929622, 'p': 0.7842857142857143,'r': 0.2185535574667709}, 'rouge-2': {'f': 0.1941196072548237, 'p': 0.3681132075471698,'r': 0.1318153364632238}, 'rouge-l': {'f': 0.385739251420621, 'p': 0.6887845303867404,'r': 0.26787985865724384}}]
-----------------------------------------------------------------------------------------------------------------------------------
p169:
Extractive Summary:
complete recovery ensued and the patient was discharged on day-16.conclusion the impact of antibiotic resistance on the treatment outcome of patients with cap is a matter of discussion [4].fq are often the primary choice for empirical treatment of patients with co-morbidity factors such as copd, diabetes, and renal or congestive heart failure [4,5].a few reports show an association between fq resistance and treatment failure in cap caused by s. pneumoniae [16,17].no cases have been yet reported from italy, possibly due to the low incidence of fq-resistant strains [3].the present report describes the first italian case of cap due to a fq-resistant strain of s. pneumoniae.failure of oral levofloxacin was observed.according to current criteria [11], the isolate was mdr i.e., intermediate to penicillin, resistant to chloramphenicol, trimethoprimsulfamethoxazole, macrolides, clindamycin and fq.the strain showed single mutations of gyra and pare genes associated with two point-mutations of the parc gene.high-level resistance to fq resulted (mic >32 mg/l).this justified the initial treatment failure when the patient was given levofloxacin alone.molecular analysis showed a group of mutations that have not yet been reported from italy [3,18].association of the reported mutations in gyra, pare and parc genes has been detected only twice in europe [3], but is relatively more frequent in the usa [7].as demonstrated by studies of s. pneumoniae strains exposed in-vitro to fq, ciprofloxacin and levofloxacin use is commonly associated with parc gene mutations, whereas use of other fq appears to favor the selection of mutations in other genes [6].the reported clinical case suggests that the following precautions need to be used for patients with specific lrti risk factors: i) before therapy, respiratory samples should be obtained to identify the responsible agents and to define antimicrobial susceptibility [16]; ii) patients previously treated with fq may be given these drugs, but need strict clinical monitoring during the first 3 days of therapy to evaluate the clinical response [19]; iii) since only 1% of fq-resistant s. pneumoniae strains are also resistant to broad-spectrum beta-lactams, seriously ill patients can be safely treated with combinations of fq plus broad-spectrum beta-lactams [20].empiric treatment of cap in patients seriously compromised using piperacillin/tazobactam (as in the reported case) appears an effective means to inhibit fq-resistant strains of s. pneumoniae.in particular, piperacillin/tazobactam appears indicated for icu patients since, in contrast to monotherapy with extended-spectrum cephalosporins (e.g., ceftriaxone, cefotaxime), does not favor the selection of esbl-positive enterobacteria and/or chromosomal beta-lactamase hyperproducers (e.g., pseudomonas aeruginosa) [21,22].additionally, this drug's spectrum is wider than that of 3rd–4th generation cephalosporins.in italy fq consumption is higher than in the rest of europe [8].thus, it seems possible that over the next years an increased prevalence of fq-resistant s. pneumoniae strains will be observed.
number of words= 444
[{'rouge-1': {'f': 0.5183499191668237, 'p': 0.6721897810218977,'r': 0.42181236673773986}, 'rouge-2': {'f': 0.2706740487213386, 'p': 0.3373992673992674,'r': 0.225982905982906}, 'rouge-l': {'f': 0.47174454339176214, 'p': 0.5758823529411765,'r': 0.39950191570881227}}]
-----------------------------------------------------------------------------------------------------------------------------------
p170:
Extractive Summary:
fifteen of the 20 repeated this process 3 months later.a total of 280 evaluation scales (160 on the first occasion and 120 on the second) were completed.all experts said that they were able to complete the scale within a 5-min period.the interrater reliability of each item and overall score, including all 20 evaluators involved on the first occasion, are shown in table 2.all icc values of all items including the total score were greater than 0.8 (0.852–0.992), and 69 % of the data were greater than 0.9.the “local anesthesia” item showed the highest interrater reliability (0.992, 95 % ci 0.982–0.998).table 3 shows intra-rater reliability (repeatability) of each evaluator.the icc values of all items and total score were greater than 0.8, and 62 % of the data were greater than 0.9, the item “conjunctival autograft acquisition” showing the highest repeatability (0.962, 95 % confidence interval 0.945–0.974).validity of the assessment scale construct validity of 257 assessment scales was analyzed (table 4).in this classification model, the χ2/ df = 2.699 < 3, goodness of fit index (gfi) = 0.931 > 0.9 and adjusted goodness of fit index (agfi) = 0.902 > 0.9, which means that model fit is fair.average variance extracted (ave) was used to reflect convergent validity.the ave values of the three categories (basic surgical skills, pterygium dissection and ocular surface reconstruction) were 0.584, 0.571 and 0.631.cr values were 0.874, 0.842 and 0.835, and the rmsea value was 0.043 < 0.05.these results showed good construct validity.discussion at the beginning of the 21st century, ophthalmology training in china remains nonsystematic, and the quality of training varies between hospitals.in shanghai, assessment of junior residents’ surgical skill is based on their ability to suture corneal ruptures on pig eyes, and senior residents’ skills are assessed by performance of pterygium excision and conjunctival autograft transplantation in theatre.we have previously developed an assessment scale for the process of suturing corneal rupture, and its validity and reliability have been confirmed in practice [13].however, the assessment of pterygium surgery remains subjective and is prone to factors such as unconscious bias [12, 20].therefore, a critical need exists for a valid and reliable assessment tool.in this study, we designed an evaluation scale for pterygium surgery conducted in china.the principles of the design are: (1) feasibility (rapid and easy to use); (2) whole-procedure evaluation; (3) surgical skill assessment at different rotation levels; (4) feedback and summative evaluation to improve skills and competencies.the final scale consists of 12 items, including five on basic surgical techniques and seven on pterygium surgery.the scale uses a 5-point likert scoring system, and each score has a detailed score description.zarei-ghanavati et al. [3] also developed an assessment rubric for pterygium surgery.our scale is similar to theirs in structure, but different in content.for example, we included the evaluation of basic surgical skills such as microscope use and instrument handling since they are important aspects of microsurgery.however, items beyond resident level such as mitomycin-c application and fibrin glue usage are not included.in the scale, percentage score categories such as 60 %, 80 % were used, for more accurate and objective evaluation of the scale.moreover, the scale is relatively simple and all evaluators reported completion within a period of 5 min, suggesting that it can be applied in rapid and large-scale resident assessment.more importantly, zarei-ghanavati et al. did not test the construct validity and repeatability level of their scale.
number of words= 558
[{'rouge-1': {'f': 0.40982393315428234, 'p': 0.6950000000000001,'r': 0.29058823529411765}, 'rouge-2': {'f': 0.17718806325355668, 'p': 0.256046511627907,'r': 0.13546644844517186}, 'rouge-l': {'f': 0.3212784286081258, 'p': 0.5253571428571429,'r': 0.23139240506329115}}]
-----------------------------------------------------------------------------------------------------------------------------------
p171:
Extractive Summary:
migration to developed countries has been common among health professionals, and growing rapidly over the past decades [1, 2].discussions about the advantages and drawbacks of migration of health workers have been taking place in both origin and destination countries [3–5].these migration flows may weaken the donor health system; however, we should equally respect the human rights of health professionals for migration as in their homelands they lack resources and education opportunities [9].iran is the second largest country in the middle east and one of the world’s oldest civilizations founded.according to the latest united nations’ data, the current population of iran is estimated 84,808,441 with the median age of 32.0 years old [16].after the islamic revolution in 1979, iran went through major social, cultural, and religious changes, and based on the 2011 iranian census, 99.98 % of iranians believe in islam, while only few of the population believe in christianity, judaism, and zoroastrianism [17–19].in iran, psychiatry training courses are available in more than 20 medical universities across the country [20].this article aimed to explore the attitudes towards migration among psychiatric trainees and early career psychiatrists in iran and to identify the reasons behind migration.methods study design this is a cross-sectional survey to explore migration trends in junior doctors in psychiatry in iran.this was a self-report, anonymous 61-item questionnaire inquiring about participants’ demographics, experiences of short-term (from 3 months to 1 year) mobility, longterm (more than 1 year) migration and trainees’ attitudes towards migration (current and future plans).the demographic data and the variables of short-term mobility and long-term migration were compared using chi-square statistic tests.participants’ detailed socio-demographic data is presented in table 1.more than three-fourths (76 %) stated that this short-mobility had influenced them in favor of migration, with preferred destination countries: the uk, germany, france, america, italy, syria, and kuwait.among them, they were either ‘very satisfied’ (n = 5, 38.4 %), or ‘satisfied’ (n = 3, 23 %) with their experiences and the rest were ‘neither satisfied nor dissatisfied’ (n = 5, 38.4 %).in relation to their 5 year-plan, most (n = 124, 67.3 %) answered they saw themselves in the country they already live in (iran), 15.8 % (n = 29) stated ‘i have not made up my mind yet’, 9.8 % (n = 18) ‘anywhere in the world’, 4.4 % (n = 9) ‘in europe’, 1.1 % (n = 2) in america, and 1.1 % (n = 2) in australia.in the chi-square test, no significant correlations were found among the participant’s variables.the main features reported for an attractive job were ‘good welfare and social security’ (87.5 %), ‘pleasant work environment’ (82.6 %), and ‘good work life balance ‘(77.7 %) (fig. 3).discussion key findings all study participants were native iranians and most of them were females (73.4 %).the main reasons to ‘stay’ in iran were mainly personal (54.9 %), followed by political (19.6 %), and academic (19 %), whereas the top reasons to ‘leave’ iran were political (32.1 %), work (28.8 %), and financial (27.2 %).in a 5-year perspective, 66.5 % planned on staying in iran, 15.8 % had not decided yet, and 16.7 % planned on leaving the country.the main reasons to migrate, among health workers, were seeking a better life, interdisciplinary discrimination, and wanting to have the experience of migration [22].comparing our findings in iran, with the results from this study from 2013 to 2014 in 33 european countries, it was then reported that 13.3 % of the psychiatric trainees working in europe were already immigrants (switzerland, sweden and uk being the top host countries).two-thirds had ‘ever’ considered migration, and half were considering it at the time of the study.the main pull factors in iran were reported personal, political, academic, and religious factors.in a 5-year perspective, 57.6 % of the estonian, lithuanian and turkish trainees [23, 24], 50.0 % portuguese and romanian trainees were considering working in another european country [25, 29].strengths and limitations this is the first study to investigate the attitudes of psychiatric trainees and early career psychiatrists in iran towards migration.
number of words= 666
[{'rouge-1': {'f': 0.4878277627592533, 'p': 0.800909090909091,'r': 0.3507262569832402}, 'rouge-2': {'f': 0.2872119022270302, 'p': 0.44226277372262773,'r': 0.21265734265734265}, 'rouge-l': {'f': 0.47319489288721267, 'p': 0.67989010989011,'r': 0.3628759894459103}}]
-----------------------------------------------------------------------------------------------------------------------------------
p172:
Extractive Summary:
background since the concept of self-directed learning (sdl) was described by malcolm knowles in 1975, medical educators have embraced the principles of this student-centered approach to enable physicians to take responsibility for and drive their ongoing learning [1, 2].[4, 5] knowles described sdl as a learning contract between a learner and an instructor and a linear process comprising six major steps: (1) climate setting (creating an atmosphere of mutual respect and support); (2) diagnosing learning needs; (3) formulating learning goals; (4) identifying human and material resources for learning; (5) choosing and implementing appropriate learning strategies; and (6) evaluating learning outcomes [4].in the ppc model, “person” refers to characteristics of the individual, such as motivation, resilience, and self-concept, “process” refers to the teaching-learning transactions such as planning and organizing, and “context” refers to the environmental and sociopolitical climates, such as learning environment and group culture [10].problem-based learning and, more recently, case-based collaborative learning have been developed as tools to help learners apply their learning to clinical cases [12, 15, 16].these approaches emphasize the development of learner autonomy and intrinsic motivation, with “supported autonomy” or scaffolding provided by the faculty tutor or facilitator guidance [17, 18].we drew on the standards for reporting qualitative research (srqr) and the consolidated criteria for reporting qualitative research (coreq) to guide our analysis and reporting of findings [24, 25].. in the u.s., medical students typically undergo training that often, but not always, is structured into two years of coursework followed by two years of clinical or “clerkship” training by participating in patient care in hospital and ambulatory settings.at harvard medical school (hms), following preclerkship training, most students enter the traditional clerkship rotation blocks (including medicine, neurology, ob/gyn, pediatrics, primary care, psychiatry, radiology, and surgery) lasting from 4 to 12 weeks in the core clerkships.a few join the longitudinal integrated clerkship (lic), which extends over a 12- month period and emphasizes continuity with faculty and patients[26].both the traditional rotation and the lic at hms share the self-study time on schedules, which is intended for reading and sdl.we invited medical students who recently finished their first year of the clerkship in four affiliated hospitals through email invitation to the class.adapting the sdl model for the clinical setting when reporting their process of engaging in sdl, participants reported steps that aligned with steps 2 to 6 in knowles’ linear model.when being asked the definition of sdl, the participants referred to a range of steps, usually only three to four per individual, but collectively they referred to all six steps.the other path was through direct patient care, when participants were directly involved in primary care and encountered a series of clinical problems.step 4: implementing learning strategies beyond reading, participants reported a variety of cognitive strategies.participants reported spending a great amount of time using question banks not only for preparing for the exams but also for gauging their level of knowledge on certain topics.when participants mentioned the framework in learning, there were two layers of meanings: learning the routine practices or standard operating procedures (sops) in the clinical setting, and consolidating their knowledge about a clinical topic or seeing the “big picture” (that is, having a holistic view) of the patient’s diagnosis and management.(table 3) motivation participants’ primary motivations in learning were to provide better patient care, followed by anticipating how they would work in the future as residents or attendings.social-emotional strategies social-emotional strategies were as follows: (1) building relationships with team members; (2) observing team members’ personalities; (3) being aware of situations (e.g., workflow) that might require a change in student behaviors; (4) being sensitive to how they (students) are perceived.the context dimension the contextual dimension refers to the learning climate, culture, and environment that may facilitate or inhibit sdl.learning environment the learning environment had five subthemes: education orientation, psychological safety, student engagement, opportunities for ownership of patient care, and a sense of urgency.discussion our findings support the need for a deeper understanding of how, and in what contexts, sdl is experienced and practiced by medical students in the clinical setting.in contrast to the classroom, where the process and context of students’ learning are likely to be both more structured and predictable, the dynamics and complexity of the clinical learning environment call for a more context-specific, nuanced understanding of sdl.our study suggests that the combined frameworks of hiemstra and brockett’s “person, process, and context” model and knowles’ self-directed learning model provide a useful lens with which to capture this complexity of learning in the clinical setting.the results here, in turn, elaborate upon and extend these models by highlighting students’ experience of sdl in a clinical context.we summarized and organized these findings into the conceptual model shown in fig. 1.one important insight from this study is that patients are both the anchor and the drivers of sdl in the clinical setting, shown in fig. 1 as the hub of all elements of student learning.further, participants who espoused goals of being prepared to provide excellent care for their future patients (that is, when they were practicing more independently) were also those who described the most proactive and well-articulated strategies for advancing their learning (fig. 1, “person” category).thus, although current definitions of sdl usually emphasize the student-centered nature of the construct, students’ experience of sdl in the clinical setting in this study is primarily patient-centered.the patient-centered focus of students’ learning also leads to a new proposed phase of sdl in the clinical setting that we have described as “building a framework” (fig. 1, “process”category, step 6).this appears to be a stage in which students consolidate their learning about a specific patient’s illness or disease processes, incorporate multiple perspectives and sources of information related to a patient’s medical and social history, integrate and prioritize information from the patient’s history, physical, and lab results, and understand how to apply content knowledge to direct patient care, often in the face of considerable clinical uncertainty.these are the competencies of practice-based learning (for example, learning to access and evaluate medical information through uptodate and other learning resources), medical knowledge (for example, seeking specific patient-related content knowledge and working to apply that knowledge to evaluation and care of the patient), and patient care (for example, learning to identify and prioritize patient care needs).further guidance and scaffolding from faculty and residents, such as that provided by faculty tutors in problem-based or case-based collaborative learning in the classroom, could enhance this important phase and promote student integration and application of their clinical knowledge.this study adds to the “person, process, context” model of sdl by describing specific themes that had major influences on medical students’ sdl in each of the three dimensions.providing students with some autonomy to pursue interests in specific fields of training, for example, may facilitate sdl in the clinical setting.a growth mindset, which dweck first mentioned in the implicit theories of learning, also has strong correlations with students’ learning behaviors [30].as stated by our interviewees, the growth mindset can be cultivated, and students with this mindset appeared to engage in sdl more easily.finally, we listed four social-emotional strategies students used in sdl, including building relationships with team members.this broad array of personal habits, strategies, and mindsets might be guided and enhanced through existing organizing tools such as an individualized learning plan (ilp).although ilps have not been used widely in undergraduate medical education, they are used successfully in other educational settings and have some evidence for effectiveness in improving sdl strategies for senior medical students [31, 32].. in addition to the phase of “building a framework” described above, the process dimension comprised all other steps of knowles’ framework except the “setting the climate” step.this is to be expected from the student perspective, as they are unlikely to feel they have the authority to influence the learning climate in their clerkships.the clerkship-specific elements of the sdl process, as shown in fig. 1, were fairly consistently described by students in this study.these may be useful guidelines for clerkship leaders to share with students to aid them in formulating and achieving their own learning goals.finally, the contextual dimension of sdl highlighted factors such as psychological safety and feeling integrated into the team, whereas high stress or high workload environments diminished students’ perceived ability to learn.clerkship leaders might consider existing tools to enhance psychological safety, such as the centre approach, which provides an explicit process for groups to discuss how to work together with respect and curiosity [33].in performance-oriented learning environments like medical schools, impression management tactics like image creation and protection have been noted in both the residency and clerkship settings [34, 35].in our study, the equivalent tactics, looking good and not seeming disinterested, were being used by medical students every day in their training hospitals and was described by some as exhausting.this study has some limitations.this was a study based on a single institution in the u.s.these may limit the transferability of findings to other institutions.it also includes only the perspectives of students, and future explorations of this topic should include faculty and other team members.sdl may also vary across specialties and levels of training; however, because we wanted to gain an overall view of sdl in the clinical setting, we did not ask specifically about student experiences in different rotations.our study explored medical students’ perception of sdl in the clinical setting and refined the model with the six steps and the three dimensions of sdl.
number of words= 1553
[{'rouge-1': {'f': 0.3387172002161503, 'p': 0.8004075235109718,'r': 0.21481044126786825}, 'rouge-2': {'f': 0.18345858503802284, 'p': 0.3467295597484277,'r': 0.12472636815920399}, 'rouge-l': {'f': 0.29336325483119674, 'p': 0.5561878453038673,'r': 0.19922173274596183}}]
-----------------------------------------------------------------------------------------------------------------------------------
p173:
Extractive Summary:
in phase 1, the research team carried out independent reviews and ‘actively’ reading the included articles to find meaning and patterns in the data [82–86].in phase 2, codes were collated into a code book to code the rest of the articles.as new codes emerged, these were associated with previous codes and concepts to create subthemes.in phase 3, the subthemes were organised into themes that best depicted the data.an inductive approach allowed themes to be “defined from the raw data without any predetermined classification” [86].in phase 4, the themes were refined to best represent the whole data set.in phase 5, the research team discussed the results of their independent analysis online and at reviewer meetings.negotiated consensual validation was used to determine the final list of themes.concurrently, three members of the research team employed directed content analysis to independently review all the articles on the final list.this involved “identifying and operationalising a priori coding categories” by classifying text of similar meaning into categories drawn from prevailing theories [87–91].in keeping with seba’s pursuit of an evidence-based approach, the research team selected and extracted codes and categories from roze des ordons (2017)‘s article entitled “from communication skills to skillful communication: a longitudinal integrated curriculum for critical care medicine fellows” [92].use of an evidence-based paradigm article to extract codes from was also in line with seba’s goal of ensuring that the review is guided by practical, clinically relevant and applicable data.in keeping with deductive category application, coding categories were reviewed and revised as required.the quality assessment of studies to enhance methodological rigour and to provide reviewers with a chance to evaluate the credibility of the conclusions and the transferability of the findings, two research members carried out individual appraisals of the included quantitative studies using the medical education research study quality instrument (mersqi) [93] and of the included qualitative studies using the consolidated criteria for reporting qualitative studies (coreq) [94].the summary of the quality assessments may be found in additional file 2: appendix 2 as well.stage 3.jigsaw perspective the themes and categories from the split approach are viewed as pieces of a jigsaw puzzle where areas of overlap allow complementary pieces to be combined.these are referred to as themes/categories.to create themes/categories, the jigsaw perspective referenced phases 4 to 6 of france et al. (2019) [95]‘s adaptation of noblit and hare (1998)[96]‘s seven phases of meta-ethnography.as per phase 4, the themes and the categories identified are grouped according to their focus.these groups are contextualised by reviewing the articles from which the themes and categories were drawn from.this process is facilitated by comparing the findings with tabulated summaries of the included articles that were created in keeping with recommendations drawn from wong, greenhalgh [97]‘s “rameses publication standards: meta-narrative reviews” and popay, roberts [98]‘s “guidance on the conduct of narrative synthesis in systematic reviews”.in keeping with france et al’s adaptation, reciprocal translation was used to determine if the themes and categories could be used interchangeably.this allowed the themes and categories to be combined to form themes/ categories.stage 4: funnelling process the funnelling process saw the themes/categories juxtaposed with key messages identified in the tabulated summaries (additional file 1: appendix 2), and reciprocal translation was used to determine if they truly reflected the data.once verified, the themes/categories formed funnelled domains and served as the ‘line of argument’ in the discussion synthesis of the ssr in seba (stage 6).results twenty-five thousand eight hundred ninety-four abstracts were identified, 257 full-text articles were reviewed, and 102 full-text articles were included.‘snowballing’ of references from these included articles saw a further 49 full-text articles added and analysed, bringing the total number to 151 (fig. 2).the split approach revealed similar themes and categories allowing the jigsaw perspective to forward six themes/categories and the funnelling process to forward six funnelled domains: curriculum design, teaching methods, curriculum content, assessment methods, integration into curriculum, and the facilitators and barriers to cst.for ease of review and given that most of the included articles did not elaborate on many of the domains, the data will be presented in tabulated form.curriculum design a variety of curricula designs were adopted due to differing curricular and program objectives, support and structure; program duration and scheduling in the learner’s training; learner and tutor availabilities, competencies, experiences and settings; assessment methods; education environment; and healthcare and education systems.the principles and models used to structure current cst programs are collated in table 2.teaching methods methods to teaching communications may be categorised into didactic and interactive methods.didactic methods include lectures [3, 13, 116–123], seminars [5, 37], presentations [35, 92, 103, 105, 114, 124–128] and are increasingly hosted on video and online platforms [14, 22, 119, 129, 130].they are occasionally supplemented by reading material [3, 119, 120].interactive methods include role-play with feedback sessions [3, 11, 13, 14, 17, 92, 103–105, 108, 118, 120, 121, 124, 127, 129–136], facilitated workshops [5, 103, 107, 118, 137–142] and group discussions [6, 13, 37, 92, 111, 121, 127, 130, 137, 143, 144].interactive methods are also used to facilitate self-directed learning such as facilitator-independent role-play [139, 142, 145–148] where participants may choose to rotate amongst themselves through the roles of patient, physician, observer and critic [149].they also include independently-held group discussions [7, 16, 35, 102, 141, 142, 150, 151] which encourage learners to learn from their peers through observation [130, 131, 142] and feedback [35, 110, 130, 152, 153], as well as engage in introspective reflection on the role and importance of good communication [5, 6, 8, 16, 103, 108, 125, 126, 142, 146, 150, 151, 153–155].feedback and reflective practice [156–158] are increasingly seen as key teaching tools critical to developing adaptive, patient-centred communication, shared decision making and negotiated treatment plans [5, 6, 8, 16, 103, 108, 125, 126, 142, 146, 150, 151, 153– 155].content of curriculum there are a diverse range of topics within current communications curricula.to remain focused upon communications training between patient and physician, we align our findings with the ‘acgme core competencies: interpersonal and communication skills’ [159] as seen in table 3.amongst this diverse array of topics, there are a few that appear more commonly within particular specialities.
number of words= 1009
[{'rouge-1': {'f': 0.3602986972704715, 'p': 0.6861616161616162,'r': 0.2442857142857143}, 'rouge-2': {'f': 0.18130820632877082, 'p': 0.2895945945945946,'r': 0.13196377502383222}, 'rouge-l': {'f': 0.3359212940936586, 'p': 0.5393877551020408,'r': 0.24391304347826087}}]
-----------------------------------------------------------------------------------------------------------------------------------
p174:
Extractive Summary:
previous studies have indicated that empathy is not well covered in medical curricula [17–20].researchers have addressed the need to measure empathy either at admission to medical school or during clinical training [7, 8, 10].health professionals should have certain degree of empathy and should put their knowledge, skills and attitude in their clinical practice to eliminate the pain and suffering of their patients [4].the jefferson scale of empathy (jse) has been used to evaluate empathy among health professionals and students of health professions in several countries such as the usa, poland, korea, italy, japan.it has been standardized for its validity and reliability [3].however, no empathy scales have been designed to measure the empathy of health professionals who are located in an area exposed to war and are practicing medicine in regions with conflicts.only one recent study has investigated the attitude of host countries’ citizens toward refugee children [29].this study has addressed the importance of designing, developing and validating scales that measure attitudes in fragile areas in which people may suffer from violence, internal displacement and adverse psychological environment.in syria, people after ten years of war are suffering from gross human rights violations, international sanction, shortage of medicine and medical equipments, chronic hunger, and the covid-19 pandemic.in these situations, health professionals should provide the life-saving assistance to community; respond to health, psychological and social needs of patients who suffer from different economic, social, psychological, and health problems regardless of their own daily suffering and daily miserable conditions.therefore, the syrian empathy scale ses was developed to measure the empathy in the context of syrian health professionals during the crisis, support decisionmaking processes, and help identifying areas that require further attention and training.the designed scale includes 20 questions and the overall score ranges from twenty to one hundred and forty in which higher scores indicate a better empathic relationship in the medical and therapeutic care.the ses was designed to be simple, cheap, readable and practical useful tool that can be used in practice settings as an attempt to shed some light on the role of syrian health professionals during conflict, with respect to health care, understanding, feeling and clinical decision-making.writing statement, which is a crucial part in designing the empathy scale to anonymous group [17, 24], has not been an easy task as it has to be simple, short, direct debatable, clear-cut, meaningful and interesting.attempt was made to make statements understandable and belonged to the same attitude variable as well as to make them relevant to the community during syrian crisis [24].for instance, issues such as feeling the pain of poor patients regardless of their social, health, and religious background (item 2) as well as recognizing the feeling of heart broken patients, (items 3) have enabled us to assess the cognitive and emotional attitude of health professionals in conflicts.however, the comparison between the ses and other designed scales has been inappropriate and testing the convergent validity would be not suitable due to differences existed in the constructs.after the factorial analysis, it was possible to identify five different components of empathy (care and understanding, feeling, health care, negative empathy impact and clinical decision making).the findings support the goodness of the factorial analysis.duarte et al. identified 6 components of empathy through the factorial analysis (compassionate care, perspective taking, cognitive dimension, standing in patient shoes clinical outcomes, no influence by others) and could also supported the goodness of the analysis [3].to increase the reliability of measurement, decrease error and save time, attempts was made to make each statement has one interpretation, contains one complete thought and one specific attitude related to one issue [24].likert scales was also adopted in order to identify the extent to which the respondent would agree or disagree with the object [26].negatively wording of half of the attitude statements was applied to provide a true measurement of an attitude, avoid the acquiescence bias and minimize extreme response that might be caused because of some respondents who might tend to agree with most statements [23].moreover, careful statistical methods and analysis such as cronbach’s alpha reliability coefficient were applied in order to verify the internal consistency of the applied scales [23].the value of cronbach’s alpha which were considered as good (0.85) provided evidence about the reliability of the applied scale [30].the alpha coefficient obtained was similar to other values obtained in some studies [31, 32] and was higher than the values obtained in other studies [2, 33, 34].the values of item-total correlations obtained for each item was higher than 0.48 indicating that an item was related to the overall scale.anonymous questionnaires to a sufficient sample size was considered in order to further validate and improve the designed scale [13].accordingly, this questionnaire can be considered as reliable for measuring empathy among syrian health professionals.the findings of the present study showed that the ses empathy score of undergraduates was significantly higher than postgraduates and it was higher in dental specialization (100.79) when compared to medicine( 97.36).similar findings were reported about the decline in empathy with increasing age or year of education [35–37].studies have attributed many factors to this consistent finding.the stress of academic performance, long work hours [38], lack of quality sleep, and increased responsibilities with age [39] are some factors that contribute to declining empathy among older individuals [40].further studies, using the ses scale, with a larger samples size are still needed to ascertain our findings.the present study reported a significant difference between males and females in the ses mean score and higher empathy scores among females.the findings were consistent with previous findings reported [41, 42] who attributed this to qualitative variance in integrating emotional information between males and females genders that can affect the decision-making process [40].similarly, hojat et al. attributed this to social learning, genetic predisposition, and evolutionary underpinnings [43].the ses has been a great tool for assessing empathy of syrian health professionals however, several procedures are still essential to increase its validity and reliability before applying it in linguistically and culturally diverse settings.for instance, multiple tests and items such as questionnaires, papers cases and observation of behavior could be developed [17].in addition, observation of medical students, during management of patients, can also be used together with empathy scale in order to improve the validity and reliability of the scale.an objective approach in which students are required to take osces by standardized patients could also be suggested to explore the association between empathy scores and ratings of clinical competence in osce stations [7, 44–46].
number of words= 1070
[{'rouge-1': {'f': 0.4196889090178512, 'p': 0.7723121387283236,'r': 0.28813285457809695}, 'rouge-2': {'f': 0.2536673861253866, 'p': 0.4265217391304348,'r': 0.1805121293800539}, 'rouge-l': {'f': 0.3526226190566424, 'p': 0.5990697674418606,'r': 0.24984189723320158}}]
-----------------------------------------------------------------------------------------------------------------------------------
p175:
Extractive Summary:
in recent decades, physician advocacy, particularly regarding social determinants of health and just distribution of resources, has been embraced as a core component of professionalism [1–3].medical organizations and codes of conduct frequently emphasize the importance of physician civic engagement [4, 5].the american medical association (ama), for example, urges physicians to “advocate for social, economic, educational, and political changes that ameliorate suffering and contribute to human wellbeing.” [6] similarly, the physician’s charter asserts that “the medical profession must promote justice in the health care system, including the fair distribution of health care resources.” [7] and physicians themselves almost unanimously agree that community participation, political involvement, and collective advocacy are important professional duties [8].yet few physicians actually engage in these tasks [1].in a 2004 survey, only a quarter of u.s.indeed, physicians take part in community and political activities less frequently than the general population and other professionals with similar levels of education and income [10, 11].while reasons for physicians’ low levels of engagement likely vary, [1] there is clearly more work to do in equipping physicians to participate in and contribute to civic life.educating medical students about their professional responsibility to advocate for health-related issues is essential to promoting more robust physician civic engagement in the future.yet relatively little is known about students’ awareness of or interest in these vital topics.we therefore set out to understand medical students’ attitudes around civic engagement -- including their interests and future plans around health policy, their sense of responsibilities around healthcare access and costs, their attitudes toward different forms of public engagement, as well as specific issues of interest -- and to determine congruence with professional obligations.we also hypothesized that students would express stronger interest in advocacy around issues directly related to health and medical care (e.g. nutrition, addiction, care access) but lesser support for engagement around indirect determinants of health (e.g. transportation, education, economic inequality).methods survey administration we conducted a cross-sectional, web-based survey of u.s.medical students.participants were recruited from student doctor network (sdn), a non-profit, online forum for current and future healthcare students and professionals.the survey link was posted on sdn’s facebook and twitter pages and on the sdn website’s homepage, online forums for allopathic and osteopathic medical students, and blog; the blog post was also distributed to selfidentified medical students who had previously opted to receive sdn emails.the survey was anonymous, but participants could opt to provide their email address to enter a lottery for 1 of 20 $100 gift cards.the survey launched on august 13, 2019, and closed on october 15, 2019.responses were collected and managed using research electronic data capture (redcap), a secure, web-based software platform hosted at memorial sloan kettering cancer center (msk) [12, 13].our findings are reported according to the checklist for reporting results of internet e-surveys [14] (supplemental digital appendix 1).the study was reviewed by the msk institutional review board (irb) and deemed exempt.survey instrument we adapted some survey items from prior studies [8, 9, 15–19] and developed additional, novel questions focusing on study objectives (supplemental digital appendix 2).a response to each question was required in order to proceed to the next.the survey included demographic items and several measures assessing participants’ interest in following or becoming involved in healthcare policy.two measures tested their views of physicians’ responsibilities to patients around healthcare access and costs (providing care regardless of patients’ ability to pay, being aware of the overall costs of care they provide) [9, 19].additional items gauged participants’ attitudes toward 3 forms of physician civic engagement: [8, 19, 20] community participation (providing healthrelated expertise to local populations), individual political engagement (being politically involved around health issues at the local, state, or national level), and collective advocacy (encouraging medical organizations to advocate for public health).the survey also assessed participants’ support for individual or collective advocacy by physicians around 18 public priorities, adapted from recent national surveys of the u.s.population [17, 18].seven issues directly related to health and healthcare: healthcare costs, healthcare coverage for the uninsured, medicare/medicaid/social security, drug addiction and treatment, abortion laws/reproductive issues, nutrition/obesity/food safety, and disability rights.eleven additional issues had connections to or implications for health: education, [21, 22] housing/homelessness, [23, 24] transportation, [25, 26] immigration, [27, 28] lgbtq (lesbian, gay, bisexual, transsexual, and queer/ questioning) issues, [29, 30] racial issues, [31, 32] economic issues, [33, 34] environmental issues, [35, 36] human rights, [37, 38] crime/criminal justice, [39, 40] and military/national security issues [41, 42].response options included likert scales for agreement (strongly agree, agree, disagree, strongly disagree) and importance (very important, somewhat important, not important).we piloted a preliminary version of the survey with 15 medical students and internal medicine residents at weill cornell medical center and made minor changes to the survey based on their feedback and responses.analysis we used descriptive statistics to summarize participants’ demographics and attitudes.we used census zones to determine geographic region of participants’ schools.we used the kruskal-wallis test to evaluate associations between demographic characteristics (including gender, race, year in school, political identification, and anticipated future field) and attitudes around healthcare policy and forms of civic engagement.we also created a composite civic-mindedness score for each participant by averaging the strength of their responses (using scores of 1 for “not important,” 2 for “somewhat important,” and 3 for “very important”) to all 18 public-priorities questions (overall score); we similarly generated composite scores for the 7 issues directly related to health and healthcare (medical score), as well as for the 11 issues addressing indirect or social determinants of health (social score).we used univariate tests and multiple linear regression to evaluate associations between demographic characteristics and overall, medical, and social scores.all analysis was performed in stata 14.2 [43].there were 815 visitors to the sdn postings linked to the survey.advocacy by individual physicians is underappreciated by students.students may incompletely understand how social determinants shape health.to ensure robust future advocacy, schools should recruit a diverse and representative student populati
number of words= 969
[{'rouge-1': {'f': 0.31341968330844877, 'p': 0.7267164179104477,'r': 0.19979351032448378}, 'rouge-2': {'f': 0.15750408663669802, 'p': 0.275,'r': 0.11035433070866142}, 'rouge-l': {'f': 0.29798033052823686, 'p': 0.5920588235294117,'r': 0.1990909090909091}}]
-----------------------------------------------------------------------------------------------------------------------------------
p176:
Extractive Summary:
introduction taking a detailed history and performing a thorough clinical examination have always been the foundation to making a diagnosis.good clinical examination skills are thought to increase the quality of care and reduce cost [1, 2].investigations are performed as an adjunct to confirm or refute the differential diagnoses.undergraduate medical education has sought to instill these skills at the earliest opportunity.the core principles of teaching a clinical examination have not evolved significantly over the last 100 years and is typically taught via a combination of large group didactic and small group seminarbased teaching styles [3].simulated encounters have aided as an adjunct to support this [4].several studies have shown that clinical examination skills are often lacking in both undergraduate and postgraduate trainees [1, 5–10].there is also a lack of agreement over the ideal time to teach clinical examination skills, and more importantly the most effective method.particular concerns have been raised over the quality of musculoskeletal clinical skills teaching [7, 11–14].the discrepancy between teaching time and proportion of patients seen with musckuloskeletal conditions is striking; oswald et al. cite in canada 2.25% of time in the curriculum is devoted to teaching the musculoskeletal system examination skills versus up-to 20% of presentations in primary care relating to the musculoskeletal system [7].furthermore, freedman et al. found only 18% of post graduates had exposure to musculoskeletal medicine and 86% of those surveyed had low confidence performing these examination skills.this shows a potential problem in both the quantity of time allocated to teaching in an undergraduate setting, and the exposure to reinforce this a a post graduate [15].modern technology means educational content can be hosted online and accessed via multiple devices.they can be accessed at times that suit the learner and support new forms of pedagogy e.g. reverse classrooms [16].education via technological platforms is however not without difficulty, with only 10% of those enrolling for ‘mass online open courses’ (moocs) completing the course.this however may not be generalisable to the well-motivated and goal-orientated medical student.there are a multitude of reasons for this, with many of them yet to be successfully addressed [17].it is clear that simply recording taught content and hosting it online is not enough to fully engage learners, and that social adaptation of technology does not immediately translate to education [18].our primary study outcome is to assess the effectiveness of different teaching modalities on naïve medical students performing a clinical examination of the shoulder joint.this randomised control trial is the first to directly compare seminar small group teaching, textbook learners and a custom-made video tutorial via an online platform.methods design this study was a prospective randomised trial comparing three different teaching modalities to teach clinical examination skills to second year medical students.the study was designed to compare three different methods of teaching clinical examination skills - face to face teaching, a custom-made educational video and a textbook chapter.an identical examination technique was taught to each group, with the mode of delivery being chosen as the intervention.detailed information on the intervention in each group is given below.the content was standardised across each modality.after permission from cambridge university press the shoulder examination chapter from examination techniques in orthopaedics [19] was reproduced in three formats.once participants had been recruited there were asked to complete a ‘vark’ learning styles questionnaire and then block randomised by a computer random number generator into the three intervention groups.the intervention in each group was on day zero of the study, to ensure standardisation of timelines.participants underwent a formal assessment of their clinical examination skills before randomisation, and at day 5 and day 19 post intervention.participants were asked to examine a patient without shoulder pathology, and were assessed by an examiner blinded to the intervention type, using a standardised scoring matrix.the study was undertaken in the medical school building then students were in a familiar environment, and was conducted in the evenings to avoid timetable clashes with scheduled teaching.the study ran for 21 days from baseline assessment to final assessment.recruitment through opportunity sampling, we aimed to recruit second year medical students at the university of sheffield (cohort size n = 290) to the study.the only exclusion criterion was previous teaching on musculoskeletal examination.the study proposal was presented to the entire year group, and contact details provided for the research team.students were emailed by the university following the presentation and provided with the contact details for the research team.the students were provided with an information booklet detailing the aims and objectives of the study and outlined the requirements.these results indicate that clinical examination skills should be taught with face-to-face teaching where possib
number of words= 757
[{'rouge-1': {'f': 0.4331168437335391, 'p': 0.6755045871559633,'r': 0.3187437185929648}, 'rouge-2': {'f': 0.18866564321487383, 'p': 0.2601840490797546,'r': 0.1479874213836478}, 'rouge-l': {'f': 0.40306093058695913, 'p': 0.5621465968586388,'r': 0.3141558441558442}}]
-----------------------------------------------------------------------------------------------------------------------------------
p177:
Extractive Summary:
professionalism is a fundamental component of medical profession which shapes the quality of medical care provided by the health care workers.the american board of internal medicine (abim), identified the core components of medical professionalism in its ‘project professionalism’, one of whose key elements is altruism described as ‘the best interest of patients, not selfinterest’ [1].at the least, there were suggestions of replacing the term with others like ‘pro-sociality’ or ‘beneficence’, to capture the obligatory nature of a doctor’s relation with the patients in a better manner [4, 7].these include sociocultural, religious, gender and economic factors, to name a few.religion also effects altruism by influencing the moral values and behaviours [9].altruistic tendencies are also affected by economic factors in health care systems.also, there is a lack of agreement on what to consider as the essential components of altruism which need to be taught to our medical students as a must feature of professionalism [8].for this, we devised the following two research questions: 1.2.methodology it was an exploratory qualitative study conducted at riphah international university between october and december 2020.research philosophy epistemological position the epistemological stance used in our study was constructivism, in which knowledge is constructed depending on interactions between individuals and their world and develops within a social context [10].the reasons for this epistemological assumption were the nature of research questions of the study, and the social context of the study.the theoretical perspective, methodology, data collection and analysis were all based on this assumption.theoretical perspective as the epistemological position of our study was constructivism, the theoretical perspective informing the methodology was interpretivism [10].this framework attempts to synergize interest of both the doctors and the patients, with the goal of promoting altruistic behaviors in physician and medical students that benefit everyone.the context of the study was clinicians doing hospitalbased practices at various clinical departments in different institutions and involved in teaching undergraduate or postgraduate medical students.this was done to probe their comprehension with regards to teaching requirement of students.there were 18 participants, including 10 females and 8 males from 6 different institutions.their clinical experience ranged from 8 to 32 years.semi-structured interview technique was chosen to gain a comprehensive understanding of participants’ views and to allow for exploration of vague answers.the interview questions aimed to find the clinicians understanding about altruism based on their clinical experiences, and the altruistic practices they considered as essential and others which they considered as optional.interview questions were refined after conducting a pilot interview.interviews were conducted by ms, si and sq, all of whom are involved in planning and development of professionalism content in undergraduate medical curriculum, are experienced in teaching undergraduate medical students and involved in conducting communication skill sessions.each interview lasted between twenty to thirty minutes.data analysis exploratory analysis of the data collected through clinicians’ responses was done.mean age was 47 years.all participants were muslim by religion.’ (interviewee 17, male).the patient quotes relevant to the context are given in italics.1. prioritizing patient interest prioritizing patients above family commitments and own interests, emerged as the most repeated concept.“there was a time my daughter was on a wheelchair with suspected cerebral malaria and i was doing rounds in the hospital” (interviewee 11, female).i will analyze the situation and communicate with my family if my presence is beneficial for my mother, i will ask for another doctor to replace me, …” (interviewee 6, female).a patient i operated on was shifted to the ward for post op care, even though not necessary i came to see him multiple times during my free time and off days, even in the evenings….“altruism is when doctor gets up in the middle of night in the winter and attend the patient and especially in covid 19 scenario using full ppe in humid weather and knowing the risk.” (interviewee 18, female).medical profession is a teamwork and when you work in an institution there are always competent people around you----” (interviewee 4, female).” (interviewee 17, male).9. tailoring treatment according to patient needs and constraints rather than denying it altogether an interesting concept suggested by one participant as a part of altruism was doing everything possible for the non-affording patients so that they are benefitted in the best possible way.so basically, catering the patients according to their social structure and financial status.10. positive psychological benefits some of the participants in our study reported a sense of satisfaction and divine connection with such altruistic acts: “sometimes we become exhausted both mentally and physically when we work beyond our capacity but at the end we feel inner satisfaction.”(interviewee 17, male).” (interviewee 14, male).” (interviewee 18, female).“so, in my personal opinion, it will be very difficult to incorporate altruism for practice.and there is a certain extent to which we can incorporate altruism into our practice.“since the system does not provide any resources for instance, transport or finance, it is not feasible for a single doctor to go out of their way to accommodate their patients.b. essential practice points for altruism when posed a question, “to what extent altruism should be an essential part of a doctor’s role as a professional and what practices do you consider as essential?the optional components with the salient relevant quotes were:  giving financial help to patients.“similarly, a doctor can excuse from doing something for a patient if it is not his designated job….” (interviewee 7, female).a doctor can excuse from doing something for a patient….under these circumstances the moral consciousness of a doctor dictates him to the right decision for a patient….discussion our study re-evaluates the practical understanding of altruism.we used an exploratory, qualitative, interview-based approach to identify clinicians’ understanding of altruism based on their clinical experience and altruistic approaches they considered essential in various clinical contexts.according to szuster, altruistic inspirations have an association with moral values [13].this was an influencing factor for few participants in our study as well.there were no definite gender differences in participant opinions about altruism, although few studies have described females as being more altruistic [14].however, when compelled to tackle the dual goals of practicing altruism and maintaining their work-life balance, they emphasized upon making decisions according to the context.the other concept was tailoring treatment according to patients’ needs and constraints and guiding them to alternate treatment options if they could not afford the primary treatment option, rather than denying treatment altogether.‘maintaining a balance’ between one’s altruistic tendencies, family life and mental health was considered as a top of the list essential for practice.the net effect of such an approach would not only benefit the patients but also promote the satisfaction and professionalism of doctors.whereas acts like blood donation or working in risky environments were taken as ‘risk-taking altruism’.it has been traditionally considered that the ‘altruistic approach’ implied giving financial help to patients, working beyond duty hours and doing non-designated work [24], as can also be deciphered from the examples narrated by the participants.for the medical educationists and curriculum developers, the findings of this study may help to reduce the ambiguity in developing professionalism component of medical curricula, for medical students of both preclinical and clinical years.revisions in curricula, with more emphasis on empathy towards patients, better communication skills, better team-work and practices which are feasible for the doctors and are in patients’ interest should be the focus.at the practical level, the study results provide a framework for the essential expected behaviours especially regarding student teaching.the context was kept limited, as the focus of the research was identifying essential altruistic behaviours to teach medical students, as part of professionalism.in this study, we identified a wide variation in clinicians understanding of altruism and shared suggestions for establishing effective and realistic altruism in medical practice.these include a balanced altruistic approach and promotion of workplace altruistic cultures through organized team-based approach rather than individual effor
number of words= 1284
[{'rouge-1': {'f': 0.32572637722526415, 'p': 0.7749808429118774,'r': 0.20619541080680978}, 'rouge-2': {'f': 0.16443766044543123, 'p': 0.29692307692307696,'r': 0.11370370370370371}, 'rouge-l': {'f': 0.3100052771004913, 'p': 0.6064238410596026,'r': 0.20822525597269625}}]
-----------------------------------------------------------------------------------------------------------------------------------
p178:
Extractive Summary:
introduction the selection of candidates to surgical training has been called a “missing link” in surgical patient safety work [1].regardless of how good the education and training system is, selection of candidates to undergo education and training is crucial.an important question is thus if the right candidates are trained to become future surgeons.currently, there is a lack of evidence on which instruments can predict future competence [2].however, there is an increased global interest in evidence-based recruitment and selection processes for specialist surgical training [3–6].assessment tools for selection have mainly been investigated in anglo-saxon countries such as australia, canada, usa, new zealand, ireland and the uk concomitant with profound reforms of the recruitment and selection processes to improve surgical training [2].this is combined with structured interviews, psychological and aptitude tests based on national templates with exact grading according to specific criteria to predict clinical performance [7–9].in contrast some countries such as sweden apply a traditional, locallybased, decentralized system for selection to surgical trainee positions without formal guidelines or firm selection criteria [10].the purpose of this report is to delineate current selection procedures in eu member-states and identify salient bases for differentiating these.cognizant of the heterogeneity that also exists within centralized and decentralized systems, the advantages and disadvantages of these two paradigmatic systems are discussed.avenues for improving selection processes with evidencebased tools independent of the underlying structure are suggested.material and methods information on bodies in eu countries responsible for surgical training programs and assessment tools for selection were obtained through google searches and pubmed.a string of keywords was used including name of country, “surgical”, “trainee”, “selection”, “specialization” or “application.” websites and “grey literature” [11] on the application and selection processes to surgical training were identified.subsequently, a letter was sent to 16 board delegates in the section of surgery of the union européenne des médecins specialists (uems) to obtain information concerning the respective country’s selection process.one reminder was sent to non-responders.the data for this article derives from an ongoing multi-disciplinary intervention project on recruitment and selection processes to specialist surgical training in sweden.all data pertaining to human subjects was obtained in adherence with relevant guidelines and regulations for the wider project’s ethical clearance obtained from the swedish ethical review authority (case number 2016–1050).the application and selection processes were sorted into three categories: centralized selection was defined as having a single national application, selection, and admission/employment process; hybrid as having one or two of the three; decentralized selection process as having none of the above undertaken at the national level.comparative case study of a decentralized (sweden) and centralized (ireland) process for trainee selection flyvebjerg argues [12] comparative case studies provide deeper and more sophisticated insight into complex, context-dependent phenomena than searching for context-independent, aggregate-based generalizations (rules).sweden was selected as an archetypical case of a decentralized selection system.data on selection procedures in sweden was obtained via available literature [10] together with semi-structured interviews with 13 experienced surgeons, as previously described [13] and knowledge within the research group.oral informed consent as per swedish ethical review authority (case number 2016–1050) was obtained from all informants, who were informed of the purpose of the study, that their contributions would be analysed and can come to comprise part of published results of the study, and that they would not be cited by name in any public documents.permission to record interviews was obtained at the opening of all recorded interviews.one interviewee did not consent to be recorded and notes were taken in this interview.the theme of selection emerged in the interview material and served as a starting point for the current study.ireland was selected as a polar opposite case to sweden, being the most centralized and formalized system for selection of surgical trainees in the eu.to obtain comparable insight, the research team conducted two study-visits to the royal college of surgery ireland (rcsi) to observe the current testing and selection procedures.visit 1 (2018) consisted of observation of interviews and selection of candidates to core surgical training.visit 2 (2019) consisted of observation of the interviewing and selection of candidates for the cut to higher surgical training which was performed after the 2 years of basic training.during the visits, relevant material describing the selection process was shared by rcsi faculty and staff, fieldnotes were taken during observation of the selection process and combined with spontaneous interviews with faculty members and involved staff [14].fieldnotes were pooled after each of the two visits to the rcsi and analysed by a team of two sociologists and four surgeons [14].focus during the study-visits was on the operationalization of the selection instruments and procedure, discussions of intended and unintended effects, challenges and benefits of the selection system in general and specific instruments, and the preconditions for their utilization.data from the site visits and supporting documents provided the core background for ireland as archetypical of the centralized system is based.results mapping selection processes in 27 countries were analysed and divided into three types of selection on a scale from centralized to hybrid to decentralized (fig. 1, table 1).slovakia was excluded due to lack of information in english.in countries where a hospital is the organizing entity for recruitment and selection of surgical trainees, there is often an employment logic, i.e., the applicant considered a part of the workforce, and thus selection for the job is conducted like any other apprentice position.in contexts where surgical training is associated with more or less permanent employment at the unit where training takes place, transition to a centralized system is unlikely.even in such contexts, a standardized selection procedure preferably composed of validated or evidence-based components, and sharing data across like units, is possible to develop and would be beneficial [57].limitations there are several limitations in our study.the countries selected were limited by language and responding uems surgical board members, leading to a possible selection bias.despite these differences, the material was considered valuable in describing the overall features for selection of trainees to surgical programs across the eu in a comparative manner.a limitation is that researchers were only able to study two prototypical examples of a centralized and decentralized system in depth.a strength of the conducted study is that the research group consisted of social scientists and surgeons, which allowed for researcher triangulation and contributions on qualitative research competence, human resource management competence and the experience of heads of department with personnel and hiring responsibilities in sweden.another strength is that data was collected over an extended period of time, which gave the opportunity for iterative data collection and discussion within the group.conclusion a wide variety of selection processes to surgical training programs and limited use of evidence-based assessment tools were found across the eu.comparing systems reveals advantages and disadvantages of different selection systems with modern evidence-based processes found in reformed centralized rather than traditional systems.based on these findings and current evidence, recruitment and selection to surgical training in many european countries has great room for improvement.a standardized procedure composed of validated evidence-based components could be developed and distributed across decentralized units as well as a fixture of centralized systems, independent of employment or educational logics without increasing cost significantly.a selection framework adopting current evidencebased instruments would include 1) ensuring multiple candidates to select from per position through wide advertisement and recruitment efforts; 2) conducting multiple mini interviews (mmi) or structured interviews by at least two assessors; 3) applying situational judgment testing (sjt); 4) conduct structured validated assessments during job trial periods with sjts (if trial work periods are employed).an unresolved issue is that the optimal weighting between these different instruments.
number of words= 1246
[{'rouge-1': {'f': 0.38583085870254197, 'p': 0.8008868501529052,'r': 0.25412942989214177}, 'rouge-2': {'f': 0.19486307117384732, 'p': 0.3368711656441718,'r': 0.13707787201233618}, 'rouge-l': {'f': 0.32863778070664573, 'p': 0.5785714285714285,'r': 0.22949820788530467}}]
-----------------------------------------------------------------------------------------------------------------------------------
p179:
Extractive Summary:
approximately 463 million people globally had diabetes in 2019, and this number is projected to rise to 700 million by 2045 [1].at the same time, over 10.8% of the world’s population is affected by severe periodontitis [2], which is the sixth complication of t2dm [3].periodontitis occurs in the tooth-supporting structures known as periodontium due to untreated gingivitis, i.e., inflammation that begins in the gums due to irritation caused by toxins produced by dental plaque [4–6].in malaysia, 18.3% of adults have diabetes and 23.6% pre-diabetes [7].meanwhile, periodontitis affects nearly half of malaysians aged 16 years and older, with 30.3% having moderate and 18.2% severe periodontitis [8].periodontitis management imposes a significant economic burden on the country [9].for a long time, healthcare professionals have emphasised managing the cause of t2dm through improvements of unhealthy lifestyles and management of obesity [10–12].more recently, there is increasing highlight on the impact of t2dm and periodontitis bilateral relationship on disease progression and treatment outcomes [13–16].since then, management of both diseases has focused on integrating diabetic care in health clinics alongside periodontitis therapy in dental clinics.however, the awareness of the diseases’ relationship among the population and non-dental healthcare professionals is still low [17–20].ethics approval was obtained from the university’s research ethics committee (reference: ukm ppi/111/8/jep-2020-106), and all participants gave written informed consent before the start of the study.a literature search via electronic databases including ebscohost and google was carried out to find relevant t2dm and/or periodontitis guidelines, articles and educational materials between august 2019 to january 2020.information was extracted and summarised in a table.the interviews were carried out in english and malay languages by a trained interviewer (c.s.h.) either in private office rooms, diet counselling room or dental clinics.a digital voice recorder (sony icd-px470, sony corporation, japan) and an android huawei mobile phone (huawei y9, huawei device co., ltd., china) were used to record all interviews, in addition to written field notes during the interview sessions, which were later summarised.the module was written in the malay language and consisted of 17 units under four topics: (i) introduction to diabetes and periodontitis, (ii) diabetes and periodontitis care, (iii) lifestyle modifications and (iv) myths and facts.all flip charts were developed using canva graphic design platform (canva pty.ltd), whereas videos were made by powtoon, the visual communication platform (powtoon ltd.).the module used simple terms and language, attractive and colourful illustrations, pictures with captions, and culturally suitable examples.the malay version of the patient education materials assessment tool for printable materials (pemat-p), which comprises 17 statements for understandability and 7 for actionability, was used to evaluate each flip chart [25].printed materials and appropriate as well as attractive audio-visual materials, preferably short videos, were proposed.culturally relevant information was also suggested by participants as lifestyle advice is largely related to culture.some participants recommended that common practice and local food should be included as part of the cultural component.phase 2: module development the developed module has 17 units consisting of 17 double-sided flip charts and 13 short videos (table 3).each unit of the flip chart has (i) a healthcare professional’s view, which consists of detailed explanations of the topic, and (ii) a patient’s view, which is more straightforward and mainly composed of pictures (infographics).the total number of pages in each flip chart unit is between 4 to 32 pages, including a front cover and an introduction section that describes the unit’s objective.meanwhile, the average length of the video is 2.35 min and ended with a take-home message.the expert panels also suggested reorganising a few medical and specific terms used in the flip charts and adding more detailed descriptions and explicit pictures.besides, short educational videos can educate and empower patients with diabetes and oral health care needs and further improve their overall quality of life.studies have well established a bi-directional relationship between periodontitis-diabetes; diabetes is one of the risk factors for periodontitis and can exacerbate the severity of periodontitis, whereas periodontal inflammation will also have adverse effects on glycaemic control [27, 28].oral health education is important as it leads to improving oral health practices [29].the present study was the first study in malaysia to integrate nutrition and oral health aspects in developing a diabetes-periodontitis module.
number of words= 688
[{'rouge-1': {'f': 0.3858280149422446, 'p': 0.6808786610878661,'r': 0.269181446111869}, 'rouge-2': {'f': 0.20862932897272674, 'p': 0.32630252100840335,'r': 0.15333333333333332}, 'rouge-l': {'f': 0.33500380337575303, 'p': 0.559655172413793,'r': 0.23904761904761906}}]
-----------------------------------------------------------------------------------------------------------------------------------
p180:
Extractive Summary:
background since early 2020, the current sars-cov-2 pandemic has challenged all the fields of knowledge, increasing the need for their interconnection.medicine, science, politics, and more specialized sectors such as biomedical engineering (bme), faced crucial ethical issues, which can no longer be underestimated.biomedical engineers design medical devices, raising many ethical dilemmas in ordinary times, which become compelling during such a crisis.the authors of this manuscript had the privilege of different points of view thanks to the president of the european society of bme (i.e., eambes) and secretary general of the global society of bme and medical physics (iupesm).three pivotal themes emerged in the global bme community: 1.the dilemma of identifying criteria for the allocation of medical devices 2.responsibilities of science and technology 3.inadequacy of regulations and norms, which lack universality this manuscript does not follow the traditional structure of scientific papers (e.g., methods, results etc.), rather it is a critical analysis, revolving around the 3 above-mentioned pillars.the first pillar focuses on the surfacing of ethical dilemmas in times of pandemic (e.g., scarce resource allocation), delving into examples from italy, where the new decision-making criteria often clashed against the existing constitutional and moral principles during the first wave.the second pillar retrospectively reports how the medical device sector was affected by the current pandemic, touching on hazardous amatorial attempts of the general public to face the scarcity of resources and the urgent needs, and on the crisis-related challenges that surfaced for manufacturers, exacerbated by the lack of dialogue with decision-makers.in the same pillar, the theoretical debate among science and politics is addressed, referring the case of the “intended use” of a medical device, from two different philosophical perspectives, i.e., substantialism and utilitarianism.the former underlines the fundamentality of substances as ontological categories [1], suggesting that the intended use of a medical device should always be respected.this invites the reader to a subsequent reflection on the inadequacy of the existing regulations on medical devices, in the third pillar.in respect to this, the manuscript proposes to offer a hermeneutic perspective close to the situational ethics that authorizes negotiations and mediations between the generality of principles and norms, and the specificity of the context.overall, the ethical considerations made in this manuscript, should be considered a valuable lesson for the future of crisis management.if and only if ethics and bioethics will be considered as effective support for science and scientists (doctors, biomedical engineers, etc.), the cartesian separation of knowledge [2] could be overcome, establishing an interdisciplinary dialogue that involves peoples and emphasizes the public relevance of such issues.in this way, such dilemmas could be anticipated by establishing a framework that could provide guidance and appropriate methodology to address arising urgent issues, without having to resort to specialists for questions that concern everyone and need a multidisciplinary approach.allocation of medical devices: clinical and ethical principles due to limited resources, decision-makers have had to compromise among all the potential useful interventions.also in the past 10 years, the most public national health systems were massively privatized, resulting in a significant reduction of prevention services and a great reduction of intensive care unit (icu) beds.the rapid spread of the sars-cov-2 resulted in an unprecedented need of sub-intensive and icu beds, which overcame the capacity of the most advanced national healthcare services.within a few weeks, the available resources (i.e., medical devices, doctors, nurses) proved to be insufficient to cover the care needs of the multitude of covid-19 patients, beyond the ordinary needs of other patients.consequently, doctors and healthcare structures ended up pondering and making difficult ethical choices in a short time and identifying priority principles that could guide them.national [3–6] and international [7–9] ethics committees, scientific societies [10, 11], and experts [12–15] soon expressed their opinion on the matter [16], identifying requirements that respect human dignity and fundamental ethical principles, enshrined in the charters of both national and international rights [17].the case of italy, the first country significantly affected outside china, is emblematic.according to the italian constitution, health is a "fundamental right of the individual" and a "collective interest" (article 32).in addition, article 2 recognizes the personalist principle and the duty of solidarity, and article 3 establishes the principle of equality.accordingly, the law founding the italian national health services (nhs) (n. 883/1978) prescribes that care must be ensured according to the principles of universality, equality and fairness.leveraging on these fundamental principles, the italian national bioethics committee [18] remarked the need to allocate medical devices and other resources based only on a clinical criterion, and without considering criteria such as age, gender, social attributes, ethnicity, disability and costs, in compliance with the principles of "justice, equity, solidarity".the envisaged method was that of "triage in a pandemic emergency", based on what the world health organization defines as "preparedness" (who) as a premise, and on two key concepts, i.e., "clinical appropriateness" and "actuality", identified by the healthcare professional on the basis of clinical criteria [19].this point of view, which values the person and opposes attempts of objectification in a series of “preestablished criteria” (except for the point of view of the clinician), is well expressed by j. habermas.in an interview with the newspaper le monde, habermas underlined the inadequacy, in moral terms, of an “objective quantification” of patients and insisted on the essential issue of the “recognition” of the individual: “when addressing a second person (you, you), the other person’s self-determination must either be respected or denied, that is, either accepted or ignored” [20].on the other hand, the italian society of anesthesiology (siaarti) introduced the identification of an age limit for accessing intensive care in case of necessity [21].siaarti’s document explained that covid-19 created a scenario, in which criteria for accessing icu may be needed beyond the clinical appropriateness and proportionality of care, but also in distributive justice and the appropriate allocation of limited healthcare resources, which means privileging those with the "greatest life expectancy" [22].even if the choice of whom to admit to treatment is a terrible reality, it must be discussed by ethicists and bioethicists to further identify solutions that support medical doctors in taking these decisions.“this is not to deny their clinical authority and responsibility, but rather to urge a commitment to give such question public relevance" so that in the future the "public health perspective" [23] to follow is clear from the beginning.responsible thinking, responsible actions, responsible silence covid-19 created a global lack of essential medical devices (e.g., pulmonary ventilators) and personal protective equipment (ppe, such as masks, respirators) [24].this led to an unprecedented amount of do-it-yourself (diy) solutions, which were fomented on media worldwide.consequently, ordinary individuals started producing ppe at home with 3d printers and everyday materials, and manufacturers converted their production facilities to develop medical devices and ppe.exactly a century, i.e., the short twentieth century, separates covid-19 from the last pandemic, the socalled "spanish flu", which flagellated europe in 1918– 1919.according to hobsbawm, “no period in history has been more penetrated by and more dependent on the natural sciences” and “yet no period, since galileo’s recantation, has been less at ease with it”.this chasm between scientists and the general public is still open and, in some cases, fomented by populisms, which leverage on people’s fears evoking war atmospheres, which have nothing to do with the catastrophic failure of many national healthcare systems’ response to this crisis.after a century, the dependence of medicine on biomedical science and engineering is evident, while their contribution to the definition of effective policies and norms is still negligible.finally, the cartesian fragmentation of knowledge, or rather “thinking in silos”, has persisted across the last century, calling for the creation of more fora where multidisciplinary discussions can be promoted.three main needs emerged clearly: the need for responsible thinking, the need for responsible action and the need for responsible silence, when required and appropriate [3
number of words= 1287
[{'rouge-1': {'f': 0.2649114284791448, 'p': 0.7928915662650602,'r': 0.15902077151335314}, 'rouge-2': {'f': 0.15581061190013284, 'p': 0.3306060606060606,'r': 0.10192279138827023}, 'rouge-l': {'f': 0.25757594909240833, 'p': 0.6044827586206896,'r': 0.16365558912386707}}]
-----------------------------------------------------------------------------------------------------------------------------------
p181:
Extractive Summary:
radiation oncology, organ allocation, robotic surgery and several other healthcare domains also stand to be significantly impacted by ai technologies in the short to medium term [6–10].the european commission has proposed legislation containing harmonized rules on artificial intelligence [16], which delineate a privacy and data principle of organizational accountability very similar to that found in the european general data protection regulation [17, 18].ai remains a fairly novel frontier in global healthcare, and one currently without a comprehensive global legal and regulatory framework.a senior advisor with england’s department of health said the patient info was obtained on an “inappropriate legal basis” [26].further controversy arose after google subsequently took direct control over deepmind’s app, effectively transferring control over stored patient data from the united kingdom to the united states [27].the ability to essentially “annex” mass quantities of private patient data to another jurisdiction is a new reality of big data and one at more risk of occurring when implementing commercial healthcare ai.the concentration of technological innovation and knowledge in big tech companies creates a power imbalance where public institutions can become more dependent and less an equal and willing partner in health tech implementation.beyond the possibility for general abuses of power, ai pose a novel challenge because the algorithms often require access to large quantities of patient data, and may use the data in different ways over time [28].strong privacy protection is realizable when institutions are structurally encouraged to cooperate to ensure data protection by their very designs [29].as we have seen, corporations may not be sufficiently encouraged to always maintain privacy protection if they can monetize the data or otherwise gain from them, and if the legal penalties are not high enough to offset this behaviour.because of these and other concerns, there have been calls for greater systemic oversight of big data health research and technology [30].given we have already seen such examples of corporate abuse of patient health information, it is unsurprising that issues of public trust can arise.for example, a 2018 survey of four thousand american adults found that only 11% were willing to share health data with tech companies, versus 72% with physicians [31].moreover, only 31% were “somewhat confident” or “confident” in tech companies’ data security [28].in some jurisdictions like the united states, this has not stopped hospitals from sharing patient data that is not fully anonymized with companies like microsoft and ibm [32].the problem of reidentification another concern with big data use of commercialai relates to the external risk of privacy breaches from highly sophisticated algorithmic systems themselves.conclusions it is an exciting period in the development and implementation of healthcare ai, and patients whose data are used by these ai should benefit significantly, if not greatly, from the health improvements these technologies generate.given personal medical information is among the most private and legally protected forms of data, there are significant concerns about how access, control and use by for-profit parties might change over time with a self-improving ai.for example, requirements for technologically-facilitated recurrent informed consent for new uses of data, where possible, would help to respect the privacy and agency of patients.also, the right to withdraw data could be clearly communicated and especially made easy to exercise; where feasible, generative data could be used to fill the data gaps created by these agency-driven withdrawals and to avoid de-operationalizing ai systems.this will require innovation, and there will also be a regulatory component to ensuring that private custodians of data are using cutting edge and safe methods of protecting patient privacy.given we are now dealing with technologies that can improve themselves at a rapid pace, we risk falling very behind, very quick
number of words= 603
[{'rouge-1': {'f': 0.4567109025318583, 'p': 0.7353061224489796,'r': 0.33121794871794874}, 'rouge-2': {'f': 0.1943121020527395, 'p': 0.27491803278688526,'r': 0.15025682182985556}, 'rouge-l': {'f': 0.3533774589041464, 'p': 0.5263758389261746,'r': 0.26596541786743516}}]
-----------------------------------------------------------------------------------------------------------------------------------
p182:
Extractive Summary:
umbilical cord blood is a rich source of stem cells that can be used in treatments for blood and immune system disorders.it is also used for research into novel therapies not yet ready for clinical application [1, 2].the use of the stem cells in these applications depends on individuals banking their cord blood in either public, private, or public-private hybrid banks.it is an individual’s choice to bank their cord blood and to do so in either a public or private bank.that decision has the potential to be influenced by numerous sources, including health care practitioners and family and friends [3].but the media – including the popular press, social media, and communication efforts of both public and private entities – can also have an impact [4–7].research shows that there are issues around the public’s awareness and knowledge of cord blood uses, and that information sources are “varied, fragmented and inconsistent” [3].private banking is a for-profit industry through which parents pay to store umbilical cord blood for potential future use.there are currently seven private banking companies marketing in canada, typically charging an initial fee of approximately $1000.00 cad, followed by a yearly charge of approximately $125.00 [8].public banking is a form of donation where the tissues are generally used to help other patients or for research, is free of cost, and is overseen by government agencies.enforcement should be feasible given that only seven private companies operate in canada, though public complaints would certainly aid in spurring the bureau into action.other relevant law and policy in addition to truth in advertising law, the common law relating to negligence, misrepresentation, and fiduciary obligation can be helpful in contexts where health professionals are presenting banking options to patients.common law requires that these professionals practice to the standard of “a prudent and diligent doctor in the same circumstances,“ [60] a standard that is generally incompatible with interventions that are not evidence-based [46, 61].physicians also have a legal duty of care to patients who are making medical decisions, and pursuant to this duty they must disclose all information a reasonable person in the patient’s position would want to know [62, 63].while the service of preserving cord blood is itself evidence-based, advertising for yet unknown future therapies is not.hence, disclosure requirements likely mandate that physicians inform patients about the problems with these representations, and the facts and probabilities relating to the low chance of utility from private banking.in canada, based on the statements of provincial governments such as those in alberta and british columbia, “doctors do not recommend that you bank cord blood on the slight chance that your baby will need stem cells someday,” and patients should “consider donating the cord blood to a public bank instead.” [14, 15] for most health-related advertising, health professional standards of practice, codes of ethics and disciplinary tribunal precedents are applicable and set a high standard of conduct [46, 64, 65].this is because marketing by health professionals often relates to a medical intervention or a health product used directly by a patient on their body, which triggers both the noted legal obligations but also these standards.however, the applicability of these policies could be questionable in the context of private cord blood bank marketing.to our knowledge, there is no requirement that patients consult with a bank’s health professionals prior to entering into an agreement to store cord blood with a bank, and while physicians and other medical professionals may hold positions at various levels in a bank’s business structure, client contact is likely to be minor and not to constitute a physician/patient relationship that would trigger additional professional standards and obligations.still, if a physician is involved, he or she would be bound by relevant standards and ethical requirements regarding advertising, and if a private bank were under a physician’s ownership it is also likely these standards would apply.conclusions when making decisions about biobanking, consumers must be presented with accurate information about the likelihood of private banking resulting in benefit.disclosures should also include a consideration of the opportunity cost of storing privately instead of donating to a public bank and possibly contributing to a life-saving therapy for someone else.both public health and personal financial harm can occur when misleading advertisements are used to convince patients to use a private instead of a public cord blood bank.here we have argued that private cord blood banking advertising that includes claims relating to speculative future health benefits from unknown or unproven therapies potentially breaches advertising law.though we have used canada as an example, and there may be legal differences in other jurisdictions that manifest as slightly different limits on the extent of permissible prognostication, the general proposition holds that such marketing is materially misleading in a way that is likely to impact consumer decision making – and especially the decision making of a "credulous" and inexperienced consumer [59].regulatory bodies such as the competition bureau, industry associations such as ad standards and nonprofit advertising watchdogs such as truth in advertising should all act to encourage enforcement of existing law, to help prevent new parents from being persuaded to privately store tissue that is likely of no personal utility and may result in significant financial and public health co
number of words= 861
[{'rouge-1': {'f': 0.46991666073539246, 'p': 0.7566096866096865,'r': 0.3407865168539326}, 'rouge-2': {'f': 0.19346863833346004, 'p': 0.27285714285714285,'r': 0.1498650168728909}, 'rouge-l': {'f': 0.3884929592701441, 'p': 0.562063492063492,'r': 0.29682926829268297}}]
-----------------------------------------------------------------------------------------------------------------------------------
p183:
Extractive Summary:
“the honour of belonging to the royal society is much sought after by medical men, as contributing to the success of their professional efforts, and two consequences result from it.in the first place the pages of the transactions of the royal society occasionally contain medical papers of very moderate merit; and, in the second, the preponderance of the medical interest introduces into the society some of the jealousies of that profession.on the other hand, medicine is intimately connected with many sciences, and its professors are usually too much occupied in their practice to exert themselves, except upon great occasions.” charles babbage, reflections on the decline of science in england, and on some of its causes – of the influence of the colleges of physicians and surgeons (1830) introduction this excerpt from babbage’s famously cantankerous treatise continues to ring true almost 200 years later.though a great deal of newspaper print, conference discussion and journal space has been dedicated to addressing the urgent concerns of scientific irreproducibility and outright fraud amongst full-time scientists [1–5] the reasons for research misconduct of medical doctors (hereon referred to simply as doctors) has attracted far less attention here in the uk.after behaving dishonestly in the operation of clinical trials involving patients at his lancashire gp practice, the erasure of jerome kerrane from the uk medical register in 2019 serves as another reminder, almost 10 years after andrew wakefield’s erasure from the medical register, of how the public image of doctors can be tarnished [6].however, reflection on whether the perpetrators are cynical wildcards or our professional culture or institutional policies may be contributory remains to be seen.conflicts of interest between researchers and drug and device companies have been discussed at length [7–10] but examination of the professional culture of medicine in the uk is uncommon, critical analysis of the training and regulatory institutions even less so.richard smith and peter wilmshurst are amongst the small number of critics of a professional culture that finds it difficult to accept the presence of dishonesty amongst its ranks and sometimes actively refuses to investigate it [11–13].in 1993 stephen lock and frank wells published the first edition of the only book in print dedicated to medical research misconduct and have also made recommendations as to how the profession can be policed to prevent abuses of power [14].the transfer of european union clinical trials legislation into uk law in 2004 made significant deviations from the gmc’s good medical practice illegal within the context of clinical trials.notwithstanding this, there has been little to no advancement in regulating doctors’ forays into research.historical context the association between medicine and scientific theory is probably as old as medicine itself, given the need for a philosophy of health and disease in which to couch the pragmatic business of diagnosis and cure.for most of british history, the practice of healthcare was delivered by an admixture of practitioners trained via informal apprenticeships and university educated physicians.it is noteworthy that the most recent world conference on research integrity had no practising doctors as speakers, while a research study from a malaysian group suggests that academics in healthcare departments were more likely to be involved with research misconduct than other academics [47].liang, mackey and lovett at the ucsd school of medicine incisively argue that the negative effects of research fraud are incalculable due to the reliance of low-income countries on our research to guide the development of their own efforts.describing the damaging effects of a dishonest culture on the mind of the developing clinician: ‘ … students and trainees do not push back against this system because of the reasonable fear of recrimination within the culture, and then over time and long hours simply become acculturated to it as a matter of survival.yet a culture where senior academic physicians are unchallenged, unethical, and reward dishonest medical trainee applications supports a never-ending cycle of inculcation of corrupting influences in each generation of academic medicine.’ [48] discussions of medical education’s role in preventing research misconduct are frequently misplaced since the reasons for refraining from research misconduct barely extend beyond any normative conception of ethical behaviour and distracts from the doctor’s agency to behave ethically [49].fenton and jones in their otherwise helpful 2002 paper state that: ‘the preponderance of misconduct occurs because many authors are not informed on ethics, as these issues have generally not been addressed in medical undergraduate or postgraduate education.’ [50] the truth of this statement is undermined by the absence of any advanced ethical framework required to understand that dishonest research activity is unethical, let alone its incommensurability with the role of a doctor.in the words of the influential british medical ethicist thomas percival: ‘let both the physician and surgeon never forget that their professions are public trusts, properly rendered lucrative whilst they fulfil them, but which they are bound by honour and probity to relinquish as soon as they find themselves unequal to their adequate and faithful execution.’ [51] whether medical education academes are best placed to instruct on these virtues remains unclear, particularly in light of the striking research findings of anthony artino jr.and colleagues, revealing that over 90% of the healthcare education researchers in their international survey admitted to having partaken in some form of questionable research practice, including 49.5% of respondents confessing to have cited papers that they had not read [52].though fora to allow researchers to discuss research ethics are likely to be helpful, it is unknown as to who would be best placed to model the attributes espoused by percival, or whether the virtues can be inculcated via professional or academic programs at all [53].conclusion the code of silence that protects senior doctors from scrutiny is rarely discussed in the uk and the gmc could help break this by issuing more sanctions on doctors who have acted in a wilfully dishonest fashion [54].there is a compelling need for the royal medical colleges to assess the degree of misrepresentation in doctors’ job applications and should reformulate how medical trainees are incentivised.demonstrating involvement with patient safety improvement or already existing research projects would be a much more constructive incentive than that which not only allows research misconduct, but appears to encourage it [31].though the medical profession still secures the trust of the majority of the british people, there is some suggestion that this has been declining [55].a meaningful discourse on how our training and reward systems should be improved would help prevent future generations of doctors from pursuing dishonest opportunities to advance their careers and would have a positive effect on public trust in the medical profession, a relationship that we still rely on to serve our patients effective
number of words= 1102
[{'rouge-1': {'f': 0.24899032868170765, 'p': 0.791311475409836,'r': 0.14773851590106007}, 'rouge-2': {'f': 0.14614526366421635, 'p': 0.30966942148760335,'r': 0.09564102564102564}, 'rouge-l': {'f': 0.23234552528827285, 'p': 0.5943902439024391,'r': 0.14439446366782008}}]
-----------------------------------------------------------------------------------------------------------------------------------
p184:
Extractive Summary:
maid was defined in bill c-14 as (a) the administering or prescribing by a clinician of a substance to a person, at their request, that causes their death; or (b) the prescribing or providing by a clinician of a substance to a person, at their request, so that they may self-administer the substance and in doing so cause their own death [1].this gap in medical training will be further complicated by the introduction of maid, as well as the lack of experience among educators to teach residents how to field requests for assisted dying.results from existing u.s.-based studies suggest that residents are hesitant to participate in maid (1–40%), but may be supportive of their colleagues’ involvement [7–11].until recently, however, there have not been any formal investigations of canadian residents’ perceptions of or willingness to participate in maid.studies have demonstrated that physicians are accepting of indirect end-of-life activities such as withholding or withdrawing life sustaining treatments (i.e., an active decision-making process to stop or not start a given intervention that would prevent a patient from dying), but not physician-assisted suicide [15].in our study, we used the term “participate in physician hastened death (phd)” to refer to participation in maid in a broad sense.in this respect, maid can be seen as a part of a continuum of end-of-life care that ranges from discontinuing or withholding treatment, to actively participating in phd, and finally the provision of maid.as the opinions and factors that shape canadian residents’ willingness to participate in maid have not been fully described, the objectives of our study were to: [1] describe canadian family medicine residents’ attitudes towards phd and the provision of maid; and, [2] identify the factors (e.g., demographics, clinical exposure to death and dying) that may influence their decision to actively provide maid.methods the survey we conducted a cross-sectional study of family medicine residents in canada to determine their opinions on phd and maid and the factors that may influence their decision to participate.all family medicine programs in canada were invited to participate.we included residents in their postgraduate year (pgy) 1 or pgy2 but excluded medical students and staff physicians.the survey was made available between december 2016 and april 2017 in english or french.metrics our survey (see additional file 1: questionnaire on maid) captured residents’ demographic characteristics (age, gender, ethnicity, training, and faith), clinical exposure to death and dying (e.g., managing pain and suffering, declaring death) and attitude towards phd and maid using an example of a patient who fulfills all the criteria for maid under bill c-14.logistic regression models were used to examine the significance of descriptive variables on residents’ willingness to participate in phd and maid.there were more female (n = 175) than male (n = 72) participants.christians (48.6%) were the largest religious group, followed by those who were non-religious (34.4%) and of other religions (15.8%), which included participants who self-identified as jewish, muslim, buddhist, hindu, sikh, aboriginal, or all others.willingness to participate in phd and maid the proportion of residents who agreed with different activities across the spectrum of end-of-life care is summarized in table 1 and is organized in the order of increasing involvement (i.e., from treatment withdrawal to administering a lethal injection).quebec residents and caucasian residents were significantly less willing to withdraw treatment.however, this correlation was only statistically significant among residents with exposure to talking to families after death.increased likelihood of willingness to withdraw treatment was seen in ontario and prairie residents compared to quebec.higher likelihood of willingness to participate in phd was observed among residents “not” (or = 17.38) or “not strictly” (or = 5.17) practicing their religion, as well as residents practicing a non-christian religion (or = 3.84).additionally, residents who had more clinical exposure to death and dying (11+ cases) in the form of talking to and counselling family members after death had greater odds of expressing a willingness to participate in phd and maid by lethal injection.however, after adjusting for the effects of other factors in our logistic regression models, only a few specific exposures to death and dying remained statistically significant.overall, the strongest predictors of willingness to participate in maid from our multivariable analyses were: [1] not practicing a religion, and [2] participating in a religion other than christianity.interestingly, our model also showed that ontario and prairie residents were more likely to withdraw treatment than their counterparts in quebec, and that females were less likely to prescribe a lethal drug than males.furthermore, the act of carrying out a request for assisted suicide represents an injustice that cannot be excused [16].nonetheless, our observation based on comparing the views of residents who self-identified as christians and those whose belief lies in other faiths is consistent with several others studies, which also found lower agreement to participate in maid among christians [8, 11, 13–15] and echoes the current opinions about maid among some christian physicians in canada [17].although this finding is interesting, the generalizability of this observation may be limited as our study population was predominantly christian or non-religious, and the category of “other religion” is a conglomerate of different faiths.female residents were less likely to be willing to participate in maid by lethal prescription than their male colleagues.we also observed that females were also less willing to participate in phd and participate in maid by lethal injection, although results were not statistically significant in our regression model.while this is noteworthy, existing literature have been inconclusive on the role of female gender and the influence it has on medical residents’ and physicians’ perspective on phd or maid; previous studies have suggested that female residents are less accepting of phd [18], but others show no difference [9, 11] or even the opposite trend [12].our study suggests that residents are more agreeable to participate in phd than physicians in canada (40.9% vs. 29%) [3] but less agreeable than canadian medical students (71%) [22].nonetheless, this difference in attitude across different stages in clinical practice echoes previous u.s., mexican and canadian studies, which found that staff physicians were more hesitant to participate in maid [6, 7, 10, 18].in addition, one study demonstrated that residents with more years in clinical practice (e.g., fellows in their 5th– 8th year of oncology training) were more hesitant to participate in phd than residents in their first 3 years of post-graduate training [13].our study showed that family medicine residents in canada may be hesitant to participate in phd and the provision of maid.despite our small sample size, this observation is consistent with other studies on residents’ standpoints on maid [7–14, 18].since most existing studies were single-centred, opinions often only reflect the local acceptance of assisted death.for example, residents in our study were less willing to participate in maid than a recent canadian study at one university [6].this difference could be explained by the sample population (being single-centred vs. multi-centred) or, possibly, because their scenario for residents was more passive (i.e., observing or participation in maid as a part of a team), which has been shown to be more acceptable to physicians.poor response rates are not uncommon for national surveys of residents in canada; a recent national resident survey by the resident doctors of canada had a response rate of 15.8% [23].further studies focusing on this specific distinction would help clarify residents’ attitudes on indirect maid activities.sex and exposure to death and dying were also found to potentially affect the willingness of residents to provide maid, although further study is needed to clarify these relationship
number of words= 1234
[{'rouge-1': {'f': 0.4179016613615262, 'p': 0.8220661157024793,'r': 0.28016166281755195}, 'rouge-2': {'f': 0.24738739425348677, 'p': 0.4374033149171271,'r': 0.1724653312788906}, 'rouge-l': {'f': 0.4034672571535944, 'p': 0.6625925925925926,'r': 0.29003929273084483}}]
-----------------------------------------------------------------------------------------------------------------------------------
p185:
Extractive Summary:
background bedside rationing and clinical ethics committees in publicly funded health services there is a need to ration healthcare as claims on healthcare exceed available resources.many countries have set up systems and institutions, such as the uk’s nice, to promote fair priority setting.important dilemmas arise in bedside rationing; however, in practice, clinicians might not be aware of priority setting principles or mechanisms for promoting fair bedside rationing, and might lack support from managers, guidelines, legislation, and politicians for the difficult rationing decisions that sometimes have to be made [1].clinical ethics committees (cecs) have been proposed to be of some value in mitigating both these deficiencies [2].cecs are multidisciplinary committees tasked with increasing clinician awareness of ethical problems and solutions in their workday and with providing deliberations and advice on difficult clinicalethical problems.cecs have long been features of many hospitals in the western world.recently it has been argued that cecs might also play several constructive roles in issues concerning priority setting, resource use and bedside rationing [2].legitimacy in rationing: two theoretical frameworks however, problems of bedside rationing often have a special feature that distinguish them from other clinicalethical problems.as bedside rationing issues concern scarce resources, the outcome is of relevance to further stakeholders than the actors directly involved, especially when the decision requires resources that could have been spent differently.for a resource driving outcome to be legitimate, therefore, more than the assent of the clinicians and patient directly involved is required.there is then a question whether cec deliberations about issues of bedside rationing can contribute to the legitimacy of processes conducted and conclusions reached.thus we ask: what does it take for the cec to be a legitimate actor in bedside rationing dilemmas in the hospital?limit-setting decisions in health carried out on a population level can be subjected to regulations by specific criteria for just allocation of health care.however, as pointed out by daniels and sabin [3], we cannot expect people to agree on what specific criteria or principles should be applied to promote justice in these decisions.rather, we must ensure that the way the criteria or principles are reached –the decision-making processes – are such that people perceive them as fair and legitimate.the procedural approach that daniels and sabin advocate, ‘accountability for reasonableness’ (a4r), stresses the importance of transparency and accountability.this framework consists of four procedural conditions that are necessary for the outcome of priority setting processes to be legitimate: “transparency about the grounds for decisions; appeals to rationales that all can accept as relevant in meeting health care needs fairly; and procedures for revising decisions in light of challenges to them”; and in addition, there must be institutions in place to enforce the previous three requirements [3].when subjected to this kind of regulating process, the decisions that are the outcomes of the process can be justified as legitimate.however, the kind of rationing decisions daniels and sabin have in mind are those concerning patient groups, not individuals.clinical priority setting decisions about individual claims on healthcare (‘bedside’) can also be perceived as indirectly subject to similar conditions of procedural justice, as clinicians are expected to comply with evidence-based clinical guidelines when there is no obvious reason to consider them irrelevant for the case at hand.the development of evidence based clinical guidelines inevitably relies upon normative considerations [4–7] which can – and should – also be subjected to accountability.norheim has developed a framework wherein the acceptability of processes of development of guidelines are evaluated, and where guidelines themselves are considered as tools for priority setting [8, 9].evidently, clinical guidelines cannot provide conclusive guidance on every clinical decision that has to be made.for instance, while emergency department triage is guided by principles, decisions ultimately rely on discretionary considerations of how these principles apply to the particular cases at hand.sometimes there might be reasons to deviate from guidelines by providing or limiting care and the choice whether to do so constitutes a dilemma.at other times there are conflicting views as to whether existing guidelines apply or not.in such situations, what can make decisions legitimate?to shed light on this question, bærøe developed a framework to complement the a4r framework [10].this approach was inspired by the identified lack of transferability of legitimacy from macro-level decisions about claims of patient groups to micro-level claims of individuals [11]; the contextual conditions for making ethical judgments do not coincide.based on analyses of the concept of a ‘healthcare claim’ and of clinicians’ scope of responsibility respectively, as well as an appreciation of the theoretical grounding of a4r, the framework details conditions for legitimacy for clinical decisions with priority-setting consequences at the micro (bedside) level.together, the a4r and bærøe’s approach can be seen as mutually supporting accountable and legitimate decisions throughout a publicly funded healthcare system.1 a role for cecs in bedside rationing?against this backdrop of two theoretical approaches to what makes rationing decisions legitimate we will explore the role of cecs in such decisions.we focus on situations of choice that have implications for priority setting/resource allocation and rationing and where the decision-makers are healthcare personnel (or managers) with the discretion to make judgments about bedside rationing.we make one crucial assumption: cecs do not have the mandate to make bedside rationing decisions, only to assist in decision-making.there can be different reasons for this.in so far as cecs at the same time also strive for impartiality in addressing and scrutinizing stakeholders’ personal values – to also critically test the established regulations and expectations – this enhances all dimensions of legitimacy.if, on the other hand, cecs insist on narrowly understanding new clinical issues in light of the values previous cases have been appreciated according to, this undermines the same dimensions of legitimacy.prerequisites for contributing to legitimacy the discussion above has shown that cecs are able to contribute to procedural, epistemic and/or political legitimacy to the extent that they fulfil the six roles that cecs can play in issues of priority setting.however, the discussion has also highlighted prerequisites for cecs being able to fulfil these roles.these prerequisites may be grouped into three main categories: 1) the ability to adequately represent all stakeholders; 2) the ability to deliberate – and critically revise this deliberation on appeals – in a systematic manner that promotes impartiality and reasonableness, drawing on relevant clinical, ethical and legal arguments; and 3) transparency with respect to the deliberation and conclusions.the extent to which cecs will be able to fulfil the prerequisites and thus contribute to legitimacy in practice is an interesting question which should be investigated empirically.the seven requirements for legitimacy of clinical decisions with priority-setting consequences and the requirements set out in the discussion of the six roles for the cec could form the basis of evaluation criteria for cec consultations on the topic.conclusions on the basis of criteria for legitimacy of clinical decisions with priority-setting consequences and roles that cecs might play in case consultations involving priority setting issues, we have shown that cecs have the potential to increase the legitimacy of such decisions.however, there are important prerequisites that must be in place for the cec to contribute to legitimacy.it seems likely that a cec consultation will serve to increase the overall legitimacy of the decision above what single clinicians would achieve on their own even though not all prerequisites are fully in place.however, there is a danger that a cec consultation that is flawed in respect of the prerequisites can confer a misleading ‘stamp of approval’ on the deliberation process and moral analysis.there is then a risk of bias and unjustified solutions and decisions based on fallacious reasoning or inadequate processes [18].involvement of the cec might then serve to decrease legitimacy of decisions made.
number of words= 1265
[{'rouge-1': {'f': 0.3089061712335514, 'p': 0.8284541062801933,'r': 0.1898473282442748}, 'rouge-2': {'f': 0.16373083137306324, 'p': 0.3224271844660194,'r': 0.1097249809014515}, 'rouge-l': {'f': 0.3131966248311563, 'p': 0.6547457627118645,'r': 0.20582677165354332}}]
-----------------------------------------------------------------------------------------------------------------------------------
p186:
Extractive Summary:
background disasters are defined as phenomena caused by environmental events or armed conflicts that lead to fatalities, injuries, stress, physical damage and economic breakdown of great significance [1, 2].they occur on a scale that overwhelms local resources, usually requiring external assistance.improving the effectiveness and efficiency of interventions, and the fairness of their distribution, is crucial in the field of disaster response.for that reason, increasing and improving the scientific evidence for disaster relief is essential.research is also vital to accurately describe phenomena in disasters, also called humanitarian emergencies or crises [3–5].conducting research during or in the aftermath of disasters poses many specific practical and ethical challenges.this is particularly the case with research involving human subjects where data collection must be balanced with the appropriate protection of research subjects.researchers play a central part in this analysis, as does the system of research ethics review.we decided to perform a systematic qualitative review of existing disaster research ethics guidelines to collect and compare existing regulations.we used the following search terms: (guidelines and “research ethics” and (disaster or emergency or crisis)).assessment of eligibility was limited to the first 200 hits retrieved in google and to the first 250 hits in google scholar ordered by relevance in accordance with the methods used in numerous similar systematic reviews.limits were not placed on the pubmed search.the screening process is summarized in fig. 1.at the first screening stage, one researcher reviewed the document titles.only documents written in english or translated into english by the guideline developers were included.titles clearly not related to the topic, as well as scientific and popular articles, books, presentations, and opinion pieces which were clearly not guidelines were excluded.this gave 110 documents which were further screened.the second eligibility screening was performed independently by two researchers.independent results were compared between the two researchers.when discrepancies existed, a third researcher was involved to resolve any eligibility disagreements.inclusion/exclusion criteria documents were included if they fulfilled all the following criteria: a. satisfied our definition of research ethics guidelines: systematically developed statements to assist with the responsible conduct of researchers and other stakeholders in the process of planning, conducting, and reporting research; b. was issued by an international or national organization/institution/meeting or developed by a group of researchers or an individual researcher; c. research ethics in disaster settings was addressed in the whole document or at least in its own part or section; d. addressed at least one of the following types of research: clinical drug research; biomedical research involving physical interventions; public health research; research on health data or biological material; psychological or social sciences research.this method of qualitative analysis combines inductive category coding with a simultaneous comparison of all obtained units of meaning.a unit of meaning is defined as a part of the text (e.g., phrase, sentence, several sentences) that “must be understandable without additional information, except for knowledge of the researcher’s focus of inquiry” [7].open coding was applied as a first step in the coding process to identify units of meaning and to allow categories to emerge from the data.according to ccm, each new unit of meaning is “compared to all other units and subsequently grouped (categorized and coded) with similar units of meaning” [7].in the process of open coding and comparison, initial categories were changed, merged and omitted when necessary.the second step involved axial coding to explore connections between categories and subcategories.selective coding as a third step involved selecting the core themes.as a result of selective coding, we identified two core themes emerging from our analysis: research ethics review process and vulnerability.results the research team reached consensus on including 14 full text guidelines for analysis (see additional file 1: table s1 for the complete list).international organizations issuing guidelines included the world health organization (who) [8], msf [9] and the international ngo training and research centre [10].at the same time, the themes overlap and a clear distinction between the two is not possible.within each of the two core themes, various categories were identified, and within each of these, further subcategories were identified.these are summarized in additional file 2: table s2 and additional file 3: table s3, and described in detail in the sections below.for each category, we identified a set of subcategories.for example, collogan et al. discussed different approaches to the definition of vulnerability and contrasted vulnerability as “a characteristic of the group” with “certain individual characteristics” [13].these arose from specific disaster situations and included, for example, political status and human rights abuses in refugee populations [16]; young and old age of research subjects, social vulnerability, physical injuries, and experience of violent and traumatic events leading to mental health problems in the aftermath of disaster [13]; psychological and physical consequences of a disaster, as well as poverty in pre-disaster settings and disempowerment in post-disaster settings [17], increased public risks and devastation [18] and substantial psychological stress in humanitarian settings [19].risks and burdens this category covered different types of risks and burdens that research subjects might face during disaster research.the category included six subcategories: physical harm (in seven guidelines), re-traumatization (in five guidelines), manipulation (in two guidelines), exploitation (in eight guidelines), unrealistic expectations (in eight guidelines) and stigmatization (in two guidelines).guidelines were more detailed in their analysis of different types of non-physical risks.while some guidelines just mentioned exploitation in general, others gave more detailed examples, e.g., schopper et al. mentioned the collection, export, and analysis of tissues as a potential source of “exploitation of communities from which tissues have been taken” [20].some guidelines noted the importance of recognising that disaster settings offer “limited opportunities for therapeutic interventions to handle adverse psychological reactions” [17].authors of these guidelines, especially after 2008, emphasized that risks in disaster research can be diminished by monitoring and control [13] and mentioned corresponding values, e.g., accountability and transparency [8].the same guidelines dealing with estimation of risks mentioned the need for empirical evidence to evaluate risks posed by research [13, 17, 21].other guidelines emphasized the need to identify available local services and to help research participants access these services when needed [17].for each category we identified a set of subcategories as summarized in additional file 3: table s3 and referenced to the specific guidelines.experience and awareness of researchers was mentioned in many research ethics guidelines for disaster settings, and our analysis showed five subcategories for this category: cultural sensitivity of researchers (in five guidelines); awareness of impact of research (in three guidelines); conflicts of interest (in four guidelines); training in research ethics (in four guidelines); and professional competence of researchers (in three guidelines).training in research ethics was mentioned as an important aspect by many guidelines, including that recs be required to ensure that researchers complete an ethics module on doing research in disaster situations.some guidelines emphasized the strong ethical mandate to do research in disaster settings “to prevent further death and illness in present or future disasters” [21], with some suggesting it might be unethical not to do such research [11].at the same time, guidelines do not provide specific methods for evaluating risks and benefits.regulation of transfer of biological material was addressed in five guidelines.another subcategory included in the guidelines was the application of standard of care in disaster settings and the justification of possible alterations [8, 9, 21].two guidelines included a provision that recs should also evaluate whether the proposed research will drain funds, resources or necessary personnel devoted to immediate disaster relief.overly burdensome bureaucracy and undue delays in the review process were mentioned in three guidelines and appeared as a problem often faced by researchers.our aim was to identify, describe and compare disaster research ethics guidelines.cioms guidelines refer to vulnerability as “a substantial incapacity to protect one’s own interests”, and accordingly state that “special provision must be made for the protection of the rights and welfare of vulnerable persons” [22].beyond vulnerability as a core theme in disaster settings, the analyzed guidelines discussed and attempted to raise awareness about specific risks that disaster research might pose for participants.generally, informed consent was seen as a necessary but also challenging requirement in situations where language and cultural barriers could be determining factors, as well as where the decision-making capacities of participants could have been impacted by disasters.a repository of innovative and evidence-based approaches to informed consent would be very valuable.in contrast, other guidelines would not prohibit such research if it posed minimal harm and had a considerable benefit to society.in a disaster setting, a proper coordination center might provide a way to involve local researchers and the local community.such a center could reduce significantly the duplication of research activities, and adequately assess the potential conflict between research and treatment or aid.it could also give proper consideration to ethical perspectives on benefit sharing with the local community and research participants.this highlights the need for more empirical research and evidence regarding the ethics of research in disaster settings.such an example is the post- research ethics analysis [25] project that attempts to collect and assess concrete, real world research experiences and ethical challenges faced by researchers and other stakeholders that could further support the development and evaluation of such guidance documents.given the tension noted above between generalized and specific approaches to guidance, such practical decisionmaking tools will be essential.these include the prevalence of ethically significant scenarios, and a typology of ethical issues, including, for example, vulnerability, re-traumatization, or lack of local rec approval.disaster researchers and the recs who review their protocols should include projects to evaluate how well the ethical issues are addressed in the research and by following rec recommendations.particular attention should be given to assessing participants’ perceptions of how ethics is addressed in specific projects.national recs and international networks of recs, or their professional associations, should reconsider their standard recommendations and procedures in light of the challenges posed by disaster situations.a comprehensive guideline for disaster research ethics that takes account of the different types of research methods, contexts and populations would be very helpful.models for proper coordination centres in disaster settings that could be responsive to the identified research ethics challenges could be advantageous.beyond the clear need for further work on disaster research ethics guidelines, we also see that it is important that specific educational resources for disaster research ethics training be developed and disseminat
number of words= 1686
[{'rouge-1': {'f': 0.28229900254710194, 'p': 0.8512500000000001,'r': 0.1692063492063492}, 'rouge-2': {'f': 0.1928150157373876, 'p': 0.47358744394618835,'r': 0.12104934770277936}, 'rouge-l': {'f': 0.2826503340757238, 'p': 0.6859999999999999,'r': 0.17799438990182329}}]
-----------------------------------------------------------------------------------------------------------------------------------
p187:
Extractive Summary:
traditional medicine had been practiced for many centuries in korea before western medicine was introduced.asian medical practitioners have traditionally been expected to be concerned healers who cure sicknesses of body and soul; indeed, they are also respected for these virtues.it emphasizes the physician’s scientific knowledge and medical skill; that is, competence is valued over moral attitude.according to an old saying, the good doctor treats diseases of the body; the better doctor heals sickness of the soul; and the great doctor cures both of these and societal illness as well.the individual and collective obligations of physicians are to restore the health of the individual and of society [20].the essence of physicians lies in their role as both healers and professionals [14].physicians have a social responsibility: they must be concerned about public health issues and understand the societal expectations of the medical profession.the public regarded the case as irresponsible behavior by the physician that caused the premature death of the patient.the second case was a physicians’ strike against a governmental regulation that stipulated separation of the privileges of prescribing and dispensing drugs.physicians must understand these roles and responsibilities towards patients, their peers, and society if medical professionalism is to be reestablished in korean society.however, the survey was designed to investigate only the overall attitudes and morals of korean doctors, because there is no question that all doctors are required to have medical knowledge and the clinical ability to practice medicine.the cronbach’s alpha for a pilot study of 37 physicians in seoul was 0.87.the survey was distributed nationwide to 950 physicians, and 721 (75.89 %) returned completed surveys between april 1 and july 31, 2011.the various virtues of medical professionalism were not described explicitly, but were expressed within descriptions of different clinical contexts.there were age-related differences (p < .0001) in this finding, with the highest frequency of affirmative responses about professionalism among those in their 60s and the lowest ratings among those in their 20s (see table 2).but, veracity in this context refers to the particular matter of truth-telling to patients or patients’ families regarding medical errors of self or other colleagues; hospital infections; and the diagnosis, development, and prognosis of disease.veracity requires a relationship of mutual respect and reliance between patients and physicians [26].in korea, the national health insurance service determines and authorizes the price and cost of each particular order or treatment in medical practice, and physicians are aware of the orders and treatments that are reimbursed by the service.medical decisions that deviate from the normally reimbursable orders or treatments can result in referral to the health insurance review and assessment service, which will often levy an enormous fine against the physician and request that the bill be returned to the patient.korean physicians tend to complain that national health insurance coverage restricts their professional discretion and encourages their unethical behaviors.in particular, the health insurance service’s preference for reducing the number of claims can discourage physicians from recommending and implementing the most appropriate treatments.” medicine is delivered by doctors who have professed to use their medical knowledge and skill only for the good of the patient.medicine as a profession is maintained by doctors who develop, comply with, and embody the medical code [27].therefore, medical professionalism is maintained and enriched by the virtuous person who makes a lifelong commitment to maintain integrity as a doctor.indeed, medical professionalism is a complex concept that defies a unitary definition.because even major figures in a medical society may be unfamiliar with the wider social context of professionalism, they might overlook important traits or exhibit bias when defining medical professionalism [5].major medical society figures represent competency, appropriate roles, and responsibility for both patients and society as essential qualities that the medical profession needs to convey to society.the ethical culture of the medical profession functions as a moral ground on which medical professionals defend healthcare values and exercise political leadership to resolve various social conflicts related to medicine.according to the academy’s understanding of medicine as a vocation, a good doctor “is an interdisciplinary hybrid person who has the moral duty and passion to treat patients with the necessary scientific and medical skills.therefore, a good doctor who embodies a high level of medical professionalism is one who practices medicine as a vocation, regardless of personal acknowledgement for doing so.regardless of whether the respondents endorsed medical professionalism as an element of the vocation, they listed veracity, respect for patient autonomy, integrity, responsibility, altruism, and honesty as important characteristics of medical professionalism.korean physicians are overwhelmed with excess documents and are expected to comply with everchanging policies.therefore, physicians find it difficult to exercise their discretion and display the virtues of a medical professional; they feel a lack of autonomy as professionals despite their altruistic mindset.the royal college of physicians has stated that all doctors must abandon their sense of supremacy, autonomy, and privilege as medical professionals [25].however, apprehensions about status and autonomy among korean medical professionals are somewhat different from those among doctors in the uk.such behavior tarnishes the value of the medical profession, and could have detrimental effects on patient care.in particular, both patients and doctors become victims in the event of a medical accident [33, 34].it is expected that experienced physicians tend to be good at communicating with patients, which helps them acquire an integral perspective of medicine.ho questioned whether medical professionalism developed by western physicians could be adapted to non-western countries with different social, historical, and cultural backgrounds, and found a difference between the core virtues of medical professionalism in western countries and those in chinese culture.this also explains why korean physicians in their 50s and 60s, who one would expect to be more influenced by confucian culture, placed a higher priority on integrity than did other age groups.in our study, korean physicians reported responsibility and obligation as core medical professionalism virtues in their practice, which reflects essential virtues according to the medical code of ethics.in conclusion, physicians tended to have an appropriate attitude toward medical professionalism, yet they felt that the social environment did not enable achievement of their potential as medical professionals.the finding that korean physicians hold the more normative values like responsibility and veracity in higher esteem than other virtues reflects their lack of autonomy due to excess governmental limitations in the healthcare system.the government forces physicians to reduce healthcare service spending rather than raising the level of reimbursement.to promote ethical culture, it is necessary to allow physician’s autonomy and to encourage a self-policing system within the medical profession [38].physicians’ autonomy requires responsible behaviors by physicians as moral agents.transformation based on social change is inevitable for korean physicians and medical societies.conclusion korea has a relatively short history of medical professionalism, which is not yet entrenched in medical education or in the clinical context.the medical community must recognize the importance of appropriate identification and implementation of medical professionalism for korea in the 21st century.korean physicians need more opportunities to exercise self-governance and autonomy to be socially responsible for medical education and medical practice.medical professionalism is the basis for trust in the patient– doctor relationship, and is a core element of being a good doctor [40–42].as medicine is practiced within a particular social and cultural context, further studies should explore the perspectives of other stakeholders, such as patients, their families, governmental officials, and other medical professionals.different opinions and appropriate feedback from various parties will contribute to establishment of a new and solid medical professionalism in kor
number of words= 1227
[{'rouge-1': {'f': 0.29638793892394394, 'p': 0.79,'r': 0.1824121779859485}, 'rouge-2': {'f': 0.18076470569760394, 'p': 0.38155778894472364,'r': 0.1184375}, 'rouge-l': {'f': 0.3100189306442791, 'p': 0.6306060606060606,'r': 0.20553113553113553}}]
-----------------------------------------------------------------------------------------------------------------------------------
p188:
Extractive Summary:
there is extensive evidence that employees in health care services, both in hospitals, nursing homes, and home care services, frequently struggle with ethical challenges [1-8].although ethics support in hospitals is relatively well developed, both nationally and internationally [9-15], the development of ethics support seems to be sparser in community health services [1,5,16].however, there is a need for ethics support in community health also [8].previous research indicates that ethics support in community health services should be closely tailored to the workplace, facilitated by a colleague [1] and concerned with the everyday ethical challenges employees experience [1,17].however, there are few evaluation studies about the impact of ethics support in community health [18].recent research indicates that systematic ethics reflection is seen as a positive learning process among those participating [19].learning to reflect on the ethical challenges the employees face in their everyday work, and experiencing the benefits of it, such as relief of moral distress, are described as key reasons and motivations for implementing ethics reflection in clinical practice [20,21].this paper presents the results from an evaluation study where ethics reflection in groups was implemented in community health services in a municipality in the central eastern part of norway.norwegian municipalities differ with regard to size, population and organisation, but health services offered are generally public.one of the facilitators constructs cases which resemble what the staff face in their workday; “we often have ethically challenging situations in our unit.when it happens, i make a case of it.often the result is that we chuckle and laugh because we see ourselves:‘oh my god this is so stupid.that’s the way it is, and it should not be that way’” (f1).whether using constructed or experienced challenges, they all found it successful to have specific cases that are relevant to their everyday practice to reflect upon.one advantage of using constructed cases is that possible issues of confidentiality are more easily are avoided.however, that ethics reflection is focused on the concrete and the practice-oriented, and the employees’ needs and challenges, makes the group discussion particularly valuable;“they [the employees] reflect over the things happening on the wards.it is not general cases that do not mean very much, but they are going into specific situations that they are involved in.they find that they benefit from it, for example in relation to particularly challenging patients that we have on the ward” (m2).if the discussion of real cases generates new and better solutions, the solutions are likely to be used afterwards.thus, the usefulness of the reflection groups becomes more obvious.discussion the aim of this study was to examine how managers, facilitators and participants in ethics reflection groups, experienced and evaluated these groups.first of all, it is obvious that some of the ethically challenging situations that have been discussed in the reflection groups will probably not always, and perhaps especially not by ethicists, be perceived as ethical problems, but more as problem of professional, technical, organisational, and collegial character.nevertheless, these are situations that staff members, facilitators and managers, experience and present as ethical problems in their everyday practice.like other recent studies from community health care, our study shows that ethics reflection in groups, above all, is experienced as a very positive and important measure [19-21].the participants describe ethics reflection as a process that makes them more aware of what patients and relatives express, of the patient’s needs, but above all, more concerned with the patient’s right to participate in decision-making processes.the participants, nurses and auxiliary nurses, understand ethics as a critical reflection on their own clinical practice, and find that this process leads to an increased ethical awareness.ethical awareness is an important part of health care workers’ clinical competence, and the measure must therefore be considered to be professionally stimulating and developing.the way the participants describe their new practices; e.g. strengthened ability to communicate with patients and relatives, sharpened awareness of patients’ wishes, feelings and well-being, finding new and better solutions, team cooperation and mutual learning, are important aspects of professionalism and good practice.ethics support in the form of systematic ethics reflection in groups also seems to affect the employees’ practice performance, and their ability to reflect on and criticise their own practice in a constructive way.this is strongly related to what the participants describe as a success factor: that ethics reflection is based on the employees’ practice experiences.it is the employee’s actual and perceived challenges they investigate; reflect on and discuss.not theoretical or potential situations or general values, but real, challenging situations that the professionals need help looking at, and trying to find new ways to cope with in respectful dialogue.this study indicates that the systematic and respectful dialogue between employees, and the reflection on ethical problems in their own workday, can be one important way to new and better solutions and thus improved practice.using a simple and systematic method of reflection, facilitating adequate training, and organisational backing seem to be key success factors.these findings also resemble findings from evaluation studies of clinical ethics support services in hospitals [30,31].assuming that our very positive findings are valid, how can we understand or explain this?first, it is obvious to us that ethics reflection groups appear to meet a need for professional development that otherwise may not have good growing conditions.increasing demands for efficiency, target management and evidence seems prominent, while dialogue, reflection, professionalism and relationships have poorer conditions in modern health care.second, it seems that being invited to reflect on clinical practice leads to a good learning process.according to argyris and schön, learning occur both as single- and double loop [32,33].which learning it is that has taken place depends on the outcome and process.doing more of the same, perhaps using new techniques, greater certainty and quicker performance, charactarises single loop learning.while double loop learning requires a qualitative change, a new stage characterised by qualitatively new content, and the result is professional development [34].the participants’ description of the significance of ethics reflection indicates double loop learning and professional development.how can this be explained?the working day without ethics reflection provides little opportunity for so-called double loop learning; this may partially explain why ethics reflection is considered “profitable” and significant.another interesting fact is that the employees seem to define ethics in accordance with double loop learning when they explain ethics as reflection on practice.unlike the automated actions that characterised the participants’ previous practice, they have discovered new aspects of their own practice, they ask questions they did not ask before, and they have become aware of new ways to deal with ethical challenges.participation in ethics reflection groups seems to have had a positive impact on the staff ’s professional development.community health care is often characterised by time constraints and the inability to meet all the patients’ needs, with little time for reflection on clinical practice.the possibility for nursing staff to sit down and reflect together can be one explanation for the overwhelmingly positive response.instead of thinking that this takes time, the managers consider ethics reflection to be a valuable investment, a win-win situation.not only has the measure led to increased knowledge and competence in ethics among the staff, and thus improved quality of service, the managers also consider the measure to be an important recruitment initiative.the employees describe ethics reflection and competence to be a necessary part of their work.although the majority has received some ethics training in their education, the employees consider ethics reflection in the workplace to be further training of great importance.a measure like ethics reflection compensates, to some extent, for the wide variation there is in the employees’ ethics education.furthermore, it seems to improve the interdisciplinary learning environment.the differences between the employees, both in cultural background and education, no longer appear to be a threat, but a source of learning.some lessons learned one important lesson learned is to base the ethics reflection on cases from the participants´ own practice right from the start.to discuss general terms, concepts, ethical values and principles are perceived as less important and fruitful.an interest in ethics, as well as seeing ethics reflection as important, is not sufficient selection criteria for ethics facilitators.participating in and observing the facilitators in the training program, as well as their reflections and discussions in network meetings, revealed how demanding some of the facilitators found their role.we learned that knowledge and skills are important criteria, criteria it is important not to under-communicate when facilitators are selected.implementing ethics reflection led by colleagues as facilitators therefore requires an education program that includes exercises to develop necessary skills.the exercises should be integrated throughout the education program, from the very beginning.one of the challenges, two years after project start, is to maintain the ethics reflection groups, particularly if the facilitators quit their job.a plan for training new facilitators is thus important.another barrier is lack of time and thus problems with prioritising participation.while some might consider it an expense, the participants evaluate ethics reflection groups to be an investment.taking the time to stop and think, are considered to be a good investment.this is a way of thinking that we have learned requires a manager who supports the measure fully.managers’, especially ward managers’ support and commitment are essential for success.therefore, we recommend postponing the initiation if it is not possible to involve the managers.initially, we wanted the reflection groups to be multidisciplinary, something that we only partially succeeded in.for example, it was very rare that doctors or physiotherapists participated.despite the limited data, we have positive experiences that suggest that multidisciplinary groups have a positive impact on the reflection process.methodological limitations our research has limitations.more research on this topic (including other services, more and other types of participants, e.g. patients/proxies and more rigorous study design) is needed to assess whether our positive findings are representative.the strength of our study is that we have different groups of informants (employees, facilitators, managers) in addition to observations and written notes.other strengths are that we as researchers have been closely involved in the project and therefore have detailed and firsthand knowledge about the intervention and the implementation process.but our closeness and involvement in the project could conceivably have influenced the findings, and is thus also a weakness of the study.the participants in these interviews are all professionals, either responsible for managing or providing health care.to the extent we know anything about what patients, residents and their families think about these questions, we know it just indirectly, through the managers’ and health care professionals’ assumptions and views.those who have received the service may consider it differently.conclusions the managers, facilitators and staff participating in this study experienced ethics reflection groups as very valuable.the time and resources needed were considered to be highly rewarding investments by all the participants.another major finding was the importance of management support and anchoring ethics sessions in the routine of daily work.ethics reflection groups, as implemented in this study, may lead to collegial support and cooperation, improved practice, better involvement of patients and their families, personal and professional development and mutual learning among employees.several aspects seem to be important for the positive experience: the reflection process focusing on the participants’ own ethical challenges from their daily work; and the reflection process taking place in the employee’s workplace and being led by a colleague trained in facilitati
number of words= 1851
[{'rouge-1': {'f': 0.30799747810709666, 'p': 0.8941758241758242,'r': 0.18603919546157816}, 'rouge-2': {'f': 0.20006350539906967, 'p': 0.47441176470588237,'r': 0.12675954592363262}, 'rouge-l': {'f': 0.3521854328425612, 'p': 0.796027397260274,'r': 0.2261119293078056}}]
-----------------------------------------------------------------------------------------------------------------------------------
p189:
Extractive Summary:
since the days of aids denialism [1-3], south africa’s policy on hiv/aids has seen a transformation with the introduction of national treatment, testing and prevention campaigns [4].despite an increase of 1.2 million new infections since 2008, the rate is stabilising and access to treatment is improving [5].by mid-2012 31.2% of people living with hiv (plhiv) were on treatment, up from 16.6% in 2008 [5] and south africa has now passed the “tipping point” whereby more people are accessing treatment than there are new infections [6].this has come on foot of a successful testing campaign which has seen rates of hiv/aids testing increase [7].however with 6.4 million people (12.2% of the population) living with hiv/aids [5], south africa has the highest number of hiv/aids infected individuals worldwide and hiv/aids continues to be a considerable challenge facing the country [8].yet life is improving for plhiv.the availability of anti-retroviral therapy (art) has helped increase life expectancy from 49.2 years prior to the availability of art to 60.5 years by 2011 [9].plhiv in south africa can now also expect to have 80% of normal life expectancy provided they start treatment before their cd4 count drops below 200 [10].yet this increased provision of treatment has a significant economic impact on the nation.although a recent renegotiation of a fixed dose arv resulted in a 40% price reduction, south africa’s programme is the largest treatment programme in the world and a considerable economic burden for the state [11].in august 2012 the us government announced that it would cut the united states president’s emergency plan for aids relief (pepfar) budget for south africa in half by 2017, at which point south africa will have to cover the full cost of its hiv/aids programme.the process of the government taking full responsibility for the programme has led to the closing of hiv/aids treatment centres created by pepfar and patients’ moving to government run centres, centres which regularly face long waiting times and medication shortages [12].equally problematic is the high prevalence rate [8].thus, while treatment and prevention campaigns can reduce infection rates and improve the quality of life for plhiv, treatment and prevention campaigns are not enough to bring the epidemic in south africa under control.news that the “berlin patient” [13] and the “mississippi baby” [14] have both been “cured” [15,16] of hiv likely brought hope to plhiv in south africa that a cure for hiv/aids is within reach, a hope which may be fuelled by further announcements of other babies who have also been cured [17].although these early reports of hiv/aids cures have been somewhat dampened by the reports of the return of hiv in the “mississippi baby” [18] and the “boston patients” [19], it appears that the search for a cure is gaining momentum.yet these new developments bring a fresh set of ethical challenges that must be addressed prior to the commencement of any trial aimed at curing hiv/aids in south africa.the recent setbacks of these early cases of cure illustrate that there is a lot that is still unknown about hiv/aids cure research and there is the real possibility that hiv may return in any early phase trial.with such unknowns, it is critical that the conduct of the trial meets the highest ethical standards.concerns about trial design and the potential withdrawal of arvs must be discussed and addressed.in particular it will discuss the experiences of the informed consent process during clinical trials in south africa and the potential implications they may have on any future trial aimed at finding a cure for hiv/aids in south africa.good ethical oversight is imperative in this process and the role that recs should play in this oversight will be discussed.discussion informed consent in south africa: the legal landscape informed consent is a fundamental ethical principle that was first codified in the nuremberg code in the wake of the nazi experiments [26,27].it has since been replicated in all major ethical codes including the international covenant on civil and political rights, the declaration of helsinki and the council for international organisations of medical sciences (cioms) guidelines.in south africa, the principle is enshrined in both the constitution and the national health act 2003, highlighting its importance.these must be understood and consent must be voluntary and based on this information.the guidelines are cognisant of the fact that due to access to health care and their education levels, many people involved in research in south africa are considered to be vulnerable and researchers must be mindful of this vulnerability [28].the clinical trial guidelines further detail what should be included in the informed consent form (icf).for all clinical trials, the original signed consent form must be kept with the trial records with one copy stored in the patients’ records and a second copy given to the participant to take home [29].these guidelines have also been supplemented with specific guidance on informed consent in hiv preventive vaccine research guidelines [30], and other international guidelines on hiv/aids research, such as the unaids documents, may also be followed in clinical trials [31].experiences with informed consent in south africa despite ample guidelines on the matter, obtaining informed consent in clinical trials is problematic in south africa.thus from a document which has its origins in promoting ethical research, it has evolved to become an increasingly legalistic document which can be difficult to read and decipher its contents [33].they fail to appreciate that decisions will be made based on the best interest of the trial.thus the participant cannot be said to be making a free, fully informed decision.these findings indicate that participants did not understand the information provided during the informed consent process and suggest that tm may also be an issue in south africa.informed consent and hiv/aids cure research in an attempt to protect them from legal liability, disclosure of information by the research team is unlikely to be a problem in south africa.furthermore in two boston patients who underwent treatment similar to the berlin patient, hiv/aids returned in both patients resulting in one becoming resistant to their treatment [46].in light of the malan study and evidence that plhiv will try unproven treatments in their search for a cure, the research team must guard against tm in hiv/aids cure research.
number of words= 1027
[{'rouge-1': {'f': 0.3369625448271084, 'p': 0.8512500000000001,'r': 0.2100560224089636}, 'rouge-2': {'f': 0.2117290709469539, 'p': 0.4521989528795812,'r': 0.13822429906542055}, 'rouge-l': {'f': 0.3497221283574955, 'p': 0.7652380952380953,'r': 0.22665236051502147}}]
-----------------------------------------------------------------------------------------------------------------------------------
p190:
Extractive Summary:
introduction rationale over the past two decades cannabis use and dependence are estimated to have increased, with cannabis use disorder (cud) reported as one of the most common drug use disorders globally [1].in canada, it has been reported that nearly 17 percent of canadians aged 15 years and older reported using cannabis between october and december of 2019, an increase from 14 percent between january to march of 2018.additionally, cannabis consumption rates are higher among males than females [2].concerningly, cannabis has been associated with substantial adverse effects.like other drugs, cannabis can result in cravings, dependence, and drugseeking behaviour [3, 4].during intoxication, cannabis can interfere with memory, motor coordination, altered judgement, and at higher doses, paranoia or psychosis [3].further, repeated use of cannabis can have long lasting effects, including altered brain development, poor education outcome, cognitive impairment, diminished life satisfaction and achievement, poor professional and social achievements, symptoms of chronic bronchitis and increased risk of chronic psychotic disorders [3, 5].heritability estimates for cannabis use initiation varied from 30 to 48%, and from 51 to 59% for problematic cannabis use, suggesting a genetic component exists [6].genome-wide association study (gwas) meta-analyses have identified possible regions of association on chromosome 3 for lifetime cannabis use (cadm2), chromosome 10 for cud (rs77300175), and chromosome 16 for age of first cannabis use (atp2c2) [7–9].moreover, candidate gene studies have detected some significant associations with cannabis use on the cnr1, gabra2, faah, and abcb1 genes, but as with genome-wide association studies (gwass), replication of these associations has been inconsistent [10].gwass provide a ‘hypothesis-free’ method of identifying novel variant-trait associations, leading to the discovery of novel biological mechanisms and diverse clinical applications [11].as such, in this systematic review, we will summarize gwas findings relevant to cannabis use or cud outcomes and discuss future directions.objectives: the main goal of this systematic review is to identify genetic variants from gwass associated with cannabis use.primary objectives of this systematic review include the following: 1.current cannabis use is defined by either self-report or positive urine drug screens within 1 month of the study being conducted.2. identify genetic variants associated with lifetime cannabis use.lifetime cannabis use is defined by any self-reported or positive urine drug screens of cannabis use within one’s lifetime.3. identify genetic variants associated with cud.cud is defined by any diagnostic and classification systems used to diagnose cud or questionnaires validated to assess cud.secondary objectives of this systematic review include the following: 1.identify genetic variants associated with the adverse outcomes of cannabis use, including psychiatric (cognitive impairment, psychotic symptoms, depression, anxiety, suicidal behavior) and non-psychiatric (chronic bronchitis, lung infections, chronic cough, increased risk of motor vehicle accidents) [12–14].2. when feasible, perform subgroup summaries by sex or ethnic differences.methods this systematic review is reported in accordance with the preferred reporting items for systematic reviews and meta-analyses (prisma) statement [15] (see prisma checklist in additional file 1).the human genome epidemiology network (hugenet) guideline was used to supplement the prisma guideline.while this review does not conform with the hugenet guideline expectations of reporting on candidate gene study findings, the hugenet is used to uphold the standard of reporting research specific to genetic association studies [16].protocol and registration the protocol for this systematic review has been registered within the international prospective register of systematic reviews (prospero) (registration number: crd42020176016) [17].the full protocol has been published in the journal of systematic reviews [18].the g allele of rs2023239 of cnr1 is linked with higher cortical cbr1 and is associated with smaller hippocampal volume in chronic cannabis users, but not healthy controls and findings that suggest only individuals with a high genetic risk of schizophrenia experience a negative impact on cortical maturation during early adolescence thus suggestive of gene × drug interactions [56–58].in addition, functional mri evidence suggest specific brain activity signatures with cannabis use such as increased functional connectivity associated with the default node network and insula networks and hippocampal and parahippocampal atrophy have been associated with chronic cannabis use [59, 60].however, neuroimaging studies of cannabis users have yielded inconsistent findings and may reflect individual differences that preceded cannabis use.the inconsistent findings in the literature highlight the need for large longitudinal studies utilizing before-and-after cannabis use neuroimaging [61].taken together, it is plausible that structural differences in brain regions could be influenced by genetic differences between individuals, explaining the mixed evidence within neuroimaging.further research is required to determine the complex interactions amongst individual genetic predispositions, prenatal environment, and postnatal environment contributing to individual cannabis use behaviour and/or the development of cud.understanding the genetic predispositions is one piece of the puzzle in understanding the complex development of cannabis use and cud.finally, it is important to consider the shared genetic basis of other substance use disorders.heritability estimates across substance use disorders vary, with heritability lowest for hallucinogens (0.39) and highest for cocaine use (0.72) [62, 63].additionally, substance use disorders are the result of gene x environment interactions, with partial risk inborn and another part determined by environmental experiences [62].previous reviews have summarized the literature on gwass for various substance use disorders including alcohol use disorder, nicotine use disorder, cud, oud, and cocaine use disorder.however, genetic studies within specific substance use disorders have had varying success in replicating previously identified associations, limiting evidence for shared genetic basis across substance use disorders [63, 64].the complexity of substance use disorder make genetic prediction efforts difficult, and while currently only alcohol use disorder have been genetically correlated with cud, continued advancements in molecular genetic studies and substance use disorder at larges further our understanding of the biological pathways underlying substance use disorders [9, 63, 65].for instance, cnr1 and cnr2, components of the endocannabinoid system, are major targets of investigation for their impact in neuropsychiatry and addiction phenotypes suggested shared genetic risk factors [66, 67].in regards to neuropsychiatric disorders, mendelian randomization studies have found mixed evidence on the causal effect of cannabis initiation and schizophrenia, finding weak evidence that cannabis initiation increases schizophrenia risk and strong evidence that schizophrenia liability increases the odds of cannabis initiation, and causal evidence of adhd on cannabis initiation [68–72].through continued advances, it is hoped that the underlying genetic basis for cud, or a shared genetic basis for all substance use disorders, will be identified to provide preventative measures and treatment for substance use disorders in the future.limitations while this systematic review was rigorous and involved a peer-reviewed protocol, it is not without limitations.first, our inclusion criteria limited our review to only gwass, meaning any gwas meta-analyses and candidate gene studies were excluded.gwas meta-analyses and candidate gene studies are often more powered due to their larger sample sizes and minimal genetic variants tested, respectfully [11].however, including only gwass was decided a priori to capture novel genetic variants associated with cannabis use and avoid the inclusion of multiple studies which could use the same genetic dataset.second, it is important to note that this review is susceptible to publication bias, as studies that do not achieve genome-wide significance may be less likely to be published, and thus, not included in this review.unpublished gwas findings may exist with snps reaching the borderline significance threshold.while we cannot eliminate publication bias entirely, we searched abstracts, gwas catalogs, and databases for any near significant findings that were not published.if a relevant abstract was identified, without the full study published, the first author was contacted to determine whether the full gwas had been published or was going to be submitted to a journal.finally, if a study met our inclusion criteria but did not report any snps that fell below the genome-wide significance threshold, study authors were contacted to confirm if any snps had reached the borderline significant threshold set for this review.third, due to the heterogeneity of the reported findings, it was not possible to conduct a meta-analysis or sex and ethnicity subgroup analyses.although we could not conduct a meta-analysis, we qualitatively summarized the studies and reported a comprehensive list of all snps reaching the significance threshold for this study.conclusion this systematic review was able to summarize gwas findings within the field of cannabis use.we were able to identify all gwass conducted on cannabis use, highlighting the need for further research as no two gwass reported the same snp or gene associated with cannabis use.further, included gwass had limited ethnic diversity, with only european or african american participants.recommendations are made for future research to replicate reported associations and include diverse ethnic populations to test whether snps associated with cannabis use reported are generalizable across study populations and if associations differ by ethnici
number of words= 1404
[{'rouge-1': {'f': 0.33550797228675777, 'p': 0.7733333333333334,'r': 0.21422419685577582}, 'rouge-2': {'f': 0.1821421034077114, 'p': 0.33755852842809364,'r': 0.12471956224350206}, 'rouge-l': {'f': 0.3011172617527488, 'p': 0.5583720930232559,'r': 0.20614262560777957}}]
-----------------------------------------------------------------------------------------------------------------------------------
p191:
Extractive Summary:
background coronavirus disease 2019 (covid-19), caused by the severe acute respiratory syndrome coronavirus 2 (sarscov- 2), was first identified in china in december 2019 and has since progressed to cause a major global pandemic [1, 2].although covid-19 shares some clinical manifestations with other respiratory viral infections such the common cold (rhinoviruses and common human coronaviruses) and influenza, it has a number of differences including prolonged viral shedding that may last many weeks, progression to more severe disease in a proportion of patients in the second week of illness, and extrapulmonary manifestations including cardiovascular and thromboembolic disease [3, 4].the disease differs in frequency and severity between ethnic groups and although these differences are most likely driven by socioeconomic factors, it has been suggested that biological factors may also contribute [5, 6].key features of the immune response to the virus are lymphopenia (possibly caused by apoptosis via the p53-signaling pathway in t-lymphocytes [7], via angiotensin- converting enzyme (ace2) protein receptor) [8], and an increase in inflammatory cytokines (cytokine storm) such as interferon-gamma (ifn-γ) and interleukin (il)-6 [9], which can result in multiple-organ dysfunction syndrome (mods) and acute respiratory distress syndrome (ards), a major cause of death in covid-19 [10].transcriptome analysis of whole blood is a useful tool for profiling the host immune response to an infectious disease.the approach has previously been applied to other respiratory infections, and has identified characteristic gene signatures associated with influenza and respiratory syncytial virus (rsv) infection [11, 12].it has shown particular value in tuberculosis where, in addition to identifying clinical cases [13], it has potential to identify contacts with asymptomatic disease exposure [14], as well as those with latent infection who will progress to develop symptomatic clinical tb disease [15].we found a number of differences in the profile of gene expression and the associated protein pathways between the two major ethnic groups in our dataset, but these were mostly in generic pathways expected in the immune response to infection which likely represent variation associated with the relatively small sub-group sample sizes rather than meaningful inter-ethnic differences.the two ethic groups studied are both at the lower end of the range of covid-19 disease severity compared with other ethnic groups, based on epidemiological (non-transcriptomic) data in the uk population [6, 31, 32], and comparison of outcomes between similar ethnic groups in malaysia did not show a difference [33].there is a strong clinical need for a test that could indicate underlying disease severity and predict those with mild covid-19 disease who are most at risk of progressing to severe forms of disease.given that the progression is most likely driven by the immune response rather than the virus per se, an immune-based test may hold the best promise for this indication.as a first step, we evaluated whether a quantitative disease score based on the magnitude of gene expression was related to clinical parameters.our finding that the score was associated with the time from illness onset (score lower with greater time from onset of illness) and ct (qpcr) on nasal swabs (score higher with greater viral burden on the nasal swab) suggest initial biological plausibility for the responsiveness of the score to clinical disease/viral burden.other studies have shown an independent relationship between qpcr ct and disease outcome [34–38].we did not find any direct relationship between the disease score and indicators of disease severity.however, we studied a group of patients with relatively mild disease and the chest x-ray we used to assess disease severity is an imprecise measure that may underestimate the extent of lung pathology.given the heterogeneity of clinical presentation a much larger study would be required to address this including greater representation of patients with more severe disease and with better markers of disease severity such a pulmonary ct imaging.this evaluation was intended as an initial proof of concept that an integrated measure of gene expression (the risk score) might relate to clinical disease parameters—for this score to be clinically useful it would need to be reduced to a smaller number of genes that could be measured using a simple test (such as lateral flow) and would need to be shown to improve the accuracy of prediction of those at risk of disease progression over and above the prediction available from readilyavailable clinical parameters.the main limitation of our study is the relatively small sample size, although it proved sufficient to identify a signature and analyse associated pathways.however, the gene signature requires further validation in an independent dataset.the number of genes in our final signature is relatively large and for the approach to have potential clinical use as a rapid test, further reduction to a small number of key genes by rtpcr would be required.the healthy controls were selected on the basis of absence of exposure to covid-19 and absence of symptoms; we did not perform a nasal swab pcr to detect asymptomatic carriage of covid-19.however, at the time of the study community transmission of covid-19 in singapore was at a very low rate (approximately 35 new cases per day, in a local community population of 4 million) at the time the study was conducted [39], and we followed the participants for four weeks after the study to rule out subsequent development of symptomatic disease.even if a small proportion of the controls had undetected infection, this would not explain the findings of differential gene expression in the covid-19 cases—if anything the bias would be towards decreasing the level of relative gene expression.clinical data collection in covid-19 cases was necessarily limited due to constraints of patient access and investigation accompanying stringent institutional infection control protocols.similarly, constraints on laboratory access and sample transportation meant that we could not collect and process other blood samples for a more comprehensive immunological profile that might provide additional context for the transcriptome findings.conclusions in conclusion, we have measured a whole-blood transcriptome in patients with covid-19 that has indicated the possible value of this approach for further characterizing disease pathogenesis and the host response to infecti
number of words= 987
[{'rouge-1': {'f': 0.4031694889809834, 'p': 0.6875637393767706,'r': 0.2852023692003949}, 'rouge-2': {'f': 0.18036509026291345, 'p': 0.2631818181818182,'r': 0.13719367588932807}, 'rouge-l': {'f': 0.3634017788438209, 'p': 0.5263106796116506,'r': 0.2775055187637969}}]
-----------------------------------------------------------------------------------------------------------------------------------
p192:
Extractive Summary:
alarmingly, the occurrence of sepsis is rising with a 17% increase between 2000 and 2010 [1].an effective prognostic biomarker that can predict the clinical outcome of sepsis patients appropriately is in high demand in clinical practice.the biomarkers containing multiple genes deliver better prognostic power.miguel reyes et al. [9] identified a unique cd14+ monocyte state which is inflated in sepsis patients.following release from the cell, s1p acts as a ligand upon binding to five subtypes of s1p receptors (s1prs) 1–5 which belong to the g protein- coupled receptor (gpcrs) family, triggering many receptor-dependent cellular signaling pathways.s1pr3 is mainly expressed in the cardiovascular system, lungs, kidney, and spleen organs [12].the s1p ligation of s1pr3 could affect various organ system functions such as vascular permeability signaling [13], heart rate, and blood pressure [14].s1p-s1pr3 axis played a pivotal role in sepsis.niessen [17] et al. suggested that the s1p–s1pr3 axis regulates latestage inflammation amplification in sepsis.hou et al. [18] also showed that s1pr3 signaling is bactericidal because it is related to the function of monocytes and the deficiency of s1pr3 could therefore increase susceptibility to sepsis, s1pr3 expression levels were upregulated in monocytes from sepsis patients.we hypothesize that multiple s1pr3-related genes, in combination with pro- and anti-inflammatory cytokines, could correlate with clinical outcomes in sepsis patients.to achieve this aim, we examined whole blood gene expression in two standalone cohorts from gene expression omnibus (geo) and identified a gene signature of 18 genes significantly associated both with s1pr3 and sepsis survival.our results propose that s1pr3-associated genes may expand the outcome prediction in sepsis.the discovery cohort gse54514 contains whole blood transcriptome data of 35 survivors and non-survivors of sepsis, samples were collected daily for up to 5 days from sepsis patients.series matrix files containing pre-processed gene expression values were obtained from series gse54514 (https ://www.ncbi.nlm.nih.gov/geo/query /acc.nlm.nih.gov/geo/query /acc.cgi?acc=gse33 118).all chip probe sets in matrix files had been transformed into corresponding gene symbols by utilizing chip platform files.identifying s1pr3‑related sepsis gene signature we detected the degs between 26 survivors and 9 nonsurvivors in the discovery cohort and set them as sepsis survival-related genes.expression score and risk score each patient was allocated with an expression and risk score from gene expression and corresponding weight values of 18 genes.statistical analyses r (version 3.5.0) was utilized to perform all the statistical calculations in this study.r package proc (version 1.16.1) was used to visualize the roc curve and compute the area under the curve (auc).1078 up-regulated and 1134 down-regulated genes (false discovery rate [fdr] < 5% and fold change [fc] > 1.5) in non-survivors were found and characterized as sepsis survivalrelated genes (additonal file 3: table s3).to find the connection between s1pr3 pathways and genes that affect sepsis survival, we intersected the s1pr3-related genes and sepsis survival-related genes and identified 18 overlapping genes (fig. 2a).these 18 genes are classified as our sepsis gene signature derived from s1pr3-related genes in this study and defined as s1pr3-related molecular signatures (s3ms) (table 1).the heatmap demonstrates that the 18 genes (s3ms) can discriminate non-survivors from survivors through different gene expression patterns (fig. 2b).graphical maps of genome sequence provide a method to rapidly look at the features of specific genes.using a circular genome plot (fig. 2c), we found that the 18 genes scattered in different genome regions suggesting that these genes are enriched in key pathways but not genetically linked due to chromosomal locations.our results confirm that the 18 genes identified in our s3ms had a strong connection and relationship.s3ms predicts clinical outcomes in both discovery and validation cohorts we developed a risk assessment scoring method to measure the possibility of sepsis risk in patients using a linear combination of the 18-gene expression values (table 1).each value was given a weighted value which indicates the direction of differential expression in non-survivors, and patients were assigned a score based on those measures.classification power of the 18‑gene signature we investigated the classification performance of the s1pr3 gene signature in the discovery and validated datasets.s1pr3, our target molecule, belongs to a family of gpcrs that regulate host innate immunity.our investigations yielded the following observations: (1) identification of an 18-gene signature in this study which could discriminate non-survivors from survivors at the gene expression level.unlike most biomarkers such as circulating protein, gene signature can be adjusted dynamically to functionally perform better.the vegf (vascular endothelial growth factor) pathway, which regulates vascular development in angiogenesis, was enriched in our gene signature.down-regulation of these immune pathways is known to worsen the prognosis of sepsis.otherwise, 18 genes had intense interactions with each other.hence, the 18-gene signature from s1pr3-related genes not only predicted the clinical outcome of sepsis patients but also revealed the signaling pathways which could play a pivotal role in the development and progression of sepsis.several studies [18, 32] already indicated that s1p-s1pr3 signaling drove bacterial killing, s1pr3 was associated with preferable sepsis outcomes.only 4 common genes among s1pr1-associated genes (557 genes) and s1pr3-related genes (226 genes).however, our work in the s1pr3-related gene signature was only based on bioinformatics methods.conclusions in conclusion, we identified a gene signature containing 18 protein-coding genes capable of being reproducible predictors of clinical outcomes in patients with sepsis.
number of words= 848
[{'rouge-1': {'f': 0.3631323579867657, 'p': 0.7267796610169492,'r': 0.24203107658157602}, 'rouge-2': {'f': 0.16261847894406034, 'p': 0.2572340425531915,'r': 0.1188888888888889}, 'rouge-l': {'f': 0.32100191335292483, 'p': 0.5455244755244755,'r': 0.22740740740740742}}]
-----------------------------------------------------------------------------------------------------------------------------------
p193:
Extractive Summary:
only the unique alignment results were considered to generate reads per kb transcriptome per million mapped reads (rpkm) values that represented for the relative expression level.the rpkm method eliminates the influence of gene size when comparing expression levels between genes.based on the expression pattern, the kolmogorov–smirnov test was applied to filter samples exhibiting large distribution bias among samples.expression data of 17 hbv-related alf samples were obtained under an accession number of gds4387.results cnv detection for 389 hbv-related aclf cases and 391 asc controls, the birdsuite and penncnv algorithms yielded 77,987 cnvs (21,891 duplications and 56,096 deletions) in total with a median size of 569,849 bp.all cnv calls were clustered into 6,819 cnves, where 4,413 (64.72%) were singletons (fig. 1a and additional file 3).the frequency distribution of cnv number per individual (cnpi) was close to the normal distribution (p value = 2.2 × 10–16, fig. 1b).meanwhile, the mean values of cnpi were statistically different between the hbv-related aclf and the asc group (p value = 0.02, fig. 1c), which were 117 ± 83 and 106 ± 26 (mean ± sd) respectively.in total, 352 and 1,874 cnves were classified as common and rare cnvs (fig. 1a), respectively, where 331 common cnvs (~ 94%) could overlap (coverage rate > 0.5) with the cnvs from the hapmap database.a total of 1805 genes were contained in the deletion regions (additional file 4).they are significantly enriched in the leukocyte transendothelial migration pathway (p value = 4.68 × 10– 3).four major sub-functions are affected, including tail retraction, cell motility, docking structure, and transendothelial migration.association study of common cnvs a total of 17 strong disease association signals were detected (threshold p value: ~ 0.01), including 9 duplications and 8 deletions, respectively (fig. 3 and table 2).the peak one was a duplicate cnv on chromosome 1 p36.13 (~ 38 kb, p value = 1.99e−04), which had the largest or value (2.66) among all associates and contained the gene mst1l (macrophage stimulating 1 like).the second-strongest associate was a deletion cnv on chromosome 6 p22.1 (~ 33 kb, p value = 3.45e−04), which was enriched in asc populations and contained a long non-coding rna gene hcg4b (hla complex group 4b).more copies of hcg4b likely resulted in greater gene expression of hla-a, and the positive correlation may be caused by the competing endogenous rnas (cerna) of lncrna.a total number of 6 potential sponging micrornas between hcg4b and 3′utr of hla-a were predicted, where mir-6823-5p had the largest prediction score (additional file 7).except for two top signals, the 6 remaining associations could also contain gene elements, notably a duplicate cnv on chromosome 8 that covered 7 beta defensin genes (table 2).discussion aiming to explore the risk cnv in hbv-related aclf, we performed a global burden analysis and a genomewide association study of 389 hbv-related aclf cases and 391 asc controls.a series of high-quality cnvs were identified using snp array technology, where over 94% of common cnvs overlapped with the hap- map database, providing a strong foundation for subsequent studies.moreover, a total number of 17 common cnvs were found to be significantly associated with hbv-related aclf.these findings suggested that host genetic copy number variations likely play an important role in disease onset.further studies implied that genes within related cnvs may participate in decreasing natural immunity and enhancing host inflammatory response during hbv infections.compared to asc controls, hbv-related aclf population exhibit a higher burden of rare cnvs with the deletion genotype (table 1), which resulted in a lower copy number of genes related to the leukocyte transendothelial migration pathway (ltmp).the most affected genes are cell adhesion molecules (cams) [20], which are the key genes regulating transendothelial migration and play an important role in the firm adhesion of leukocytes during transmembrane transport [21] (additional file 5).transcriptome data revealed that low cam gene copies may further decrease its expression level (fig. 2a).nk cells are main cellular responders after hbv infection, and the abnormal status can induce severe liver injury [22, 23].low gene dosage of cams may reduce the migration activity of nk cells and further reduce its cellular immunity.for association studies, the strongest association signal was a duplication segment with a length of ~ 38 kb, covering only the mst1l (macrophage stimulating 1 like) gene (table 2).mst1l is homologous to macrophage stimulating protein (msp), and its first 6878 bp sequence was 96.1% identical to msp [26].mst1l was once thought a pseudogene of msp due to the frameshift and termination mutations.however, yoshimura et al. found that msp homologous genes could express in hepg2 cells [27], suggesting that mst1l may have transcriptional activity.transcriptome data indicated that the expression of hcg4b was positively correlated with hla-a, which may be regulated by competing with the sponging micrornas.chen et al. also identified a similar expression relationship (r = 0.45, p value = 1e−3), and predicted the potential sponging micrornas [29].low copies of hla-a and its expression level in asc controls may alleviate inflammation and reduce the risk of aclf under unknown situations.secondly, the transcriptome data is not from the samples of this study, and only reflects gene expression in monocytes.
number of words= 838
[{'rouge-1': {'f': 0.36963422127880174, 'p': 0.7750691244239631,'r': 0.24268623024830702}, 'rouge-2': {'f': 0.22387780820500314, 'p': 0.4125925925925926,'r': 0.15361581920903955}, 'rouge-l': {'f': 0.3965795291686572, 'p': 0.6661538461538461,'r': 0.2823287671232877}}]
-----------------------------------------------------------------------------------------------------------------------------------
p194:
Extractive Summary:
this syndrome is characterized by global developmental delay, intellectual disability, absent or severely delayed speech, hypotonia, minor dysmorphic features and autism spectrum disorder (asd) [1].no abnormality was identified on prenatal ultrasonography.the serum triple-marker (including alpha-fetoprotein, free-β- human chorionic gonadotropin, and unconjugated estriol) screening test result for down syndrome was negative.infant development was assessed using the development quotient (dq) according to the following criteria: normal (dq ≥ 85), borderline (75 ≤ dq < 85) and abnormal (dq < 75).the girl had dq of 60 for “gross motor”, 55 for “fine motor”, 35 for “language”, 47 for “adaptive behavior” and 48 for “personal- social behavior”.the boy had dq of 58 for “gross motor”, 54 for “fine motor”, 32 for “language”, 45 for “adaptive behavior” and 46 for “personal-social behavior”.they had abnormal social interactions, were no shy with poor eye contact and stereotypic behaviors and were interested in only one children’s song.the karyotype analysis of the parents was also normal.the boy had abnormal genitalia.the girl had sleep disturbances (easy to wake) and recurrent upper respiratory tract infections, while the boy did not.a timeline of historical and current information is shown in fig. 4.discussion and conclusions in this study, we described identical clinical and molecular findings in one pair of boy-girl twins with intellectual disability, speech absence, facial dysmorphism, cyanosis, large fleshy hands and feet, dysplastic fingernails and abnormal behaviors.using third-generation sequencing, we identified a 6.0 mb de novo interstitial deletion of the 22q13.31-q13.33 region encompassing 45 protein-coding genes.several reports have indicated that although probands obviously have de novo deletions, siblings can have the same deletions, probably due to germline mosaicism in a parent, which is similar to our report [12, 13].germline mosaicism may be a significant mechanism for the generation of de novo pathogenic cnvs [14].if the 22q13.3 variant found in the proband cannot be detected in the leukocyte dna of either parent, the recurrence risk in siblings is estimated to be 1% because of the theoretical possibility of parental germline mosaicism, which is marginally greater than in the general population [15].therefore, prenatal diagnosis is essential for normal parents with affected children.previous genotype-phenotype studies have revealed that many clinical features of pms are associated with deletion size.samogy-costa et al. found that renal abnormalities, lymphedema, and language impairment were positively associated with deletion sizes [5].several case reports of 22q13 interstitial deletion support this view.the sult4a1 and parvb genes have been suggested to be related to neurological features and macrocephaly/hypotonia, respectively [9].additional file 2: table s2 lists and compares the clinical and molecular findings of the probands and 14 previously reported cases, and a molecular comparison is shown in fig. 5.analysis of the data shows that consistent findings included dd/id (15/16), delayed speech (15/16), hypotonia (12/16), macrocephaly (10/16), dolichocephaly (3/16), feeding problems (9/16), hands/feet anomalies (10/16) and facial dysmorphisms (14/16).our findings further confirmed that other deleted genes in the 22q13 region likely contribute to the phenotypic characteristics of pms.the deletions identified in our two probands were identical and encompassed 45 protein-coding genes.our hypothesis is similar to that of palumbo et al. [10].the functions of the four candidate genes mentioned above are listed in additional file 3: document s1.among them, 8/13 had hands/feet anomalies.the most common abnormalities were large fleshy hands and feet.other mechanisms may affect the expression of the fbln1 gene, and larger case series and basic research are needed to confirm this hypothesis.however, more studies of smaller interstitial deletions with 22q13 are needed to corroborate our hypothesis and better define the genotype-phenotype correlation.our findings contribute to a more comprehensive understanding of pm
number of words= 588
[{'rouge-1': {'f': 0.4957993903561711, 'p': 0.8178991596638656,'r': 0.3557142857142857}, 'rouge-2': {'f': 0.3212156903141421, 'p': 0.5045991561181435,'r': 0.23559485530546626}, 'rouge-l': {'f': 0.45499182961534934, 'p': 0.6966666666666668,'r': 0.3378062678062678}}]
-----------------------------------------------------------------------------------------------------------------------------------
p195:
Extractive Summary:
the risk score was calculated with the following formula: risk score ¼ 0:139hoxc6 − 0:046hoxc4 þ 0:165heyl þ 0:106znf556 − 0:032hoxc9 the final coefficients of the model have been modified automatically to achieve better performance and to increase accuracy overall.thus, the coefficients of hoxc9 and hoxc3 are adjusted to slightly below zero, which are much smaller than those positive coefficients.then we performed the km analysis and the log-rank test result over these five selected tfs.the results and the p-value from previous cox ph analysis, along with the hazard ratio for each of these genes are summarized in fig. 3.it can be seen that all selected 5 tfs has cox p-value < 0.01, which indicates all these tfs are highly related to the overall survival of patients according to cox ph analysis.for the log-rank p, only the znf556 has a p-value of 0.107, while all the other four have p-value < 0.05.the hazard ratios of all these five tfs are more than 1.0, indicating higher risks of colon cancer prognosis.results on validation of the five-tf based prediction model based on the median value of the predicted risks scores of all the patients in both the training and validation set, patients are classified into high-risk and low-risk subgroups.km curve analysis and log-rank test were conducted to evaluate the performance of predicting power in colon cancer prognosis on tcga coad dataset.the results are shown in fig. 4.the scatter plot (fig. 4a(b)) shows the distribution of patients’ overall survival status.the red point indicates the patient belonging to a high-risk group while a blue point indicates the patient belonging to a low-risk group.from the scatter plot, we can observe that the red points are more concentrated in the lower part of the figure.this is an indication that high-risk patients have a shorter survival time comparing to low-risk patients.the heatmap (fig. 4a(c)) shows that the five selected tfs in our predictive model were highly expressed in tcga coad dataset.moreover, the km curve (fig. 4b) shows a distinctive survival difference between the high-risk and low-risk groups in a time span of more than 10 years.all these results prove the prediction power of our predictive model on tcga coad dataset.to test the five-tf based signature as colon cancer survival predictor, we further validated the predictive model on another four independent microarray datasets with a total of 1584 samples for geo with gse39582 (n = 563), gse17536 (n = 177), gse37892 (n = 130) and gse17537 (n = 55).the risk score of each patient in validation dataset was calculated by using the same formula established with tcga training dataset.the same coefficients were utilized to assign weight to each of the selected tf.by using the same median cutoff strategy to divide patients to the high-risk and low-risk groups, the km curve analysis shows the consistent patterns with the tcga coad dataset.patients in the high-risk group have a significantly shorter survival time than patients in the low-risk group (fig. 5a–d), which suggests the clinical robustness among multiple centers.therefore, our five-tf based signature is proved to be a robust predictor for colon cancer survival.results on pathway analysis the gene set enrichment analysis (gsea) [15] was conducted to investigate the biological function of this five-tf based signature, including its molecular function and gene-gene network.gsea is performed on the tcga coad dataset with predicted high-risk subgroup versus low-risk subgroup.in conducting the gsea study, the reference gene pathway database is the kyoto encyclopedia of genes and genomes (kegg) pathway database [23].the gsea number of permutations is set to be 1000, and the phenotype labels are determined according to whether a patient is in the high-risk subgroup or the low-risk subgroup.as illustrated in fig. 6, the gsea results showed that several cancer-related pathways were alternated in patients with high-risk scores, such as the pathways for the epithelial-mesenchymal transition, the ecm receptor interaction, the cytokine-cytokine receptor interaction, and the cell adhesion molecules (fig. 5ad).taken together these findings, it’s indicated that the five tfs in our model may highly associate with tissue morphogenesis, intercellular regulations and cell adhesion.by affecting these cell processes, these tfs may promote the tissue malignant then result in a poor overall survival rate of colon cancer patients.discussions we implemented an innovative machine learning approach for signature variables, which combines the cox ph method with the random forest algorithm.our signature selection process can find the minimum subset of tfs to build the prognosis prediction model with satisfying performance.a five-tf predictive model was developed by training the classifiers on tcga coad dataset.the trained multivariable linear predictive model was validated with multiple datasets from the geo database.three out of the five selected genes, i.e., hoxc4, hoxc6, and hoxc9, belong to the homeobox family of genes.the homeobox genes are highly conserved tf family and play an essential role in morphogenesis in all multicellular organisms.dysregulation of hox gene expression implicated as a factor in malignancies, and up-regulation has been observed in malignant prostate cell lines and lymph node metastases [24].hoxc6 was also reported to be overexpressed in colorectal cancer tissue, and highly correlated with poor survival outcome and acts as a significant prognostic risk factor [25].for the other two genes selected in our predictive model, heyl belongs to the hairy and enhancer of split-related (hesr) family of basic helix-loop-helix (bhlh)-type transcription factor.a recent study shows that heyl may be a tumor suppressor of liver carcinogenesis through upregulation of p53 gene expression and activation of p53-mediated apoptosis [26].znf556 belongs to zinc finger protein (znf) family.despite the large size of znf gene family, the number of disease-linked genes in this family is very small [27].to the best of our knowledge, the research on znf556 related to cancer is very limited.therefore, our study provided new insight on potential relationships between overexpression of znf556 and the development of colon cancer.
number of words= 957
[{'rouge-1': {'f': 0.411333493472825, 'p': 0.7151612903225806,'r': 0.28868787276341945}, 'rouge-2': {'f': 0.19620324564242453, 'p': 0.29647058823529415,'r': 0.14661691542288557}, 'rouge-l': {'f': 0.3965650229839208, 'p': 0.5945901639344262,'r': 0.29748815165876774}}]
-----------------------------------------------------------------------------------------------------------------------------------
p196:
Extractive Summary:
this fact is very important for the development of safe dna-editing systems with low risk of off-target events.here we describe all known bes.we also performed analysis to find all possible pathogenic variants which can be efficiently targeted by any of the described systems and present them for further selection and development of targeted therapies.methods clinvar database (grch37_clinvar_20171203) was used to search and select mutations available for current single-base editing systems.we included only pathogenic and likely pathogenic variants for further analysis.genome assembly hg19 was used as a reference.generally in order to target the specific mutation the cas9-based system needs a pam sequence.for every potentially editable mutation the pam sequence should be in the interval dependent on the sgrna length and width of the editing window of the specific be.so the pam sequence was searched in the window with coordinates [lengthsgrna – y; lengthsgrna – x + lengthpam] starting from mutation location (fig. 3, a).where lengthsgrna is typically 20 for most of the systems, lengthpam is typically 3 and x and y are the coordinates of the editing window for the particular be if to count nucleotides from the 5′ end of the sgrna.these calculations allowed to find the pam in such a distance from the mutations that if and when be would be applied the mutation will be found in the editable window.if a pam was found, we analyzed the editing window to find sequences with only one nucleotide (mutated) which can be edited without risk of changing neighboring nucleotides (fig. 3, b).detailed characteristics of the analyzed bes are presented in the table 1.the code of the script to search the database and to analyze the sequences was written in r and is available in the additional file 2.results editing systems are able to convert g(c) > a(t) and a(t) > g(c), which allows in theory to correct 68% of all mutations registered in clinvar (a(t) > g(c) – 21% and g(c) > a(t) – 47% respectively) (fig. 1,c).we selected only pathogenic and likely pathogenic mutations – 21% of all clinvar records.therefore, the total number of analyzed mutations was 27,310.we developed the r script to analyze 21 editing system currently reported in 9 publications.every system has different working characteristics such as the editing window and pam sequence which are summarized in the table 1.c > t bes have a lot of pams with the most popular ngg, and editing window is in the range of − 20 to − 5.for g > a mutations there are 2 systems with ngg/ngv/gat pams and typical window from − 17 to − 12.firstly, we searched for available pams near the target mutation (fig. 2,a).exact area of searching depends on the length of the editing window and length of the sgrna.it was possible to find several pams in the designated area, which were analyzed individually.for all c > t bes, we found 6415 potential targets which constitutes 93% of all t > c pathogenic mutations.abe systems can edit 13,683 mutations (67% of g > a pathogenic mutations).then we analyzed editing windows around selected mutations to check for the presence of other c(g) or a(t) nucleotides which could be nonspecifically edited together with targeted mutations.we selected only those mutations, which have no other targets near them (table 2).as a result, for c > t systems we select 3196 variants, it is approximately 46% of all pathogenic mutations, and 6900 mutations (34% of all pathogenic) for a > g systems.the first successful single-base editor was presented in 2016 by a. komor with colleagues [6].the editor consists of nuclease-deficient cas9 fused with apobec1 cytidine deaminase.cas9 with sgrna targets the complex to dna.deaminase converts any cytosine into uracil in the range of 8 nucleotides from − 18 to − 11 of the targeted sequence from pam with the overall frequency of 37%.uracil is later repaired to thymine.
number of words= 638
[{'rouge-1': {'f': 0.5030843246315618, 'p': 0.8238461538461539,'r': 0.36210134128166915}, 'rouge-2': {'f': 0.3569417664877638, 'p': 0.5642084942084942,'r': 0.261044776119403}, 'rouge-l': {'f': 0.5282078826264873, 'p': 0.7485714285714287,'r': 0.40807829181494665}}]
-----------------------------------------------------------------------------------------------------------------------------------
p197:
Extractive Summary:
background oesophageal atresia (oa) is the most common congenital anomaly of the oesophagus with an incidence of around 1 in 3500 births [1, 2].the malformation is characterized by a distal tracheoesophageal fistula, classified as gross type c, in 85% of all cases [3].isolated oa occurs in approximately 50% of cases whereas the remaining are syndromic [4–7].although the genetic basis for syndromic oa has been identified in a proportion of syndromic cases, the genetics behind isolated forms remains elusive.the recurrence risk for isolated oa is estimated to approximately 1% and twin studies have shown a concordance rate of 2.5% [8–10].the few familial cases of isolated oa that are reported suggest an autosomal dominant inheritance with reduced penetrance [8, 11, 12] whereas epidemiological studies on isolated oa indicate a multifactorial aetiology, with a contribution from both gene variants and environmental factors [4].the genetic architecture of both rare mendelian diseases and multifactorial disorders such as birth defects may be explained by a continuum based primarily on the frequencies of the relevant variant alleles.complex traits can be seen as omnigenic and thereby driven by large numbers of variants of small effects and, in this context, we also need to identify modifier loci that contribute to penetrance of variation [17].we therefore set out to analyse the genomic sequences of three families with recurrent cases of a low tracheoesophageal fistula (tef), the most common form of congenital oa.in family 3, a girl and a boy with oa were second cousins.for the analysis of structural variants (svs), we used manta, delly, cnvnator and tiddit as described previously [19–23].tiddit is mainly designed to identify larger svs (> 1 kb) using coverage and insert size distribution to identify svs based on discordant read pairs.variants that potentially affect splicing were identified using spliceai (score cut off 0.2 (high recall/likely pathogenic), 0.5 (recommended/pathogenic), and 0.8 (high precision/pathogenic)) [25].the svs were filtered against 1000 genomes and the swegen dataset using the aforementioned sv callers [23].we annotated identified svs with allele frequencies from 1000g, gnomad and the swegen dataset for all four sv callers.we did pathway analysis using enrichr (amp.pharm.mssm.edu/enrichr).we investigated associations between candidate genes identified in our study and oesophageal disease using genedistiller [27].a candidate gene list was generated in genedistiller by adding omim entries using the keyword ‘esophageal’ and manually adding genes previously associated with oa or similar phenotypes (complete reference gene list: aldh2, c2orf40, chd7, col4a6, ctag1b, dec1, dlec1, eftud2, fancb, fgf8, foxc2, foxf1, foxl1, gaec1, ger, gli3, hoxd13, hsn1b, lzts1, mthfsd, mycn, pten, rfx6, sox2, spg9, tmprss11a, and zic3).associations identified by genedistiller included gene reference into function (generif) and search tool for the retrieval of interacting genes/proteins (string; string-db.org).results whole genome sequencing quality control measurements show that the mean coverage for the 10 samples was 32.6x (min 21.9x; max 39.9x) with a median insert size of 394 (min 371; max 419).the total number of reads was 1,057,991,431 (min 794,472, 217; max 1,279,622,464) per sample and the number of aligned reads was 1,052,483,855 (min 791,300,824; max 1, 270,865,080) corresponding to 99.5% aligned reads (min 99.3%; max 99.7%).the duplication rate was 35% (standard deviation = 5.4%; min 28%; max 42%).we excluded sample bias by looking at the gc-content distribution compared to a pre-calculated distribution for the reference genome (hg19; additional file 1).the variants have a predicted effect on splicing of the prim2, fam182b, map2k3 and ccdc144nl genes (additional file 3).no variants identified was inherited from the non-sequenced parent, indicating co-segregation from the obligate carrier parents in family 2 and 3 and from the affected parent in family 1.analysis of sv shared by the two affected individuals in each separate family resulted in the identification of 339 del, 47 dup and 3 inv in family 1, 172 del, 19 dup and 2 inv in family 2 and 135 del, 17 dup and 1 inv in family 3 (fig. 2a; additional files 5, 6 and 7).all identified variants were found also in the parents of each family confirming segregation.the major proportion of variants consisted of svs (involving 95 genes) followed by splicing (involving 4 genes).the 100 genes were then subject analysis by genedistiller to identify genes associated to the oesophagus.our initial analysis of rare protein coding snvs or small indel (filtered for an af < 0.001) did not show any shared variant among all six cases.the variants predict perturbed splicing of the prim2, fam182b, map2k3 and ccdc144nl genes, respectively.cancer is usually associated with loss of function variants, while in our cases non-coding svs may alter expression of genes that are required for proper specification and elongation of the oesophagus during embryogenesis, ultimately resulting in oa [36].conclusion in summary, we explored the possible association between familial forms of isolated oa and rare gene variants using whole genome sequencing.in two out of the three families, the affected members were second cousins, whereas one family consisted of an affected mother and son.among all variants (snvs, splice sites and svs) in 100 genes shared by the six individuals from the three families, we identified variants in three genes associated with oesophageal disease, namely ccdc144nl, defb4a and pds5b, bringing further support for a molecular link between oa and oesophageal cancer.we anticipate that the identification of additional families segregating isolated oa, in combination with our findings, will facilitate the identification of specific gene variants contributing to this serious malformati
number of words= 880
[{'rouge-1': {'f': 0.3216461622680838, 'p': 0.8215527950310559,'r': 0.19996777658431794}, 'rouge-2': {'f': 0.16500331533579615, 'p': 0.31375,'r': 0.11193548387096774}, 'rouge-l': {'f': 0.31915286511831475, 'p': 0.662920353982301,'r': 0.2101673640167364}}]
-----------------------------------------------------------------------------------------------------------------------------------
p198:
Extractive Summary:
clinical severity of thalassemia varies greatly and could be classified as non-transfusion dependent thalassemia (ntdt) which requires occasional blood transfusion in specific situations and transfusion dependent thalassemia patients (tdt) which requires regular blood transfusion [2, 3].there is limited data about telomere length in patients with thalassemia.it was also demonstrated that the telomere shortening was associated with the clinical severity of thalassemia [11].we conducted the study in patients with tdt who had severe clinical symptoms.clinical and laboratory measurement we collected clinical and laboratory parameters at the time of enrollment to identify factors associated with telomere length including age, sex, history of splenectomy, iron chelation, and factors associated with clinical severity including: pre-transfusion hb level (mean steady-state hb level in the previous 10 visits), units of red blood cell transfusion per month, serum ferritin (maximum, minimum, mean of serum ferritin at multiple times every year).the specificity of all amplifications was determined by a melting curve analysis.the primers for amplification of the scg albumin are albumin forward primer, albu: 5′-cggcggcgggcggcgcgggctgggcgga aatgctgcacagaatccttg-3′ and albumin reverse primer, albd: 5′-gcccggcccgccgcgcccgtcc cgccggaaaagcatggtcgcctgtt-3′; telomere forward primer, telg, 5′-acactaaggtttgggttt gggtttgggtttgggttagtgt-3′ and telomere reverse primer, telc, 5′-tgttaggtatccctatccctatccctatccctatccctaaca- 3′. the thermal cycling profile was stage 1: 15 min at 95 °c; stage 2: 2 cycles of 15 s at 94 °c, 15 s at 49 °c; and stage 3: 32 cycles of 15 s at 94 °c, 10 s at 62 °c, 15 s at 74 °c with signal acquisition, 10 s at 84 °c, 15 s at 88 °c with signal acquisition.all samples were assayed in triplicates in order to minimize the sample-to-sample variation.multivariable regression analysis was performed using a forward selection technique.we proposed to examine age, pre-transfusion hemoglobin level and type of iron chelator.the correlation coefficient between the t/s ratio and each parameter value was calculated using pearson’s correlation.this study was approved by the human research ethics committee of faculty of medicine, chiang mai university.(study code: med-2559-03967).the median age was 27.0 years (range 18–57).the mean pre-transfusion hb level was 7.1 (± 1.07) g/dl.the median age was 30 (18–50) years.there were 19 females (63.33%).the age and gender distribution were not different between the patient and control groups (p = 0.07 and p = 0.55 respectively).the meant/s ratio of the control group was 0.99 ± 0.25.tdt patients had a statistically significant shorter t/s ratio compared to the control group (p < 0.0001).(fig. 1).moderate negative correlations were seen between the mean t/s ratio and age (r = − 0.4435, p = 0.0001), fig. 2, and there was a positive correlation with pre-transfusion hb level (r = 0.2508, p = 0.044), fig. 3.the multivariate analysis factor showed that telomere length shortening was associated with age (p < 0.0001) and pre-transfusion hb level (p = 0.02).patients who received combination therapy had higher serum ferritin level more than patients who received monotherapy.there were 2 patients who did not receive iron chelation because of severe adverse effects from iron chelators.our study demonstrated that patients with tdt had a shorter telomere length as reflected by the lower t/s ratio when compared with healthy controls.the telomere shortening was associated with the age and pre-transfusion hb level.there was no statistically significant correlation between telomere length and other factors such as sex, reticulocyte count, ferritin, cardiac t2*, lic, ntbi, and hemolysis parameters.telomere length associated with aging was only found in patients with mild clinical symptoms but not in the groups with moderate and severe clinical symptoms.reticulocyte count showed a correlation with telomere shortening.tissue iron may represent the body iron status of patients and is associated with accelerated shortening telomere length.however, this finding about iron chelators needs to be interpreted carefully as patients who needed combination iron chelators may have had higher iron burden and experienced more chronic oxidative stress.
number of words= 619
[{'rouge-1': {'f': 0.47725939956264696, 'p': 0.7513186813186814,'r': 0.34969924812030073}, 'rouge-2': {'f': 0.25877726998361644, 'p': 0.3788235294117647,'r': 0.19650602409638554}, 'rouge-l': {'f': 0.40303064466836586, 'p': 0.5666887417218542,'r': 0.31271844660194176}}]
-----------------------------------------------------------------------------------------------------------------------------------
p199:
Extractive Summary:
background congenital malformations account for ~ 20% of infant mortality and ~ 18% of pediatric hospitalizations [1–3].many are recessive disease syndromes that are severe, untreatable, and adversely affect quality or length of life, prompting parents to seek prenatal counseling for future pregnancies.over 3000 individual genes are currently linked with mendelian disease, and this list continues to grow rapidly [4].furthermore, there is growing consensus around standardized interpretation of genetic variants within a clinical context [5] and diagnostic decision support software (ddss) can provide an additional level of certainty as to pathogenicity [6, 7].autosomal recessive diseases account for ~ 26% of severe pediatric conditions undergoing diagnostic sequencing [8], and impart a standard 25% recurrence risk.most of these diseases are not evident with fetal ultrasound prior to 20 gestational weeks (gw), especially neurological conditions, since much of human brain development occurs after mid-gestation [9, 10].however, there are few reports documenting population-level effectiveness of fetal genotyping supporting the prevention of disease recurrence through elective termination of pregnancy (etop), and despite this approach gaining acceptance, each prenatal center follows its own best-practices.here, we leverage family-specific ngs sequencing, combined with fetal targeted genotyping to measure the impact of this approach on the eventual disease recurrence risk in families, by providing predictive affectation status information to couples carrying a pregnancy that were at 25% risk for disease recurrence.most recessive pediatric brain disease (rpdb) seen in our center do not have obvious structural fetal anomalies, rendering fetal imaging sub-optimal for determining affectation status.omim features with standard diagnostic criteria were used to determine affectation status of newborn children, as assessed by the neurologist and geneticist.exome and genome sequencing exome on dna samples was performed using idt exome capture kit.hybrid selection libraries covered > 80% of targets at 20x with a mean target coverage of >80x.the exome data were demultiplexed and each sample’s sequence was aggregated into a single bam file.genome on dna samples (500 ng to 1.5 ìg) was performed using a pcr-free protocol.these libraries were sequenced on the illumina hiseq 4000 with 150-bp paired end reads and target mean coverage of >30x.in 17 pregnancies, pnd was declined at that point.each case presented at least two neurological features, with the majority displaying 4 to 5 features (fig. s4), consistent with severe rpbd.features such as spasticity, ataxia, seizures and developmental regression were noted.secondary outcome measures were all negative for losses of pregnancy, extensive bleeding or post-procedural complications, there was no detectable maternal blood contamination in collected fetal samples, and no equivocal genotyping results.there were 39 that genotyped as heterozygous for the pathogenic variant and 21 as homozygous reference, all predicted to be unaffected by the disease for which they were tested.among these were one case of a spontaneous loss of pregnancy that was not attributed to the amniocentesis procedure, and one with a skeletal deformity (absent sternum) at birth that was unrelated to the phenotype of the affected older sibling, and that remained otherwise neurologically normal.out of 24 that tested positive for the biallelic pathogenic variant, 16 ultimately decided on etop and 8 carried the pregnancy to term.none of the 16 women experienced notable complications of the etop.thus, while positive predictive value (ppv) of pnd could not be assessed, negative predictive value (npv, i.e. probability that offspring does not have disease given negative pnd) was 100%.we determined that a sample size of 80, given etop decision in 70% of affected pregnancies, would yield > 90% power to detect a significant difference in outcome from this study (fig. s5).this resulted in a p-value of 0.0017, indicating a significant reduction in observed recurrence risk over expected in this study.assessment of decision regret by parents we assessed parental experiences with their decision about etop at ~ 6 months after the predicted due date for all 24 pregnancies.none of the 8 parents of pregnancies carried to term had feelings of regret about their decision, despite clinical features of disease apparent in most.these 8 couples cited several reasons why etop was not pursued, including social, logistical, religious or economic reasons contributing to their decision.none of these couples were attempting to become pregnant again at the time of the 6-month follow up, and no further pregnancies were observed from these 8 couples.none of the 16 parents of the etops had feeling of regret about their decision at follow-up.even though reproductive specialists often provide genetic testing based upon experience, widespread acceptance by public health authorities and insurance providers could benefit from future prospective studies demonstrating ngs implementation that ultimately lead to reduced disease recurrence.decisions ultimately lie with the mother or couple, but can be influenced by numerous ethical, social, familial, legal and logistical considerations.the npv of 100% that we observed may not be representative for larger applications of this design, taking into account the 0.1% estimated sequencing error and the possibility of human error, which would negatively impact diagnostic accuracy.additionally, ppv could not be calculated due to the inability to independently confirm the phenotype of fetuses predicted to be affected, and the absence of clinical features prior to etop in nearly every case.minimal variability in expressivity of each condition in each family was observed, where there were two or more affected children diagnosed prior to pnd, and each of the 8 children later diagnosed as affected displayed features nearly identical to their older affected sibling(s).the benefit of this approach is that each family had prior experience with the disease in their older offspring, which may have facilitated the decision about subsequent pregnancies.although the method chosen for fetal sampling was amniocentesis, chorionic villus sampling (cvs) is a comparable alternative, if available, as it can be performed earlier than amniocentesis.additionally, the variants identified in probands could be alternatively used in preimplantation genetic diagnosis (pgd) to reduce recurrence risk in subsequent pregnancies.pgd is not widely available in egypt where this study occurred, limiting its impact, and suffers its own limitations [23–25].in our study we focused on direct sanger sequencing of the pathogenic allele in the fetus that was previously identified in an older sibling(s).recent publications highlight that between 4.9–13% of cases show two different diseases or disease-causing mutations in the same child [30, 31].we observed a skeletal deformity in an otherwise healthy case newborn, suggesting a second genetic condition, but the genetic origin of the skeletal deformity was not investigated.ideally, assessment of all such conditions relevant to the fetus could be performed simultaneously.for instance, recessive traits in the ashkenazi population, like tay-sachs disease (1:27–1:30 carrier frequency) formed the basis for genetic premarital efforts that reduced the incidence by > 90% [34].with an expansion of ngs accessibility, preconception carrier screening for the majority of inherited diseases will be feasible, but will suffer from imperfect predictive power in many instances.conclusions identification of causes through ngs, coupled with prenatal fetal genotyping in subsequent pregnancies, allows families to make informed decisions to reduce recessive disease recurrenc
number of words= 1132
[{'rouge-1': {'f': 0.37107835828734437, 'p': 0.7335220125786164,'r': 0.24836010143702453}, 'rouge-2': {'f': 0.17470464547881953, 'p': 0.2813564668769716,'r': 0.12668358714043992}, 'rouge-l': {'f': 0.3033381038739117, 'p': 0.5550299401197605,'r': 0.2086986301369863}}]
-----------------------------------------------------------------------------------------------------------------------------------
p200:
Extractive Summary:
type b aortic dissection (tbad) is an acute cardiovascular disease with high mortality and disability rates [1–3].medical treatment is routinely used in the clinic to treat the uncomplicated aortic dissection.however, for the complicated tbad, the thoracic endovascular aortic repair (tevar) is an alternative and effective tool compared to open surgery [4–6].since the stent placement is mainly guided by digital subtraction angiography (dsa), an inaccurate viewing angle may lead to 3d vascular structures overlap in the projected 2d images.therefore, the viewing angle of the c-arm plays an important role in placing the stent grafts.during the process of tevar, physicians manually determine the angiographic viewing angles according to their personal experiences in order to display the whole aortic arch while eliminating any overlap of branches.this task is subjective and may not only be time-consuming for young physicians but also increase the patient’s intake of x-ray and contrast agent.furthermore, the x-ray angiographic images with improper viewing angles lose much aortic topological information [7, 8], thus, affect the accuracy of the stent placement and lead to some later complications such as aortic rupture or retrograde dissection [9].the current clinical imaging angle selection requires the participation of expert cardiologists, hence an additional burden in the clinical setting.therefore, the automatic selection of angiography viewing angle is extremely important and clinically required.the issue of automatic optimum viewing angle determination has attracted many researchers’ attention because of its major clinical implications.dumay et al. [10] assumed that the interested vessel is a cylinder and selected two projection images of different angles.then, the optimal viewing angle was obtained through the geometry relationship of corresponding vectors.nevertheless, this method is only suitable for slightly-curved vessels and requires at least two different angles of projection images.it fails in case of large and extended curvature.moreover, obtaining projection images at different angles increase the radiation dose.considering the intake of x-ray and contrast agent, a series of studies used cta data to obtain the optimal angle in the coronary artery [11–16].the vessel vector was represented by the line between two adjacent points on the centerline of the vessel of interest.chen et al. [17] used a predefined threshold to calculate the minimum vascular projection foreshortening rate.the optimal angiographic angle was obtained by refining the overlapping rate.considering that the aortic branches are much larger than the coronary arteries, the method applied in coronary arteries is not suitable for the aorta.the imaging angle automatically obtained through mathematically minimum methods does not fit the clinical scenario in the aorta.the optimal angle obtained by adding the constraint design of the realistic clinical scenario is more suitable for the clinic.in order to solve the aforementioned drawbacks, we proposed an adaptive optimization method that automatically determines the angiography viewing angle for the tevar operation.the projection foreshortening rate (pfr) is used to obtain a full display of the aortic arch, and the projection overlapping rate (por) is used to avoid the overlap of branches on the aortic arch.by combining an empirical regularization term, the optimal angle can be automatically obtained before tevar, which ensures doctors a better imaging angle to place the stent more accurately.the contributions of this work are summarized as follows: 1. to our knowledge, our solution is the first attempt based on pre-operative cta images for providing an automatically and reliably optimal imaging angle view for tevar.2.the results are very close to the angle chosen by the experts and no statistically significant difference is observed.therefore, the c-arm angle can directly use the result obtained by our algorithm before the intervention.after the contrast injection, if the angle is suitable, the stent can be directly implanted.even if the angle needs to be fine-tuned, the slight error will reduce the radiation time compared to the previous angle adjustment from zero.our future work will focus on the overall benefit of our approach in terms of radiation amount and contrast usage by either experts or junior physicians.conclusion in this paper, an adaptive optimization algorithm based on cta data acquired before tevar is proposed to determine the clinically best c-arm angle to use during the intervention.the optimal viewing angle estimated using this method does not show a significant difference with expert settings.hence, this automatic method has the potential to assist junior doctors while providing shorter procedure time, less radiation exposure and contrast injectio
number of words= 706
[{'rouge-1': {'f': 0.38807163621749435, 'p': 0.7864948453608247,'r': 0.2575843454790823}, 'rouge-2': {'f': 0.216345893638417, 'p': 0.38088082901554404,'r': 0.1510810810810811}, 'rouge-l': {'f': 0.3962609559666943, 'p': 0.6526771653543306,'r': 0.2844927536231884}}]
-----------------------------------------------------------------------------------------------------------------------------------
p201:
Extractive Summary:
there were no significant differences between c-dwi and sms3-dwi in the sharpness of the right lobe edge (p = 0.066) or conspicuity of the left lobe (p = 0.131).c-dwi exhibited better scores than sms3-dwi for the visibility of intrahepatic vessels, artefacts, and overall image quality (all p < 0.01) in the images with a b-value of 800 s/mm2.for the images with a b-value of 2000 s/mm2, sms2-dwi exhibited better scores than sms3-dwi for all image quality parameters (all p < 0.05).artefacts of the liver could be observed in the high b-value (800 and 2000 s/mm2) sms3-dwi images, and the artefacts became more obvious with increasing b-values (figs. 1, 2).quantitative analysis of signal‑to‑noise the snr values measured in the three dwi sequences are shown in table 3.there were no statistically significant differences in the snr between the three sequences in images with a b-value of 2000 s/mm2 (p = 0.110).for the images with a b-value of 800 s/mm2, there were no statistically significant differences between sms2-dwi and c-dwi (p = 1.000) and no statistically significant differences between sms2-dwi and sms3-dwi (p = 0.059), whereas sms3-dwi had a significantly lower snr than c-dwi (p = 0.024).quantitative analysis of dki parameters and adc values the mk values of c-dwi, sms2-dwi and sms3-dwi were 0.67 ± 0.15, 0.68 ± 0.11, and 0.70 ± 0.18, respectively.the md values of c-dwi, sms2-dwi and sms3- dwi were 1.19 ± 0.15 × 10− 3 mm2/ s, 1.17 ± 0.21 × 10− 3 mm2/ s, and 1.17 ± 0.18 × 10− 3 mm2/ s, respectively.the adc values of c-dwi, sms2-dwi and sms3- dwi were 0.85 ± 1.12 × 10− 3 mm2/ s, 0.81 ± 1.24 × 10− 3 mm2/ s and 0.80 ± 1.56 × 10− 3 mm2/ s, respectively.there were no statistically significant differences between the three sequences (mk, p = 0.606; md, p = 0.831; adc, p = 0.264) (fig. 3).although there were no statistically significant differences in the dki parameters between sms2-dwi and c-dwi in this study, increased mk values and decreased md values were observed with increased afs in the liver.although the scan time was significantly shortened for sms3-dwi sequences, it had a negative impact on the image quality, especially at a high b-value of 2000 s/mm2.the dki model potentially reflects the non-gaussian diffusion behaviour of water diffusivity in tissues using ultrahigh b-values above 1000 s/mm2 (1500–2000 s/mm2 in body imaging) [5, 6, 18]; thus, dki could provide further information on tissue characteristics and has additional value in the prediction of microvascular invasion (mvi) of hepatocellular carcinoma (hcc) and in the assessment of post-therapeutic response in hypervascular hcc [3–5].obtaining ultrahigh b-value dwi images will induce a decreased snr, increased distortion, susceptibility artefacts, and increased scan time [19].in conventional diffusion acquisition, a single slice is excited, whereas in sms-dwi, multiple slices are excited simultaneously, and the corresponding read-out contains these multiple slices.this permits the tr to be decreased, thus shortening the acquisition time, or allows more slices with thinner slice thicknesses to be scanned under the same tr [7, 9, 10].notably, the maximum number of simultaneous acquisition slices depends on the number of coil channels [7].in theory, image acquisition in sms-dwi can be accelerated to a large extent by increasing the applied acceleration factors.however, an arbitrary increase in the acceleration factors is expected to be associated with a decrease in image quality.our research indicates that sms can shorten the scan time without reducing the image quality even in images with ultrahigh b-values of 2000 s/mm2 if an acceleration factor of 2 is used.although there were no statistically significant differences in the dki parameters between sms-dwi and c-dwi in this study, sms3-dwi should not be chosen in future studies due to the obviously decreased image quality and pronounced artefacts, especially in images with high b-values of 2000 s/mm2.the mk values (unitless) of the dki parameters reflect complex tissue microenvironments, such as tumour cells, necrosis, and inflammation.the md values are the diffusion coefficients (unit: 10− 3 mm2/ s) related to gaussian behaviour and similar to the adc values that decrease with restricted diffusion.mk is determined by the si decay curvature away from the plot that would be predicted by a monoexponential fit, whereas md is determined by the slope of the si decay plot as b approaches 0. decreases in md values and increases in mk values indicate abnormal diffusion behaviour in the organs.although there were no statistically significant differences between the three sequences, with an increase in the acceleration factors, mk values showed an increasing trend and md and adc values showed a decreasing trend in the liver.this result could be due to a decrease in image quality that potentially affects the signal accuracies and influences the dki parameter calculations in the sms-dwi sequences.several studies have indicated that adc values decrease when using sms-dwi compared with c-dwi in the liver, pancreas and kidney [12, 15, 20].one possible reason for the lower adc values with higher acceleration factors is the shorter repetition time (tr) of the sms sequence.in the present study, the tr was 5200, 2400, and 1800 ms for c-dwi, sms2-dwi, and sms3- dwi, respectively.the lower tr may lead to a reduced signal due to t1 saturation effects, particularly with the high b-value images that had higher afs, resulting in decreased adc values.obele et al., who investigated the sms technique with an acceleration factor of 2 in free breathing, noted that there were slightly lower adc values compared with those of a conventional dwi sequence without statistical significance in the liver [12].
number of words= 915
[{'rouge-1': {'f': 0.5474653603071618, 'p': 0.8399530516431926,'r': 0.4060655737704918}, 'rouge-2': {'f': 0.37902311707680836, 'p': 0.5641176470588236,'r': 0.2853846153846154}, 'rouge-l': {'f': 0.4842751008278497, 'p': 0.7006818181818182,'r': 0.37}}]
-----------------------------------------------------------------------------------------------------------------------------------
p202:
Extractive Summary:
background ganglioneuroma (gn) is a rare benign tumor which originates from neural crest cells.it is mainly composed of mature schwann cells, ganglion cells and nerve fibers.gn may arise anywhere along the paravertebral sympathetic plexus.the retroperitoneum and posterior mediastinum are the two most common locations of gn [1–4].although gn is regarded as an uncommon tumor, more and more gns have been detected with the dramatically increased use of computed tomography (ct) and magnetic resonance imaging (mri).though gns are benign tumors, they may cause pain and compression symptoms and recurrence or malignant transformation has been reported [2, 5, 6].due to these characteristics, surgical resections and postoperative monitoring are recommended for gn patients.to date, few studies have focused on the primary non-adrenal retroperitoneum gn.the precise diagnosis of non-adrenal retroperitoneum gn prior to surgery remains challenging.the non-invasive diagnosis of gn is important, because the surgical strategy and surgical approach can be different for benign and malignant tumors.prior to the surgical procedure, ct and mr images can demonstrate important features of the tumors and help to narrow the differential diagnosis.moreover, ct and mr images enable a multiplanar evaluation revealing the exact localization of the lesion and could heip to accurately evaluate the association of the mass with adjacent structure.therefore, preoperative imaging of gns could substantially facilitate preoperative planning.in this study, we reviewed a relatively large series of histopathological proved non-adrenal retroperitoneum gn cases at our institution and interpreted the ct and mri features of them.furthermore, we analyzed risk factors of increased surgical blood loss based on radiographic images for the first time.methods this study was a single-center retrospective study approved by the committee on ethics of medicine of changhai hospital and the study was carried out in accordance and relevant guidelines and regulations.the requirement to obtain written informed consent was waived by the committee on ethics of medicine of changhai hospital.computerized patient record systems and the laboratory, pathology and radiology databases were searched from january 2012 to june 2019.the inclusion criteria were as follows: pathologically proven retroperitoneum gn; undergone contrast-enhanced ct or mri before surgery.the exclusion criteria were: adrenal gn cases; absence of a presurgical contrast-enhanced ct or mri record in our hospital; not undergoing surgical resection of tumors.follow-up data were obtained by searching medical records in our hospital and making phone calls to the patients.the written informed consents were obtained from all patients.patients a total of 35 patients (18 male, 17 female) were pathologically diagnosed with retroperitoneum gn during the 7-years-period in our hospital.their age ranged from 14 to 66 (median, 40 years).all of them were treated operatively.the mean time interval from preoperative imaging to surgery was 5 days (range, 0–27 days).modalities among them, 24 patients had undergone ct scans and 19 patients had undergone mr examination before treatment.the ct scans were carried on by a 320-multidetector row ct system (toshiba medical systems, tokyo, japan).the ct scan parameters were as follows: 120 kv, 150 effective mas, a matrix of 350 × 350, and a gantry rotation time of 0.5 s. the intravenous iodinated contrast agent (iopamiro, bracco sine pharmaceuticals, shanghai, china) was administered for contrast media enhancement.the contrast-enhanced ct scans were performed in arterial (20–25 s) and venous (60–70 s) phases after the contrast agent injection.mr imaging was performed by a 1.5 t mr system (ge medical system, usa) with a body coil.the mr protocol contained horizontal t1 (repetition time (tr) 2.58 ms, echo time (te) 1.18 ms) and t2 weighted (tr 6316 ms, te 87 ms) images, coronal t2 weighted images (tr 7000 ms, te 1230 ~ 1270 ms) and diffusion-weighted imaging (dwi) (tr 5000 ms, te of 80 ms, b = 800).images were obtained with a field of view of 440 × 440 mm, an image matrix of 224 × 270, and a slice thickness of 5 mm.gadolinium-diethylenetriamine pantaacetic acid (gd-dtpa, magnevist, berlin, germany) was intravenously administrated for contrast media enhancement.the delayed time was 30 and 70 s after injection of contrast agent.image analysis the images were reviewed by two experienced radiologists (zhang & yang) independently, and discrepancies were resolved by consensus.the two readers were blinded to the operation records.the following radiological characteristics were assessed: tumor size (maximal diameter measured on axial and coronal 2d images), shape (round, oval or irregular), calcification (present or absent), density (ct)/ signal (mr) features, pre-contrast appearance (homogeneous or heterogeneous), the pattern of enhancement, specific signs including vessel encasement and whorled sign.the vessel encasement sign represented that the tumor showed the trend to surround major blood vessels (> 180°).the vessel encasement signs are categorized into 3 types: tumor encasing the origin of the coeliac axis, and/or of the superior mesenteric artery; tumor encasing one or both renal pedicles; tumor encasing the aorta and/or vena cava [17, 18].though gns are uncommon tumors, they should be taken into consideration when specific features are presented.these features are as follows: (a) relatively low density on unenhanced ct images, (b) the whorled sign on t2-weighted images, (c) the possible tendency to surround major vessels but with no narrowing, (d) the delayed progressive enhancement on enhanced ct and mr images.larger tumor size measured on axial images and encasing one or both renal pedicles may lead to increased blood loss during surge
number of words= 858
[{'rouge-1': {'f': 0.4244244781395831, 'p': 0.7665517241379309,'r': 0.29345132743362834}, 'rouge-2': {'f': 0.26055089885685045, 'p': 0.4333217993079585,'r': 0.18627906976744185}, 'rouge-l': {'f': 0.4157205614050891, 'p': 0.7090532544378698,'r': 0.2940663900414938}}]
-----------------------------------------------------------------------------------------------------------------------------------
p203:
Extractive Summary:
a total of 14,972 visible bone lesions were recognized in all wbs images and 51.23% of them were identified metastasis.the lesion-based metastasis rate was 50.13% in lung cancer, 57.39% in prostate cancer, and 44.61% in breast cancer, respectively.the detailed information was listed in table 1.the performance of the proposed network after fivefold cross validation, the cnn model demonstrated an average sensitivity, specificity, accuracy, ppv and npv for all visible bone lesions were 81.30%, 81.14%, 81.23%, 81.89% and 80.61%, respectively.when compared with the other three start-of-art cnns, our proposed network achieved the best accuracy in identification the bone lesions at bone scintigraphy (tables 2, 3).subgroup analysis of proposed network based on the number of lesions per image, we found that the ai model reached the highest sensitivity (89.56%, p < 0.001), accuracy (82.79%, p = 0.018) and ppv (87.37%, p < 0.001) in the extensive lesions group as shown in table 4. whereas, the highest specificity (89.41%, p < 0.001) and npv (86.76%, p < 0.001) of the ai model were captured in few lesions group.we also calculated the auc to evaluate the diagnostic performance of the ai model, which was 0.847 in the few lesions group, 0.838 in the medium lesions group, and 0.862 in the extensive lesions group.and the confusion matrix directly demonstrated the true labels and predicted labels in the three groups (fig. 3).the detailed results based on the primary tumor types were shown in table 5, the results demonstrated the highest diagnostic sensitivity (84.66%, p = 0.002) in the prostate cancer group.albeit slightly higher accuracy (82.30%) in the prostate cancer group, there was no statistical significance (p = 0.209) comparing with the lung cancer group (79.40%) and breast cancer group (81.82%).the specificity in lung cancer (82.52%), prostate cancer (79.07%) and breast cancer (81.78%) group also did not indicate statistical significance between each other (p = 0.354).furthermore, the auc was 0.870 for lung cancer, 0.900 for prostate cancer, 0.899 for breast cancer.the confusion matrix directly demonstrated the true labels and predicted labels in the three groups (fig. 4).additionally, we also evaluated the lesion-based diagnostic performance of the ai model according to the different number of lesions per image (few, medium and extensive lesions group) in lung cancer, prostate cancer and breast cancer, respectively.the results were supported as additional file 1: table 1 and additional file 2: figs. 1, 2, and 3.discussion the definitive identification of abnormal bone lesions is beneficial to proper personalized treatment and subserves the patients who were suffering from advanced malignant cancers [26].however, nuclear medicine physicians usually take other lesions and additional cues into account when determining one single lesion itself.for example, an isolated lesion without other nearby lesions would be more difficult to assert benign or malignant, while multiple lesions that occur within a narrow region would be more likely malignant.we input corresponding lesion masks to the cnn and take the whole wbs image into account, and this might be a possible reason for the improved accuracy of the extensive-lesions group.previous studies also reported ai for bone lesion identification from wbs images.the authors used a ladder network to pre-train a nerual network with an unlabeled dataset [28].on the metastasis classification task, it reached a sensitivity of 0.657 and a specificity of 0.857.another similar study also build a model to detect and identify bone metastasis from bone scintigraphy images through negative mining, pre-training, the convolutional neural network, and deep learning [29].the mean lesion-based sensitivity and precision rates for bone metastasis classification were 0.72 and 0.90, respectively.in our study, the lesion-based sensitivity, specificity and precision values for metastasis classification were 0.813, 0.811 and 0.819, respectively.it is difficult to compare the difference of algorithms, all studies have used in-house datasets of a gold standard and these datasets were not open.we were not able to try other datasets using our algorithm.therefore, the performances reported by other researchers can only be used as references, rather than for objective comparison.it is worth mentioning that the aforementioned ai was focused on the chest image instead of the whole body.
number of words= 668
[{'rouge-1': {'f': 0.4355284288539066, 'p': 0.6785526315789474,'r': 0.32067750677506773}, 'rouge-2': {'f': 0.21510262927395216, 'p': 0.30432343234323433,'r': 0.16633649932157396}, 'rouge-l': {'f': 0.3889272398776017, 'p': 0.5214285714285714,'r': 0.3101215805471125}}]
-----------------------------------------------------------------------------------------------------------------------------------
p204:
Extractive Summary:
the regimens commonly suggested for adjuvant chemotherapy in the guidelines for pdac include gemcitabine, 5-fluorouracil (5-fu)/leucovorin, s-1, etc.[4–6].s-1 is a newly developed oral antitumour agent consisting of tegafur (a prodrug of 5-fu), gimeracil [a potent dihydropyrimidine dehydrogenase (dpd) inhibitor], and oteracil (an inhibitor of the phosphorylation of 5-fu in the gastrointestinal tract).tegafur is transformed into 5-fu in the liver after oral administration [7].it has been reported that monotherapy with s-1 demonstrates noninferiority to commonly used gemcitabine in overall survival for locally advanced and metastatic pancreatic cancer [8].how can a suitable postoperative regimen be personalized from a variety of chemotherapy regimens listed in the pdac guidelines?because adverse reactions are common and the proportion of patients benefiting from chemotherapy is not high [10, 11], it is urgent to determine how to identify patients who are more likely to benefit from certain adjuvant chemotherapy regimens and how to maintain their quality of life while pursuing better clinical outcomes.however, the guidelines have not provided answers to these questions or solutions to these problems.currently, there is no definite standard for the selection of drugs in the adjuvant chemotherapy of pdac [4], and predictors of chemotherapy response are needed for personalized precision medicine.the same questions and problems exist in the clinical application of s-1, especially which population can benefit from postoperative adjuvant chemotherapy with s-1.radiomics technology, which has emerged in recent years, offers important advantages for the assessment of tumour biology.radiomics analysis can aid in evidencebased clinical decision making in oncologic management and help achieve individualized precision medical care [12–14].only a few radiomics analysis studies on mri have been performed in pdac.whether mri and radiomics features could be used for the prediction of therapy response to adjuvant chemotherapy in postoperative pancreatic cancer patients has not been reported in previous literature.after curative resection, 31 of them who subsequently received adjuvant chemotherapy with s-1 were included in this study as the primary cohort.the follow-up period was from the time of surgery to november 2018.the inclusion and exclusion criteria of our study were as follows.inclusion criteria: (1) patients received abdominal contrast-enhanced mri examination with magnetom aera (siemens healthcare, germany, 1.5 t) at our institute and were suspected of having pancreatic cancer; the image quality was satisfactory for the study; (2) patients underwent curative resection of the tumour at our institute, and the diagnosis of pdac was confirmed by pathological examination; (3) the pre-operative laboratory tests and operation were within 1 month from the date of the mri examination; (4) patients received postoperative adjuvant chemotherapy with s-1 and follow-up; (5) clinical information including demographic characteristics, laboratory tests, surgery, chemotherapy regimen, pathological findings, and follow-up were collected.s‑1 regimen chemotherapy with s-1 started within 8 weeks after the operation.the dosage was determined based on the body surface at 40–60 mg per dose with 2 doses per day.s-1 was orally administered after breakfast and dinner for 28 days, followed by 14 days of rest.this administration of s-1 was repeated every 6 weeks for up to four cycles until the disease progressed or until patients were intolerant.factor to evaluate the efficacy of s‑1 for adjuvant chemotherapy of pdac: disease‑free survival (dfs) time dfs was defined as the time from the date of surgery to that of the first recurrence of the disease, the date last known to have no evidence of disease, or the date of the most recent follow-up with no disease.kaplan–meier analysis was used to calculate the median dfs of the patients in our study.the efficacy of s-1 for adjuvant chemotherapy of pdac was evaluated by the median dfs.t1- weighted images with spoiled gradient-echo using volumetric interpolated breath-hold examination (vibe) sequence, t2- weighted turbo spin-echo (tse) sequence and dwi (b = 0, 500 s/mm2) using single-shot spin-echo echo-planar imaging were obtained before contrast was administrated.detailed parameters of each sequence are shown in table 1.extraction of whole‑tumour radiomics features whole‑tumour segmentation whole-tumour segmentation was performed semiautomatically by a radiologist (with 11 years of experience in abdominal imaging), then checked and corrected by another radiologist (with 13 years of experience in abdominal imaging).after univariable analysis and radiomics features selection, a multivariable cox regression model (cox proportional hazards model) of survival analysis was subsequently used to select statistically significant factors associated with postoperative dfs.then, the patients were grouped by the factors selected through multivariable cox regression analysis.predictive capacities of the selected factors were also tested on the validation cohort by using the kaplan–meier method and survival curves that evaluated by the log rank test.results patient characteristics the demographic information and clinical characteristics of the patients in the study are shown in table 2.there were no differences found between the primary and validation cohorts, which enabled their use as primary and validation cohorts.the median dfs of the patients was 10.7 months in the primary cohort and 11.0 months in the validation cohort (p = 0.772).according to the results of the analyses, wholetumour radiomics feature of t1wi_ ngtdm_strength and tumour location were significantly associated with postoperative dfs (p = 0.005 and 0.013), with hazard ratios (hrs) of 0.289 and 0.293, respectively (table 6).further survival analysis with the kaplan–meier method and the log rank test showed that grouping by t1wi_ ngtdm_strength or tumour location prompted significantly different postoperative dfs between the respective subgroups in the primary cohort (p = 0.006 and 0.016).in the primary cohort, patients in the low- t1wi_ngtdm_strength group had shorter dfs (median = 5.1 m) than those in the high-t1wi_ ngtdm_strength group (median = 13.0 m) (table 7, fig. 2a).patients with pdac on the pancreatic head exhibited shorter dfs (median = 7.0 m) than patients with pdac in other tumour locations (median = 20.0 m) (table 7, fig. 2b).validation of the factors relevant to therapy efficacy of s‑1 in the validation cohort, different subgroups of t1wi_ ngtdm_strength or tumour locations also prompted different postoperative dfs, with marginally significant (p = 0.073 and 0.050), respectively.additionally, compared to gemcitabine, s-1 has the convenience of oral administration, no complications of intravenous chemotherapy, good tolerability with fewer adverse reactions and might contribute to improving patients’ quality of life [9].some studies proposed that the intratumoural expression levels of dpd and thymidylate synthase (ts) might be related to therapeutic outcomes in pdac patients receiving s-1 chemotherapy [21–23], but the conclusion remains controversial; some studies even had contradictory results [24, 25].in the validation cohort, patients with pdac on the pancreatic head also exhibited shorter dfs.in our study, s-1 used for postoperative adjuvant chemotherapy of pdac was much more effective for patients with tumours located in areas other than the pancreatic head, suggesting that the selection of s-1 for pdac located on the pancreatic head should be made more cautiously.interestingly, another study found that gemcitabine improved overall survival in a subgroup of postoperative patients with pancreatic head tumours compared to 5-fu [30, 31].we performed all the mri examinations of the same cohort with the same scanner in our study to avoid the influences of equipment and parameter differences.the results of our study could be the feasible basis of evidence and further studies for personalized precision medicine to select the right treatment for the right patient at the right time, thereby reducing the adverse reactions of ineffective treatments and improving the quality of life as well as prognosis of pdac patients.however, the differences of dfs between different subgroups in the validation cohort were marginally significant with the same survival trends as the primary cohort.
number of words= 1220
[{'rouge-1': {'f': 0.4561745623826191, 'p': 0.9333720930232559,'r': 0.3018501170960187}, 'rouge-2': {'f': 0.3410645265815768, 'p': 0.6647521865889212,'r': 0.229375}, 'rouge-l': {'f': 0.43653542822187646, 'p': 0.7876470588235294,'r': 0.3019391634980989}}]
-----------------------------------------------------------------------------------------------------------------------------------
p205:
Extractive Summary:
background medical imaging became a standard in diagnosis and medical intervention for the visual representation of the functionality of organs and tissues.through the increased availability and usage of modern medical imaging like magnetic resonance imaging (mri) or computed tomography (ct), the need for automated processing of scanned imaging data is quite strong [1].currently, the evaluation of medical images is a manual process performed by physicians.larger numbers of slices require the inspection of even more image material by doctors, especially regarding the increased usage of high-resolution medical imaging.in order to shorten the time-consuming inspection and evaluation process, an automatic pre-segmentation of abnormal features in medical images would be required.image segmentation is a popular sub-field of image processing within computer science [2–7].the aim of semantic segmentation is to identify common features in an input image by learning and then labeling each pixel in an image with a class (e.g. background, kidney or tumor).there is a wide range of algorithms to solve segmentation problems.however, state-of-the-art accuracy was accomplished by convolutional neural networks and deep learning models [7–11], which are used extensively today.furthermore, the newest convolutional neural networks are able to exploit local and global features in images [12– 14] and they can be trained to use 3d image information as well [15, 16].in recent years, medical image segmentation models with a convolutional neural network architecture have become quite powerful and achieved similar results performance-wise as radiologists [10, 17].nevertheless, these models have been standalone applications with optimized architectures, preprocessing procedures, data augmentations and metrics specific for their data set and corresponding segmentation problem [14].also, the performance of such optimized pipelines varies drastically between different medical conditions.however, even for the same medical condition, evaluation and comparisons of these models are a persistent challenge due to the variety of the size, shape, localization and distinctness of different data sets.in order to objectively compare two segmentation model architectures from the sea of oneuse standalone pipelines, each specific for a single public data set, it would be required to implement a complete custom pipeline with preprocessing, data augmentation and batch creation.frameworks for general image segmentation pipeline building can not be fully utilized.the reason for this are their missing medical image i/o interfaces, their preprocessing methods, as well as their lack of handling highly unbalanced class distributions, which is standard in medical imaging.recently developed medical image segmentation platforms, like niftynet [18], are powerful tools and an excellent first step for standardized medical image segmentation pipelines.however, they are designed more like configurable software instead of frameworks.they lack modular pipeline blocks to offer researchers the opportunity for easy customization and to help developing their own software for their specific segmentation problems.in this work, we push towards constructing an intuitive and easy-to-use framework for fast setup of state-ofthe- art convolutional neural network and deep learning models for medical image segmentation.the aim of our framework medical image segmentation with convolutional neural networks (miscnn) is to provide a complete pipeline for preprocessing, data augmentation, patch slicing and batch creation steps in order to start straightforward with training and predicting on diverse medical imaging data.instead of being fixated on one model architecture, miscnn allows not only fast switching between multiple modern convolutional neural network models, but it also provides the possibility to easily add custom model architectures.the threefold cross-validation of 120 ct scans for kidney and tumor segmentation were evaluated through several metrics: tversky loss, soft dice coefficient, class-wise dice coefficient, as well as the sum of categorical cross-entropy and soft dice coefficient.these scores were computed during the fitting itself, as well as for the prediction with the fitted model.for each cross-validation fold, the training and predictions scores are visualized in fig. 4 and sum up in table 1.the fitted model achieved a very strong performance for kidney segmentation.the kidney dice coefficient had a median around 0.9544.the tumor segmentation prediction showed a considerably high but weaker performance than the kidney with a median around 0.7912.besides the computed metrics, miscnn created segmentation visualizations for manual comparison between ground truth and prediction.as illustrated in fig. 5, the predicted semantic segmentation of kidney and tumors is highly accurate.discussion miscnn framework with excellent performing convolutional neural network and deep learning models like the u-net, the urge to move automatic medical image segmentation from the research labs into practical application in clinics is uprising.still, the landscape of standalone pipelines of top performing models, designed only for a single specific public data set, handicaps this progress.the goal of miscnn is to provide a high-level api to setup a medical image segmentation pipeline with preprocessing, data augmentation, model architecture selection and model utilization.miscnn offers a highly configurable and open-source pipeline with several interfaces for custom deep learning models, image formats or fitting metrics.the modular structure of miscnn allows a medical image segmentation novice to setup a functional pipeline for a custom data set in just a few lines of code.additionally, switchable models and an automatic evaluation functionality allow robust and unbiased comparisons between deep learning models.a universal framework for medical image segmentation, following the python philosophy of simple and intuitive modules, is an important step in contributing to practical application development.use case: kidney tumor segmentation challenge in order to show the reliability of miscnn, a pipeline was setup for kidney tumors segmentation on a ct image data set.the popular and state-of-the-art standard  u-net were used as deep learning model with up-to-date data augmentation.its predictive power was very impressive in the context of using only the standard u-net architecture with mostly default hyperparameters.in the medical perspective, through the variety in kidney tumor morphology, which is one of the reasons for the kits19 challenge, the weaker tumor results are quite reasonable [29].also, the models were trained with only 38% of the original kits19 data set due to 80 images for training and 40 for testing were randomly selected.the remaining 90 cts were excluded in order to reduce run time in the cross-validation.nevertheless, it was possible to build a powerful pipeline for kidney tumor segmentation with miscnn resulting into a model with high performance, which is directly comparable with modern, optimized, standalone pipelines [12, 13, 16, 32].we proved that with just a few lines of codes using the miscnn framework, it was possible to successfully build a powerful pipeline for medical image segmentation.additionally, fast switching the model to a more precise architecture for high resolution images, like the dense u-net model, would probably result into an even better performance [15].however, this gain would go hand in hand with an increased fitting time and higher gpu memory requirement, which was not possible with our current sharing schedule for gpu hardware.nevertheless, the possibility of swift switching between models to compare their performance on a data set is a promising step forward in the field of medical image segmentation.road map and future direction the active miscnn development is currently focused on multiple key features: adding further data i/o interfaces for the most common medical image formats like dicom, extend preprocessing and data augmentation methods, implement more efficient patch skipping techniques instead of excluding every blank patch (e.g. denoising patch skipping) and implementation of an open interface for custom preprocessing techniques for specific image types like mris.next to the planned feature implementations, the miscnn road map includes the model library extension with more state-of-the-art deep learning models for medical image segmentation.additionally, an objective comparison of the u-net model version variety is outlined to get more insights on different model performances with the same pipeline.community contributions in terms of implementations or critique are welcomed and can be included after evaluation.currently, miscnn already offers a robust pipeline for medical image segmentation, nonetheless, it will still be regularly updated and extended in the future.miscnn availability the miscnn framework can be directly installed as a python library using pip install miscnn.additionally, the source code is available in the git repository: https ://githu b.com/frank krame r-lab/miscn n. miscnn is licensed under the open-source gnu general public license version 3.the code of the cross-validation experiment for the kidney tumor segmentation challenge is available as a jupyter notebook in the official git repository.conclusions in this paper, we have introduced the open-source python library miscnn: a framework for medical image segmentation with convolutional neural networks and deep learning.the intuitive api allows fast building medical image segmentation pipelines including data i/o, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation).high configurability and multiple open interfaces allow users to fully customize the pipeline.this framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code.we proved the miscnn functionality by running an automatic cross-validation on the kidney tumor segmentation challenge 2019 ct data set resulting into a powerful predictor.
number of words= 1457
[{'rouge-1': {'f': 0.3125156274143, 'p': 0.9389320388349514,'r': 0.18745406824146982}, 'rouge-2': {'f': 0.2485452317795881, 'p': 0.6797560975609755,'r': 0.15207485226526593}, 'rouge-l': {'f': 0.4151635592048226, 'p': 0.8755555555555556,'r': 0.2720905923344948}}]
-----------------------------------------------------------------------------------------------------------------------------------
p206:
Extractive Summary:
it has received increasing attention for being a rare risk factor for ischemic stroke.because of the unclear or missed diagnosis of imaging diagnosis, clinical treatment is often not timely, so that patients can not get correct and timely treatment, thus aggravating the condition and affecting the quality of life.so its clinical and imaging definite diagnosis need further exploration.a 65-year-old male patient was admitted to tianjin huanhu hospital on october 20, 2018, due to left limb weakness accompanied by a headache and dizziness for 4 h. the computed tomography (ct) examination of the head indicated infarction of the right basal ganglia.however, mra seems to be less sensitive and specific compared with cta in the diagnosis of a carotid web.few studies explored the correlation between mra and carotid web.differential diagnosis of a carotid web a focal atherosclerotic plaque (asp) is characterized by an endometrial flap on axial cta [9], and therefore it is easily misdiagnosed as an asp [10].the typical pathological features of an asp are a lipid-rich necrotic core covered with a fibrous cap, multiple filling defects, and niches seen on the mip map of cta.the nonatherosclerotic and noninflammatory histopathological features of a carotid web are significantly different from those of an asp; a membrane-like septal shadow is seen on a cta image.most patients with a carotid web have no vascular risk factors, differentiating it from atherosclerosis.mild stressful actions (such as coughing, vomiting, exercise, or neck manipulation) do not directly cause spontaneous dissection, but they trigger spontaneous dissection in patients with the underlying arterial disease [11].spontaneous dissection is also the reason why cerebrovascular events are increasingly recognized in young and middle-aged patients.further, 19.7% of patients with fmd have artery dissection [12], mostly involving the carotid artery.the principle of preventing stroke by removing a carotid web is also obvious.a study on carotid endarterectomy in patients with a carotid web reported no recurrence of stroke [10].also, pathological tissues obtained in operation provides the conditions necessary for further studying the pathology and pathogenesis of a carotid web.some clinicians choose carotid stent placement for treating patients with ischemic stroke.in the study by haussen [13], 16 patients were treated with carotid artery placement.the number of patients in a similar study was the largest, the average follow-up time was 4 months, and no recurrence of stroke was found.further, the short-term prognosis was good [14].the present study had a small sample size.however, carotid artery stent therapy may have a great prospect with the development of neurointerventional technology and materials.conclusion although carotid web is a rare disease, careful comparison of axial thin-layer cta and mr axial t2 images, combined with cta sagittal reconstruction, can greatly improve the diagnostic rate of carotid web, thus providing useful imaging information for clinic
number of words= 450
[{'rouge-1': {'f': 0.41213763097174594, 'p': 0.6560215053763441,'r': 0.30044397463002115}, 'rouge-2': {'f': 0.23194192764027796, 'p': 0.3402702702702703,'r': 0.1759322033898305}, 'rouge-l': {'f': 0.3538069051334414, 'p': 0.5301769911504425,'r': 0.2654887218045113}}]
-----------------------------------------------------------------------------------------------------------------------------------
p207:
Extractive Summary:
since we only do the segmentation in a one-shot manner without considering temporal information in other frames.to improve the segmentation results and reduce computational complexity, we use a simple temporal filter to carry out a weighted averaging of successive frames [26].where g(n, k) is the recorded image sequence, k is the number of the sequence.n=(n1, n2) refers to the spatial coordinates.h(l) are the temporal filter coefficients used to weight 2k+1 consecutive frames.in case the frames are considered equally important we have h(l) = 1/(2k + 1).the motion artifacts can greatly be reduced by operating the filter along with the image elements that lie on the same motion trajectory [27], as shown in fig 3.results data in our work, we choose two parts of the dataset for training, one is the public drive [28] dataset which is widely used for vessel detection, and the other dataset comes from the angiography video of coronary interventional surgery.train loss function this paper uses the end-to-end learning process based on u-net.blood vessel segmentation is a pixel-level classification task.usually, the cross-entropy loss function is used for this kind of segmentation task.but, in the coronary segmentation task, the background occupies a large area of the image, and the blood vessels are unevenly distributed in the image, so dice loss [29] is selected to replace the cross-entropy loss function.if the dice coefficient is higher, the similarity between the predicted result and the ground truth is higher.also, it is more feasible to train for minimizing the loss value.the loss function is defined as: ldice = 1 − mm 2wmni p(m,i)g(m,i) ni p2( m,i) +ni g2 (m,i) (2) where m is the number of the pixel, the p(m,i) and g(m,i) are predicted probability and ground truth for classm.and m wm = 1 are the class weights, here wm = 1/m .the final loss function is defined as: lloss = ldice + lreg (3) here lreg represents the regularization loss [30].the loss curve is shown in fig. 5.implementation environment the model in this method is implemented using pytorch based on geforce gtx 1070 gpu.the system is ubuntu 16.04, and the cpu is intel i5, the ram is 16g.method results and performance in this section, we show the results and evaluate the performance of the proposed method on the tasks of coronary angiography video segmentation.figure 6 shows the segmentation result of one image.figure 7 shows the results of a video sequence.from the results we can see the detail of the coronary arteries.to evaluate the performance of the vessel segmentation, we compute the sensitivity and the accuracy, which are also calculated in[31].sen = tp tp + fn (4) acc = tp + tn tp + tn + fp + fn (5) where tp is the number of true positives, tn is the number of true negatives, fp and fn represent the number of false positives and false negatives.in addition, we also compute the iou overlap to measure segmentation performance as shown in fig. 5.table 1 shows the performance comparison with other method.what needs to be explained here is that we use not only the public data set drive [28] but also our own data set.our work achieves 0.8739 in sen, 0.9895 in acc, the average values of tp, tn, fp, and fn are 3929912.0, 115029685.0, 747732.0, and 860167 for all the test frames.from the performance of the results, the accuracy of our method is significantly improved.the performance benefit from the algorithom architecture in [23] and the enlarged dataset.discussion as shown in fig. 8, two situations can cause segmentation results to fail.first, there are some tissue structures in the video image which have close grayscale value and similar shape with the angiographic vessels, which will cause the tissue to be segmented together.the second is that when the contrast agent is very thin or uneven, the coronary blood vessels will not be segmented.
number of words= 638
[{'rouge-1': {'f': 0.38385495985076745, 'p': 0.6673451327433628,'r': 0.2694091580502216}, 'rouge-2': {'f': 0.21576797515725774, 'p': 0.33666666666666667,'r': 0.15875739644970416}, 'rouge-l': {'f': 0.3335677030672319, 'p': 0.5018181818181818,'r': 0.24981072555205047}}]
-----------------------------------------------------------------------------------------------------------------------------------
p208:
Extractive Summary:
a t1-weighted, contrast materialenhanced), fat-saturated, spoiled, volumetricinterpolated gradient-echo sequence was performed after a dynamic breast sequence.imaging parameters were as follows: tr/te, 5.6/2.6 ms; flip angle, 10°; matrix size, 298 × 352, slice thickness, 1.5mm for 1.5-t scanner, tr/ te, 4.1/1.3 ms; flip angle, 12°; matrix size, 340 × 380; slice thickness, 1.0 mm; 160 slices for 3-t scanner.the field of view was optimized to involve the bilateral axillae (levels i–iii), supraclavicular area, and inferior neck (levels iv–vii).contrast medium (0.1 mmol/kg; dotarem, guerbet, aulnay-sous-bois, france or 0.2 ml/kg; uniray, dongkook, seoul, korea) was injected at a flow rate of 1 or 2 ml/s followed by a 20-ml saline flush, using an mrcompatible power injector (spectris; medrad, pittsburgh, pa).image analysis all patients were reviewed by two dedicated breast radiologists (s.m.h., and e.y.c., with 7 and 9 years of clinical experience in breast imaging) in consensus.image interpretation was performed in two reading sessions: (1) standard mr imaging and (2) dedicated axillary mr imaging sequences.we analyzed axillary lymph nodes first using standard mr imaging –sequence and then -using dedicated axillary mr sequence.the minimum interval between the reading sessions was 1 month to reduce the possible bias caused by the likelihood of readers remembering what they had previously read.the readers were blinded to the result of other reading session, clinical and histopathologic information.the axillary lymph nodes were evaluated using standard mr imaging sequence consisting of axial t2- weighted and t1-weighted images obtained with fat saturation.if lymph node was suspected of metastasis [10, 11, 13, 18], the most suspicious node was selected and recorded for analysis.a lymph node was considered suspicious if it had one or more of the following features: (1) round or macrolobulated shape, which was determined when a node larger than 4mm was not visible as an oval structure on two contiguous images; (2) loss of fatty hilum, with fat signal intensity not seen in the node; (3) uneven cortex, when a focal increased cortical thickness was noted but not placed in the center of the node but placed on one side; or (4) lobulated margin, when the node had an irregular outer contour.using both mr imaging sequences, a confidence level scale of 0 (definitely benign) to 4 (definitely malignant) was recorded for each patient, by using the criteria as described by baltzer et al. [14].in addition, the largest dimension (ld) and the cortical thickness (ct) of the lymph node were measured and the ratio between cortical thickness and largest dimension of lymph node was calculated.when there was no suspicious lymph node in the axilla, the largest benign looking lymph node was selected for analysis.dedicated axillary imaging sequence consisting of axial, fat saturated t1-weighted image was also evaluated in the same manner.clinicohistopathologic analysis the reference standard for axillary nodal status was a combination of pathological results (n = 340) and clinical follow-up results (n = 147) at least one year.the pathological data were reviewed from samples obtained by slnb (n = 281), alnd (n = 38), core needle biopsy (n = 6) or fine needle aspiration (n = 15).lymph node status was recorded as benign, isolated tumor cells (size≤0.2 mm), micrometastases (> 0.2mm and ≤ 2.0 mm), or macrometastases (> 2.0 mm).for the purposes of this study, isolated tumor cells and micrometastases were considered as negative.in addition, the clinical indications for breast mr imaging was reviewed for each patient.statistical analysis diagnostic performance parameters, including sensitivity, specificity, positive predictive value (ppv) and negative predictive value (npv), were analyzed for standard mr imaging and dedicated axillary imaging sequences, using the confidence level scale.the confidence level scale for axillary nodal status was dichotomized, with lymph nodes scored 2 or lower considered as negative and those scored 3 and higher as positive test result.the diagnostic ability to identify patients with positive lymph nodes was also assessed based on the area under the receiver operating characteristic curve (auc), the summary measure of the accuracy with auc of 0.5, no ability to diagnose, 0.7–0.8 acceptable, 0.8–0.9, excellent and > 0.9 as outstanding performance.the diagnostic performance of both sequences was compared using the exact binomial or generalized score or mcnemar’s test and auc was compared using the delong test.a mann-whiney u test or student’s t-test was used to compare the quantitative measurements recorded (largest dimension, cortical thickness, and the ratio of cortical thickness to largest dimension) between positive and negative axillary lymph nodes.a correlation coefficient was calculated to evaluate the agreement of quantitative features (largest dimension, cortical thickness and the ratio of cortical thickness to largest dimension) between standard mr imaging and dedicated axillary mr imaging sequences.a p value of less than 0.05 was considered statistically significant.all statistical analyses were performed using spss software (version 23.0, statistical package for the social sciences, chicago, il).result patient characteristics of the 487 women who underwent breast mr imaging, 68 (14.0%, 68/487) were node-positive and 419 (86.0%, 419/487) were node-negative.the clinical indications for breast mr imaging included preoperative evaluation for known breast cancers in 354 patients, surveillance for women with a personal history of breast cancer in 106 patients, silicone implants or free injections in 14 patients, screening for high risk group with genetic mutation in 4 patients, or others in 9 patients.among 354 patients with known breast cancers, the mean tumor size was 20.5 ± 16.6mm (range, 1–103 mm).the tumors were predominantly invasive ductal carcinoma (222/354; 62.7%).diagnostic performance of standard and axillary mr imaging for axillary lymph nodes table 1 summarizes the diagnostic performance parameters of standard mr and dedicated axillary sequences for the evaluation of axillary nodal status.
number of words= 916
[{'rouge-1': {'f': 0.39164278594905727, 'p': 0.760909090909091,'r': 0.26367991845056066}, 'rouge-2': {'f': 0.22525758943301147, 'p': 0.3875182481751825,'r': 0.15877551020408165}, 'rouge-l': {'f': 0.34986966097946737, 'p': 0.5857232704402515,'r': 0.24943107221006566}}]
-----------------------------------------------------------------------------------------------------------------------------------
p209:
Extractive Summary:
though cus is non-ionizing, non-invasive, readily available and inexpensive, it is limited in attempting to differentiate rcc from aml [7].between august 2012 and january 2019, 165 patients with 172 renal masses were recruited for the study, included 145 patients (117 males and 33 females, age range 25–86 years, mean age 61.2 ± 12.4 years) with 150 rccs and 20 patients (5 males and 17 females, age range 22–75 years, mean age 55.7 ± 16.0 years) with 22 amls.initial cus was conducted to obtain the position, shape, echogenicity, size, margins, homogeneity, and orientation of the tumor.imaging interpretation and data evaluation the images and video clips saved on the local hard disk were independently reviewed in random by two radiologists (d.x.h. and z.j.), both blinded to the pathological results.both radiologists had more than 10 years of experience in urinary us and 8 years in reading ceus images.the cus characteristics included the mass position, shape, echogenicity, size, margins, homogeneity, orientation, and color flow signals.referring to the normal renal cortex adjacent to renal mass, the enhancement characteristics of renal mass were analyzed.the ceus features included the enhancement intensity at peak time, the homogeneity of enhancement, the perilesional rim-like enhancement and the “wash in” and “wash out” mode.the enhancement intensity at peak time was described into hyper-, iso-, and hypoenhancement.the homogeneity at peak enhancement was classified into homogeneous and heterogeneous.the homogeneous was defined as a renal mass with uniform enhancement, and the heterogeneous was defined as a renal mass with inconsistent enhancement.the perilesional rim-like enhancement, more distinct in the late phase of enhancement, was classified as present or absent.both the “wash in” and “wash out” of renal masses contrast enhancement were classified as fast, synchronous, or slow.if conclusions of the two radiologists were different, they consulted with a third reviewer to reach a final conclusion by discussions.statistical analysis continuous variables were expressed as mean ± standard deviation (sd), and discrete variables as numbers and percentages.the shapiro-wilk w test was used to determine whether data was normally distributed, and the f test determine whether the variables had homogeneity of variance.we determined the optimal cut-off values for the represented indices that showed the highest auc.the corresponding sensitivity, specificity, and auc were calculated with 95% cis.statistical analysis was performed by using ibm spss statistics version 22.0 for windows (ibm corp., armonk, ny, usa) and medcalc statistics version 15.2.a p value < 0.05 was considered statistically significant.results characteristics of the enrolled patients a pathologic diagnosis was obtained for all masses via a laparoscopic or open radical or partial nephrectomy.one single mass was detected in 142 patients with rccs and 16 patients with amls, and two masses were detected in the remaining 7 patients.thus, a total of 165 patients with172 renal masses were recruited, 150 (87.2%) were rccs and 22 (12.8%) were amls.cus characteristics of renal masses the diameter of rccs (mean, 40.4 ± 22.1 mm; range, 7- 130 mm) and amls (mean, 34.7 ± 22.2 cm; range, 9-80 mm) made no significant difference (p = 0.258) (table 2).ceus characteristics of renal masses ceus characteristics of renal masses are listed in table 3.all the indices (peak intensity, homogeneity, wash in, wash out, and perilesional rim-like enhancement) are significantly different between rccs and amls (p < 0.05 for all).the auc of perilesional rim-like enhancement was 0.838 (95% ci: 0.774–0.890) with 76.7% sensitivity and 90.9% specificity, while the auc of fast wash out was 0.833 (95% ci: 0.768–0.885) with 74.7% sensitivity and 81.8% specificity (table 5).aml, the most common renal benign neoplasm, contains varying proportion of thick-walled blood vessels, smooth muscle, and fat tissue [4].therefore, it is important to differentiate them for the prognostic evaluation and clinical treatment decision.to the extent known, hypoechoic renal masses are mostly considered to be malignant while hyperechoic and iso-echoic renal masses are often referred to as benign.four amls (18.2%) were hypoechoic and one (4.5%) was isoechoic on cus in this study.this may be related to the size of the tumors included in this study.in this study, 22.7% (5/22) amls showed heterogeneous enhancement (fig. 3), and all the masses were larger than 4 cm.the main limitation of our study is the relatively small number of amls (n = 22) cases, so the possibility of selection bias should be considered.
number of words= 702
[{'rouge-1': {'f': 0.3840173494770317, 'p': 0.7040425531914893,'r': 0.2640104166666667}, 'rouge-2': {'f': 0.2265207438996602, 'p': 0.3734188034188034,'r': 0.16256844850065189}, 'rouge-l': {'f': 0.39557122341798306, 'p': 0.6394444444444445,'r': 0.28635883905013193}}]
-----------------------------------------------------------------------------------------------------------------------------------
p210:
Extractive Summary:
inappropriate test selection can lead to misdiagnosis, suboptimal treatment and unnecessary costs [1].patients with a genetic disease often require special considerations for test selection to screen and manage prevalent chronic diseases [2].the test measures the three-month average plasma glucose concentration, reflecting the average normal lifespan of red blood cells.data derived from ehrs loaded into multi-institutional data warehouses provide a powerful resource for investigating these issues.these data warehouses provide a comprehensive and longitudinal collection of patient health care data and an emerging resource for health services analysis.several studies have demonstrated that ehr systems can promote cost-effective and sustainable solutions for improving quality in medical care [24].multi-institutional data warehouses aggregating ehr data from multiple sites allow national level assessment, comparison of practices and analysis of outcomes across independent, non-affiliated, organizations to guide quality improvement initiatives and identify gaps [25].one such data resource, cerner health facts™ (hf), has been demonstrated to have frequency of diagnosis codes consistent with the hcup national inpatient survey, indicating that multi-site ehr data warehouses can be representative of national trends [26].we explore the trends in a1c testing among the scd patient population in hf and evaluate whether facility characteristics affect these trends.the analysis also serves to establish a baseline that is important for assessing the effect of potential interventions to mitigate any quality gap.methods data source this study used the de-identified hf data warehouse (cerner corporation, kansas city, mo), which contains longitudinal patient data systematically extracted from the ehr at participating institutions and includes encounter data (emergency, outpatient, and inpatient), patient demographics (age, sex, and race), diagnoses and procedures, laboratory data, and facility characteristics.all admissions, inpatient medication orders and dispensing, laboratory orders, and specimens are date and time stamped, providing a temporal relationship between treatment patterns and clinical information.consistent with hf policies, all data were de-identified in compliance with the health insurance portability and accountability act (hipaa) before being provided to the investigators.the facilities contributing data were each assigned a unique identification code.longitudinal relationships between patient encounters within the same health system are preserved.we excluded patients with a diagnostic code for sickle cell trait (icd-9-cm: 282.5, icd-10-cm: d57.3).the codes were selected based on clinical judgement and the phenotype knowledgebase (phekb) [27].the resulting definition groups (from icd-9-cm codes) were combined with the appropriate icd-10-cm codes to identify the sickle cell disease patient cohort (table 1).a1c and fructosamine encounters before 2010 were excluded from the analysis because the hf data architecture was updated in 2008–2009.if the date and time stamp of the a1c test were the same as that of the fructosamine test ordered for the specific patient, that a1c test was considered to be co-ordered with fructosamine.we identified three facility groups (adherent, minor non-adherent and major non-adherent) from the study cohort based on the prevalence of the scd patients having had a1c testing ordered.a non-adherent facility has at least one patient encounter with an scd diagnosis code and at least one a1c test order for any scd patient.we stratified the non-adherant institutions by quartile and focused our investigation on the first quartile (< 25th percent adherant) (na-1) and the fourth quartile (> 75th percent non-adherant) (na-4).of the 393 facilities, 151 facilities (5039 patients) had no a1c test orders for patients classified as having scd while the remaining 242 facilities (32,112 patients) had at least one a1c test order for an scd patient.from the 151 facilities with no a1c encounters for scd patients, 77 facilities (2518 patients) were excluded from the analysis because there was no record of an a1c encounter for any patient regardless of scd diagnosis.out of 37,151 scd patients in the cohort, 89 patients had both an a1c and fructosamine test while 3838 patients had only an a1c test and no fructosamine test, 45 patients had at least one fructosamine test and no a1c test and 33,179 patients had no a1c test ordered and no fructosamine test ordered (fig. 1). of the 89 patients who had both an a1c and fructosamine test, 12% of a1c tests (63 of 533 a1c tests) from 42 patients were ordered with the fructosamine test during the same encounter.adherent, na‑1 and na‑4 facility cohorts patient level characteristics such as percentage of a1c utilization, age, sex, race, and sickle cell diagnosis groups were compared among the baseline hf cohort and the three sub-cohorts (adherent, na-1 and na-4) (table 3).evaluating the annual frequency of a1c encounters shows that the scd patients with a1c tests at the na-4 was consistently several possibilities exist for the overall low utilization of fructosamine.collectively this analysis shows that inappropriate a1c orders and underutilization of fructosamine test orders for scd patients are quality gaps.it is worth exploring the usefulness of integrating clinical decision support within ehrs to intercept a1c tests ordered for sickle cell patients and recommend more appropriate testing modalities.
number of words= 791
[{'rouge-1': {'f': 0.43210807996373013, 'p': 0.7726022304832714,'r': 0.2999270072992701}, 'rouge-2': {'f': 0.20952410365688132, 'p': 0.3274626865671642,'r': 0.1540438489646772}, 'rouge-l': {'f': 0.3597488810870206, 'p': 0.566551724137931,'r': 0.2635483870967742}}]
-----------------------------------------------------------------------------------------------------------------------------------
p211:
Extractive Summary:
examination of the performance of bert for the automated detection of actionable reports • investigation of the difference in detection performance upon adding order information to the input data methods task description this study was approved by the institutional review board in our hospital, and was conducted in accordance with the declaration of helsinki.we define two collective terms: (1) “report body,1” referring to the findings and impression in radiology reports, and (2) “order information,” referring to the free texts that are written in the ordering form by the referring clinician (e.g., the suspected diseases or indications), as explained in introduction.our task is thus defined as the detection of radiology reports with actionable tags using the report body alone, or both the order information and the report body.clinical data we obtained 93,215 confirmed radiology reports for computed tomography (ct) examinations performed at our hospital between september 9, 2019, and april 30, 2021, all of which were written in japanese.next, we removed the following radiology reports that were not applicable for this study: (1) eight radiology reports whose findings and impressions were both registered as empty, (2) 254 reports for ct-guided biopsies, and (3) 2030 reports for ct scans for radiation therapy planning.the remaining 90,923 radiology reports corresponded to 18,388 brain, head, and neck; 64,522 body; 522 cardiac; and 5673 musculoskeletal reports; and 3209 reports of other ct examinations whose body parts could not be determined from the information stored in the radiology information system (ris) server.the total was greater than the number of reports because some reports mentioned more than one part.class labeling and data split each of the 90,923 radiology reports was defined as actionable (positive class) if it had been provided with an actionable tag by the diagnosing radiologist, and it was otherwise defined as non-actionable (negative class).in other words, the gold standard had already been given to all of the reports in the clinical practice, which enabled a fully supervised document classification without additional annotations.the radiologists in our hospital are requested to regard image findings as actionable when the findings were not supposed to be expected by the referring clinician and were potentially critical if left overlooked.specific criteria for actionable tagging were not determined clearly in advance but left to clinical decisions of individual radiologists.the numbers of actionable and non-actionable reports were 788 (0.87%) and 90,135 (99.13%), respectively.then, these radiology reports were split randomly into a training set and a test set in the ratio of 7:3, maintaining the same proportions of actionable and non-actionable reports in each set, i.e., in the training set, there were 63,646 reports, where 552 were actionable and 63,094 were non-actionable, and in the test set, there were 27,277 reports, where 236 were actionable and 27,041 were non-actionable.preprocessing of radiology reports to apply machine learning methods in the following sections, the same preprocessing was carried out on all radiology reports (fig. 1).first, the contents in the order information and report body were respectively concatenated into passages.then, the passages were individually tokenized with the sentencepiece model, whose vocabulary size is 32,000 [33, 34].bert bert is one of the transformer networks [27, 28].in general, “transformer” refers to neural networks using multiple identical encoder or decoder layers with an attention mechanism [35].transformer networks have outperformed previous convolutional and recurrent neural networks in nlp tasks [27].bert has been proposed as a versatile transformer network.bert takes one or two documents as input, passes them into the inner stack of multiple transformer encoder layers, and characteristically outputs both document-level and tokenlevel representations.bert can thus be applied to both document-level and token-level classification tasks [28].various bert models pre-trained with large corpora are publicly available, which has established a new ecosystem for pre-training and fine-tuning of nlp models.we used the japanese bert model developed by kikuta [34].this model is equivalent to “bert-base” with 12 transformer encoder layers and 768-dimensional hidden states.the model has been pre-trained using a japanese wikipedia corpus tokenized with the sentence- piece tokenizer [33].we constructed a binary classifier (hereafter, a bert classifier) by adding a single-layer perceptron with softmax activation after the pre-trained bert model.the perceptron converts a 768-dimensional document-level representation vector output by the pre-trained bert model into a two-dimensional vector.the procedure is shown in fig. 2.for the detection experiment without order information, the sequences generated from the report body were fed to the bert classifier.for the detection experiment with order information, each sequence pair generated from the order information and report body was fed to the bert classifier.fine-tuning was performed on all embedding and transformer encoder layers of the bert model, and none of these layers were frozen.the maximum sequence length was set to 512 and the batch size2 was set to 256.we used adam optimizer [36] and binary cross-entropy loss function.as in table 2, the learning rate and the number of training epochs were set as follows.the learning rate was set to 5.0 × 10− 5 for the experiment without order information and to 4.0 × 10− 5 for the experiment with order information.the number of training epochs was set to 3 for both experiments.the learning rate and the number of training epochs were determined by the grid search and five-fold cross-validation using the training set.we tried all of the 25 direct groups of five learning rates, 1.0 × 10− 5, 2.0 × 10− 5, 3.0 × 10− 5, 4.0 × 10− 5, and 5.0 × 10− 5, and the five training epochs, 1 to 5.we calculated the averages of the area under the precision-recall curve (auprc) [37, 38] for the five folds, and chose the learning rate and the number of training epochs that gave the highest average auprc.the learning environment was as follows: amd epyc 7742 64-core processor, 2.0 tb memory, ubuntu 20.04.2 lts, nvidia a100-sxm4 graphics processing unit (gpu) with 40 gb memory × 6, python 3.8.10, pytorch 1.8.1, torchtext 0.6.0, allennlp 2.5.0, pytorch-lightning 0.7.6, scikit-learn 0.22.2.post1, transformers 4.6.1, tokenizers 0.10.3, sentencepiece 0.1.95, mlflow 1.17.0, and hydra 0.11.3.baselines: lstm as one of the baselines against bert, we performed automated detections of actionable reports using a twolayer bidirectional long short-term memory (lstm) model followed by a self-attention layer [27, 39].as in bert, the inputs to the lstm model were report bodies in the experiments without order information and were concatenations of order information and report bodies in the experiments with order information.the lengths of the input documents in a batch were aligned to the longest one by adding special padding tokens at the end of the other documents in the same batch.next, each document was tokenized and converted into sequences of vocabulary ids using the sentencepiece tokenizer, and was then passed into a 768-dimensional embedding layer.in short, the preprocessing converted radiology reports in a batch into a batch size × length × 768 tensor.the final layer of the lstm model outputs two batch size × length × 768 tensors corresponding to the forward and backward hidden states.we obtained documentlevel representations by concatenating the two hidden states.the representations were further passed into a single-head self-attention layer with the same architecture as proposed by vaswani et al. [27].the self-attention layer converts the document-level representations to a batch size × 1536 matrix by taking the weighted sum of the document-level representations along the time dimension effectively by considering the importance of each token.then, the matrix was converted into twodimensional vectors using a single-layer perceptron with softmax activation.the resulting two-dimensional vectors were used as prediction scores.hereafter, we collectively refer to the lstm model, the self-attention layer, and the perceptron as the “lstm classifier.” we trained the lstm classifier from scratch.the same optimizer and loss function as those in bert were used.the batch size was set to 256.as in bert, the learning rate and the number of training epochs were determined by grid search and five-fold cross-validation.table 2 shows the hyperparameter candidates on which the grid search was performed and the hyperparameters that were finally chosen for each experiment.baselines: statistical machine learning logistic regression (lr) [40] and the gradient boosting decision tree (gbdt) [41] were also examined for comparison.figure 3 shows the procedures.the tokenized report body and order information were individually converted into term frequency-inverse document frequency (tfidf)- transformed count vectors of uni-, bi-, and trigrams (one, two, and three consecutive subwords).the two vectors were concatenated for the detection experiment with order information, and only the vector from the report body was used for the detection experiment without order information.
number of words= 1392
[{'rouge-1': {'f': 0.3340588992937134, 'p': 0.7513880126182966,'r': 0.2147721179624665}, 'rouge-2': {'f': 0.1681308752796012, 'p': 0.2946835443037975,'r': 0.11761904761904762}, 'rouge-l': {'f': 0.3469995790224576, 'p': 0.5960416666666666,'r': 0.24474048442906576}}]
-----------------------------------------------------------------------------------------------------------------------------------
p212:
Extractive Summary:
as part of making decisions about the prevention or management of a health condition, patients need to know about more than just treatment options; they also need to know about the prognosis of the condition, with and without treatment.communication of prognostic information is essential as it helps patients to know what may happen to their health over time, to make appropriate preparations, and to make informed decisions about whether to intervene and if so, how.clinicians sometimes try to avoid or delay this kind of communication, while patients often wait for their clinicians to initiate the process [8].as with the communication of treatment information, a contributor to the challenge of communicating prognostic information well is the difficulty that clinicians and patients can have understanding relevant quantitative information [9–11].methods the protocol of this systematic review is registered at (crd42020192564) and can be found in the open science framework osf.io/ze26g.objective this systematic review aimed to identify and synthesize research that has evaluated visual presentations that communicate quantitative prognostic information to patients or the public.information sources we searched for studies in six databases: medline, embase, cinahl, eric, psycinfo, and the cochrane central register of controlled trials (central), each from date of inception till december 2020.we used a tailored search strategy for each database (see additional file 1).the only participation restriction was the exclusion of health professionals or health professional students.for this reason, only some of the questions asked were eligible.secondary outcomes included: preferences for any of the presentations evaluated; satisfaction with the presentation; and behavioral intentions relevant to the information presented (e.g. intention to be screened).intervention details (including type of presentation (e.g., bar graph), the presented information, who delivered the information, how, where and when the information was delivered), outcome details (including the eligible outcomes, how they were measured and at what timepoints) and result details (including number of responses analysed, follow up rate, results of eligible studies) are tabulated in the additional file 1 and show the data extracted.studies that compared alternative statistical formats (e.g. relative risk reduction vs. absolute risk reduction) and studies that compared the framing (i.e. positive or negative) of health information were excluded as they have been previously synthesized [13, 14].also excluded were studies that only compared methods of wording free text for conveying prognostic information.the search strategy (see additional file 1) was slightly modified by adding more mesh terms (e.g., comprehension, knowledge, data display, communication, perception, “decision making, shared”) at the request of reviewers during the peer-review process.after the full-text screening, we identified 9 articles: in 2 of these, 2 separate studies were reported, resulting in 11 eligible studies [15–23] (fig. 1).characteristics of included studies all 11 included studies were randomized trials.details of the included studies are presented in the table of characteristics (see additional file 1).all included articles were judged to have “some concerns” for at least one domain of risk of bias.(fig. 2a, b).(fig. 3).there was no statistically significant difference in comprehension between these types of graphs [17, 18, 21].the question about prognosis was answered correctly by 91% of those who viewed the pictograph, compared to 64% who viewed the bar graph [18].a group of participants in the same study who were shown both survival and mortality curves performed slightly better than the group who only saw mortality curves, but the difference was not significant [15].tabular format versus text (3 studies) three studies compared a tabular format to a free text format (fig. 4d).two studies (reported in the same article) found no significant difference between using a tabular format and free text to communicate prostate cancer prognosis to male participants and prognosis of breast cancer to female participants.one of these found that pictographs with information about the outcome of two treatment alternatives, compared to those with four, were significantly better understood [21].the pictograph was rated the lowest on both preference and expected understanding, regardless of the format they were randomized to.overall, there was no statistically significant difference between graph preference and comprehension in this study [17].two studies on the communication of prostate cancer prognosis screening outcomes found no association between the type of presentation and the intention to be screened for prostate cancer at any measurement time point in two studies (one conducted online, one conducted face to face) [20].in the few studies that examined this, simpler formats (such as one outcome instead of multiple, and fewer intervention options presented at one time) were generally better understood and achieved higher levels of satisfaction.the impact of various types of visual presentations on behavioral intention is inconsistent.while there are similarities between the communication of treatment benefits and harms and the communication of prognosis information, the extent to which methods identified as superior for communicating treatment quantitative information are also suitable for facilitating the comprehension of prognosis information is unknown.similar to the findings of a review of methods of communicating quantitative treatment information [9], we found little difference between bar graphs and pictographs in facilitating comprehension and that there is no superior single method for conveying quantitative information.research on the comprehension of information from a survival curve with different variations found that comprehension was generally good across each variation [32].
number of words= 851
[{'rouge-1': {'f': 0.4222354316227585, 'p': 0.7965917602996255,'r': 0.28724524076147817}, 'rouge-2': {'f': 0.22322655486242918, 'p': 0.3707518796992481,'r': 0.1596860986547085}, 'rouge-l': {'f': 0.45969178530097793, 'p': 0.7432026143790851,'r': 0.3327551020408163}}]
-----------------------------------------------------------------------------------------------------------------------------------
p213:
Extractive Summary:
each term is the term of one concept.each definition is the definition of one concept.each semantic type is the semantic type of one concept.term source selection and term extraction a bilingual terminology system towards a worldwide emergency disease was supposed to be correct, authoritative and highly correlated to the theme, where exact bilingual concepts, semantic types, etc. should be demonstrated.therefore, the information resources we took were limited to authority publishment from the situation report or document of world health organization, journal articles such as preprint, open access, etc., nationwide regulation, policy document, professional textbooks, etc.bilingual terms were mostly extracted from bilingual who documents, textbooks, and related papers.definitions were located from textbooks and related papers under most conditions.hierarchical structure construction we adopted top-down and bottom-up synthesis approach to formulate the final hierarchical structures.on the one hand, related clinical classification architectures were identified and reused in associated documentation, literatures, textbooks (e.g. textbooks in epidemiology, virology, and preclinical medicine), etc.on the other hand, measures were taken for terms extracted from literatures and other resources, i.e. synthesis from bottom up.agile model was adopted during the procedure, i.e. adjusting the structure by adding, altering or deleting specific substructures when necessary.the finalized hierarchical structure (fig. 2) was reviewed and assessed by one expert on clinical medicine and one two professionals on medical informatics.relationship and property development relationship and property of each concept were developed in this step.each concept was assigned with properties including concept id, term id, bilingual semantic type, chinese preferred term, and english preferred term as the obligatory items, and chinese synonym, english synonym, bilingual definition, definition source as alternative items.we combined terms with same meanings as one concept with different synonyms; integrated terms within the same classification as one subset with various concepts.the editing date and time were automatically generated in the system.among these properties, concept id and termid could be directly linked to other systems through automatic mapping; each definition was required with a source for users to look up to.synonyms were not a prerequisite element but more synonyms would help with the search scope and term location.quality control to guarantee the correctness of the terminology, we performed quality control after each round editing and before each version update.after editing in each round, two examiners with professional background and related practice experience were invited to validate the accuracy of the terminology.a third party with clinical experts would be involved when disagreement was reached.before each releasing round, we performed quality control via cross assessment, i.e. automatic checking and expert review.the former was responsible for repeated terms detection (i.e. repeated terms with different concept identification), language detection (i.e. english terms marked as chinese or vice versa), abnormal character detection (i.e. term that cannot be read by machine), closed hierarchical relationship detection (e.g. whether there is circularity in a hierarchical tree), hierarchical depth detection (whether a term is too deep for users to browse), spelling detection (whether there is questionable spelling).the expert review covered classification checking (whether the classification is appropriate from professional perspective and whether a term is suitable under a specific category) and content checking (whether the definitions or synonyms of a certain term is correct and related).based on the positive feedback of quality control, we updated and released the terminology online.web service we built a website for covid term, making it available for users to access each updated data version.for each update round, enriched sub branches with abundant information were required, where up to date covid resources e.g. the lancet coronavirus theme, nih 2019 novel coronavirus theme, who covid-19 theme, the new england journal of medicine covid-19 theme [37–40], etc. were constantly followed by covid team to provide most recent terminology.the terminology towards covid-19 was named as covid term.earlier versions were also released on the phda (population health data archive) [41].results covid term overview we constructed a 6-level bilingual terminology system for covid-19 involving 464 concepts, 724 chinese terms, 887 english terms, 464 chinese preferred terms, 464 english preferred terms, 260 non-preferred chinese terms, 423 non-preferred english terms, 42 chinese definitions, and 5 english definitions.the first-level category was designed as disease, living organism, clinical manifestation, diagnosis and treatment technique, qualifiers, demographic and socioeconomic characteristics, epidemic prevention and control, medical equipment, instruments and materials, psychological assistance, and anatomic site.detailed term statistics were shown in table 1.concept distribution in this 6-level structure, all first-level terms were root nodes with leafs.there were 10 first-level terms, same as first-level categories, 104 second-level terms, 180 third-level terms, 138 fourth-level terms, 31 fifth-level terms, and 9 sixth-level terms.sixth-level leaf nodes only existed in category ‘disease’, ‘living organism’, and ‘anatomic site’.one example was ‘living organism—pathogen— virus—coronavirus—human coronaviruses—severe acute respiratory syndrome coronavirus 2’.concept distribution in different categories was calculated.concepts in ‘clinical manifestation’, ‘diagnosis and treatment technique’, ‘epidemic prevention and control’ ranked top 3, which accounted for over half concepts, with proportion respectively 20%, 18%, 14%.concept distribution was shown in fig. 3.terminology property analysis we performed data statistics on each property of every category including concept, bilingual term count, bilingual synonym count, and bilingual definition count in descending order of concept.concepts in category ‘clinical manifestation’ ranked first with 94 concepts, together with 99 chinese synonyms and 125 english synonyms.most english terms took a major proportion in all properties among different categories.
number of words= 870
[{'rouge-1': {'f': 0.3686205743278102, 'p': 0.6831386861313868,'r': 0.25241042345276876}, 'rouge-2': {'f': 0.1596648615510371, 'p': 0.23849816849816852,'r': 0.12000000000000001}, 'rouge-l': {'f': 0.3289953849023854, 'p': 0.47512820512820514,'r': 0.25160919540229887}}]
-----------------------------------------------------------------------------------------------------------------------------------
p214:
Extractive Summary:
the term “readmission” refers to the readmission of a patient to the same department within a certain period due to the same disease after discharge.an accidental readmission is caused by many reasons, including improper initial diagnosis, relapse, premature discharge, and others [1, 2].the 30-days readmission rate after an index hospitalization has become an important hospital performance measure used by the centers for medicare and medicaid services and is receiving increased scrutiny as a marker of poor patient care [3, 4].in 2014, a record-breaking fine was issued to 2610 hospitals by the centers for medicare & medicaid services because too many patients had been readmitted to the hospital within a short period of time.accidental readmission not only increases patient financial burden but also leads to a repeated waste of medical resources.in 2017, the estimated cost of diagnosed diabetes in the united states was about $327 billion, of which $237 billion was direct medical costs [5–8].rowley et al. [9–11] recently estimated that the prevalence would increase by 54% between 2015 and 2030, reaching a total cost of over $622 billion.nevertheless, compared with the overall 30-days readmission rate of inpatients [12, 13], those who were diagnosed with dm have a much higher readmission rate (14.4–22.7%).in addition, based on agency for healthcare research and quality nationwide inpatient sample data from 2012, if a modest 5% reduction can be achieved, there would be far fewer admissions per year at an estimated annual cost savings of $1.2 billion [14, 15].undoubtedly, readmission plays an essential role in the increasing hospital-related costs and is becoming more common among elderly dm patients; as a result, dm readmissions become a growing and costly economic burden on both patients and public finance budgets, thus deserving our intensive attentions.in clinical settings, real-world data from electronic health records (ehrs) can have more potential value than recording disease purely.in addition to effective predictive models, identifying features associated with risk factors related to readmission in medical records will enable more careful and effective treatments in future.to manage this concerning issue, practical models that can precisely predict the possibility of individual readmission are critical.therefore, this study analyzed the medical records of 100,244 diabetes patients in the us health facts medical database from 1999 to 2008.a total of 55 related attributes were included, such as admission times, sex, age, admission type, length of hospital stay, number of laboratory tests, glycosylated hemoglobin results, diagnosis, and medication.the dataset consisted of clinical records of diabetic inpatients with a length of 1–14 days hospital stay, and laboratory tests as well as medications used during hospitalization [16].characteristics of the diabetes dataset the patients’ general demographic data, such as sex, age, and race as well as the clinical records of drug use, clinical operations, admission times, and others were analyzed as shown below.nearly half (46.15%) of the total patients were male, while the majority of patients (76.49%) were white americans (fig. 1).the records of male patients were marked as green, whereas those of female patients were marked as red.the histogram of the age distribution with different length of stay was demonstrated (fig. 4).figure 5 showed a statistical box chart of hospitalization time distribution of different races, demonstrating a mean 4-days hospitalization time of white and black americans in the united states, which was slightly longer than the hospitalization time of patients of other races in the dataset.further data verification and cleaning was conducted in cooperation with other data of these patients in the subsequent analysis.to create a 30-days readmission prediction model, the binary classification model was constructed to identify high-risk patients who were readmitted < 30 days after discharge (including no readmission) and > 30-days admission records, which revealed that the proportion of the two outcomes in the dataset were very unbalanced.the area under the operating characteristic curve (auc) value, which was used to evaluate the merits of the binary classification algorithm, was adopted as a main criterion to judge the performance of the prediction model in this study.some patterns could be found from the order of importance of characteristics in patients with readmission.moreover, the elderly patients had more emergency department visits due to the sudden deterioration of their condition.in summary, medical staff must provide health education and follow-up for diabetic patients with repeated admissions, especially elderly patients, to prevent the occurrence of complications and choose appropriate treatments for patients at different ages.in a readmission risk model for patients hospitalized with cirrhosis in 2020, the auc was 0.670 compared to existing models (0.649, 0.566, 0.577), similar to the predictive ability of the model in this study [21].more admission times was the main component of patients who had more readmissions, primarily elderly patients with more serious conditions, and the length of hospitalizations were much longer than that of general patients.besides, the frequent diagnoses indicated that these patients had a higher probability of developing diabetes-related complications.so many known and unknown risk factors in medical activities can affect readmissions, and model performances will be greatly improved through the analysis of realworld data and the data-driven mining of potential risk factors affecting patient readmission rates.rf was more suitable for predicting accidental readmissions in this study.as one of the most commonly used algorithms in current classification work, rf has better predictive performance and can give variable importance measures during classification.this study adopted the algorithm encapsulated in the knime tool, which is relatively mature and can be convenient for clinical practitioners who are not capable of algorithm programming but want to be able to perform analyses themselves.however, there is little scope for self-modification of these algorithm parameters.nevertheless, some measures such as the gini index and out-of-bag data error rate for calculating the feature importance score were not considered when integrating the algorithm functions to facilitate personalized use.as an alternative, the tree ensemble learner node was adopted instead of the rf algorithm to determine the importance score, which was also a limitation to this study.moreover, our study did not exclude the planned readmissions after discharge, and researchers can adjust the training dataset as needed to predict unplanned returns.prediction accuracy may be further improved if clinical data related to patient hospitalizations can be extended to larger sample sizes with more included features.conclusion in conclusion, ml could help healthcare providers to identify those patients who are prone to short-term readmission and might reduce the probability of readmission within 30 days by altering the risk factor
number of words= 1053
[{'rouge-1': {'f': 0.41917685079416106, 'p': 0.8043750000000001,'r': 0.2834423251589464}, 'rouge-2': {'f': 0.20487047914175127, 'p': 0.3364576802507837,'r': 0.14727272727272728}, 'rouge-l': {'f': 0.3754934632151557, 'p': 0.6328415300546448,'r': 0.26694072657743784}}]
-----------------------------------------------------------------------------------------------------------------------------------
p215:
Extractive Summary:
kidney transplantation is currently the optimal kidney replacement therapy for the treatment of patients with advanced kidney disease compared to haemodialysis and peritoneal dialysis [1].in 2018, the annual number of kidney transplant recipients in china was 13,029 [2].after kidney transplantation, patients should take medication for their whole life, nevertheless, complex medication regimens [3], medication-related side effects [4], and the lack of professional self-care knowledge and guidance in these patients [5], could lead multi-complications [6] and increase readmission rates [5], furthermore, impair the quality of life (qol) [7].previous studies showed that transitional care could reduce the complication rate and readmission rate and improve the quality of life in kidney transplant [8, 9].high-quality indexes for effectiveness evaluation could promote quality of nursing care because it was not only to assess the quality of transitional care but also to identify the insufficiencies in existing transitional care.however, at present, there are no standard evaluation indexes and debatable scientific of existing indexes in chronic disease transitional care in china [10].some researchers have explored the construction of a standardized evaluation index system in people with stroke [11], coronary heart disease [12] and other diseases, and some researchers have also explored the construction of a transitional care evaluation index system for the entire chronic disease population [13].however, in the field of kidney transplant recipients, a standardized and systematic index system for evaluating the effectiveness of transitional care has not been developed, and the index system for evaluating the chronic disease population is not disease-specific and lacks evidence-based studies.therefore, present study used the omaha system as the theoretical framework and delphi method combined with analytic hierarchy process (ahp) to construct an index system for evaluating the effectiveness of transitional care in kidney transplant recipients.methods objective the objective of this study is to provide a reference for scientific and effective evaluation of transitional care in kidney transplant receipeints and promote the improvement of nursing quality.description of a research group there were six team members in the research group, including two graduate nursing students, one associate professor of kidney transplant nursing research, one director of the kidney transplant unit, one head nurse of the kidney transplant unit, and one expert related to scale statistics.the main tasks of the research team included: review of relevant literature, preliminary construction of the evaluation index entry pool, selection of experts, preparation and distribution of expert correspondence forms, and collation and analysis of expert opinions.inclusion criteria of correspondence experts experts were selected by snowball sampling method in china.inclusion criteria for experts were: (1) nursing experts: intermediate or above title, who have conducted research related to kidney transplantation and/or transitional care; 10 years or more in the field of nursing clinical/management/psychology/education; bachelor’s degree or above; (2) clinical medicine experts: above intermediate title, 10 years or more in kidney transplantation clinical work; master’s degree or above; (3) all included experts were interested in supporting the study and could guarantee sustained attendance for the duration of the present study.development of evaluation indicators system an initial effectiveness evaluation index system consisting of 5 primary indicators, 19 secondary indicators, and 61 tertiary indicators was developed through literature review and patient interviews.systematic literature reviews the keywords "renal transplant*/kidney transplant*/ renal transplant recipients/kidney transplant recipients, transitional care/continuity of care/continuing nursing/ patient discharge planning/discharge planning/telemedicine/ comprehensive care/multidisciplinary care" were searched in electronics database.based on the systematic literature review, the omaha system was used as the theoretical framework, and 42 questions in four major domains were referred to the omaha problem classification system, which were combined with the effectiveness evaluation indexes and disease characteristics in the literature to form a preliminary pool of entries for evaluating the effectiveness of transitional care for kidney transplant recipients.patient interviews a comprehensive understanding of kidney transplant recipients’ experiences of transitional care and the indicators of transitional care effectiveness that transplant recipients consider important were obtained through interviews with transplant recipients, and the views of transplant recipients were considered in the construction of the indexes of transitional care effectiveness.patients for this interview had to be 18 years or older.all patients provided their written informed consent in accordance with the helsinki declaration.eleven selected patients were interviewed after their clinic follow-up.this indicator was retained and renamed to "social function" in accordance with the experts’ opinion.the weighting coefficient of satisfaction was 0.06, indicating that experts considered satisfaction to be less important than other level 1 indicators, which may be related to the subjective nature of satisfaction.one expert stated that patient experience should be distinguished from quality of care and that the level of care should not be too dependent on the subjective feelings of patients.in addition, present study was constructed as a gross indicator, and the appropriate indicators or weights should be adjusted for transplant recipients of different ages.for example, elderly transplant recipients have a higher incidence of post-transplant diabetes mellitus [35], lower medication adherence [36], and greater susceptibility to infection [37] compared to young and middle-aged transplant recipients, so the weight of indicator "blood glucose" may need to be higher for elderly transplant recipients, and the weight of indicators related to medical resource utilization, family and social support, and medication adherence may be higher.also, indicators related to the assessment of frailty and cognitive impairment may be increased.based on the present study, the weighting of the indicators may be increased or decreased for different age groups in order to better match the clinical application.limitations expert consultation has the limitation of population selection, and this study has tried to select representative experts as much as possible, but it is undeniable that a different group of experts may get different results.this study can only represent the importance of the selected experts for the indicators considered.conclusion this study used the omaha system as the theoretical framework, combined with literature review, semistructured interviews, expert consultation and hierarchical analysis to construct a scientific and systematic index system for evaluating the effectiveness of transitional care for kidney transplant recipients.this index system also provides the evidence for nursing staff to determine the priority of transitional care when making decision.in future, a preliminary clinical experiment is going to be conducted to examine the effects of this index system on transitional care in kidney transplant recipient
number of words= 1025
[{'rouge-1': {'f': 0.3508166725461259, 'p': 0.8610447761194029,'r': 0.2202835538752363}, 'rouge-2': {'f': 0.24393578025488577, 'p': 0.535,'r': 0.1579848628192999}, 'rouge-l': {'f': 0.3382391006587591, 'p': 0.6736036036036037,'r': 0.2258139534883721}}]
-----------------------------------------------------------------------------------------------------------------------------------
p216:
Extractive Summary:
medical diagnoses and procedures are reported using standardized codes that are updated periodically to keep up with the latest clinical knowledge and practices.transitioning from an old medical coding system to a new one can be challenging, especially when the two systems are significantly different.one such transition took place in the united states (us) in 2015 when the country switched from the 9th revision of the international classification of diseases (icd) clinical modification (icd- 9-cm) to the 10th revision (icd-10-cm).this newer revision was accompanied by a very different procedure coding system (pcs) (icd-10-pcs), as compared to the icd-9-cm procedure coding system (volume 3, abbreviated here as vol.3).for example, each icd-10-pcs procedure is made of 7 multi-axial characters where each axis encompasses up to 34 alphanumeric values [1].this arrangement is a significant departure from the procedure code structure in icd-9-cm vol.3, where all codes are numeric and can only be between 2 and 4 characters long.in 2015, icd-10-pcs had about 72,000 procedure codes as compared to only about 4000 codes in icd- 9-cm vol.3. the diagnosis codes between these two revisions of icd are also quite different.for example, all diagnosis codes in icd-10-cm are alphanumeric and can be 3 to 7 characters long, whereas icd-9-cm diagnosis codes are mostly numeric and can only be between 3 and 5 characters long.in 2015, there were about 14,500 diagnosis codes in icd-9-cm as compared to about 69,800 codes in icd-10-cm [2].given these differences, some analysts had predicted a costly and challenging transition from icd-9-cm to icd-10-cm/pcs [3].indeed, some of the feared problems did materialize after the changeover, such as the loss in productivity [4, 5], the lack of readiness of computer systems, the inability to find some icd-9-cm concepts in the icd-10-cm system, and difficulties mapping icd-10-cm to other coding systems such as snomed-ct [6].some icd-10-cm clinical classes were also found to have more coding deficiencies than others, such as the class of external causes of morbidity (v00-y99) [7].in one post-icd-10 implementation audit, it was found that one of the most significant challenges for coders was selecting the correct character in the 3rd position (root operation), the 4th position (body part), and the 5th position (approach) of an icd-10-pcs code [8].while little evidence exists to suggest that reimbursement was significantly impacted by the transition, in some practices, a statistical increase in the codingrelated denials was noted [9].a few of the post-transition qualitative studies concluded that training and education were critical in overcoming many of the previously anticipated challenges [6, 10].besides the us, other countries have also faced challenges while transitioning to new medical coding systems.the issues ranged from coding errors to discrepancy problems when the same condition was coded in both coding systems.for example, in one analysis [11], it was found that the swiss transition from icd-9 to icd-10 resulted in the initial increase of the number of coding errors for co-morbidities, but, over time, the accuracy improved as the learning curve waned.while the authors did not find much difference in the validity of the codes from these two systems, the discrepancy was apparent for some conditions (e.g., hiv/aids, hypothyroidism, and dementia).the authors also observed that the quality of data had not yet improved in icd-10 as originally expected.now that many countries are preparing to migrate from icd-10 to icd-11 [13], one can expect similar transition challenges to occur, as these two coding systems have different code structures [14], and the equivalence is at times lacking [15].this research aims to introduce entropic measures to help users prepare for the migration to a new medical coding system by identifying and focusing preparation initiatives on clinical concepts with more likelihood of documentation deficiencies, coding errors, and longitudinal data comparison issues.related work not many studies have considered how to quantify the complexity of codes between two medical coding systems.in some studies, the equivalence in the number and structure of the codes between two coding systems is considered, but without accompanying measures of the dissimilarity in the codes [15].in a few studies, an attempt is made to address the complexity between two medical coding systems.for example, in boyd et al. [16, 17], the authors proposed using the science of networks to evaluate the difficulties of transitioning from icd- 9-cm to icd-10-cm in the us.as the number of candidate codes m increases, h(m) increases as expected, which should also increase h(a).while such a mutual increase in both h(a) and h(m) occurs in most maps, a few maps exhibit more variation in the codes’ alphabets relative to the corresponding number of candidate codes or vice versa.an example here is map 721 (low forceps operation with episiotomy) where h(a) = 6, but h(m) = 0.5 since there are only two candidate codes.in this map, h(b)– the entropy of the valid combinations (v) of m candidate codes–is zero since v = 1 .in map 7392 (replacement of prolapsed umbilical cord), the opposite divergence exists.there are more codes ( m = 3 ) relative to the corresponding variation in the alphabets.as a result, h(m) = 1.59 whereas h(a) = 0.92.besides the disparity between h(m) and h(a), h(b) and h(m)–two entropic methods that mostly agree–may also significantly diverge when there is a significant difference in the number of candidate codes m and the number of valid combinations v. examples include map 0050 (implantation of cardiac resynchronization pacemaker without mention of defibrillation, total system [crt-p]) where v = 216 but m = 16 and map 688 (pelvic evisceration) where v = 2 but m = 16 . regardless of the source, a divergence in the entropic measures’ rankings complicates implementing the proposed methods in actual settings.unless one method proved superior to others, clinical concepts or classes where the rankings of entropic measures significantly disagree should be audited by medical providers and coding professionals.subsequently, training efforts for clinical documentation and medical coding should be adjusted as appropriate.given this recommendation, during the transition from icd-9-cm to icd-10-cm/pcs, audits of clinical classes 0, b, and f (in fig. 2b) and o00–o9a and st00–t88 (in fig. 2d) would have been necessary to ascertain any transition challenges and training needs.besides ranking maps or clinical classes by their entropic measures, the user may also prioritize transition efforts from outlier and pattern analysis.that is, instead of working with predefined clinical classes, the user would try to assess the impact of the transition using major themes or ontological groups from the descriptions of outlier maps.many thematic analysis [24] and ontological learning methods [25, 26] are applicable here.for demonstration purposes, a simple graph was constructed and patterns were examined using network algorithms [21, 27] (see fig. 4).for example, a close examination of fig. 4c, d reveal a collection of terms that relate to the vascular, skeletal, integumentary, and cardiac body systems.in terms of the eigenvector centrality, the most central words were tissue, graft, subcutaneous, skin, repair, and incision.combining these keywords, one may conclude that the procedures for the musculoskeletal, integumentary, and cardio-vascular systems likely involved significant complex coding in icd-10-pcs, a deduction that is consistent with the results in fig. 2a.conclusion transitioning from an old medical coding system to a new one can be challenging, especially when the two coding systems are significantly different.this research aimed to propose methods that could help users prepare for the transition by identifying and focusing preparation initiatives on clinical concepts with more likelihood of transition challenges.to this end, two entropic measures of coding complexity were introduced.the first measure was a function of the variation in the map’s alphabets, and the second measure was based on the possible number of valid combinations of candidate codes in a map.the primary assumption was that the more entropy, the more likelihood of coding errors.so, more prudent documentation was recommended for clinical concepts with high rankings of entropic measures, not only to increase the chances of accurate coding but also code validity and longitudinal data comparisons.it was also recommended that the resulting entropic measures be normalized and adjusted by the probability of a given code before isolating clinical concepts of interest.medical professionals should conduct audits to ascertain transition challenges and training needs, particularly in the instances of diverging entropic measures.the proposed techniques are suitable for establishing coding complexity between any two medical coding systems, provided mappings or crosswalks exist.
number of words= 1365
[{'rouge-1': {'f': 0.3344436794348223, 'p': 0.937579908675799,'r': 0.20352073085031624}, 'rouge-2': {'f': 0.25736185426590236, 'p': 0.6571559633027524,'r': 0.160014064697609}, 'rouge-l': {'f': 0.37938540835581225, 'p': 0.8667479674796748,'r': 0.2428395061728395}}]
-----------------------------------------------------------------------------------------------------------------------------------
p217:
Extractive Summary:
the problem-oriented medical record—a structured organization of patient information per provided medical problem—is successful in helping healthcare providers to get a good understanding of the temporality of patients [1–3].however, in order to realize these benefits, problem lists need to be accurate and complete, and problem list entries should be coded [4–7].although healthcare providers acknowledge the importance of accurate problem lists [8–10], problem lists often remain inaccurate and incomplete [11–14].uncertainty can be defined as the expressions of hypotheses, tentative conclusions, and speculations [24].uncertainty described in diagnosis descriptions could indicate a change in the meaning of a modified description compared to the default description.problems that should be specified by laterality are not always available in clinical terminologies, which therefore requires adding laterality in the description [26].furthermore, healthcare providers may argue that listing temporality is important (e.g. glaucoma 2002), explaining the timeline of a disease, symptom or event [23, 27].specifying a diagnosis with temporality could also be useful for prompting more frequent testing, for instance for breast cancer [28].again, it is important to identify temporality, because temporality described in a modified description indicates that the problem is a former problem, while the code indicates it is a current problem [17, 27, 29–32].the identification of context of information in terms of uncertainty, laterality and temporality is therefore an important task [17, 21, 35, 39].for instance negex is an algorithm that uses regular expressions to identify negated (i.e. ‘ruled-out’) diagnoses [32].context, an algorithm that was developed based on negex [32], identifies several contextual properties in clinical free text, including whether a condition is negated, but also hypothetical, historical, or experienced by someone else [17].the dt is used by healthcare providers to select the best-fitting code for their patients’ problems.a multicenter dataset including data from five anonymous dutch hospitals, all using the same ehr (epic), was used for second validation of the algorithm.further characteristics of the two datasets are shown in table 1.a modified freetext description could be suspected hypertension.development of the algorithm unlatem we developed an algorithm that uses regular expressions to identify whether a modified diagnosis description contained (removal of) uncertainty, laterality and/ or temporality.in this study, we treat uncertainty as expressions of belief, including at least one tentative conclusion or speculative fragment described in diagnosis descriptions [24].based on all found variations, regular expressions were established.for instance, laterality could be described as left and right, but also le and ri (dutch: links and rechts, li and re).the translation of the terms to english can be found as additional file 1.validation and performance of the algorithm the single-center and multicenter validation sets were independently manually annotated by the same two authors (esk and fjp).the specificity is defined as the proportion of modified descriptions that were correctly identified not to have type(s) of properties (i.e., true negatives).this method calculates the weighted mean of the two validations samples, by taking into account the standard errors of the recall, specificity and prevalence.in line with context, four error classes were distinguished [17].results development of the algorithm the regular expressions and trigger terms for (removal of) uncertainty, laterality and temporality for the algorithm are shown in fig. 1, in the second column.that is, because both expressions indicate that having glaucoma was suspected.for instance, glaucoma and suspected glaucoma exist as default diagnosis descriptions but hypertension and the corresponding suspected hypertension do not.only a few records that included removal of laterality were retrieved.also, note that one modified description can have multiple properties (2015 left eye infection is categorized as both laterality as well as temporality for the default description eye infection).table 3 shows the actual prevalence, which was determined using the reference standards and the recall, specificity and precision of unlatem to identify the properties for the validation samples.error analysis tables 4 and 5 show the results of the error analysis in the internal validation set (n = 980) and multicenter validation set (n = 996).application of unlatem table 6 shows the apparent and actual prevalence of the contextual properties which was determined within the modified descriptions for both the amsterdam umc dataset (n = 73,280) and multicenter dataset (n = 175,210).temporality additions were highest for internal medicine (4822/30,457, 15.8%) and lowest for audiology (0/1127, 0.0%).removal of uncertainty was highest for urology (49/1306, 3.8%) and lowest for audiology (0/1127, 0.0%).the most-frequent modification was adding od (right eye) to cataract, which occurred 1,237 (0.7%) times in the dataset (n = 175,210), followed by adding os (left eye) to cataract, which occurred 1,191 (0.7%) times.as expected, the percentages of laterality modifications that were considerably higher than the mean value occurred failure.we applied the weighted prevalence, recall and specificity to determine the weighted precision.temporality additions were highest for internal medicine (4822/30,457, 15.8%) and lowest for audiology (0/1127, 0.0%).removal of uncertainty was highest for urology (49/1306, 3.8%) and lowest for audiology (0/1127, 0.0%).discussion in this study, we developed and evaluated an algorithm called unlatem to identify (removal of) uncertainty, laterality and temporality in dutch modified diagnosis descriptions.nlp could automatically transform clinical text into structured data that can guide clinical decision-making and data reuse such as research.however, current nlp algorithms cannot accurately identify diagnoses in dutch or english clinical free text yet [56].strengths and limitations a strength of this study is that we developed a successful algorithm for recognizing four contextual properties in dutch descriptions, as it showed high performance scores in both validation sets.we expect to find more information on diagnoses in other free-text fields, such as history notes, discharge letters and medication letters.error analysis it is important to note that unlatem is meant to identify simple sentences including uncertainty, laterality and temporality using regular expressions, and was never expected to capture all properties.contextd, an algorithm adapted from context [17] identifies whether a condition is temporal, but also whether terms were negated in clinical dutch reports and letters [27].for example, ad means right ear but was also used for the pregnancy duration (dutch: amenorroe duur).the algorithm also misclassified diagnosis descriptions containing values such as mmol/l.in the error analysis, we found that possible also appeared in the middle of sentences.extending the rules for existing trigger items for laterality and (removal of) uncertainty could improve the performance of the algorithm even further.in the third error class, “outside the framework”, we discovered that the algorithm did not take into account the term order of the modified descriptions, though this is needed to extract meaningful information [27].finally, the annotators sometimes misclassified a modified description as an uncertainty, or they overlooked a laterality.relation to other literature we compared the results of temporality of unlatem and the temporality module of contextd.laterality was not included in contextd.it is important to note that there were more trigger items (i.e. since) for the temporality function of contextd than for the temporality function of unlatem.another comparable algorithm is pycontextnlp [21].the performance scores for uncertainty of pycontextnlp (recall = 0.94, precision = 0.93) are comparable to the performance scores for uncertainty of unlatem (recall = 0.98, precision = 0.90).pycontextswe is the swedish version of pycontextnlp, and distinguishes four different classes (definite existence, probable existence, probable negated existence and definite negated existence) [65].additionally, unlatem included laterality triggers.this means that unlatem could also be used to detect patterns of laterality for implementation in context-sensitive user interfaces and identify terms that should be further specified by laterality.however, no checkbox should appear when the healthcare provider selects terms for non-lateralizable concepts, for instance diabetes.additionally, based on the presence of (removal of) uncertainty, laterality or temporality in the modified descriptions, the algorithm could trigger alerts to a decision support system or module to no longer rely on the captured underlying codes.it thereby provides insights in the correctness of diagnosis descriptions and potential discrepancies, that should be identified before reusing these diagnosis data.unlatem could be implemented eventually in dutch hospital systems, improving quality of diagnosis data for research and clinical decision support.for example, the records for audiological centres (dutch: audiologische centra) are combined with audiology (dutch: audiologi
number of words= 1314
[{'rouge-1': {'f': 0.38004249201920653, 'p': 0.7755555555555556,'r': 0.2516881258941345}, 'rouge-2': {'f': 0.19779817449708625, 'p': 0.3401949860724234,'r': 0.13943450250536865}, 'rouge-l': {'f': 0.35961563656403184, 'p': 0.5991005291005291,'r': 0.25691588785046726}}]
-----------------------------------------------------------------------------------------------------------------------------------
p218:
Extractive Summary:
due to the esthetical disturbance of the disease to adolescents and adults, especially when considering people with longstanding, childhood-onset strabismus, the relating prejudice to eye deviation extends beyond social interactions.in this context, the collaborative or shared decision is seen as a model for clinical practice [2] and was defined by weston in 2001 as being one of the key components to patientcentered care [3].this patient-physician alliance results in empowering patients to develop their autonomy [6, 7] and in finding better healthcare choices.furthermore, earlier research performed by wisdom et al. [10] outlined that teens are eager to exchange information with clinicians and they want to express their opinion and to have the autonomy to choose between treatment options whenever possible.a young person’s ability to consent depends more upon his understanding of the nature of the proposed treatment and its possible consequences [11].it consists of versions of the patient (sdm-q-9) and the doctor (sdm-q-doc), which can be used to evaluate the patient’s involvement in the decision-making process from two points of view [14, 15].it is often used in various clinical situations, including primary [16] and specialized care as oncology [17], otorhinolaryngoglogy [18], mental health practice [19], vascular surgery [20], dialysis [21] multiple sclerosis [22] and even among patients’ family members [23].several available publications in the ophthalmological field directed towards managing patients with cataract, diabetic retinopathy, glaucoma, age-related macular degeneration, and exotropia in providing a qualitative analysis by using standard interviews with healthcare workers and/or patients [25–29] and support the need for more patient involvement in shared making decision process.although, sdm has become a priority of health policy in many european countries in the past two decades [30] there are no studies on the involvement of patients in taking medical decisions in western europe and the republic of moldova.as the main tool, we have used the sdm-q-9 questionnaire and confronted the score with a face-q scale: to compare the postoperative satisfaction and the decisionmaking score.the outcome had to help us improve the quality of the surgical consultations in the process of sdm and help both sides move towards taking an optimal decision.we wanted to clarify whether in the process of sdm the patients were satisfied with the amount and clarity of the options and outcomes and it led to a higher satisfaction rate vs. having the physician offer the best option based on.methods study design and context we conducted a prospective observational pilot study among teens (15–17 years old) and adults over 18 years old undergoing strabismus surgery.participant recruitment and eligibility among the participants were 93 adult patients with manifest strabismus who needed a surgical correction and in whom more than one technique was advised (eye muscle recession or resection by symmetric surgery, asymmetric surgery, or adjustable sutures).the patients were enrolled in this study if they met the following criteria: (1) 15 years and older at the time of surgery, (2) a confirmed diagnosis of manifest concomitant strabismus through orthoptic examination, (3) candidates for surgical correction of strabismus, and (4) romanian communicative skills and written agreement of patients aged 18 and up, and from the teenagers’ parents (for the teenagers between 15–17).therefore the written informed consent was obtained from all participants prior to their enrollment in the study.the patients were informed about the study by their healthcare provider.procedure before the study, basic demographic data on age, gender, and type of strabismus were collected.all patients underwent a detailed ocular examination and an orthoptic evaluation before undergoing strabismus surgery.after the final consultation, the patients completed the translated romanian version of a validated patientreported measure of shared decision making (the 9-item shared decision making questionnaire, sdm-q-9) questionnaire.in this previously approved questionnaire, the sdm level was assessed subjectively by evaluating the nine phases of the decision-making process from the patient’s perspective on a 6-point likert scale ranging from 0 (not at all) to 5 (fully applicable) [13] (see additional file 1: appendix sdm-q-9 questionnaire).an excellent rating meant that the patients did not notice any eye deviation after the surgery interval.the mean sdm-q-9 score assigned by esotropic female patients was 32 (iqr = 2) and a score of 33 (iqr = 2) was given by males.correlation between the sdm score and postoperative patient satisfaction the postoperative patient satisfaction was ranked as excellent by 16 (17.2%) patients, good by 38 (40.9%), fair by 32 (34.4%), and poor by 7 patients (7.5%).the interrelation between sdm-q-9 score and patients’ satisfaction, following to bubble plot, had to be adjusted to other potential covariates like age, gender, and strabismus types.patients with exotropia, especially men, reported lower sdm and pos scores compared to esotropic patients.the sum sdm norm determination effect was the important one having a value of 0.679, agenorm represented only 0.01.the model included a constant (b = − 1.175, 95% ci − 2.417, − 1.013) and two transformed continuous variables, agenorm (b = − 8028, 95% ci − 15.952, − 0.105) and sumsdmnorm (b = 0.005, 95% ci 0.004, 0.005), the sumsdmnorm effect estimated after coefficient standardization being higher in comparison with agenorm (0.84 versus − 0.118).residuals were normally distributed (fig. 4), shapiro–wilk test = 0.993, df = 93, p = 0.906.discussion shared decision making (sdm) comprises three main elements: the exchange of information (personal and medical) between the patient and the doctor, the discussion of the diagnosis and the treatment options, and the building of consensus [33, 34].patients’ views on involvement in strabismus decision‑making treatment in this study we wanted to outline the patients’ perception on the treatment decisions about strabismus shared by both the doctors and the patients.a study performed by sharma et al. [43] on a sample of 60 patients revealed that only 30% of the patients were aware of the strabismus pathology and the treatment alternatives.another concern revealed in this study was related to the fact that some patients felt excluded from the treatment decision making process.our study results clearly show that the information providing about existing treatment options is clearly explained, and patients need this information to be able to participate in sdm.in this study the participants reported an overall positive experience with their care: 58.1% were satisfied or very satisfied with the results of their surgery, and 7.5% were unsatisfied.having been compared to other publications, a higher result of overall 90% of parents’ satisfaction with strabismus treatment rated as "good" or "very good" was reported by mruthyunjaya et al. [46] and kaszli et al. [47].communication that provides the evidence-based, weighted level of assistance to patients in their decision making appears to be an important non-desease- specific factor that influence the sdm process and the patients’satisfation with received healthcare.furthermore, in our study, no significant difference was observed in reported satisfaction levels among males and females and among esotropic and exotropic patients.on the other hand, burke et al. [49] found that people with an esotropic strabismus recorded significantly greater appreciation of the effects of surgery than those with an exotropic strabismus.this study has some limits.the described model needs correction by efficient variables enrolment because of reduced determination coefficient (about 70%), another 30% of variance being unexplained.secondly, one of the parents were always present during consultations and shared decision making evaluation process, which may influence the adolescents’ answers – so they should probably be assessed in a separate study or as a separate group.these findings can serve as a springboard to further communicative improvements related to the sdm process and between patients and physicians, thereby consequently leading to patients’ satisfaction raise in strabismus care.the study underlines the importance of further analysis and validation of on-ground interactions among the adolescent and adult patients and the clinicians across the strabismus management trajectory.a multicentral study and its validation will follo
number of words= 1268
[{'rouge-1': {'f': 0.4624488932321192, 'p': 0.782215320910973,'r': 0.3282582582582583}, 'rouge-2': {'f': 0.26718281861546206, 'p': 0.41854771784232364,'r': 0.19622088655146508}, 'rouge-l': {'f': 0.39264013455432245, 'p': 0.6224193548387096,'r': 0.2867721518987342}}]
-----------------------------------------------------------------------------------------------------------------------------------
p219:
Extractive Summary:
mms calculating absolute and relative counts per variable, per value in this variable (absolute and relative frequencies) or counted for a certain dimension, e.g. number of values in a variable per patient, can give hints on completeness [12].checking constraints for valid variable’s values, correctness- issues.mms describing the distribution of values, e.g. mean and standard deviation, or extreme values like minimum and maximum, could also indicate implausible data.furthermore, which mms provide sensible information and assessment of their results may depend on the planned data usage [14] and the role of the person assessing the dq [8, 15–19].stausberg et al. [20] suggest in their review that research should take into account proposals for formal definitions of dq-indicators as well as standards for data definitions.changes to mms governed separately from the applying software do not require changes to the software’s source code, thus shifting the ability for mm-governance away from software developers towards domain experts.juarez et al. [26] recently published work based on such a metadata repository, in which simple constraints are stored centrally together with metadata like variable definitions.utilizing standards like iso/iec 11179 for data definitions as proposed by stausberg et al. is one aspect of interoperability.knowledge-based implies that the mms themselves, which mms are applied as well as assessment of mm-results, can be expressed in a formalized way.first, the specification provides definitions for basic building blocks from which complex clinical information models (cim) can be built.some of these basic definitions, the reference model types, can be used to automatically generate mms similar to mms based on a datatype, e.g. measures of distribution for numeric variables or frequencies for string variables.second, cims provide shared machinereadable definitions of the clinical concepts the data represents.cims in openehr are called archetypes or templates, depending on their purpose.yet, the archetypes for the same clinical concepts are still common, enabling tools to work on data from different sources through archetype-paths.beyond that, cims can express constraints on valid data instances for variables, such as ranges, formats, value sets, datatypes, cardinalities.opencqa can automatically derive applicable mms from openehr cims to check these constraints.the second type of domain path relates to other mms, i.e. by specifying a filter-expression defining the mms and which attributes of the mms shall constitute the input data for this domain path.the second type of domain path enables multi-level mms, e.g. for using results from mms as input for another mm.other means considered for expressing these parts of the mms were drools [38], arden syntax [39], object constraint language [40] and the openehr guideline definition language [41].when mixing characterization (e.g. mean for a variable’s values) with rule checking (e.g. is value in permissible range), grouping (e.g. per hospital), adding some aggregation (e.g. a mean over some mm-results with different weights) and a visualization or assessment (e.g. dataset is ok), much variability between two mms is introduced, since each step can slightly differ.as a result, even mms quantifying almost similar aspects of dq may differ in minor details, and results may not contain all necessary information from the dataset to make their results comparable.separating these steps and defining preferably plain mms aims to maintain comparability as long as possible.one intention of this application was to test if our described theoretic concepts for interoperable and knowledgebased dq-assessment work, e.g. regarding portability and whether the generated mms provide useful and correct results.a common problem mentioned in the literature is that domain experts are often left alone with this complex and resource intensive task, ending up in single-use projectspecific mms [21, 22].we used the opencqa client (fig. 1a) to derive mms based on the reference model types calculating simple characterizations, e.g. mean value for scale variables or frequencies for categorical data, and simple visualizations, e.g. a barplot.the measures calculated by is in spss defined which measures were of interest and et checked if these were present in opencqa’s generated mms.(3) tdconstraints summarizes the checks of constraints for variables defined in given cims.cims can serve to express both domain and taskdependent constraints.a regular cim used for data processing should already include sensible constraints, e.g. the height archetype could constrain, that the height of a person should not exceed 300 cm or a template could define mandatory context variables according to local clinical processes.this is why constraints in regular cims should be set with caution to prevent excluding unusual but correct data (cf.in addition to that, cims could define task dependent constraints, e.g. when a multi-disciplinary project team collaboratively decides to make fields mandatory, considering a certain planned analysis (cf.the presented method can deal with an unlimited number of cims (archetypes and templates) for the same clinical concept.in our study, we derived constraints from the consented templates without adding any more restrictive constraints.therefore, our created example knowledge base containing the hdqf-mms is applicable in any sensible use case and applying the measure tdconstraints on mms derived from different cims is possible without adapting the tdconstraints- mm.this example shows how existing work proposing well-thought-out means for dq-assessment can be integrated with our approach and demonstrates possibilities for summarizing and assessing mm-results using multilayered mms.the client resolved dependencies of multilayered mms and invoked the mm execution on serverside (fig. 1b).opencqa derived 245 and 381 mms respectively from reference model types and cims.the automatically derived mms included the frequencies and percent values of interest, measures like minimum, maximum, median and mean as well as appropriate visualizations as needed for the assessment of the study’s data.thus, such a knowledge base entails mms creating histograms as characterization (cf.such mms could be loaded from the knowledge base and applied on other sensible datasets to visualize age or gender distributions without the need for modifications.intention of table 5 is to illustrate mm-results while not revealing any clinical study results, which are not subject of this work.the corresponding mm was excluded from opencqa’s mms before comparing results since it involves grouping in dimensions (cf.the measure tdconstraints used 12 mms derived from constraints contained in the corresponding cim.to improve comparability of mm-results and to support collaborative knowledge governance for dqassessment, our knowledge-based approach proposes a formalization for dq-assessment knowledge.the generated mms derived from reference model types and cim-constraints could serve as basic assessment, e.g. regarding completeness (absolute and relative frequencies) and correctness (constraint checks, frequencies, distribution measures).the presented concepts for formalizing mms (cf."mm formalization" section), basing mms on standardized data definitions (cims and archetype-paths), portability (aql, openehr restapi and archetype-paths) and collaborative governance of dq-assessment knowledge (openehr cim governance and compilations of mms for domains and/or tasks managed using git) worked, produced useful results and showed satisfactory practicability in a real world use case.nevertheless, we took particular care to facilitate expansion of our approach to other data sources and to simplify comparing results between mms based on different cim standards.juarez et al. store constraints on valid variable values together with the variable definitions.comparing mm-results based on constraints defined in a metadata repository and mms based on openehr cims merely requires mappings between the iso/iec 11179 variable definitions and corresponding archetype-paths.alternative implementations or extensions of opencqa, e.g. to apply our concepts on data sources relying on iso/iec 11179 metadata definitions, to the omop data model [50] or complying with other cim-standards like fhir [51], would increase the value of collaborative knowledge governance.implications on portability depend on the means other standards provide to support standardized data retrieval, e.g. equivalents of aql, rest-api and archetype-paths.we already approximated application of our concepts to other standards by applying our mms generated with opencqa (r-scripts) to comparable data in non-openehr data sources [52].and on the possibility to govern domain and task specific constraints.knowledge‑based dq‑assessment considering the possible combinations of variables, checks, groupings and characterizations and keeping in mind that sensible combinations as well as the assessment of mm-results are task and domain dependent, the amount of resultant information could become overwhelming.the finding that selecting sensible mms and their assessment for a certain task is challenging agrees with findings from the literature, e.g. diaz-garelli et al. [21] stress that defining dq-requirements is complex, resource intensive and typically requires a multidisciplinary team.the intention of our approach is to support dq-assessment by providing means for flexible generation (mms from cims and from reference model types), reuse and collaborative governance of formalized dqassessment knowledge.from the mms in our use case, we already identified two sensible knowledge bases, i.e. “initial data analysis in studies” and “hdqf”.beyond diaz-garelli’s approach, we deem most other processes for the elaboration of mms or proposing mms for a certain domain or task to be complementary to our approach, not competing, e.g. hdqf [11], 3 × 3 dqa [12] or henley et al. [54].similarly, once implemented as mm-compilation (like the hdqf example), existing dq-frameworks are enabled to be extended with additional mms, e.g. for mms assessing the results of the framework for a certain task.juarez et al. discuss in which stage of a dataset’s lifecycle dq-assessment is most worthwhile and focus their framework on this stage [26].we assume dqassessment is sensible at different stages with different perspectives, e.g. a data integration specialist validates data integration locally during implementation, a quality manager continuously monitors dq in a data integration center and a researcher assesses dq in a research data network specifically for the research question [8, 15–19].a common practice to agree on a set of sensible mms and their assessment for a project is to conduct surveys and reviews with experts and stakeholders [12, 18, 19].this is similar to openehr cim governance which typically involves domain- and technical experts working together to define a cim’s core data elements, contextual data elements and sensible constraints for a clinical domain (archetypes) or a particular use case (templates).cim-drafts are refined in multiple review rounds in which experts discuss the draft and suggest improvements finally leading to a consented cim.tools to support these review rounds and cim management over the whole lifecycle are available [32].we can directly make use of these well-tested processes and tools for cims, to govern constraint checks on the data (fig. 1e).likewise, for all other types of mms we can manage knowledge bases (mm-compilations, cf.fig. 1f) using git [37].git primarily supports version control but also comprises features for documentation, discussion and issues tracking that can support similar processes as for cims, although less optimized for knowledge governance.using these two means (cims and knowledge bases) for knowledge-management entails the challenge of keeping constraints consistent through updates, which will probably need attention.if we imagine a knowledge base for a certain task that includes mms derived from a cim, these mms are not updated if constraints in the original cim change and vice versa, the cim will not change when the respective mms are adapted.however, we just started collaborative governance of dq-assessment knowledge and so far did not evaluate different processes regarding their goal to improve task and domain specific dqassessments while keeping efforts justifiable.we need more experience in how to combine different means.conclusions the presented work describes a method for interoperable and knowledge-based dq-assessment.we provide applicable concepts and a tested exemplary open source implementation.we propose a formalization for mms and show means for collaborative governance of dq-assessment knowledge striving to base dq-assessment on formalized knowledge.we applied our concepts in a real world use case with satisfactory results, using opencqa as our implementation.important next steps would be to work on methods for learning of dqassessment knowledge, on integrating existing processes for mm elaboration, integrating existing frameworks proposing mms [11, 12, 21, 54] as well as to gain experience with collaborative governance of dq-assessment knowled
number of words= 1902
[{'rouge-1': {'f': 0.2445119549669978, 'p': 0.846595744680851,'r': 0.14289066400399403}, 'rouge-2': {'f': 0.16029206842415267, 'p': 0.39620320855614977,'r': 0.10046953046953047}, 'rouge-l': {'f': 0.2686411391171044, 'p': 0.693931623931624,'r': 0.16656084656084658}}]
-----------------------------------------------------------------------------------------------------------------------------------
p220:
Extractive Summary:
the enumerators were trained to sync completed forms for review by the team leader immediately following each section of the interview.this means that completed survey forms were uploaded to a central server so that the team leader could access the data on her tablet and address any errors in real time.team leaders also visited a randomly selected 5% sample of households to verify collected information; however, no such discrepancies were found.we hired all female enumerators and team leaders based on the local norms in karachi; typically females are permitted to enter the household and talk to women, but males are not.we developed a protocol for revisiting cases with major errors such as when an enumerator skipped collecting pregnancy-related information even though the respondent had a birth in the last 30 months (as identified in the contraceptive calendar module).the data management unit documented the data cleaning notes and ran rounds of stata cleaning codes to fix case specific errors following discussions with the field manager, then re-ran the stata quality checks to confirm that all issues were resolved.in addition, enumerator refresher trainings were conducted weekly to discuss common issues with data and fieldwork, and to review updated field protocols where appropriate.in order to minimize loss to follow up between the household listing and re-visits for the survey, when a selected woman could not be found at their household, the enumerator made three separate attempts to contact her on different days.after each interview or contact attempt, enumerators completed an ‘interviewer contacts’ form in commcare, which documented the status of each case (e.g. completed, not eligible, moved away, rescheduled, etc.).to improve response rates, enumerators also scheduled interviews on days/times most convenient for participants (for example, on weekends or evenings, outside of typical working hours).ethical approval the ethical review committee of the aku (4964-ped- erc-17) and the national bioethics committee of pakistan approved the study.the enumerator read aloud the consent form before asking survey questions to eligible women and signed the consent form on their behalf.the enumerator also provided a hard copy of consent form to the respondent.results this study documents various issues encountered during data collection and potential solutions.these are broadly categorized into four main themes with further sub-categories (table 1).the four main themes are: (1) technical issues related to gis usage and computer assisted personal interviews (capi); (2) household listing issues; (3) respondent issues, and (4) field issues.technical issues related to gis and capi we found that using gis technology to demarcate clusters, particularly in urban slums, was an efficient approach to establishing a sampling frame; however, its use is subject to issues around cluster boundary interpretation and household identification, particularly for narrow and congested streets.this approach required extra time and additional human resources to validate the boundaries through in-person field visits.we also worked closely with program staff to ensure that the boundaries around areas intended for program implementation were accurately demarcated.this was a timeintensive, but critical step in the process.collecting data using electronic devices had many advantages but was also prone to challenges such as power outages, low connectivity in some areas, and slow uploading speed.we ensured that ample tablets were available as backup in such circumstances.when data issues/queries emerged in the field, waiting to receive a response from the technical support team at aku (who had access to the central server) was a time-consuming procedure.in areas of low connectivity, enumerators struggled with the intermittent internet to sync the data, especially when conducting interviews on ground floors of tower buildings.nevertheless, the application was programmed such that the survey itself could be run without any connectivity, and syncing could occur later (i.e. each evening upon return to the office).aku’s dedicated research staff were also on board to deal such queries in the field.household listing issues karachi is a metropolitan city comprised of quickly growing urban dwellings and settlements.residential apartments and multi-story buildings are common.in a dynamic population like our study sample, having a long gap between the household listing and the household survey can lead to high loss-to-follow-up, and recall errors among enumerators, such as forgetting important sites and other neighborhood landmarks.a majority of the housing areas do not have formal addresses, which makes it difficult to identify and track individual households.to address this challenge, the team marked a unique identification number (id) on the front door of each household during household listing with the permission of participants.this was helpful to relocate and confirm the households randomized for data collection during the household survey.we trained two separate teams to conduct the household listing and household survey.initially, our plan was to complete the entire household listing in all selected clusters before beginning the household survey.however, there was a shift in strategy mid-course when the study team identified a need to reduce the time lag between the two activities.in our revised field protocol, the household listing and survey teams worked in tandem so that interviews began immediately after each cluster was listed, rather than listing all clusters before beginning the household survey; this modification proved critical in minimizing loss to follow-up and maintaining an efficient data collection process.respondent issues the study revealed that respondent issues were the most important barriers to high quality data collection (table 1).in karachi, people’s religious beliefs affect their willingness to discuss srh topics.in addition, perceived security threats and gender-based household dynamics made some women reluctant to participate in surveys.these findings are similar to those reported in other studies [10].respondents were initially suspicious of enumerators and viewed them as strangers due to recent thefts and child kidnapping incidents in the neighborhood.we found that discussing sensitive topics such as women’s socioeconomic status (which raised concerns regarding robbery) and sexual and contraceptive history made fieldwork challenging and led to high refusal rates if not approached with sensitivity.likewise, certain community sub-groups (e.g. muhajirs and pashtuns) were reluctant to participate for these reasons.to counter this challenge and to build strong rapport, male field supervisors coordinated with male community leaders personally using a gatekeeper script to initiate the discussion, and we trained an allfemale team of enumerators on strategies to improve mutual trust prior to interviews.as a result, refusal rates were low (jamshed town 9.8%, yousuf goth 2.5%, korangi town 5%, and pib & dalmia/shanti nagar 4.3%.).in some instances, respondents expressed difficulties understanding urdu language and had to be assigned to a new enumerator who could communicate in their native language.to maximize inclusion in our study sites, we ensured that our enumerator team included women who spoke at least one of the four most common languages in karachi: english, urdu, pushto, or sindhi.we found that engaging respondents for more than an hour conflicted with women’s busy daily schedules, which created problems for successful data collection.this also increased participant discomfort and unwillingness to participate in the data collection process.in these cases, we rescheduled the remaining portion of the interviews at a time more convenient for respondents (e.g. on sundays, outside of working hours, etc.).another issue this study revealed was that some respondents demanded material benefits, extra healthcare services or sometimes lodged complaints against the healthcare system when visited for surveys, thus enumerators had to be properly trained to deal with such situations.this report might inform policies that could benefit respondents in the long run.we also provided a tea mug to all respondents as a token of appreciation for their time and effort.field issues this study also highlighted the difficulties of conducting long interviews in a dense, urban environment.our protocol was to conduct the interview privately in women’s homes, but this was sometimes difficult because other family members were present and did not want to leave.selecting an alternative private interview location was a challenge in some areas since data collection did not take place in a room, particularly in joint family systems, but rather in an open space (for example, at the doorstep where loud background noises and warm temperatures posed additional challenges).in addition, ensuring privacy away from other family members and neighbors was a challenge.we addressed this issue by scheduling a revisit after confirming another time when most family members were away from the home.the data collection period was affected by some seasonal events such fasting and eid holidays, and field activities were postponed for 2 months in retrospective sites due to political events and general elections in july, 2018.these data collection protocol adaptations have serious time and budget implications, but we view them as necessary investments to ensure high quality data.discussion this study demonstrates how conducting surveys in urban karachi presents unique challenges that can influence the data collection process.however, we found that appropriate mitigation measures to address the environmental and socio-cultural context enabled successful data collection in this setting.lessons learned may be usefully applied in similar urban settings in other lmics.using gis technology to demarcate clusters for our sampling frame is a unique and efficient approach of present study.however, the process is time-intensive, as accurate interpretation of cluster boundaries and household identification, particularly within neighborhoods with congested and narrow streets, can be challenging.working with experienced field teams and program staff who know the area well is critical to success.this finding is consistent with other studies conducted in lmics, where researchers lacked accurate and detailed spatial images, and encountered challenges tracing the exact location of zoning areas [23, 24].
number of words= 1534
[{'rouge-1': {'f': 0.31830206683214635, 'p': 0.7461904761904763,'r': 0.20229813664596275}, 'rouge-2': {'f': 0.13789484802218707, 'p': 0.22286624203821656,'r': 0.09983219390926042}, 'rouge-l': {'f': 0.3197198086189543, 'p': 0.6197382198952879,'r': 0.21542936288088643}}]
-----------------------------------------------------------------------------------------------------------------------------------
p221:
Extractive Summary:
(1) this true generating mechanism contains only the true predictors.the assumed coefficients of the above true generating mechanism were adapted from the published regression parameter estimates in the paper, but the interaction terms fromthe original study were excluded for the sake of simplicity.as a consequence, the intercept used in here deviates from the original publication in order to create reasonable values of the outcome.moreover, the covariance structure of the exemplary model was chosen as reasonable as possible, but does not encode specific causal assumptions.two frequently applied variable selection methods are univariable selection and backward elimination with the akaike information criterion (aic).although from a theoretical point of view, the bayesian information criterion (bic) as a model selection criteria may be preferred to identify the true underlying model [17], the aic is more commonly applied in practice.in univariable selection, the final model includes only those predictors which were significant in univariable regressions.backward elimination starts with the full model and iteratively cycles between identifying the least significant predictor and refitting the model without that predictor.the procedure is stopped if no predictor can be removed without increasing the aic.note that although all variables in the true data generating mechanism are true predictors, the clinical relevance of the predictors and the size of the coefficients are different, so the impact of not selecting a true predictor is different as well.therefore, we calculated the standardized regression coefficients of our data generating mechanism, which are 0.528 for xsex, −0.406 for xcbp.first, 0.315 for xantihyp, −0.268 for xage, 0.201 for xcbp.change, −0.161 for xpp, −0.093 for xbmi, −0.050 for xcvd and −0.004 for xhistory.this should be kept in mind, when interpreting the simulation results below.simulation and analysis our considered simulation study consisted of three steps.in the first step, data were simulated and in steps 2 and 3, the simulated data were analysed further.in step 1, three different “preceding” study data sets were generated according to the model specified above.this first step is highlighted in blue colour in fig. 1.subsequently, in step 2, variable selection was performed within each preceding study, and for the final model of the “present” study, a variable was considered as a “known” predictor if at least one, at least two, or all three preceding studies identified it as relevant.this part of the simulation study is graphically highlighted in green in fig. 1.in step 3, the reliability of background knowledge based on the preceding studies was evaluated with different performance indicators.thereby, the performance indicators assess performance aspects related to descriptive and to predictive behaviour.this third step of the simulation algorithm is highlighted in orange in fig. 1.in the following, the three steps are described in more detail.in the first step of the simulation study, three preceding study data sets were generated according to the true data generating mechanism, including predictor and non-predictor variables as specified in the following: step 1: data generation to define the candidate predictors, we additionally added a set of non-predictor variables denoted by xnone1 to xnone11 to the true predictor set.to simulate candidate predictors (including true predictors and non-predictor variables), we used the r-package “simdata” [18].this package is inspired by a technical report by binder et al. [19].it simulates data for covariates with a predefined realistic joint distribution mimicking data from real biomedical studies.this is achieved by first drawing multivariate normal deviates with a predefinded correlation structure, and then transforming them to achieve specific realistic marginal distributions of simulated predictors and a realistic correlation structure between them.note that the application of transformations might change the correlations.figure 2 visualizes the respective discrete or continuous marginal densities for the simulated variables.the resulting average correlations are presented in the supplement [figure s1].while the distributions of the true predictors were generated to derive clinically meaningful values in accordance with the above true generating mechanism, the distributions of non-predictor variables were chosen with the intention to create variables with complex correlation structures and a range of different distributions.the data generating code including the applied transformations is provided in the supplementalmaterial.for data generation, the variance σ2 of the random error was set to 2 resulting in a r2 of about 0.75.this seemed to represent a plausible situation where still some variance is present.we considered the following specific simulation settings: • preceding studies with equal sample sizes n = n1 = n2 = n3, where n ∈ {200, 500, 2000}. • in addition, we considered seven scenarios where at least two preceding studies show unequal sample sizes, where n1, n2, n3 ∈ {200, 500, 2000}. step 2: variable selection within each preceding study data set, variable selection was now performed to identify the respective predictors.we thereby relied on the following two variable selection techniques known to be often applied in applications: • univariable variable selection was considered with upper p-value thresholds of αin ∈ {0.05, 0.2} meaning that variables which showed a p-value smaller or equal to αin were included in the full model of the preceding study.• in addition, we also considered backward elimination with the aic as selection criterion.subsequently, each of the 20 candidate predictors (9 true predictors, 11 non-predictors) was considered as a “known predictor” if it was identified by only one preceding study (rule 1), by at least two preceding studies (rule 2) or by all three preceding studies (rule 3).the set of predictors identified by these rules within the preceding studies was then considered as the set of “known predictors” (background knowledge) for the current study.step 3: performance evaluation as the true predictors are known, the reliability of background knowledge based on the preceding studies was then evaluated by different performance indicators.the following different performance measures were investigated, where we focused on correct predictor identification (descriptive aim) and prediction performance (predictive aim).a discussion of suitable performance measures can also be found in [19].• first, we evaluated how often a specific rule to quantify background knowledge from preceding studies identified all and only the true predictors, referred to as “model selection frequency” (msf) [12].the rates were calculated as relative frequencies over all 10,000 random replications.a value of 1 indicates that a rule is perfectly able to identify all true predictors.• second, we also evaluated the average relative frequency for each rule resulting in a correct identification of the true predictors referred to as “true positive rate” (tpr).here, the identified predictor set might include additional variables with a zero effect.the rates were again calculated as relative frequencies over all random replications.the tpr is always at least as high as the msf.a value of 1 indicates again an ideal performance.• third, we calculated the average false positive and false negative rates (fpr, fnr), also denoted as type i and ii errors as defined in [19].in each random replication, the number of falsely selected non-predictor variables divided by the true number of non-predictor variables (here 11) and number of falsely not-selected true predictors divided by the number of true predictor variables (here 9) were evaluated.both numbers were then averaged over all random replications to give the fpr and fnr for a scenario, respectively.
number of words= 1166
[{'rouge-1': {'f': 0.30403561227706505, 'p': 0.7583720930232558,'r': 0.19012987012987015}, 'rouge-2': {'f': 0.16061875962058494, 'p': 0.29897196261682246,'r': 0.10980503655564583}, 'rouge-l': {'f': 0.3222286028468447, 'p': 0.6220000000000001,'r': 0.21743589743589745}}]
-----------------------------------------------------------------------------------------------------------------------------------
p222:
Extractive Summary:
the organ procurement and transplantation network (optn) is responsible for allocating deceased-donor organs in the united states.the optn utilizes separate policies to govern the allocation of livers, kidneys, hearts, and lungs [1].for example, liver transplant candidates receive a model for end-stage liver disease (meld) score; those wait-listed for kidney transplantation receive an estimated post-transplant survival (epts) score, and lung transplant candidates receive a lung allocation score (las) [1–3].more specifically, the las is derived from models that predict both pre-transplant and posttransplant survival and aims to balance each patient’s predicted transplant benefit (i.e., difference between survival with versus without a lung transplant) against their waitlist urgency [1–3].such models are particularly susceptible to selection bias.estimates of post-transplant survival similarly are subject to a type of selection bias – “survivor bias” [4–7] – because they have been derived using information only among patients who received a transplant.lung transplantation represents an important example to study because it is a highly effective treatment, but organs are scarce.waitlist mortality is high, waitlist times vary, and there are concerns about inequities in waitlist mortality and organ allocation [8].in fact, concerns about these inequities led the department of health and human services to mandate the development of the las based on medical need rather than wait time [3, 9– 11].donor lungs are now allocated to recipients based on the las [3, 9–11] which is calculated using the predicted difference between transplant benefit and waitlist urgency, with transplant benefit defined as one-year post-transplant survival minus one-year waitlist survival, and waitlist urgency defined as one-year waitlist survival.conceptually, the las aims to determine the number of days of life a person would gain over the next year if they receive transplant compared to if they do not receive transplant, and prioritizes patients for whom this comparison is more favorable.in appendix 1, we use directed acyclic graphs to illustrate how selection bias can lead to inaccurate predictions for both the pre-transplant and post-transplant prediction models.this bias can occur even with measured covariates, because: 1) not all measured covariates which are available in the unos database are included in the las models (e.g., geographic differences in transplant listing and outcomes [12]); and 2) even if a measured covariate is included in the las, the association between such a covariate and post-transplant survival can be different in the post-transplant subset than it is in the full waitlist population.biased estimates of preand post-transplant survival in turn imply that the current prioritization of lung transplant recipients may be inaccurate.although prior research has incorporated weights in the pre-transplant survival model [13], the models did not capture important geographic differences in patient selection and survival, and no work, to our knowledge, has estimated weights to the post-transplant survival model.in this study, we attempt to bring principles from causal inference into the existing prediction model framework employed by the las to improve organ allocation.specifically, we develop a modified las using inverse probability weighting to improve the accuracy of the las by accounting for selection bias in the pre- and post-transplant survival models.our development cohort consisted of all patients 18 years or older who were listed for single or bilateral lung transplantation in the united states between january 1, 2010 and december 31, 2013.to avoid concerns about positivity violations associated with the likelihood of receiving a transplant, we removed individuals who had clinical contraindications to receiving transplant (e.g., those with panel reactive antibodies greater than 90%), and individuals with both restrictive lung disease (diagnosis group d) and height less than five feet who require such small donor organs that they rarely find a match.after cleaning (see appendix 3), data were divided into pre- and post-transplant subsets, with the pre-transplant subset containing daily time intervals, and the posttransplant subset consisting of a single record per patient.appendix 4 and 5 provide methodologic details on how we constructed these weights.analyses were conducted using stata (statacorp llc, college station, tx) and r (r foundation for statistical computing, vienna, austria).thus, the variables in the outcome model are the same in the modified las as in the existing las, but the coefficient estimates vary (see appendix 6).this approach accommodates censoring by viewing survival time as a “time-varying binary outcome” at each possible time point, and estimating the sensitivity and specificity of the model among all patients who are still alive and at risk of the outcome at those time points [15, 16].comparing the modified las to the existing las the current las is composed of a pre-transplant outcome model and a post-transplant outcome model, where only the pre-transplant outcome model is weighted using a select number of covariates [13].to assess the difference between the modified and existing las models, we constructed bland-altman plots of 1) the modified las score versus the existing las score; and 2) the modified patient rank versus the existing patient rank [18].in all cases, the auc of the modified model is higher than that of the existing las, indicating that the modified model has better discrimination.in contrast, predicted survival estimates from the modified post-transplant outcome model (fig. 1c) and the existing posttransplant las model (fig. 1d) closely match the observed survival curves.in the testing cohort, the predicted survival estimates from the modified pre-transplant outcome model were consistent with the observed survival curves over time, regardless of risk group (fig. 2a).the modified and existing post-transplant models exhibit similar calibration in the testing cohort (fig. 2c and d, respectively).patients at the extremes tend to receive similar scores under the two models, while patients with intermediate scores tend to experience more changes under the modified las.in both figures, however, differences in pre-transplant survival explain a greater proportion of the variability in the outcome (i.e., change in las score or change in priority) compared to differences in post-transplant survival.the fact that discrimination and calibration improve under the modified pre- and post-transplant outcome models compared to the existing las models suggests that regional differences in patient selection may be important to consider when estimating pre- and posttransplant survival.this approach ensures a fair comparison between the modified and existing models in both the development and testing cohorts.additionally, it can be applied to any organ allocation system that relies on estimates of pre- and post-transplant survival to prioritize patients, including those used for different organs and in other countries.
number of words= 1040
[{'rouge-1': {'f': 0.31294616124539065, 'p': 0.7958064516129033,'r': 0.19476894639556377}, 'rouge-2': {'f': 0.18189526764377809, 'p': 0.3672972972972973,'r': 0.12087881591119334}, 'rouge-l': {'f': 0.3286396345113765, 'p': 0.6330252100840337,'r': 0.22192743764172337}}]
-----------------------------------------------------------------------------------------------------------------------------------
p223:
Extractive Summary:
in the presence of positive autocorrelation, statistical methods that do not account for this correlation will give spuriously small standard errors (ses) [11].in sections 4 and 5, we present the methods and results from the statistical simulation study.in section 6, we return to our motivating example and demonstrate the impact of applying the methods outlined in section 3.finally, in section 7 we present key findings and implications for practice.motivating example healthcare-associated infections (hais) are a common complication affecting patients in hospitals.c. difficile (c difficile) infection is an example of one such infection that can cause serious gastrointestinal disease.as such, many countries require mandatory surveillance of c difficile infection rates in hospitals.when outbreaks of c difficile occur, the cleaning and disinfection regimes in hospitals are often changed in an attempt to reduce the infection rate.the routine collection of data in this context has led to many retrospective investigations of the effects of different interventions (e.g. novel disinfectants) to reduce c difficile infection using its data [16].hacek et al. [17] provides an example of such a study, where they examined the effect of terminal room cleaning with dilute bleach (fig. 1) on the rate of patients (per 1000 patient days) with a positive test for c difficile.data were aggregated at monthly intervals.the series was relatively short – a scenario which is not atypical of these studies – with 10 data points pre and 24 post the intervention [16].in the context of hais, there is a tendency for consecutive data points to be more similar to each other, manifesting as ‘clusters’ of data points in time (fig. 1).fitting a segmented linear regression model to the data shows an apparent immediate decrease in the infection rate (level change), as well as a decrease in the trend (slope change).statistical model we use a segmented linear regression model with a single interruption, which can be written using the parameterisation proposed by huitema and mckean [18, 19] as: yt ¼ β0 þ β1t þ β2dt þ β3 t−ti ½ dt þ εt ð1þ where yt represents the continuous outcome at time point t of n time points.dt is an indicator variable that represents the post-interruption interval (i.e. dt = 1 (t ≥ ti) where ti represents the time of the interruption).the model parameters, β0, β1, β2 and β3 represent the intercept (e.g. baseline rate), slope in the preinterruption interval, the change in level and the change in slope, respectively.a lag-1 error means that the influence of errors on the current error is restricted to the value immediately prior.longer lags are possible but in this paper we confine attention to lag-1 only (ar (1) errors).estimation methods a range of statistical estimation methods are available for estimating the model parameters.if there is still some residual autocorrelation these steps are iterated until a criterion is met (e.g., the estimated value for autocorrelation has converged [23]).parameters in this more general family of models are estimated by maximum likelihood, enabling the uncertainty in the autocorrelation estimate to be taken into account in the standard error of the regression coefficients, unlike pw or co. this approach has been variously labelled in the literature, including use of the terminology ‘maximum likelihood arima’ [14].often, the test is used as part of a two-stage analysis strategy to determine whether to use a method that adjusts for autocorrelation or use ols (which does not adjust for autocorrelation).simulation study methods we undertook a numerical simulation study, examining the performance of a set of statistical methods under a range of scenarios which included continuous data with different level and slope changes, varying lengths of series and magnitudes of lag-1 autocorrelation.we now describe the methods of the simulation study using the ademp (defining aims, data-generating mechanisms, estimands, methods and performance measures) structure [32].we found a median standardised level change of 1.5 (inter-quartile range (iqr): 0.6 to 3.0), n = 190), median standardised slope change of 0.13 (iqr: 0.06 to 0.27, n = 190) and median autocorrelation 0.2 (iqr: 0 to 0.6, n = 180).lag-1 autocorrelation was varied between 0 and 0.8 in increments of 0.2 to cover the full range of autocorrelations observed in the its studies included in the review.the increment between the number of data points per series varied; initially it was small (i.e. 2) so as to detect changes in the performance metrics that were expected to arise with smaller sample sizes and was increased to 4 and then 20.all combinations of the factors in table 1 were simulated, leading to 800 different simulation scenarios (table 1, fig. 2).estimands and other targets the primary estimands of the simulation study are the parameters of the model, β2 (level change) and β3 (slope change) (eq. 1).an exception to this occurred for the ols estimator (and to a lesser extent arima) which exhibited unusual behaviour for an autocorrelation of 0.8, with the se initially increasing with an increasing number of points in the series, and then decreasing.the relationship between autocorrelation and the empirical se was modified by the length of series.in contrast to the level change, there were no important differences in the empirical ses across the statistical methods, even when the autocorrelation was large.the nw method performed only slightly better than the ols (except when the autocorrelation was zero); however, the nw model-based ses were still downwardly biased across all scenarios, were worse than ols for small series lengths, and only marginally better than ols for large series lengths.the pw model-based ses were smaller than the empirical ses for all magnitudes of autocorrelation, though the modelbased ses approached the empirical ses with increasing series length.aside from these scenarios, the arima model-based ses were approximately equal to the empirical ses.the reml method behaved similarly to the pw method but, relatively, did not underestimate the ses to the same extent.for the slope change parameter (β3), the ratios of model-based to empirical ses followed similar patterns as for the level change parameter (β2).for any given series length, as the magnitude of autocorrelation increased, model-based ses became increasingly smaller compared with the empirical ses for most statistical methods (supplementary 1.5).for reml and arima, the pattern of ratios of model-based to empirical ses for β3 slightly differed compared with β2.specifically, the reml model-based ses were smaller than the empirical ses for small series, and then increased to be slightly larger as the number of points increased.for arima, the model-based ses were smaller than the empirical ses for large underlying values of autocorrelation (ρ ≥ 0.6 ) for small to moderate length series.the observed patterns did not differ for any of the eight level and slope change combinations (s 1.3.5 for level change, s 1.3.6 for slope change).confidence interval coverage for all combinations of level change, slope change, number of time points and autocorrelation, most methods had coverage (percentage of 95% confidence intervals including the true parameter) that was less than the nominal 95% level for both level and slope change (fig. 7 for level change and fig. 8 for slope change, both with a level change of 2 and slope change of 0.1, supplementary 1.3.7 for level change and supplementary 1.3.8 for slope change for other parameter combinations).however, coverage of the ols method decreased with increasing autocorrelation as well as with increasing series length (with the exception of the zero autocorrelation scenario).reml with the satterthwaite small sample adjustment yielded coverage greater than the nominal 95% level when the number of data points was greater than 30 in the presence of autocorrelation.confidence interval coverage patterns generally reflected those observed with the comparisons between the model-based and empirical se.in scenarios where coverage is less than 95%, examining power is misleading.for scenarios with a level change of two, power was low for series with a small number of points, but predictably, increased as the number of points increased for all methods, except the reml method with satterthwaite adjustment (fig. 9).this was due to the reml method with satterthwaite adjustment having greater than 95% coverage in these situations and hence substantially lower than 5% type i error rates.the reml method always yielded estimated autocorrelations closer to the true underlying autocorrelation compared with the other methods.durbin-watson test for autocorrelation the dw test for detecting autocorrelation performed poorly except for long data series and large underlying values of autocorrelation (fig. 12).the reml model regularly failed to converge (approximately 70% convergence) for short data series (fewer than 12 data points) at all values of autocorrelation, however convergence improved substantially as the number of points in the series increased.the point estimates for level and slope change are similar across methods, but notably, the width of the confidence intervals vary considerably.the confidence intervals are narrower for ols, nw and pw, but wider for reml (with and without the satterthwaite adjustment) and arima.for level change, this led to corresponding p-values that ranged from 0.002 to 0.095; and for the slope change, p-values ranging from 0.069 to 0.531.estimates of autocorrelation also varied, with reml yielding an estimate of 0.23, while arima and pw yielded much lower estimates of 0.07.confidence interval coverage, however, was generally below the nominal 95% level, except in particular circumstances for specific methods.the reml method with and without the satterthwaite adjustment had improved confidence interval coverage compared with the other statistical methods, particularly for slope change.coverage improved for most methods with increasing series length (with the exception of ols and nw in some circumstances).was the only method that yielded at least the nominal level of confidence interval coverage, however it was overly conservative in some scenarios, with a resultant reduction in power compared with other methods.this underestimation of autocorrelation had a detrimental impact on the estimates of se, which were too small, and in turn, this led to confidence interval coverage that was less than the nominal 95% level.arima confidence interval coverage was similar to reml for level change, though reml showed improved confidence interval coverage for slope change.further, the reml method yielded less biased estimates of autocorrelation than the other methods, even for small series lengths.when deciding whether to use the satterthwaite adjustment, consideration therefore needs to be made between the trade-off in the risk of type i and type ii errors.a further issue we identified with the satterthwaite adjustment was that the adjusted d.f.were very small in some series, leading to nonsensible confidence intervals.to limit this issue we set a minimum value of 2 for the d.f., but other choices could be adopted.in our case, we found that for short series (fewer than 12 data points), the dw test failed to identify autocorrelation when it was present, and for moderate length series (i.e. 48 points), with an underlying autocorrelation of 0.2, autocorrelation was only detected in 7% of the simulations.mcknight et al. [14] similarly found that pw and arima yielded liberal type i error rates for the regression model parameters.other simulation studies have investigated the performance of methods for general time series, and our findings also align with these.alpargu and dutilleul [13] concluded from their simulation study examining the performance of reml, pw and ols for lag (1) time series data over a range of series lengths (from 10 to 200), that reml is to be preferred over ols and pw in estimating slope parameters.cheang and reinsel [20] examined the performance of ml and reml for estimating linear trends in lag (1) time series data of length 60 and 120 (both with and without seasonal components) and concluded that the reml estimator yielded better confidence interval coverage for the slope parameter, and less biased estimates of autocorrelation.smith and mcaleer [12] examined the performance of the nw estimator for time series of length 100 with lags of 1, 3 and 10, and found that it underestimated the ses of the slope parameter.our parameter values were informed by characteristics of real world its studies [4].we planned and reported our study using the structured approach of morris et al. [32] for simulation studies, and we generated a large number of data sets per combination to minimise mcse.implications for practice we found that all methods yielded unbiased estimates of the level and slope change, however, the methods differed in their performance in terms of confidence interval coverage and estimation of the autocorrelation parameter.in practice, however, most analysts will only have knowledge of the length of the time series to guide in the choice of method.in rare cases, knowledge of the likely size of the underlying autocorrelation may be available from a previous long time series study in a similar context, which could help inform their choice.
number of words= 2072
[{'rouge-1': {'f': 0.279683229727516, 'p': 0.9026693227091633,'r': 0.16547738693467337}, 'rouge-2': {'f': 0.19375465205274567, 'p': 0.506,'r': 0.11981718464351007}, 'rouge-l': {'f': 0.34095911870930196, 'p': 0.7774829931972789,'r': 0.21835948644793154}}]
-----------------------------------------------------------------------------------------------------------------------------------
p224:
Extractive Summary:
nevertheless, the vast majority of rcts with time-to-event outcomes are analysed using methods that are maximally powerful under an assumption of proportional hazards, implying timeindependent or ‘fixed’ magnitude treatment effects.when designing trials, as well as the assumption of time-independent treatment effects, there is often an explicit or implicit assumption of constant event rates – constant baseline hazards - used to determine the number of events required and hence the number of patients that need to be recruited for the trial to have the desired power in the sample size calculations methods employed [4, 6].two broad classes of timedependent treatment effects, early effect that attenuates and lag to effect, have emerged as there has been a shift to biomolecular-targeted and immunotherapy-based treatments implemented either alone or as an adjunct to surgical and chemotherapy-based approaches.this circumvents the problems observed with toxicity and resistance to the biological-based agents.however, this mechanism of action via immune system activation is typically associated with a delay of varying months’ duration until any treatment effect may be observed, an example of a lag until treatment effectiveness.recent reappraisals using reconstructed data of published phase iii oncology trials have highlighted how prevalent timedependent treatment effects may be, and that the use of standard analytical approaches assuming time-fixed treatment effects may underestimate the magnitude of, or miss completely.under ph, these two approaches are known to be maximally powerful and provide an asymptotically equivalent test of significance.these include the fleming-harrington (fh) family of weighted lr test statistics which can be differentially weighted to emphasise events that occur earlier, in the middle, or later over the survival time horizon of interest [9].when the assumption of proportionality of the treatment effect is met, the summary hr from a cox ph model is a suitable parameter to provide a clinically meaningful measure of the relative difference between two survival curves.when not met, the clinical interpretation of a single summary measure such as the hr is not clear.an alternative estimand of treatment effect for timeto- event outcomes that does not rely on the ph assumption is the restricted mean survival time (rmst) [14].by combining this p-value and the p-value from the cox ph model, an overall p-value for the combined test (designated pct) can be derived and has the correct distribution under the null hypothesis of equal survival curves.sample size calculations assuming constant, or at the most, piecewise constant event rates were applied even when prior information on the shape of the underlying event rate was available [6].the cox model makes no assumption about this shape whereas parametric modelling approaches, including fractional polynomials [24] or splines [25] model the underlying shape of the baseline hazard function.the properties of three modelling approaches will be examined, the semiparametric cox ph model, the royston-parmar (rp) models utilising flexible restricted cubic splines and parametric models assuming the exponential or weibull distributions.in the methods section we describe the aims of the simulation study, the data-generating models used for the different non-ph scenarios, the estimands of treatment effect and tests of equal survival functions to be compared and the measures used to assess the performance of the analysis methods.characteristics of the design model used in the simulations are detailed in table 1, along with the data- generating models (dgms) for the simulation and analysis models that could be chosen for prespecification in a trial protocol.data-generating processes for simulation scenarios using a weibull data-generation model, three different event rate scenarios were considered by selecting a scale parameter λ and a shape parameter γ such that there was a near zero probability of survival by the end of an administratively imposed time in each scenario.use of more extreme values of the shape parameter may have resulted in far more impactful effects on simulation performance measures, but would not have been reflective of typical experiences with clinical trials.the baseline hazard, cumulative hazard and survival functions for the three event rate scenarios for the control and treatment groups are shown in fig. 1.a binary covariate for treatment group status (xtrt) was simulated from a bernoulli random variable with probability p = 0.5 to mimic 1:1 randomisation.the hazard, cumulative hazard and survival functions for the ph and increasing lag until effect times for the control and treatment groups under the decreasing, constant and increasing event rate scenarios are shown in fig. 2.the period prior to tearly was the period in which the treatment had the anticipated design effect (β = − 0.4), and the period after tearly was when there was no effect of treatment (β = 0).the early effect period lengths investigated were tearly = 3, 10, 20 and 50 months, with the setting tearly = 50 representing ph.supplementary figure s1 presents the hazard, cumulative hazard and survival functions for the ph and early effect that ceases non-ph scenarios for the decreasing, constant and increasing event rates in additional file 1.difference in restricted mean survival time (δ rmst) the rmst μ of a time-to-event random variable t is the mean of min (t, t∗) where the cut off time t∗ is greater than zero.the δ rmst with t∗ taken to be the last uncensored observed event time was obtained by predicting the log cumulative hazard functions for the treatment and the control groups over a grid of time values, transforming into the survival functions and integrating over (0, t∗).standard errors were estimated using the delta method [29].by using the last uncensored observed event time, the same events were used for the estimation of δ rmst as were used for the estimates of hr and tr.¼ exp β1 ð þ in the ph parameterisation of a weibull regression model, the effect of a covariate is multiplicative by a factor of exp(β).if non-proportional hazards are anticipated, landmark analyses can be obtained by undertaking a cox analysis conditional on individuals being event free at the pre-specified lm time point tlm.to accommodate a non-constant hazard, a useful extension is the piecewise exponential model which allows the time scale to be split into an arbitrary number of intervals each of differing lengths, with a constant hazard rate assumed within each interval.weibull accelerated failure time (aft) model an alternative parameterisation of the weibull model is the accelerated failure-time model which has the parameterisation ln(ti) = xiβ + ϵi where ϵi has an extreme value distribution.tests of equal survival functions many tests of difference between two survival curves have been proposed that aim to achieve acceptable power under ph and under anticipated non-ph patterns whilst maintaining type i error rates close to the nominal level.in this simulation we included tests from two broad categories of test statistics - weighted variants of the lr test designed to improve power under particular non-ph patterns, and omnibus global tests that combine results of several individual tests of significance in an attempt to improve power across a wider range of non-ph patterns.we compared the performance of estimators from an analysis model against the design model knowing that the design model would not necessarily accord with the data-generating model.power, the first performance measure, was obtained as the proportion of simulations where the p-value was less than the nominal significance level α.the anticipated power specified at the design stage was 80%.the second performance measure was the scaled treatment effect (ste).the scaling was calculated as ð1−mean½chrþ=ð1−hrdesignþ 100 for the hr estimands, as ðmean½ctr−1þ=ðtrdesign −1þ  100 for the tr estimand, and as ðmean½δrdmstþ =δrmstdesign  100 for the δ rmst with the δrmstdesign value obtained empirically from a large n = 250,000 simulation of the design setting.the final measure, coverage was calculated as the proportion of simulations in which the 100 × (1 − α)% confidence interval around analysis model ^β included the anticipated β from the design model.this allowed assessment of whether the empirical coverage rate approached the desired rate.the anticipated coverage specified at the design stage was 95%.number of simulations we generated 2000 simulated datasets for each scenario.in this worst-case scenario, the mcse for the simulation would be 1.1%.results type i error prior to comparing performance measures such as power for scenarios with a known treatment effect, it is important to assess that analytical approaches are controlling the type i error level at the same or similar nominal value when there is truly no effect.we compared that empirical type i errors were maintained reasonably well and similar to other simulation studies [33, 34].additional detail of the type i error assessment is presented in additional file 1.for all methods, there was an appreciable loss of power in these non-ph scenarios.conversely, the losses in power observed under a decreasing event rate in the presence of a lag until effect were magnified as a result of more events occurring during the period where the treatment had no effect.this pattern of relative power loss with non-constant event rates was observed for the hr, tr and δ rmst.scaled treatment effects (ste) estimates of regression model approaches the middle panel of fig. 3 presents the ste results.in the scenario of no lag until treatment effect (tlag = 0) estimates close to the design model values are observed except for the hr from the pe2 model and the tr from the aft model.for these two estimators, an increasing event rate resulted in a lower ste under ph whilst a decreasing event rate resulted in a higher ste.the presence of non-constant event rates has less impact on this performance measure.in the scenario equivalent to ph, the lr, cox, versatile and combination tests achieved power values close to the design model value of 80%.the power dropped swiftly with increasing lag times.early effect that ceases the early effect that ceases non-ph scenario is the inverse in treatment effect timing to the lag until treatment effect.under an increasing event rate, some reduction of the losses observed under the constant event rate were observed.results are described in more detail in the supplementary results section in additional file 1.non-proportionality of treatment effects has been increasingly observed in clinical trials [16, 35].for these trials, non-constant event rates will also be more likely to be observed, yet the interplay between non-ph and the shape of the baseline hazard rates has received little attention before now, despite the reasonable anticipation that it could also to have important design implications for clinical trials.the fh late test was more powerful when there was longer lags until effect, but was less powerful under shorter lags and ph scenarios more likely to be encountered in trials compared to the versatile test.as part of cross-pharma non- proportional hazards (nph) working group, lin et al. (2020) compared nine tests of survival curve difference in the presence of non-ph covering the lr and weighted lr tests, weighted kaplan-meier based tests (incorporating the rmst) and combination tests [38].there is substantial overlap between the tests included in this simulation study and the three other studies, with similar focus on early (treatment effects that cease) and late (lag until treatment effect) forms of non-ph.the δ rmst has an additional advantage of being a summary measure of survival time distribution that does not rely on the ph assumption although it does require specification of the cutoff timepoint.by design, the last uncensored event would have been expected to occur at a time very close to the maximum follow up time.the impact of non-constant event rates in the presence of non-ph was to partially diminish or further exacerbate losses in power and treatment effect magnitude.designing trials with non-constant event rates in the presence of non-ph simulation studies can only ever include a limited range of scenarios.we restricted attention to simplified forms of non-ph - piecewise constant hrs with a single change point - comparing ph with early and late forms of non-ph.our simulations also featured almost complete follow up of all events before undertaking analysis which, whilst unrealistic in some applications, resulted in almost identical numbers of total events being observed in each scenario, and hence provided a fair basis for comparison.it is a desirable strategy to design trials to use analysis methods that can accommodate delayed treatment effects, or early treatment effects that cease if these are to be anticipated with the treatment under study.our simulations provide some guidance on this choice.
number of words= 2016
[{'rouge-1': {'f': 0.32333796513593677, 'p': 0.8594736842105264,'r': 0.1991248206599713}, 'rouge-2': {'f': 0.19308028868027263, 'p': 0.41310850439882696,'r': 0.12598086124401914}, 'rouge-l': {'f': 0.3502489796917187, 'p': 0.6895652173913043,'r': 0.2347398843930636}}]
-----------------------------------------------------------------------------------------------------------------------------------
p225:
Extractive Summary:
age-related macular degeneration (amd) is a leading cause of blindness, especially for people in developed countries older than 60 years [1, 2].amd has two late stages: choroidal neovascularization (cnv) and geographic atrophy (ga).here we consider ga, which is thought to be the end stage of amd when cnv does not develop [3] and which is responsible for vision loss in approximately 20% of all patients with amd [4].more than five million people are estimated to be affected by ga worldwide, a number which is supposed to increase with the aging of the population [2].to date, there is no effective standard treatment available [5].ga is defined by atrophic lesions of the outer retina resulting from loss of retinal pigment epithelium (rpe), photoreceptors and underlying choriocapillaris (reviewed by [6]).these areas enlarge with time and lead to irreversible loss of visual function [7].a relevant clinicalmeasure of disease progression is the eye-specific size of ga which can be quantified based on imaging techniques including color fundus photography, spectral domain optical coherence tomography imaging, or fundus autofluorescence (faf) imaging [8, 9].a better understanding of the risk factors that accelerate ga size progression is necessary for the development of treatment options, in particular for the design of (interventional) clinical trials.to date, empirical evidence on ga size progression is usually collected through longitudinal observational studies (e.g. [10–12]).in these studies, it is essential to analyze ga size trajectories over time using an adequate statistical model.specifically, in the absence of a randomized study design, data analysis needs to account for confounding issues as well as correlation patterns, for instance when both eyes of a patient are included in the study.in the latter case, the correlations between the eyes within one patient need to be incorporated as well as the correlations due to repeated measurements over time.the aim of this analysis is to systematically derive a statistical approach for modeling ga size in observational ophthalmologic studies.as will be demonstrated in the following sections, the proposed approach generalizes various statistical models for ga size progression that have been used in previous publications (see below).special focus will be given to the following issues, which are considered to be of particular importance for the planning and design of future interventional trials: (i) transformation of ga size.before model fitting, it is important to consider whether the response (here, ga size) should be transformed.finding an appropriate transformation can provide information about the underlying natural processes that drive the progression of ga.in recent publications on ga size progression, there has been an ongoing discussion about the optimal choice of transformation [11, 13–15].this implies a constant enlargement of ga size over time.examples of this modeling approach can be found in [13, 14].the second approach assumes a quadratic enlargement of the lesion size.this is motivated by the thought of circular atrophic lesions that constantly enlarge with their radiuses [11, 15].the third model type is an exponential model in which atrophic lesions enlarge exponentially.compared to a linear growth model, dreyhaupt et al. [13] found that the assumption of exponential growth led to improved model fits.to this purpose we fitted a model to a training data set excluding the last observation while performance was measured on the last observation.the root mean squared difference between observed atrophy sizes and the mean predicted atrophy sizes was avg(( ¯ˆ y − y)2) = 1.67mm2.discussion despite a high prevalence and extensive research efforts, there are currently no effective standard treatment options for ga.it is therefore essential to develop accurate models for disease progression that enable researchers to efficiently plan and design clinical trials.in this article, we presented a comprehensive framework for modeling the course of ga size progression in longitudinal observational studies.our modeling approach was derived from a linear enlargement model using transformed ga size as response variable.as shown in the results section, the resulting model can be embedded in the class of linear mixed-effects models [27], allowing for the incorporation of risk factors, confounding variables, and measurements taken repeatedly from the same patients and eyes.since the assumption of linear enlargement imposes numerous restrictions on the model parameters, it is necessary to adapt standard (unrestricted) mixed-effects modeling approaches to the specific structure of the proposedmodel.to this purpose, we developed an algorithm for ga size modeling that can be implemented using readily available software for fitting linear mixed-effects models.to obtain the best transformation of ga size, we conducted a systematic search within the class of box-cox transformation models that included both parametric and non-parametric approaches.our experiments yielded an optimal transformation that was close to the square-root function, thereby justifying earlier modeling strategies that assumed linear trajectories of square-root transformed ga size over time [18].of note, the square-root transformation has a straightforward interpretation in terms of a linear enlargement of the atrophy radius [15].a convenient feature of the proposed modeling approach is that it yields estimates of the disease age of the eyes at study entry.this is important because patients can only be included in trials when the disease has already manifested.when applied to the analysis data set consisting of patients included in the fam-study, disease age at study entry was estimated to range between 3.5 and 13.4 years (model (6)).these estimates are in line with estimated prevalence values reported in the literature [4], but the resulting ages of disease onset were smaller than previously modeled ages using data partly from the same study [28].since the proposed modeling approach employs a transformed response variable, care has to be taken when making predictions of future values of atrophy size.as argued in the results section, predictions with a naive back-transformation may show a bias due to the nonlinearity of the square-root function.to address this issue, we proposed a sampling approach that allows for drawing valid conclusions and making undistorted predictions of ga size on its original scale.in the analysis data set, estimated expected ga size values derived fromthe proposed model deviated 1.10mm2 on average from the respective observed values.generally, the model proposed here allows for performing statistical hypothesis tests on a set of risk factors suspected to accelerate or slow down ga size enlargement.this strategy was illustrated in the results section, where an analysis of a ga patient sample of the famstudy identified significant interaction effects between hypercholesterolemia, hypertension and time.although a number of studies have shown a link between cardiovascular risk factors and amd, the role of hypertension, atherosclerosis, high bmi, diabetesmellitus, higher plasma fibrinogen and hyperlipidaemia remain equivocal owing to inconsistent findings (reviewed in [29]).high blood pressure is shown to be associated with lower choroidal blood flow and disturbed vascular homeostasis [30].since perfusion deficits in the choriocapillaris, the innermost layer of the choroid, are associated with future ga progression [31], an associate between hypertension and increased ga progression appears biologically plausible.regarding the association of hypercholesterinemia and decreased ga progression, the biological plausibility remains elusive.the majority of previous studies did not find any relationship between systemic cholesterol levels and progression to early amd, ga or namd (reviewed in [29]), although two studies found an association between serum cholesterol on the development of late stage amd [32, 33].interestingly, one of these studies reported that serum cholesterol levels have a protective effect on the development of namd, while they are a risk factor for the development of ga [32].these observations apparently are in contrast to our results; however, there is evidence that different mechanisms may be involved in driving ga enlargement than those increasing the risk of de novo ga development [6].further validation of the risk factors, especially on an external data set, is necessary while it has been established that so-called nascent ga progresses to manifest ga [34], the trajectory of early ga – prior to the minimum lesion size requirement for clinical trials (e.g., 2.5mm2) – is poorly understood.the information derived by this modeling strategy can be used to design future intervention studies, for example regarding the stratification of patient groups and the definition of inclusion criteria.of note, the proposed modeling approach is not restricted to established epidemiological covariables like hypertension but may also incorporate novel markers of disease progression such as patientreported outcome measures [35], digital biomarkers, and machine-learning-based scores derived from structural imaging data [36].the proposed model constitutes a flexible framework to systematically investigate the transition from intermediate to late amd in large observational studies such as themacustar study (clinicaltrials.gov: nct03349801) [37].conclusions we have provided a comprehensive framework for modelling the trajectories of uni- or bilateral ga size progression in longitudinal observational studies.our analysis shows that a square-root transformation of atropy size is recommended before model fitting.the proposed modelling approach allows for the estimation of age-of-onset, identification of risk factors and prediction of future ga size.
number of words= 1431
[{'rouge-1': {'f': 0.2884178325084759, 'p': 0.8422772277227724,'r': 0.174}, 'rouge-2': {'f': 0.18889897953735302, 'p': 0.443134328358209,'r': 0.12003335557038025}, 'rouge-l': {'f': 0.3130734005446885, 'p': 0.7035877862595419,'r': 0.20132911392405065}}]
-----------------------------------------------------------------------------------------------------------------------------------
p226:
Extractive Summary:
hometime is graded, with longer home-time being associated with higher post-stroke disability [2–6], unlike other outcomes available in administrative data such as mortality.in prior studies, a substantial range of statistical methods have been used to analyze hometime including negative binomial regression, ordinal logistic regression, median regression, linear regression, spearman rank correlation, t-test and chi-square analyses, propensity score matching, and categorizing hometime into quartiles [3– 7, 11–13].we aimed to study the use of random forests regression for modelling 90-day hometime in a populationbased cohort of stroke patients, and to determine the relative importance of several covariates in the prediction of hometime using random forests regression.advantages of random forests for the analysis of hometime one of the biggest advantages of random forests is that they do not make any distributional assumptions about underlying data structures, meaning they can be used on data which exhibiting highly unusual distributions, such as those in hometime.methods cohort identification using the canadian institute for health information (cihi) discharge abstract database (dad) we identified all patients with a main diagnosis of stroke (ischemic or intracerebral hemorrhage) admitted to an acute care hospital in ontario between april 1, 2010 and december 31, 2017.ninety-day hometime calculation we calculated 90-day hometime using linked data from the following sources: dad (inpatient hospitalization), national ambulatory care reporting system (emergency department), the national rehabilitation reporting system (rehabilitation), the continuing care reporting system (complex continuing care or long-term care), and the ontario registered persons database (mortality data).data linkage occurred through unique encoded identifiers at ices; these datasets have been validated extensively for research purposes [25].for patients who survived to day 90, 90-day hometime was calculated as 90 minus the sum of length(s) of stay in ed, acute care, rehabilitation, and long-term care.for example, a patient with whose sum of lengths of stay in healthcare institutions = 20 days and died on day 70 would have a hometime of 50 days.patients who died during the index admission have, by definition, hometime of 0 days.hometime accumulation does not have to be continuous.the data sets used for this study were held securely in a linked, de-identified form and analyzed at ices.the cohort selection flow chart is presented in figure a1.we removed 202 observations with small cell counts upon cross tabulation of baseline characteristics to avoid potential re-identification of individuals as per ices policy; aggregate demographics of these patients are given in table a.2.baseline characteristics of the final cohort are given in table 1.at day 90, 68.54% of patients were home and 17.49% of patients had died.the pairwise correlation between all covariates is given in table a.3.some of the predictors exhibited moderate correlation with the highest magnitude being between passv score and admission via ambulance (ρ = − 0.45); however, as random forests regression is robust to multicollinearity all variables were included as candidates in the model.using random forests regression to predict 90-day hometime the random forests model predicted 90-day hometime with reasonable accuracy (adjusted r-squared = 0.3462).extreme values of hometime, both low and high, were predicted with the least accuracy.low hometime values were systematically overestimated and high hometime values were systematically under-estimated (figure a.2).interpretation of random forests model whether determining variable importance using model accuracy (out-of-bag error estimation) or node purity (gini index), four of the top five ranked variables were the same: frailty, stroke severity, age, and ambulance use (fig. 2).influence of individual covariates on hometime predictions using both one and two-way partial dependence plots, we examined the relationships between the four co-variates ranked of high importance in predicting hometime.these partial dependence plots are interpreted as the relationship between the predictor variable(s) and 90-day hometime after averaging out the effects of all other predictors.frailty and stroke severity were the top predictors of hometime, and the associations were non-linear. for patients with low or moderate risk of frailty (scores ≤15), as frailty increased predicted hometime decreased; however, for patients at high risk of frailty (scores > 15), there was little change in predicted hometime as frailty score increased (fig. 3).for higher stroke severity, estimated hometime remained relatively constant regardless of frailty.age displayed a non-linear relationship with hometime, with predicted hometime decreasing with increasing age, especially beyond age 45 (fig. 3).the s-shaped relationship between hometime and stroke severity also persisted across all ages (fig. 7).patients presenting via ambulance had less hometime than those who did not across all ages, but the difference in predicted hometime between the two groups increased with age (fig. 4).discussion we found that a random forests regression model predicts hometime with reasonable accuracy without predicting implausible values.the characteristics of these two groups may be different and using a single model to predict these outcomes may not be ideal.interestingly, the model also systematically under predicted hometime values for patients with high hometime.our findings are consistent with prior work showing that frailty [27], stroke severity, [12, 13] and age [6, 12, 13] are associated with disability after stroke, but the association between these variables and hometime specifically is not yet well understood.sex was not associated with hometime, which has been previously reported by some studies [5] but not others [6, 12].we did not see a difference in hometime based on thrombolysis use in this study.this is consistent with our understanding of the effects of multi-morbidity on stroke outcomes [28, 29].there are limitations to using random forests.random forests are complex, consisting of hundreds of regression trees.this means that 1) a large amount of computation power and time are needed to generate them, and 2) they don’t produce readily interpretable coefficients like those produced in linear regression or other parametric models.predictive accuracy was lowest for extreme values of hometime which may warrant future study.
number of words= 935
[{'rouge-1': {'f': 0.3508877736217153, 'p': 0.7972727272727274,'r': 0.22494393476044852}, 'rouge-2': {'f': 0.19923679539751518, 'p': 0.3776923076923077,'r': 0.1353061224489796}, 'rouge-l': {'f': 0.34816939692664545, 'p': 0.6446268656716418,'r': 0.23849015317286654}}]
-----------------------------------------------------------------------------------------------------------------------------------
p227:
Extractive Summary:
background systematic reviews are essential to locate, appraise and synthesize the available evidence for healthcare interventions [1].citation-screening is a key step in the review process whereby the search results identified from searches often performed across multiple databases, are assessed based on strict inclusion and exclusion criteria.the task is performed through an assessment of a record’s title and abstract (what we term ‘citation’).the aim is to remove records that are not relevant and determine those for which the full-text paper should be obtained for further scrutiny.this is no easy task.one study found a mean of 1781 citations were retrieved in systematic review searches (ranging from 27 to 92,020 hits retrieved from searches), from which a mean of 15 studies were ultimately included in each review: an overall yield rate of only 2.94% [2].in part driven by the resources required to undertake citation-screening, reviews are typically time and labour intensive, taking an average of 67.3 weeks to complete [2] (borah 2017).moreover, the challenge of locating relevant evidence for reviews is becoming ever greater: over the last decade, research output has more than doubled, and approximately 4000 healthrelated articles are now published every week [3–5].new approaches are needed to support systematic review teams to manage the screening of increasing numbers of citations.one possible solution is crowdsourcing [6].crowdsourcing engages the help of large numbers of people in tasks, activities or projects, usually via the internet.such approaches have been trialled in a number of health research areas, using volunteers to process, filter, classify or categorise large amounts of research data [7, 8].more recently, the role of crowdsourcing in systematic reviews has been explored, with citation-screening proving a feasible task for such a crowdsourced approach [9–12].cochrane, an international not-for-profit organization and one of the most well-known producers of systematic reviews of rcts, is an early adopter of the use of crowdsourcing in the review process.since the launch of their cochrane crowd citizen science platform in may 2016, over 18,000 people from 158 countries have contributed to the classification of over 4.5 million records [13].to date, crowdsourcing experiments in citationscreening have often focussed on identifying studies for intervention reviews, with included studies often limited to randomised or quasi-randomized controlled trials [9– 11].whilst this supports traditional systematic reviews concerned with evidence of effectiveness, an increasing number of reviews in health and healthcare now address research questions requiring the identification and synthesis of both quantitative and qualitative evidence [14– 16].one study by bujold and colleagues used a small crowd (n = 15) to help screen the search results for a review on patients with complex care needs.the study was not a validation study and so does not provide crowd accuracy measures; however, the authors concluded that crowdsourcing may have a role to play in this stage of the review production process, bringing benefit to the author team and crowd contributor alike [17].aims and objectives the aim of this feasibility study was to investigate whether a crowd could accurately and efficiently undertake citation-screening for a mixed studies systematic review.our objectives were to assess: (1) crowd sensitivity, determined by the crowd’s ability to correctly identify the records that were subsequently included within the review by the research team (2) crowd specificity, determined by the crowd’s ability to correctly identify the records that were subsequently rejected by the research team (3) crowd efficiency, determined by the speed of the crowd in undertaking the task and the proportion of records which were sent to crowd resolvers for a final decision (4) crowd engagement, determined by qualitative assessment of their satisfaction with the citationscreening task and readiness to participate methods the systematic review this study was embedded within a mixed studies systematic review exploring training for healthcare professionals in intrapartum electronic fetal heart rate monitoring with cardiotocography [18].cardiotocography is widely used in high-risk labours to detect heart rate abnormalities which may indicate fetal distress, in order to intervene or expedite birth as required.the aim of the review was to examine the effects of training for healthcare professionals in intrapartum cardiotocography and to assess evidence for optimal methods of training.all primary empirical research studies that evaluated cardiotocography training for healthcare professionals were eligible for inclusion in the review, irrespective of study design.crowdsourcing platform the citation-screening task was hosted on the cochrane crowd platform [19].this citizen science platform offers contributors a range of small, discrete tasks aimed at identifying and describing evidence in health and healthcare.there is no requirement for contributors to have any relevant background in research or healthcare: anyone with an interest in helping may volunteer to do so.the main activities available to contributors to cochrane crowd are tasks aimed at identifying or describing reports of rcts.in these, contributors are asked to look at a series of citations (titles and abstracts of journal articles or trial registry records) and classify them as either reporting an rct or not reporting an rct (see fig. 1).cochrane crowd employs two strategies to ensure accuracy of contributor screening decisions.our findings are inevitably limited by their dependence on citation-screening of search results from only one systematic review.different search results from different reviews may generate different sensitivity and specificity estimates in crowdsourced citation-screening.however, it is notable that there is little published evidence on how screening decisions vary: different expert review teams could also be anticipated to make different screening decisions when presented with the same set of search results.with a complex mixed studies review, the likelihood of human error, whether from the crowd or the ‘expert’ review team, is further increased: there is often limited information in abstracts to judge topical relevance.it is not clear what an acceptable level of crowd accuracy is, to be able to confidently use crowdsourcing without comparing crowd decisions to those of an expert review team.for reviews of evidence of effectiveness, there may be very little tolerance for divergence of decisions.for other reviews – such as the current example on training in the use of cardiotocography – overall review results may be little influenced by the inclusion or exclusion of a few studies of marginal quality and depth of information.the level of error deemed to be acceptable in relation to a specific degree of time saving may depend on both the type of review being conducted and the breadth and volume of potentially includable studies.these are factors that require determining if crowdsourcing in this way is to become an acceptable model of research contribution.finally, in terms of the generalisability of these results we should address the characteristics of the crowd participants.whilst we recruited a non-selective crowd (contributors did not need to have any topic knowledge or expertise to be able to participate) we can see from the survey responses that many participants did have a healthcare background which may have made the task easier.in addition, in order to be able to participate in this study, potential participants had to have completed 100 assessments in another cochrane crowd task, rct identification.the rct identification task on cochrane crowd requires contributed to complete a brief training module made up of 20 practice records.while this task is different from the study task, it does mean that the participants were already familiar with screening citations within cochrane crowd.we therefore must exercise caution in generalising that a crowd consisting of either fewer healthcare professionals or those without any experience of screening citations, would perform as successfully.finally, the very positive responses from this study’s participants were highly encouraging.however, successful, widespread implementation of crowdsourcing in this way brings with it a number of important ethical considerations.providing meaningful opportunities for people to get involved with the research process must be matched by appropriate measures of acknowledgement and reward.in this study named acknowledgement in the review proved a suitable reward but as crowdsourced tasks become more involving or challenging, as they no doubt will, it stands that the requisite reward should be greater.this then potentially presents a conflict with current academic publishing guidance, with criteria for authorship often requiring full involvement of all authors across all or many parts of the study.in some circumstances, payment might be appropriate, yet micro-payment or piece-rate models such as those used by amazon mechanical turk have come under fire in recent years with studies revealing poor working conditions of an “unrecognised labour” [32, 33].as crowdsourcing in this way becomes more accepted as an accurate and efficient method of study identification, these ethical factors will need to be understood and addressed in parallel, for the benefit of both contributor and task proposer alike.conclusion in support of a complex mixed-studies systematic review, a non-specialist crowd tasked with undertaking citation-screening performed well in terms of both accuracy and efficiency measures.importantly, crowd members reported that they enjoyed being part of the review production process.further research is required to develop effective approaches to pre-task training for contributors to crowdsourced citation-screening projects, the refinement of agreement algorithms, and establishing ‘acceptable’ levels of performance (for example, by investigating the variation in performance by both crowd and ‘expert’ screening teams, such as clinicians).review teams, particularly those engaged in locating a broad range of evidence types, face significant challenges from information overload and long production times.
number of words= 1508
[{'rouge-1': {'f': 0.33199627875956683, 'p': 0.7681132075471697,'r': 0.2117624521072797}, 'rouge-2': {'f': 0.1942531884840763, 'p': 0.3728391167192429,'r': 0.13134185303514379}, 'rouge-l': {'f': 0.3283237674589347, 'p': 0.6268181818181817,'r': 0.22241057542768275}}]
-----------------------------------------------------------------------------------------------------------------------------------
p228:
Extractive Summary:
however, the average number of rooms per member nearly doubled from 0.61 in 1967 to 1.13 in 2017–18.land ownership decreased from 1975 (74%) to 2002 (53%) and subsequently increased (67%).we observed a large increase in sanitation (5% in 1967 to 99% in 2017–18).only a small proportion of households (11%) had electricity in 1975 but nearly all (97%) did by 2017–18.ownership of appliances like refrigerator (1% in 1967 to 71% in 2017–18), television (1% in 1975 to 92% in 2017–18) and computer (1% in 2002 to 31% in 2017–18) also increased over time.by 2017–18, nearly all households (96%) possessed a telephone (fixed or cell phone).harmonized index construction table 3 shows loadings on each indicator for the harmonized index in the first column and, then indices constructed for sensitivity analyses.the first principal component of the harmonized index explained 32.4% of the variance in the pooled data.standardized loadings of the principal components are positive for ownership of each individual asset except radio (− 0.05).the largest positive loadings were for television (0.34), high quality floor (0.33), sanitary installation (0.33) and electricity (0.33).the lowest loadings were for record player (0.004), house ownership (0.03) and land ownership (0.07).cronbach’s alpha for the items and kaiser-meyer- olkin (kmo) measure of sampling adequacy were 0.84 and 0.91 respectively.visual inspection of histograms (supplementary figure 1) of the benchmark harmonized index (pooled, and separately by study wave) indicates clumping (limited range of data values) for early study waves (1967, 1975 and 1987).the index displayed truncation at the lower tails of the distribution in the early study waves but not for later ones or for the upper tails of the distributions.as households acquired additional assets the mean increases across study waves.the mean harmonized index score in the observed sample increased from − 3.76 in 1967 to 1.92 in 2017–18 while the sd increased from 0.91 in 1967 to 1.51 in 1987 but then declining to 1.09 by 2017–18.the inter-quartile range (3rd quartile – 1st quartile) of harmonized index scores increased from 1.35 in 1967 to 2.01 in 1987, after which it decreased to 1.26 in 2017– 18.most cohort members (99.8%) observed in 2017–18 experienced gains in the absolute level of the harmonized index for their household in adulthood relative to early life (fig. 1).for those who did not participate in 2017–18, we observe a similar increase to the wave in which they were last observed.the harmonized index is positively associated with measures of schooling, another important indicator of socio-economic status: maternal schooling (with 1967 or 1975; r = 0.16), paternal schooling (with 1967 or 1975; r = 0.10) and attained schooling in adulthood (with 2015–16 or 2017–18; r = 0.35).sensitivity analysis cross-sectional indices constructed separately for each individual study wave using the same set of assets (s1; r ≥ 0.91) and urban-rural stratified indices for each of the final two waves (r ≥ 0.90) were also correlated with the harmonized index.we display the loadings of these different cross-sectional indices in table 3 and their correlations with benchmark harmonized index in table 5.an index including newer available assets (s2; video player, sound system, computer, telephone, washing machine and sewage) was correlated (r ≥ 0.91) with the benchmark harmonized index.the harmonized index was robust to omission (s3) of any pair of assets (r ≥ 0.97; supplementary table 2), any one or two study waves (r ≥ 0.96; except for omission of 1967 and 1975 where r = 0.91; supplementary table 3) and joint omissions of each single asset with each study wave (r ≥ 0.95; supplementary table 4).these results suggest that the index is stable even when we do not include assets or study waves such that an index created from a sparser dataset would be largely similar to the benchmark index.alternative specifications of the correlation matrix and factor extraction methods (s4) on the pooled sample indicated a high correlation (table 5) with the harmonized index based on pca (range: 0.96– 1.00).asset indices constructed by re-specifying housing characteristics (as described in supplementary table 1) into three categories (s5; low/medium/high) were also highly correlated (pca: 0.96, efa: 0.92) with the original index.comparing cross-sectional indices for survey waves when cohort members were in adulthood (2002, 2015– 16, 2017–18), ownership of refrigerator (loadings: 0.35 to 0.40), television (loadings: 0.34 to 0.37) and high quality flooring (loadings: 0.31 to 0.38) have the highest loadings.in general from 1967 to 2017–18, the loadings of housing characteristics such as the roof and walls decrease while those of assets increase.however, items such as house ownership (2015–16; loading = 0.37) and land ownership (loading = 0.36) have high loadings for the urban sample.ownership of a television (loading = 0.35) and high quality floor (loading = 0.36) have high loadings within rural sample.discussion we attempted to develop a temporally harmonized asset index from consistently administered surveys in a cohort setting.such an index can be used to study the impact of socio-economic mobility on measures of human capital in adulthood.for cohort members followed over a period of 50 years, an asset index created by pooling study waves shows an increase in absolute wealth over time.the constructed harmonized asset index was robust to various sensitivity analyses.our analysis demonstrates wealth gains over time in a birth cohort from a lmic setting.a harmonized index for a birth cohort is an improvement over repeated cross-sectional surveys because it permits the estimation of both the population mean trajectory, quantifying cross-sectional variation within the cohort and understanding trends in household wealth of the analytic sample over the followup period.additionally, a harmonized index allows examination of trajectories of absolute wealth mobility over the life course and sensitivity of timing of wealth gains for human capital [34].the results suggest a divergence or increased inequality in household wealth from 1967 to 1987 followed by a partial convergence in 2002.the observed pattern could reflect the transition of cohort members residing in the villages (typically in their parental homes) until adolescence and then the process of forming their own households.the period from 1987 to 2002 was marked with economic changes such as the transition from agricultural to non-agricultural jobs, increased access to electricity, piped water and increased ownership of electronic appliances such as televisions [22].a cross-sectional analysis of item loadings demonstrate how importance of housing characteristics and assets in differentiating households changes over time.the temporally harmonized index was highly correlated with cross-sectional indices for each study wave on its own.cross-sectional comparisons of urban and rural households indicate how house ownership differentiates households in urban areas while housing characteristics and ownership of electronic goods are better differentiators in rural areas.this potentially reflects the higher cost of owning houses in urban areas rendering it a stronger indicator of wealth.similar to a study from zimbabwe, we observed correlations of the pooled index with indices stratified by rural (and urban) residence [35].despite the few observed differences, the loadings are similar in magnitude over time and between settings for most items included such that developing a temporally harmonized index was feasible in our sample.the index displayed internal consistency (or monotonicity) such that loadings for all assets, except radio, were positive [11].the largest loadings were for electricity, television, high quality flooring and sanitary installation.descriptive analysis shows that ownership of radios increased until 1987 and subsequently decreased reflecting changing consumption patterns.
number of words= 1198
[{'rouge-1': {'f': 0.3696954477452447, 'p': 0.77,'r': 0.24324052590873937}, 'rouge-2': {'f': 0.2219677350766515, 'p': 0.405423197492163,'r': 0.15281733746130033}, 'rouge-l': {'f': 0.3742231605555181, 'p': 0.5957731958762886,'r': 0.27278330019880714}}]
-----------------------------------------------------------------------------------------------------------------------------------
p229:
Extractive Summary:
unlike other study types, there is no universal checklist to ensure vignette studies are understandable, transparent and of high quality [38].the aim of this scoping review was to locate, methodologically evaluate, and synthesise the contribution of experimental vignette studies that seek to identify drivers of unwarranted variations in the delivery of healthcare.methods we conducted a scoping review in accordance with prisma-scr guidelines [39].study selection we used a phased approach to title/abstract screening.next, js manually screened the remaining titles and abstracts to exclude papers that did not examine healthcare variations using vignettes, or that measured rather than sought to identify drivers of variation.jb double screened 10% of the full sample to confirm accuracy and clarify inclusion criteria.inter-rater agreement was assessed using cohen’s kappa for a subset of papers.prior to consensus discussions, kappa was 66%, which can be interpreted as moderate agreement [41].for both title/abstract and fulltext screening, all differences were resolved by discussion.methodological assessment there is no existing standardised approach to evaluating the robustness of experimental vignette studies.we therefore conducted a review of methodological reviews of vignette studies within and beyond healthcare.within this framework, we identified factors considered important in maximising internal and external validity of experimental vignette studies in two broad areas: (a) the design and description of vignettes used, and (b) the wider study design and methods within which vignettes are employed, as outlined below (table 1) [46, 50, 52].we use the term vignette to refer to the overall description or depiction of each situation as presented to the participants.for four considerations (vignette credibility, evaluation, description and study analysis) the scores primarily reflected the extent to which sufficient methodological detail was provided.for two considerations in the wider study design (concealment and sampling/response), the scores reflected both provision of methodological detail and the quality of study execution.two considerations were not given a score; mode of vignette delivery and whether multiple vignettes were provided.both these considerations - while important for researchers to consider when designing vignettes – are not, intrinsically, markers of quality.registration as a methodological scoping review, the study was not eligible to be registered on prospero.results study selection and characteristics we identified 23 papers and 21 unique studies for inclusion within the review (see prisma flowchart, figure 1).drivers of variation were also examined in relation to patient gender (n = 9), socioeconomic circumstances (n = 7) and age (n = 9).methodological assessment we assessed ten studies as being of good methodological quality (table 3).we focused on these studies in exploring how vignettes may produce insights into drivers of variations of care.vignette design credibility most studies provided comprehensive descriptions of how vignettes were constructed.for example, burt et al. based vignettes on previously video-recorded patientclinician encounters [42].modality in six studies, vignette information was purely textual; here, manipulated characteristics and their variations were therefore stated clearly to participants.in interactive modalities the content of the vignette could vary across participants, as the vignette evolved in response to respondent behaviours, such as the questions they asked [42].evaluation three high quality studies comprehensively reported how their vignettes performed, most commonly in tests of credibility [58, 60, 68].mckinlay et al., hirsh and lutfey et al. used post-study quantitative surveys of participants to find out whether vignette 'patients' were typical of the real patients they encountered [64, 66, 68].mckinlay reported that 91% of participants viewed the vignettes as typical of their patients [68].burt et al reported the expert clinical raters’ scores of their high and low performing vignette consultations as an indication of their credibility [58].vignettes performance was evaluated in other ways too.for example, sheringham et al. had developed an online interactive vignettes application specifically for the study [72].the authors quantified system errors that occurred when the software could not answer a question entered by a participant.system errors occurred on average in just under 5% of all participant interactions.analysis was adjusted to examine whether system errors could have been responsible for the findings and this was found not to be the case [72].access to video or interactive vignettes in a journal article is not straightforward, but five out of the nine video or interactive papers did include sufficient aspects (e.g. using video stills [60]) or online links (e.g. to a multimedia demonstration [72]) to enable readers to judge vignettes’ quality and credibility.wider study design concealment while eleven papers reported that the study’s purpose was not divulged to participants, only seven high quality described strategies they actively employed to conceal it.lutfey et al. (2009) alerted half their sample to the potential of chd as a diagnosis; primed doctors made different decisions on the same vignettes to those not explicitly primed [66].sampling and response rate risk of bias was common due to sampling flaws, low or unreported response rates.of these, three reported response rates of less than 30%.insights were also on occasion limited due to challenges of recruitment.it is clear that research assessments of decision-making recorded in quiet environments without time constraints do not replicate this pressure.one study provided evidence that bias is more likely to arise in high pressure situations: increasing cognitive load through the provision of a competing task to do under time pressure altered ethnic inequalities in physicians’ prescribing patterns [57].this supports not only the notion that cognitive load leads to variations in care, but that such variations may be systematically biased against certain patient characteristics.of note, additional cognitive load altered inequalities in prescribing in different ways for male and female physician, highlighting the complexity of contextual influences on disparities in care [57].combining insights from observational and vignette data may be particularly helpful in clarifying the relevance of research findings to policy or practice.as an example, in an initial descriptive analysis of case records, begeer et al. identified that minority ethnic groups were under-represented in autism institutions [54].for example, burt et al. noted that certain minority ethnic groups report lower patient experience scores compared to the majority population across a wide variety of settings [58].they found south asian respondents consistently rated gps’ communication skills higher than white respondents, thus challenging the hypothesis that poorer reported experiences of care in south asian patients relative to white british patients arise from higher expectations of care [58].as noted above, vignette approaches may provide insights into decision-making processes.obtaining generalisable evidence on this is rarely feasible in real life due to the specific dynamics of individual clinician-patient pairs.adams et al. asked physicians to reflect on video consultations about depression, analysing these narratives in detail to identify micro-components of clinical decision making [53].by applying a novel methodological framework for conducting vignette studies to this review, we demonstrated that their insights have been limited in many cases by a lack of evaluation of the credibility of vignettes and flaws in their wider study design.what this study adds the application of a novel methodological framework to appraise vignette studies illustrated the variation in quality and conduct of such studies.experimental vignette studies can help with this, but they need careful design and effort to be conducted to a high standard.to date, most experimental vignette studies have concerned themselves with exploring the attitudes and behaviour of healthcare professionals.the framework developed in this paper to appraise vignette studies covers dimensions of relevance beyond inequalities.
number of words= 1187
[{'rouge-1': {'f': 0.3528042288354756, 'p': 0.8076425855513307,'r': 0.22569823434991976}, 'rouge-2': {'f': 0.1921561237114432, 'p': 0.3600763358778626,'r': 0.13104417670682733}, 'rouge-l': {'f': 0.33469525646410236, 'p': 0.6618367346938776,'r': 0.22398230088495577}}]
-----------------------------------------------------------------------------------------------------------------------------------
p230:
Extractive Summary:
background physical activity may benefit health and reduce risk of chronic complications in normal people and people with diabetes mellitus (dm) and peripheral vascular diseases [1].the american diabetes association (ada) recommends that people with dm should perform moderate-intensity aerobic exercise for 150 min/week or vigorous aerobic exercise for 75 min/week [2].moderateand vigorous intensity exercise has been demonstrated to improve peripheral circulation, but light intensity exercise cannot [1].walking is the most common exercise in normal people and people with dm [3, 4].however, walking is a weight-bearing exercise and fast walking such as brisk walking may cause high plantar pressure for increasing risk of plantar tissue injury and stress fracture of metatarsals [1, 3, 5–7].previous studies have confirmed that prolonged physical pressure over the skin is a major contributing factor for developing plantar tissue injury [5, 6].research studies have shown that muscle fatigue after intensive walking exercise reduces the body’s ability to absorb impact forces during walking [7, 8].the reduced shock absorption often causes muscular and joint pain of the lower extremity and fall injury [7, 8].researchers usually focus on the postural adjustment and balance after leg muscle fatigue because of severe impact of fall injury [9].in these studies, changes of plantar pressure, plantar tissue injury and metatarsal stress fracture are not well investigated [9].in the literature, there are only few studies investigating changes of plantar pressures after various physical activities [7, 10, 11].ferris et al. found that plantar pressure values were affected by activities of lower extremity muscles [10].bisiaux and moretto demonstrated that muscle fatigue of the lower extremity caused increased plantar pressure under the forefoot [11].however, to the best of our knowledge, there is no study comparing changes of plantar pressure after various walking intensities and relationship between leg muscle fatigue and plantar pressure patterns.although moderate intensity exercise, such as brisk walking is recommended for the normal people and people with dm, it is unclear whether these moderate and vigorous exercise are associated with increased leg muscle fatigue and increased plantar pressures as well as risk for plantar tissue injury.because various healthcare clinicians have been strongly advocating physical activity in people with dm and peripheral vascular diseases, there is an increasing need to understand the relationship between muscle fatigue and plantar pressure changes.to the best of our knowledge, this is the first study to compare leg muscle fatigue and plantar pressure patterns after various walking speeds and durations.the results from this study in healthy participants can provide a foundation for understanding the effect of walking intensities on leg muscle fatigue and plantar pressure in people with dm and peripheral vascular diseases.we hypothesized that different walking speeds (ranging from slow walking, brisk walking to slow running) and durations cause different degrees of leg muscle fatigue and plantar pressure patterns.methods a 3 × 2 factorial design, including three speeds (1.8, 3.6, and 5.4 mph) and two durations (10 and 20 min), was used in the current study.this study was part of a larger project investigating the effect of walking intensities on plantar skin blood flow responses [3, 4].the emg data has not been reported elsewhere.these three walking speeds represent slow and normal walking at 1.8 mph, brisk walking at 3.6 mph, and slow running at 5.4 mph.we also intended to examine the differences between two common walking durations of 10 and 20 min [3, 4].the specific walking tasks were as follows.• walking at 1.8 mph for 10 min in the first week • walking at 1.8 mph for 20 min in the first week • walking at 3.6 mph for 10 min in the second week • walking at 3.6 mph for 20 min in the second week • walking at 5.4 mph for 10 min in the third week • walking at 5.4 mph for 20 min in the third week participants healthy participants between 20 and 45 years of age were recruited from the university and nearby community.exclusion criteria were active foot ulcers, diabetes, vascular diseases, hypertension, the inability to walk for 20-min independently, the inability to walk at the speed of 5.4 mph independently, or the use of vasoactive medications [3, 4].the examinations were performed at the rehabilitation engineering laboratory, university of illinois at urbana-champaign with room temperature at 24 ± 2 °c. the study was approved by the institutional review board, university of illinois at urbana-champaign (irb #19,225).each participant signed the informed consent for this study.experimental procedures the research participant took off the socks and shoes and was in the supine position for 30 min before the walking protocol to avoid the influence of previous weight-bearing activities on muscle fatigue and plantar pressure.all participants were asked to walk with an appropriate size of standard shoes (1-inch heel, altrex, teaneck, nj, usa) at a speed of 1.8 mph on a treadmill in the first visit.electromyographic (emg) electrodes (model el507 (bipolar ag/agcl electrodes) and emg100c, biopac systems, inc., goleta, ca, usa) were taped on the tibialis anterior (ta) and gastrocnemius medialis (gm) of the dominant leg according to seniam locations [12].the sampling rate of emg signals was 1,000 hz.the skin sites for placing the electrodes were cleaned using alcohol wipes and shaved.the plantar pressure insole measure system was used to measure plantar pressures (tekscan, south boston, ma, usa).the participant wore the insole sensors inside the shoe for 3-min walking before the calibration.the insole sensor was then calibrated according to the manufacturer’s guidelines [13, 14].plantar pressure data were collected at 300 hz.participants were randomly assigned into the 10- or 20-min walking duration first and the other duration later, with a balanced crossover design.although the participants felt comfortable before participating in the second walking protocol of a day, the influence of muscle fatigue from the first walking protocol may affect the second walking protocol of a day.the temporal variations of median frequencies of emg signals and plantar pressure may affect our findings.researchers may use established fatigue protocols to examine the effect of leg muscle fatigue on plantar pressure.conclusions the results of this study demonstrate that the walking speed (1.8, 3.6, and 5.4 mph) is a significant factor on inducing leg muscle fatigue and the walking duration (10 and 20 min) is not.the median frequency of tibialis anterior significantly decreased after brisk walking (3.6 mph) and slow running (5.4 mph) speed compared to slow walking (1.8 mph) speed.the multiscale entropy based complexity of ta emg showed a similar trend to the median frequency.another significant finding is that muscle fatigue assessed by the median frequency and complexity were significantly correlated to the plantar pressure changes after walking at various intensiti
number of words= 1086
[{'rouge-1': {'f': 0.45548073241751874, 'p': 0.7286433260393874,'r': 0.3312847222222222}, 'rouge-2': {'f': 0.2755349267005801, 'p': 0.4142982456140351,'r': 0.20640312771503042}, 'rouge-l': {'f': 0.46151725359969553, 'p': 0.6912121212121212,'r': 0.3464044943820225}}]
-----------------------------------------------------------------------------------------------------------------------------------
p231:
Extractive Summary:
when considering the vm muscle, significant greater shear modulus of the senior adults was only observed at 105° of knee flexion, which was 11% greater than young adults (p = 0.020).the mivc of the senior adults was significantly lower than young adults at 60° and 90° of knee flexion (table 2).the correlations between passive shear modulus and mivc are depicted in table 3 and fig. 3.as regards the association between the shear modulus and muscle force, we have only detected moderate and negative association between the passive rf shear modulus at 60° of knee flexion and knee extension force in the senior adults (r = − 0.485, p = 0.030).in addition, there was a trend for a significant correlation in the young adults (r = − 0.395, p = 0.085).however, no significant association was found for other conditions both in the senior and young adults.discussion in this study, we aimed to assess age-related changes in muscle stiffness in the female senior adults.we have chosen the quadriceps muscle for analysis because of its importance in daily function.the senior female adults exhibited greater muscle stiffness at the rf and vl muscle heads when the muscles were positioned over 60° of knee flexion (knee extension = 0°) and at the vm muscle at a more lengthened position (105° of knee flexion).this present result is consistent with previous studies reporting higher passive stiffness of aged muscles at long muscle length in either animals [11, 28] or humans [16].however, the difference was not observed when assessed at shortened muscle length [11, 16, 22].in this connection, it is well known that change in muscle length affects passive muscle stiffness [26, 29, 30].the muscle fiber will become taut and even reach to a cross-bridges detached length when the muscle is stretched beyond its slack length which is just over 40° of knee flexion for quadriceps muscle [26, 31].the underlying mechanisms responsible for the observed increased passive stiffness in the senior female adults could be multifactorial.these include the increased extracellular matrix with aging, which, in turn, increases the muscle stiffness nearly three times in aged quadriceps femoris muscle [32, 33], the decrease of the amount of highly compliant element in the extracellular matrix such as elastin and collagen type iii [28], as well as the changes in muscle fiber type and the declined level of estrogen in senior female adults.it has been reported that the area and the percentage of type i muscle fibers increase with age [34].in response to passive stretch, the type i muscle fibers in isolated vl muscle exhibit greater passive stiffness than type ii fibers [35].the aged muscles with greater amount of type i muscle fibers might display greater passive stiffness than young muscles.estrogen deficiency might be another factor that reduces passive muscle stiffness in females by means of mediating collagen content within the muscle [36].therefore, estrogen deficiency in senior female adults, in particular, after menopause, might contribute to the increase in passive muscle stiffness.however, age-related increase in passive stiffness of vm was not identically observed as rf and vl over 60° knee flexion.the possible reason could be the imbalanced reduction in muscle mass with aging.it has been reported that the volume of vm was the least affected when it comes to aging among the four heads of quadriceps femoris muscle [37].the loss of muscle mass with aging would then be replaced by fat and connective tissue, which, in turn, could induce increase in passive muscle stiffness [8].the relative preservation of vm volume might be one of the reasons for the less increased stiffness at 60° and 90° knee flexion.we have also detected a negative relationship between passive stiffness of rf and the maximal isometric extension force in senior adults.more specifically, individuals with stiffer rf would produce lower knee extensor force when measured at 60° of knee flexion.in older adults (mean age of 73), no association between the rf stiffness and quadriceps extension strength was observed [15].the authors used strain ratio as an index of passive muscle stiffness.the strain ratio was computed from the strain of the muscle to that of an acoustic coupler.more importantly, the strain of the muscle was measured at its shortened length (knee in extension).similarly, we could not delineate a significant relationship between muscle stiffness of rf when measured at 30° of flexion and knee extension force.as muscles with higher echo intensity displayed lower muscle shear modulus and greater muscle strength [38], and echo intensity of rf was negatively associated with maximal isometric force at 60° of knee flexion and functional performance [39, 40], it might be possible that muscle stiffness of rf is associated with knee extension force.however, further investigation is required to understand the underlying mechanism of this correlation.this study had limitations.first, the results of this present study should be interpreted in regard to the characteristics of our population, i.e., females aged between 50 and 70 years old.
number of words= 810
[{'rouge-1': {'f': 0.46191120641498623, 'p': 0.8072013651877132,'r': 0.3235211267605634}, 'rouge-2': {'f': 0.280915383596299, 'p': 0.456986301369863,'r': 0.2027849588719154}, 'rouge-l': {'f': 0.4187256923353324, 'p': 0.6231914893617021,'r': 0.31528301886792454}}]
-----------------------------------------------------------------------------------------------------------------------------------
p232:
Extractive Summary:
anomalies of the medial meniscus are rare compared with those of the lateral meniscus.they include discoid variants, hypoplasia of anterior horn, and anomalous attachment of the anterior horn.usually, the most common anomaly of medial meniscus is related to the anterior insertion that often shows anatomic variability [1–3].however, anomalous attachment of posterior horn has been reported in only two cases [4, 5].in the first case [4], the authors described the presence of a fibro-cartilaginous fold covering the medial femoral condyle.the posterior horn was described as normal.in the second case [5], the authors described the presence of a fibrous band arising from the posterior horn and inserting to femoral insertion of the acl.both cases were described prior to the common use of arthroscopy.recently, sadigursky et al. [6] reported a case of abnormality in medial and lateral posterior horns.we present the first report of an anomaly of both the anterior and posterior roots of the medial meniscus, documented by mri and confirmed by arthroscopy.failure to respond to non-operative management.he first experienced pain on the medial side two years prior that was noted after sports activities although no clear injury was identified.the knee was intermittently painful and with time these symptoms progressively worsened despite several attempts at non-operative management to include anti-inflammatories and physical therapy.the symptoms limited the ability of the patient to participate in sports.physical examination revealed no effusion, complete range of motion, and no signs of meniscal pathology.there was no tenderness to palpation of the medial or lateral joint line, a negative mcmurray, and a normal ligamentous examination.there was no pain with hyperextension, and the only positive examination finding was mild pain on hyperflexion.radiographs were normal.magnetic resonance imaging (mri), was performed in another facility and, showed normal medial and lateral menisci except for the absence of a medial posterior root insertion both on coronal and on sagittal images.at the present time, the patient still actively practices and competes in soccer with no symptoms that limit participatio
number of words= 324
[{'rouge-1': {'f': 0.5164896847772366, 'p': 0.6565384615384615,'r': 0.4256851311953353}, 'rouge-2': {'f': 0.23695526020217145, 'p': 0.2873913043478261,'r': 0.20157894736842105}, 'rouge-l': {'f': 0.4330010728749867, 'p': 0.48911764705882355,'r': 0.38843575418994414}}]
-----------------------------------------------------------------------------------------------------------------------------------
p233:
Extractive Summary:
to compare the apparent stiffness of the samples under different loading conditions, other unwanted experimental sources of alteration in sample stiffness must be determined.these uncertainties often arise from simplifications introduced during the experimental procedures and can be investigated using a parametric approach.this study design is a powerful tool to isolate the influence of all players and compare their relative significance systematically [30, 31].most studies have only reported the repeatability measures of the experiments via repeating each test case multiple times [2, 32] or by re-orienting and re-installing the setup between each test [26].others have studied the effect of fe modeling methodological determinants on predicted femoral strength [3].to our knowledge, there are no studies investigating the effect of typical experimental parameters on the structural properties of the samples using a systematic approach.the aim of this study was to parametrically determine the influence of common steps involved in an experimental validation study on the apparent stiffness of the proximal femur under multiple loading directions, namely: repetition, pre-loading, re-adjustment, re-fixation, storage, μct scanning, and loading direction (15° deviation from neutral alignment) in stance and side-fall configurations.to our knowledge, this is the first study in which both stance and fall configurations have been tested on every sample.these results could add an aggregated reference to the currently scattered pool of data required for planning experimental protocols with reduced uncertainties affecting the measured structural properties of bone samples.methods & materials samples ten proximal femoral samples from five donors (table 1) were harvested and kept frozen in -23° (center for anatomy and cell biology,medical university of vienna).the specimens originated from voluntary body donations for scientific and teaching purposes to the center (according to protocol accepted by the ethics committee of karl landsteiner university of health sciences).samples were screened for lack of any pathological disease.all procedures were performed in accordance with relevant guidelines.experimental setup to perform a parametric study, we developed a new femoral experimental setup based on two main criteria: 1. possess fully defined boundary conditions, and 2.provide the means for fast multiple non-destructive tests in stance and fall configurations with variable loading direction.the test setup was comprised of the following main components: alignment setup, embedding components, testing apparatus, scan chambers, and dic setup.alignment setup: a custom-made alignment setup was used to maintain the femur’s physiological neutral stance alignment in the final prepared sample.it was comprised of two cross lasers, in-house 3d printed holder devices, and manufactured pom (polyoxymethylene) supports (fig. 1).the intact femur was laid down on two distal and proximal supports.the bone was axially tilted until the neck axis, passing through the femoral head center and middle of the femoral neck, was coincident with the horizontal line of a cross laser (fig. 1a).metal spacers with various thicknesses were placed between the condyles of the femur and the distal support to maintain the torsional alignment.then, the proximal support was elevated using a screw mechanism so that the femoral head center and the distal mid-condylar line were coincident with the horizontal laser line (fig. 1b).finally, using the second cross lasermounted above the dissection table, a 3° adduction angle with the reference line of the alignment supports was formed.the femoral head center and the femoral intercondylar fossa, hence the mechanical axis, were aligned with the laser line (fig. 1c).once all angles were determined, a custom-made c-shaped device was used to fix the alignment on the proximal portion of the bone making it ready for the cutting and potting steps.this device was fixed 105 mm below the bounding plane coincident to the most proximal point of the femoral head (fig. 1e).the proximal portion of the bone was cut using a bone saw 45 mm below the fixed device, resulting in a total sample length of 150 mm (fig. 1f).the total sample length was restricted by the maximum field of view from the μct scanner, which was necessary for future studies using the micro-fe models of the samples.embedding components: the proximal femoral samples were held in their neutral stance alignment on the potting block, and their shaft was embedded in a 50 mm diameter cylinder.a 5-mm gap was left between the bottomof the sample and block to account for uneven cutting surfaces.two pins were attached to the walls of the potting block to prevent the shaft from rotating in the holder (fig. 2a).the trochanter and the head were embedded in spherical segments to provide defined boundary conditions at the contact points and avoid local crushing.the alignment was done using 3d printed adapters (fig. 2).using printed holders and pins, 4-marker clusters were later attached to the femoral head, major trochanter, and shaft of the samples for displacement tracking.testing apparatus: a 25 kn load cell with 6 degrees of freedom (dof) (hottinger baldwin messtechnik (hbm) gmbh, germany) was mounted on the 30 kn electromechanical axial testing machine (z030, zwickroell ulm, germany).hardened iron disks were manufactured and used with ring ball-bearings to apply a purely axial load (fig. 3).a rotating milling machine table was equipped with a hinge bearing to hold the shaft of the sample and allow for 5 degrees of freedom for the sample alignment (x, y, rx, ry, rz).an additional uni-axial 25 kn load cell (see fig. 3) was used in the fall configuration to support the head while the trochanter is loaded (hbm, germany).in the stance configuration, the shaft block was fixed on the rotating table.the abduction\adduction angle was adjusted on the table.the table was fixed on the testing machine.in the fall configuration, the shaft block was free to rotate in the abduction\adduction direction while the head was resting on the support load cell.the table was fixed on the machine as well (fig. 3).scan chambers: μct: a custom-made chamber was manufactured using pom (polyoxymethylene) and plexiglas (fig. 4).the cylindrical chamber was 15 cm in diameter and 17 cm inner height.a clamping mechanism was fitted in the chamber so that the femur could be stood upright and fixed to avoid movement artifacts during the scan.a sealing cap with a pressure valve was used tomake sure the sample is not dehydrated during the scan while the heated air can escape the chamber.ct: to mimic the clinical conditions in which two legs with surrounding soft tissues are present in scanner’s field of view, wemodified a rectangular translucent storage box (polypropylene) using embedding material, 3d printed adapters, and ready-made pvc holders.each pair of samples were fixed side-by-side, in their neutral alignment, 20 cm apart, and fully submerged in saline solution (fig. 4).dic setup: digital image correlation (dic) system (aramis 3d camera, gomgmbh, braunschweig, germany) with two ccd cameras was used for optical displacement tracking.the 6-megapixel cameras were 150 mm apart and positioned at a perpendicular distance of 350 mm from the sample, capturing images at a 10 hz rate from a measurement volume of 160 x 130 x 95 mm (lxwxd).all measurements were done according to the manufacturer’s standard protocol (gom gmbh, braunschweig, germany).the system was calibrated before beginning of each session using the standard calibration plate and according to the manufacturer’s protocol keeping the calibration deviation below 0.05 pixels.clusters of markers comprised of four markers (gom gmbh, braunschweig, germany) attached to a 3d printed holder were placed on the head, trochanter, and shaft of the samples in order to measure the apparent stiffness of the bone (fig. 2).additional markers were placed on the loading plate and holder block in order to be able to measure the apparent stiffness of the full specimen as well (fig. 5).all markers were covered within the measurement volume at all time.there was a 1 micron displacement noise in the marker displacement data at zero load.parameters while all steps involved in our experimental validation study are described below, not all the acquired data were relevant and hence presented in this manuscript’s scope (e.g., scan data acquired for the fe modeling phase elsewhere).the following parameters were tested for stance and side-fall configurations: • repetition: repeating a test five times without touching any parts of the setup or sample.•pre-loading: testing a load case once right after fixing the sample into the desired place and alignment and comparing the results with the average of the immediately following five repetitions, without touching the sample or setup in between.• re-adjustment: distorting the sample configuration and placement on the machine and re-adjusting it back to the initial condition without taking the sample out of the setup.•re-fixation: taking out the sample and distorting the setup adjustments, then putting everything back to their initial condition.• storage: storing samples in a -23°c freezer for four weeks.•
number of words= 1409
[{'rouge-1': {'f': 0.32340223515860916, 'p': 0.7863636363636364,'r': 0.20355932203389832}, 'rouge-2': {'f': 0.17389977029696657, 'p': 0.3291240875912409,'r': 0.11816824966078698}, 'rouge-l': {'f': 0.3000495622509978, 'p': 0.5731055900621118,'r': 0.20322368421052633}}]
-----------------------------------------------------------------------------------------------------------------------------------
p234:
Extractive Summary:
introduction musculoskeletal diseases are the main causes of disability among adults [1, 2].much of this disability burden is associated with populational aging, but also with behavioural and work-related risk factors [3].the 12-month prevalence estimates vary from 21 to 42% among children and adolescents to 30 to 50% among adults and the elderly [6].neck pain is a complex condition, in which the association of individual, ergonomic, socio-cultural and psychosocial risk factors contributes to its occurrence and chronicity [8].the main individual factors are age, sex, increased body mass index and smoking [9, 10].among the environmental factors are those related to ergonomics, such as strenuous physical activity, use of force and vibration, improper posture and repetitive movements [6].the economic impact of neck pain is significant and includes costs related to treatment, decreased productivity, non-attendance to work and social security [11].in 2012, neck pain was responsible for job absences of 25.5 million americans, who missed an average of 11.4 days of work [1].in brazil, neck pain was responsible for 7.2% of disability pensions granted to workers with musculoskeletal diseases [15].since 2014, several investigations on the burden of disease were conducted by the gbd brazil network, which was created as a collaboration among the ihme, the brazilian ministry of health, and a network of academic institutions led by the federal university of minas gerais [17].the definition of neck pain was based on the codification by the international classification of diseases, tenth edition (icd-10): neck pain (m54.2); cervicocranial syndrome (m53.0); cervicobrachial syndrome (m53.1); back pain with cervical radiculopathy (m54.1); cervical spondylosis with radiculopathy (m47.2); cervical spondylosis with myelopathy (m47.1); cervical disc disorder (m50); cervicogenic headache (r51); sprain of ligaments of cervical spine (s13.4); and sprain of joints and ligaments of other parts of neck (s13.8) [20].the disability weights used for estimating the diseases’ ylds are measured on a scale of zero to one, in which zero is equivalent to full health and one is equivalent to death.disability weights were obtained from surveys conducted in several countries from different regions.the surveys used paired-comparison questions, in which the respondents considered two hypothetical individuals with different, randomly selected health conditions and were asked to indicate which individual they considered to be the healthiest.more methodological details are available elsewhere [22].in addition, the position and changes in the classification rank of neck pain prevalence and ylds were analyzed in the selected locations, in the same years of 2000, 2010 and 2019.to describe the degree of confidence in the metrics, the modeling process enables estimates to be calculated 1000 times and then distributed from the lowest to the highest value.all estimates are presented with their 95% ui, and differences between estimates are statistically significant if the intervals are not coincident [16].all estimates were produced by the institute for health metrics and evaluation (ihme) and available in the ihme website at: http:// ghdx.healthdata.org/gbd-results-tool [7].the authors are responsible for describe and analyze these data.healthdata.org/countries).prevalence rates in brazil showed an inverse trend when compared to the other locations, increasing between 2000 and 2010 but decreasing from 2010 to 2019 (table 1).although there is a difference between these variations, the magnitude of the curves for prevalence patterns in brazil, the state of são paulo, and the rest of the states was similar.in 2019, there was a statistical difference in the global prevalence of neck pain for both sexes in the age group of 15 to 49 years, where the estimates were 2770.4 (95% iu 2027.9 – 3851.4) per 100,000 population; in the age group of 50 to 69 years, where the estimates were 6064.2 (95% iu 4070.1 – 9132.7) per 100,000 population; and in the age group of 70 years and older, with prevalence estimates of 5971.6 (95% iu 4080 – 8621.7) per 100,000 population.contrary to the prevalence estimates, there was no statistical difference between the yld values comparing brazil, mexico, the usa and england in 2010 and 2019.the age-standardized yld rates were very similar among the brazilian states.discussion this study shows a similar prevalence of neck pain among the global and countries with lower sdi (brazil and mexico), in contrast to higher estimates in countries with higher sdi (england and usa).previous studies pointed out the higher prevalence rate of neck pain among women [9, 28, 29], and their lower tendency to recover from such pain [30], different from data of more recent studies [7, 23].younger men and women have a higher prevalence of neck pain, but recover more often from it than older individuals.this fact suggests that neck pain affects a younger population in brazil, the long duration of the disease make the ydl similar to countries with a higher prevalence.musculoskeletal conditions are neglected in most national health surveys: among 170 national health surveys from different countries, only 37 cited cervical pain [38].besides, the different definitions of neck pain prevent the comparison [38].low income is a risk factor for chronic spinal pain and is associated with a worse prognosis for chronic neck pain [7, 31].more recent studies have revealed a higher prevalence of chronic neck pain and a notable association with people with low education [39, 40].this study presents the current brazilian status of the burden of neck pain with the best available data.considering brazilian subnational diversity, obtaining reliable estimates is challenging.in addition, it highlights the importance of diagnosis, early treatment and adequate rehabilitation, which tend to reduce the disability generated by neck pa
number of words= 889
[{'rouge-1': {'f': 0.36511159567852614, 'p': 0.6245171339563862,'r': 0.25796198521647307}, 'rouge-2': {'f': 0.17082990812168034, 'p': 0.248125,'r': 0.13025369978858353}, 'rouge-l': {'f': 0.3023563748079877, 'p': 0.4411340206185567,'r': 0.23}}]
-----------------------------------------------------------------------------------------------------------------------------------
p235:
Extractive Summary:
classification methods overview we performed four existing classification methods, i.e., random forest algorithm (rfa), artificial neural network (anns), support vector machine (svm) and logistic regression (lr), to predict the disease status (i.e., kbd or normal).the classification models were trained with default parameter settings companying with four feature selection methods (i.e., rfa, mrmr [21], svm-rfe [22] and relief [23]), which are falling into three categories, i.e., wrappers, embedded methods, and filters [24–26].all methods were implemented by python (version 3.6.10) within sklearn framework (v 0.23.1).the performance of all four classification methods were evaluated by fivefold cross validation (5-cv) over four popular measures, i.e., sensitivity, specificity, accuracy, and the area under the roc curve (auc).random forest algorithm (rfa) the first classification method we performed in our analysis is random forest algorithm (rfa) [27], which is an ensemble learning method for kbd diagnosis purpose by constructing a multitude of decision trees (supplementary figure 1).we performed the rfa with ensemble.randomforestclassifier function to construct rfa object; then utilized fit function to train the model, in which the training data sets and its corresponding class labels (i.e., disease status, kbd or normal) as inputs; finally carried out the predict function to predict the disease status with testing data sets.we performed various experiments to determine the optimal parameter, the number of variables randomly sampled as candidates at each split mtry, and the number of trees ntree (supplementary figure 2), and finally, mtry = 3, and ntree = 300 were used in the following analyses.artificial neural networks (anns) there were three layers in anns classification models, an input layer, a hidden layer and an output layer [28].the scheme of classification models using ann was showed in supplementary figure 3 (f. s3.).for convenience, neural_ network.mlpclassifier was implemented as anns classification model.by gridsearchcv which provide convenience for finding optimal parameter, there was only one hidden layer and the number of neurons was 5.in addition, the activation function was set as relu.learning rate was set as adaptive.lbfgs was taken as optimization algorithm.support vector machine (svm) non-linear svm algorithm was applied in this study.the hyperplane in non-linear svm algorithm used kernel function to transform the decision function in the low dimensional plane.we establish non-linear svm model by using svm.svc, then other training and prediction steps is the same as rfa.for chosen of the kernel function and the coefficient (gamma) of it, with the help of gridsearchcv which provide convenience for finding optimal parameter, different settings are tried.finally, we adopt rbf kernel and gamma value is set as the inverse of the number of features included.the fivefold cross-validation are also implemented for the propose of stabilized results.logistic regression (lr) logistic regression algorithm adds a sigmoid (for binary classification) or softmax (for multi-classification) based on linear regression to solve dichotomous classification task.in this study, linearmodel.logisticregression was applied and fit function was also implemented for training process.predict labels and probabilities are available.similar to svm, gridsearchcv determines optimal parameter, and optimization algorithm was lbfgs and none penalties were chosen.results general characteristic of study samples eight hundred adolescents (400 kbd and 400 non- kbd) were recruited.general characteristics of all study subjects were demonstrated in table 2.there were significant differences of the age distribution between kbd and non-kbd group (  2 = 343.17, p < 0.001).gender of the two groups showed no statistical differences (  2 = 0.18, p = 0.669).for clinical manifestations, significant differences between kbd and non-kbd group were observed in clinical grading of kbd, joint pain, short fingers, flexion of the distal part of fingers, wrist joint movement limitation, elbow joint movement limitation, ankle joint movement limitation, knee joint movement limitation, squatting limitation, enlarged finger joint, enlarged elbow joint, enlarged ankle joint, and deformed joints.in addition, more adolescents in kbd group showed pathological x-ray images in metaphysis alterations and distal end of phalanges alterations than those in non-kbd group and the differences were statistically significant.prediction performance of classification models classification models applied four different algorithms i.e., rfa, anns, svm and lr were built based on 26 features.the prediction performance of four models were listed in table 3.all four models showed good predictive efficacy with accuracy ranged from 93.63 to 99.76% and auc value ranged from 0.94 to 1.00.among four models, classification models of rfa and anns showed better predictive efficacy with higher auc value (1.00, 1.00) and accuracy (99.76%, 99.63%) than models based on based on lr (0.97, 96.50%) and svm (0.94, 93.63%).rfa model presented highest sensitivity with 100.00% and model svm had lowest with 88.64%.sensitivities of lr model and anns were 96.20 and 99.86%, respectively.the specificity of four models including rfa, anns, svm, and lr model were 99.22, 99.66, 98.51 and 96.89%, respectively.to conclude, rfa and anns models had the best comprehensive predictive efficacy with highest auc values.feature selection in this study, four algorithms including rfa, mrmr, svm-rfe and relief were applied to select discriminative features for kbd diagnosis.the importance ranking of 26 features ranked by different algorithms were presented in table 4.the order from 1 to 26 represented the range from the most important to the least important.the rankings of 26 features in rfa and mrmr algorithm were the same.
number of words= 846
[{'rouge-1': {'f': 0.37214125399169407, 'p': 0.7575000000000001,'r': 0.24665952890792292}, 'rouge-2': {'f': 0.22140248827855336, 'p': 0.3963598326359833,'r': 0.15360128617363344}, 'rouge-l': {'f': 0.370456502435363, 'p': 0.5922929936305732,'r': 0.26951338199513386}}]
-----------------------------------------------------------------------------------------------------------------------------------
p236:
Extractive Summary:
the other parameters included in this model (opa vigorous, opa high vigorous, opa total, and gender) were statistically insignificant.different models were not found.discussion the aim of this study was to assess the occurrence of self-reported msd among male and female territorial army soldiers during the covid-19 pandemic and to investigate whether there was a relationship between opa, ltpa, and msd.an investigation on the 12-month and 7-day prevalence of msd revealed that the most common was low back pain, followed by neck and knee pain.a total of 56 and 40% of territorial army soldiers had experienced pain or other discomfort in one or more of nine body regions during the past 12 months and during the past 7 days, respectively.in serra et al., police officers often reported pain in the lower back (47%) and dorsal region (33%) [29].we observed relatively high and highly diverse pa among territorial army soldiers.most of them declared ee on total pa above 6000 kcal peer week, and the mean and median of the total pa in the studied soldiers was above 6000 metmin per week.in this cross-sectional study, we investigated the relationship between pa and msd among soldiers.the study revealed that occupational pa, especially vigorousintensity and high vigorous-intensity opa, is positively correlated with a higher prevalence of msd in several regions of the body, i.e. the lower back, elbows, wrists or hands, hips or thighs, and ankles or feet.we observed that along with an increase in energy expenditure on total pa, a greater percentage of respondents experienced low back pain.we also observed that undertaking high vigorous-intensity ltpa is positively correlated with a higher prevalence of wrist or hand pain.our study indicates that participants who were less physically active experienced msd less often during the last 12 months.this probably results from the declared high levels of pa among most of the studied soldiers.our findings are partially in line with previous studies.as showed in weyh et al., welders who had a higher physical work load demonstrated a higher prevalence of msd.the 12-month prevalence of low back pain in welders was 71%, neck pain 61%, and shoulder pain 55%.the authors stated that insufficient ltpa (< 600 met/ week) was associated with low back pain [30].lópez-bueno et al., based on their study including 10,427 active danish employees, observed an association between ltpa and long-term sickness absence (ltsa).the authors noticed that high ltpa reduced the risk of ltsa by a borderline significant 23%, whereas moderate ltpa also reduced ltsa but a lower percentage than high ltpa [31].søgaard et al. revealed that employees with high work activity had a higher frequency of recurring lower back and hand/wrist symptoms, whereas employees with mainly sitting work experienced a higher frequency of recurrent neck symptoms [32].our findings were similar; however, we did not find an association between time spent sitting per day and the prevalence of msd, including neck pain.the comparison of opa, ltpa, and msd between physical education teachers (pet) and other teachers indicated that pet who were more physically active had a significantly lower risk of all msd during the past year then teachers who were more sedentary [33].these findings point out that long-term pa is associated with a lower risk of msd, which is not in line with our study.this is likely due to different levels of pa between teachers and soldiers.due to the demands of the annual fitness test, soldiers are obliqued to maintain high fitness levels, which are related to performing physical training.for the same reason, this occupational group might differ from other working groups and should not be compared directly.the lack of the positive effects of pa, the relationship between vigorous and high-vigorous pa, and the occurrence of msd in the hips or thighs or ankles or feet observed in our study is supported by studies that indicate physical training as the cause of injuries in the lower limbs [34, 35].the opposite findings were obtained by serra et al. who indicated that undergoing physical activities during the last 12 months reduced the odds of getting msd by 30%.police officers who had performed pa during the last 12 months reported less occupational stress than those who did not.however, the authors mainly focused on the assessment of stress perception, whereas information about pa was limited to one question regarding performing some type of physical exercise more than three times a week [29].our results, based on the cross-sectional study design, do not allow us to point out a clear causal relationship between pa and msd.however, we may assume that performing high-intensity pa, the majority as opa, causes the more frequent occurrence of msd.strengths and limitations of the study the study was conducted during the covid-19 pandemic among a specific group of workers with relatively high pa.these results should neither be generalized to other working populations nor to non-pandemic periods.there are several limitations to this study which are worth mentioning.
number of words= 807
[{'rouge-1': {'f': 0.47162185581011307, 'p': 0.8917054263565891,'r': 0.32059101654846334}, 'rouge-2': {'f': 0.3554003972111498, 'p': 0.645875486381323,'r': 0.24514792899408286}, 'rouge-l': {'f': 0.5088265137428761, 'p': 0.8004964539007093,'r': 0.3729411764705882}}]
-----------------------------------------------------------------------------------------------------------------------------------
p237:
Extractive Summary:
periprosthetic joint infection (pji), one of the most troublesome complications of total hip or knee arthroplasty, exacerbates the burden on the individual and health care system [1–4].as the number of surgeries surged yearly, the total number of pji patients increased [5].however, the diagnosis and treatment of pji remain challenging for clinicians.current diagnostic methods of pji include serological testing, synovial fluid testing, and intraoperative histological pathology [2, 5].however, although the diagnostic criteria are well defined, no gold standard has yet been established [4, 6].similarly, treatment of pji is difficult for clinicians because there are no widely accepted criteria [7, 8].the two-stage revision is currently the standard procedure for pji, but the proper time to perform the second-stage revision is still debatable [9].as recent literature revealed the close correlations between the coagulation cascade and infection course, coagulation-related biomarkers are gaining attention.some biomarkers, such as d-dimer and fibrinogen (fib), have been proven promising for pji diagnosis [10–13] and determining the reimplantation timing [10, 14].thromboelastography (teg) is a routine coagulation test that assesses the whole process of clotting over time in the body [15] and provides a full-scale evaluation of clot formation, elasticity, and duration.additionally, various coagulation elements are measured as follows [16].the clotting time (k value) reflects the rate of blood clot formation and is an indicator of fibrinogen function [17].the α-angle (angle) represents the clot growth rate, while the maximum amplitude (ma) is the maximum clot amplitude [18].the amplitude at 30 min (a30) measures clot strength at 30 min after ma [19], and thrombodynamic potential index (tpi) was derived from the ma and k values [20].moreover, teg yields information about all phases of coagulation and provides further information on standard coagulation tests [16, 21].numerous studies have proved that teg is useful in evaluating coagulation status, predicting bleeding in patients with severe sepsis, monitoring haemostasis during cardiac surgery and liver transplant procedures, etc.[15, 21, 22].however, no study has reported its value in diagnosing pji and guiding the timing of reimplantation for the second-stage revision.therefore, this study aims to investigate (1) the value of teg in distinguishing pji from aseptic loosening and (2) the ability of teg parameters to guide the proper time for the second-stage revision.furthermore, the measured teg parameters were compared with the esr, crp, and d-dimer levels.methods we conducted this retrospective study including all revision total hip and total knee arthroplasties performed in our hospital from october 2017 to september 2020, under the ethical approval of the institutional review board of our hospital.among the 145 patient records acquired, 61 patients were diagnosed with pji according to the musculoskeletal infection society criteria [23], and 84 patients were diagnosed with aseptic loosening.patients were excluded if at least one of the following are present: (1) lack of needed data, (2) recent use of anticoagulants, (3) presence of inflammatory arthritis such as rheumatoid arthritis and ankylosing spondylitis, (4) blood diseases such as thrombocytopenic purpura, (5) formation of deep vein thrombosis of the lower limbs, (6) liver diseases, (7) malignancy, and (8) infection of other tissues or organs.finally, 62 patients who underwent revision arthroplasty were included in this study: 23 in group a (treated for pji) and 39 in group b (treated for aseptic loosening).a total of 83 patients were excluded due to lack of needed data (n = 55), deep vein thrombosis in the lower limbs (n = 24), recent use of oral warfarin due to coronary stent implantation (n = 2), urinary tract infection (n = 1), and rheumatoid arthritis (n = 1).the patients’ fasting venous blood samples were collected routinely on the second day of admission and sent to the clinical laboratory of our hospital for blood examination, including routine blood examination, conventional coagulation tests, and teg.the test results were acquired approximately 30 min after blood collection.first, the samples in this study were small; a larger sample size might have produced different results.second, lower extremity doppler ultrasound was routinely performed to exclude venous thromboembolism of the lower limb, which did not rule out clots in other parts of the patient’s body.moreover, we did not consider pji patients’ use of antibiotics before admission to our hospital.furthermore, this study has some inherent biases due to its retrospective nature.finally, we only checked the teg of pji patients before spacer insertion and before reimplantation, rather than checking them regularly.hence, the changing trend of the biomarker levels in pji patients was not clear.conclusion this study reports 5 measured teg parameters (k value, angle, ma, a30, tpi) that are statistically different between patients with pji, with aseptic loosening, and those who were re-admitted for reimplantation in twostage arthroplasty.with high specificity, ma was considered a valuable biomarker in diagnosing pji and assessing infection control after the first-stage surge
number of words= 778
[{'rouge-1': {'f': 0.384879822706962, 'p': 0.6509859154929578,'r': 0.27320197044334976}, 'rouge-2': {'f': 0.19440958777236692, 'p': 0.28908127208480566,'r': 0.14644882860665845}, 'rouge-l': {'f': 0.34854914172915574, 'p': 0.522513966480447,'r': 0.26148936170212767}}]
-----------------------------------------------------------------------------------------------------------------------------------
p238:
Extractive Summary:
however, a systematic search of pubmed, the web of science, and the cochrane library revealed only a handful of preliminary reports describing the use of pte for infected tha, highlighting the slow adoption of this procedure in clinical practice [7–11].in this retrospective study, we examined the use of pte for infected tha, providing additional clinical evidence regarding the efficacy and reliability of this surgical procedure.the mean age at the time of surgery was 61 years (range, 40–78 years).the average body mass index was 23.7 kg/m2 (range, 18.4– 31.7 kg/m2).the harris hip score upon admission to the hospital was 39.8 ± 10.7 points.the baseline demographic data were listed in table 1.the diagnosis of deep infection was made based on the new definition for pji established by the musculoskeletal infection society workgroup [12].briefly, diagnosis of infection was defined as the presence of a discharging sinus communicating with the joint, growth of a microorganism from joint fluids or at least two separate tissues from the affected prosthetic joint, or by the presence of four of the following six criteria: (a) elevated c reactive protein (≥ 10 mg/l) and erythrocyte sedimentation rate (≥ 30 mm/h); (b) elevated synovial white blood cell count (≥ 2000/μl); (c) elevated synovial neutrophil percentage (≥ 65%); (d) presence of purulence in the affected joint; (e) isolation of a microorganism in the culture of periprosthetic tissue or fluid; (f) detection of > 5 neutrophils per high-power field on histopathologic examination [12].the definition of chronic infection was based on the criteria of tsukayama et al. [13].possible loosening of all femoral components was further confirmed during the operation.for well-fixed cementless stems where radiographic evidence indicated bone ingrowth along the entire length of the stem, the femoral stems were retained without any attempt to remove them [8, 15, 16], and a pte revision was made.surgical technique all surgeries were performed by the same experienced, fellowship-trained arthroplasty surgeon.the basic principles of pte revision were as follows: radical debridement, removal of the acetabular component and artificial femoral head, retention of the uninvolved femoral stem component, and insertion of an antibioticloaded cement spacer, followed by two-stage reimplantation.the acetabular component and artificial femoral head were then removed; the femoral stem component was retained in place.for each patient, at least two separate tissues from the affected prosthetic joint were harvested for intraoperative frozen biopsy and bacterial culture.afterwards, the wound was successively flushed with 0.9% nacl solution and hydrogen peroxide solution, followed by pulsatile lavage for 15 min in iodine solution.the removed components were then replaced with antibiotic-loaded cement spacers.these spacers consisted of high-viscosity cement (palacos, zimmer, warsaw, in, usa) and vancomycin [17].the dose of vancomycin was determined based on previous studies (4 g vancomycin per 40 mg bag cement) [18, 19].spacers were handmade using a pediatric ear and ulcer syringe (outer diameter, 44 mm; cr bard, inc., covington, ga, usa) or other suitable spherical tools when necessary.additional antibiotic-loaded cement was applied around the proximal end of the femoral stem prosthesis to reduce bone loss and prevent bacteria from affecting the femoral stem components (shown in fig. 1).at this stage, the acetabular bone defect would not be treated, and the patient would be told to avoid stress on the surgical side.serum c reactive protein level, erythrocyte sedimentation rate, and white blood cell count were monitored monthly.second-stage reimplantation was performed based on a combination of the patient’s general condition, wound healing without signs of infection (determined using the same criteria as initial diagnosis), and normalization of laboratory data.intraoperative pathological examination of frozen biopsy confirmed that the level of neutrophil granulocytes was < 5 per high-power field.routine flushing of the wound was performed using 0.9% nacl solution and hydrogen peroxide solution, followed by pulsatile lavage for 15 min in iodine solution.generally, paprosky type i, type iia and type iib defects were managed with a noncemented, porous-coated hemispheric implant with or without the use of adjunctive screw fixation.in this study, we defined failure as recurrence of infection in the same hip, requirement of additional surgical procedures for infection control, or use of longterm (> 6 months) suppressive antibiotics.p values < 0.05 were considered statistically significant.the interval from the appearance of infection to the first surgery was 17 months (range, 6–72 months); the second-stage surgery was accomplished after a mean interval of 4.1 months (range, 3–12 months).the success rate of treatment in this study was 85.7% (24/28).except for recurrent infection, no complications were observed in this study, such as deep vein thrombosis, implant loosening, nerve injury, dislocation, or death (shown in figs. 2, 3).at the final follow-up, inflammatory indicators such as erythrocyte sedimentation rate, c reactive protein level, and white blood cell count were within the normal ranges.the esr of men declined more slowly than women after surgery in an age-dependent manner.the gee results showed that there was a positive correlation between postoperative pain score and crp values.the average harris hip score at 1 month postoperatively was 51.8 ± 8.9 points, which increased to 61.3 ± 8.2 points at 3 months postoperatively and 76.2 ± 11.7 points at the final follow-up, revealing significant improvement in hip function (table 2).a total of four treatment failures were observed in this study.three were due to persistent infection before second-stage revision, while the fourth was attributed to recurrent infection after two-stage revision (table 3).failure 1: a 69-year-old woman had undergone prior hip operations due to trauma; she presented with poor local soft tissue condition, including severe scarring.
number of words= 900
[{'rouge-1': {'f': 0.3244598021912687, 'p': 0.6979069767441861,'r': 0.21136125654450263}, 'rouge-2': {'f': 0.16339854132301265, 'p': 0.275607476635514,'r': 0.11612159329140462}, 'rouge-l': {'f': 0.3069624361101567, 'p': 0.5664028776978418,'r': 0.21052953156822812}}]
-----------------------------------------------------------------------------------------------------------------------------------
p239:
Extractive Summary:
most of all, increased femoral anteversion is notable in patients with cp.femoral anteversion in cp patients increases according to the gross motor function classification system (gmfc s) level [3], and it is believed to not improve with age.therefore, femoral derotational osteotomy is one of the common procedures used in single-event multilevel surgery to improve gait function in patients with cp [4–6].this lack of research is owing to the paucity of pathologic tibial torsion compared with increased femoral anteversion, and physical examination depicting tibial torsion is not as good as that depicting femoral anteversion in terms of validity [7, 8].the exclusion criteria were as follows: (1) inadequate 3d ct scan for measuring femoral anteversion or tibial torsion, (2) patients with a history of orthopedic intervention (bony or soft-tissue procedures) for the treatment of cp before assessment, and (3) patients with neuromuscular diseases other than cp (fig. 1).data collection after implementing the inclusion and exclusion criteria, two authors (mjj and pms) reviewed the patients’ medical records.previous studies on ct measurements were reviewed, and 3d images were used for measurements.definitions on an axial 3d ct scan, femoral anteversion was defined as the angle between a line connecting the centers of the femoral head and greater trochanter and another line connecting the posterior margins of the medial and lateral femoral condyles (fig. 2 a).tibial torsion was defined as the angle between a line connecting the posterior margins of the medial and lateral tibial condyles and another line connecting the midpoints of the medial malleolus and syndesmotic articular surface of the lateral malleolus (fig. 2 b).the restricted maximum likelihood estimation was used to estimate parameters for the linear mixed model [9, 10].eighteen right legs and 18 left legs were randomly selected for statistical independence and included for reliability testing [13].all statistical analyses were performed using the sas statistical package, version 9.4 (sas institute, cary, nc, usa) and r version 3.5.1 (r foundation for statistical computing, vienna, austria; isbn 3–900,051–07-0, url http://www.r-project.org) with the stats package 2.3.cis were considered significant when they did not include zero, and pvalues < 0.05 were considered significant.results overall, 639 patients were screened.after implementing the inclusion and exclusion criteria, 472 patients were enrolled in this study.both involved and uninvolved sides of hemiplegic patients were statistically significant factors affecting tibial torsion, with the involved side showing 2.63° (p = 0.0471) and the uninvolved side showing 3.87° (p = 0.0047) greater external tibial torsion in hemiplegic patients than in diplegic patients.in addition, factors affecting femoral anteversion were age, gmfcs level, and uninvolved limb in hemiplegia.with 1-year increase in age, femoral anteversion decreased by 0.28° (p < 0.0001).the uninvolved limb showed 13.31° lower femoral anteversion in hemiplegic patients than in diplegic patients (p < 0.0001).this evidence was refuted in our study with cp patients; the analyzed patients tended to show a decrease in femoral anteversion as they aged, even in the affected limbs of hemiplegic patients.previous evidence regarding tdc has shown an increase in external tibial torsion as children age [15], yet only a few studies examined the rotational profiles of the tibia in patients with cp.our study results showed that external tibial torsion in cp patients also followed the pattern seen in tdc, showing increase with age.this pattern may be a developmental change, or it may be a compensatory change to decreased femoral anteversion.previous studies have shown that femoral anteversion and femoral neck-shaft angle tend to be higher in patients in gmfcs levels iv and v [3].our results are consistent with the findings of previous studies, probably because of increased spasticity, delay in motor development and weakness expressed in patients with an aggravated functional status.there is no evidence as to why hemiplegic patients tended to show higher femoral anteversion than diplegic patients.further study is required for a reasonable explanation of this phenomenon.external tibial torsion tended to increase with increasing femoral anteversion.this may be a compensatory phenomenon to increase the femoral anteversion to maintain neutral foot progression.the relationship between pelvic external rotation as a result of increased femoral anteversion has been proven in a previous study [2].additionally, a study has shown that correction of increased femoral anteversion also corrects the external pelvic torsion [5, 18].
number of words= 691
[{'rouge-1': {'f': 0.40712057664688717, 'p': 0.7822641509433963,'r': 0.27516304347826087}, 'rouge-2': {'f': 0.23850570950493133, 'p': 0.4112322274881517,'r': 0.1679591836734694}, 'rouge-l': {'f': 0.3574342677093678, 'p': 0.5910084033613445,'r': 0.25618618618618616}}]
-----------------------------------------------------------------------------------------------------------------------------------
p240:
Extractive Summary:
background ambulatory blood pressure monitoring (abpm) allows serial blood pressure (bp) measurements over a 24-h period.the normal bp profile follows a circadian pattern where night-time values are at least 10% lower than daytime values (dipping status).blunting or absence of this physiologic phenomenon (non-dipping status) is common in chronic kidney disease (ckd) and has been associated with decline of kidney function and progression of cv diseases [3–5].whether dipping status is correlated to adverse outcome independently of hypertension (ht) and other confounders is debated as various studies showed conflicting results [6–8].as in the general population, ht is frequently misclassified in ktx patients and abpm has proved to be a valuable tool in detecting white-coat ht, masked ht and non-dipping status, which are highly prevalent after successful transplantation [12–14].however, circadian bp patterns were not considered and the evolution of egfr over time was not specifically described.measures validity intervals were as follows: systolic bp (sbp) > 50 mmhg and diastolic bp (dbp) > 30 mmhg and < 150 mmhg [5].egfr was estimated by the ckd-epi equation using idms measured creatinine [20].data were considered to be missing completely at random and therefore patients with any missing value were excluded from the multivariate analyses.thus 123 (87.8%) patients without missing values on abpm were included in the present analysis.patient’s characteristics according to systolic dipping status at the time of abpm (t1) are described in table 1.median graft vintage at abpm (corresponding to the timespan between t0 and t1) was 2.5 (0.7 – 6.0) years.compared to the non-dipping group, dippers had higher egfr, lower prescription rate of cni and lower tacrolimus trough levels (p < 0.05 for all).other considered characteristics were similar between groups.abpm readings according to systolic dipping status at t1 are described in table 2.systolic dip was 0.6 ± 6.9% in non-dippers and 14.5 ± 3.5% in dippers.in univariate analysis, dipping status was positively associated with egfr (p = 0.012) and compared to nondippers, dippers had a 9.1 ml/min/1.732 higher egfr.an interaction between time and dipping status was present (p = 0.085 for lrt): egfr slope was -0.3 ml/ min/1.73m2/year in non-dippers (p = 0.59) and -2.4 ml/ min/1.73m2/year in dippers (p = 0.010).egfr evolution over time between dippers and non-dippers is illustrated in fig. 1. in multivariate analysis, dipping status was positively associated with egfr (p = 0.009) and compared to non-dippers, dippers had a 10.48 ml/ min/1.73m2 higher egfr (table 4).interaction between time and dipping status was not significant (p = 0.17 for lrt) and egfr slopes were thus not different between dippers and non-dippers.ht was negatively associated with egfr (p = 0.003).other covariates were not associated with egfr.sensitivity analyses ht defined by abpm values instead of related medication was substituted in the final multivariate model.dipping status remained positively associated with egfr (p = 0.005).ht based on abpm definition was negatively associated with egfr (p = 0.019).diastolic and mean dipping status instead of systolic dipping status were substituted in the final multivariate model.mean and diastolic dipping statuses were positively associated with egfr (p = 0.029 and p = 0.010 respectively).systolic dip as continuous variable instead of systolic dipping as a binary variable was substituted in the final multivariate model.systolic dip was not associated with egfr (p = 0.06).discussion in this longitudinal study, we described circadian bp patterns in ktx recipients, compared obpm and abpm value in this setting and identified systolic dipping status as a major determinant of kidney function.in our cohort, prevalence of ht when combining obpm and abpm measurement was 82.6%.prevalence of ht based on abpm only was 71.9%.systolic, diastolic and mean dipping statuses were wellcorrelated and systolic dippers tended to be mean and diastolic dippers as well.finally, dipping status was not associated with the presence of ht on abpm in our cohort and dippers were as likely as non-dippers to have ht.this absence of a direct relationship between ht and dipping status was also highlighted in ckd patients where non-dippers with controlled bp were almost as prevalent as non-dippers with ht [8].in 48 ht ckd patients followed by timio et al., non-dippers had faster rates of renal function decline and higher proteinuria compared to dippers over a 3-year follow-up [23].the same year, agarwal et al. reported on 217 ckd patients followed during 3.5 years where non-dipping status was an independent predictor of end-stage renal disease [24].as these studies globally concluded that dipping status was an independent determinant of renal function over time, some evidences suggest that such a relationship does not exist.as such, gabbai et al. noted that, although 24 h sbp was associated with subsequent renal and cv outcomes, dipping status did not per se predict progression of renal disease in 617 afro-american with ht ckd [26].in ktx patients, description of bp patterns is much more sparse.the impact of these profiles and renal function is however less well defined.this study was however cross-sectional in nature and very few potential confounders were considered.compared to previous publications on ktx patients, our study differs on several aspects.this is of prime importance in this field as several factors, particularly bp control and proteinuria, were shown to confound the intricate relationship between dipping status and renal function.in our study, a preserved dipping status was associated with a higher egfr independently of potential confounders.the purpose of this study was thus rather different than ours and results are not directly comparable.finally, in previous studies, whether alteration in circadian bp profile was the cause or the consequence of kidney function decline was not entirely clear.in our study, although dippers had higher egfr compared to non-dippers overall, differences seemed most striking at t1 when abpm was recorded.when considering potential confounders however, kidney function decline rates were similar between groups.in a previous study on ckd patients with established diabetic nephropathy, time trajectories of renal function according to dipping status were similar to ours [33].first, compared to our study, minimal adjustment only was considered and those results could represent residual confounding.thus, the longer time span between transplant and initial evaluation in our study could have allowed sufficient influence of established circadian bp patterns to impact renal function.limitations our study has limitations that should be considered when interpreting the results.while the longitudinal design, statistical methodology and pathophysiologic considerations could indicate a causal effect of circadian bp patterns on renal function, a reverse impact of egfr on dipping status is possible.
number of words= 1048
[{'rouge-1': {'f': 0.34393290403306076, 'p': 0.7565079365079366,'r': 0.2225573192239859}, 'rouge-2': {'f': 0.1897750758593513, 'p': 0.3449003984063745,'r': 0.13090026478375993}, 'rouge-l': {'f': 0.3689943058723289, 'p': 0.6143037974683545,'r': 0.26369369369369366}}]
-----------------------------------------------------------------------------------------------------------------------------------
p241:
Extractive Summary:
the segregation of the disease in the pedigree was not compatible with x-linked disease and therefore only autosomal linkage analysis was performed under a fully penetrant dominant model, with the affection status of 5 individuals as affected (6237, 6238, 6463, 6464, 7825), 3 individuals as unaffected (7826, 7827, 7828) and 3 as unknown (7014, 7015, 7824).the maximum lod score was 0.9028, and it was observed at 388 markers across 11 different chromosomes (chromosome 1, 2, 3, 4, 6, 7, 8, 10, 12, 13 and 18), narrowing the candidate gene(s) to only 3% (388 snps/11,335 snps) of the genome (supplementary fig. 1).copy number variants (cnv) were also analyzed but there was no co-segregation of any particular cnv in all affected individuals of the family.whole exome sequencing was performed in 6237, 6238 and 6463, identifying 5 heterozygous rare variants (minor allele frequency or maf < 0.01) in the linked regions (fig. 1).sanger sequencing of the 5 variants in 11 individuals where dna was available identified 2 of these to be segregating in all affected individuals, while the other 3 did not (fig. 1).this included the variants in lama2 (chr6, nm_000426.3:c.380a > g (p.thr127ala); maf 1.76 × 10– 5) and loxl4 (chr10, nm_0002211:c.1684_1686del (p.glu562del); maf 3.871 × 10– 5), which affected amino acid residues that were found to be highly conserved across species (fig. 1).mafs were determined by gnomad v.2.1.1, which contains 125,748 exome sequences and 15,708 whole-genome sequences.both variants were predicted to be deleterious by in-silico programs (supplementary tables 1 and 2).lama2 contributes to laminin networks and localizes to the mesangium while loxl4 catalyzes cross linking of collagens and is expressed in glomeruli and tubules.detailed examination of electron microscopy of basement membranes was undertaken in 4 individuals: 6237, 6238, 6463 and 7825.in 6237, regions of dense mesangial matrix were observed (fig. 2) but this was not observed in other biopsies of the same individual or the 3 other individuals.the glomerular basement membrane (gbm) thickness was also compared.the average gbm widths (± sem) were: 574.3 nm ± 11.8 (6237; male), 345.7 nm ± 8.1 (6238; male), 375.6 nm ± 8.5 (7825; male), 495.9 ± 7.2 (all individuals) (fig. 2).two of these patients, 6237 and 7825, had wider gbms than historic control averages (male 373 ± 42 nm, n = 59 male kidney donors; and 326 ± 45 nm, n = 59 female kidney donors) [17].discussion and conclusions our comprehensive genetic analysis in this fsgs family consisting of linkage analysis narrowed candidates to 3% (388 snps/11,335 snps) of the genome.whole exome sequencing subsequently identified segregating rare variants in lama2 and loxl4 as candidate disease genes.in the glomerulus, it heterotrimerizes with laminin beta-1 or 2 (lamβ1, lamβ2) and laminin gamma-1 (lamc1), called laminin 211 or 221, to form the mesangial extracellular matrix [18, 19].the variant lama2 t127a exists in the laminin n-terminal (ln) domain, which is responsible for trimertrimer interaction of laminin polymer formation involved in the initiation of basement membrane assembly [20].certain variants in lama2 have reported to cause lama2-muscular dystrophy, an autosomal recessive disorder caused by loss of laminin-211 in skeletal muscle [21].none of the affected family members had evidence of lama2-muscular dystrophy.lama2 protein is also expressed in most tubular segments with the exception of proximal tubules (https:// esbl.nhlbi. nih.gov/ ktea/) (22).loxl4 encodes an amine oxidase enzyme that is copper dependent and hypothesized to catalyze the cross linking of collagens and elastins [23].it is expressed in both glomeruli and tubules (https:// www.prote inatl as.org/ ensg0 00001 38131- loxl4/ tissue/ kidney; https:// gtexp ortal.org/ home/ gene/ loxl4; https:// esbl.
number of words= 585
[{'rouge-1': {'f': 0.41177548364455235, 'p': 0.6442187500000001,'r': 0.30259493670886073}, 'rouge-2': {'f': 0.18427596320324394, 'p': 0.2543137254901961,'r': 0.1444849445324881}, 'rouge-l': {'f': 0.3617165967645296, 'p': 0.5164285714285715,'r': 0.2783333333333333}}]
-----------------------------------------------------------------------------------------------------------------------------------
p242:
Extractive Summary:
rt dose thresholds for dysphagia suggest that risk persists even with emerging investigational efforts in swallowing optimized rt planning [11].while acute dysphagia improves for the majority of patients in the early months after rt ends, many survivors suffer chronic or progressive radiation-associated dysphagia years after treatment.in fact, among those with late (beyond 5 years) toxicities, 66% have severe dysphagia requiring lifelong feeding tube dependence [12].recent work highlights the severity of this endpoint to cancer survivors, who ranked feeding tube dependency as one of 6 outcomes of their cancer treatment that were worse than death [13].there is a rapidly growing pool of hnc survivors at risk for chronic dysphagia, with 68,785 incident cases annually in the usa and canada [14, 15].a large proportion of hncs are now human papillomavirus (hpv)- driven oropharyngeal cancers, the incidence of which is expected to sharply rise through 2030 [16].almost all of this fast growing, large subgroup of hnc survivors have been treated with curative rt at doses ≥60 gray, sufficient to induce chronic dysphagia.hpv-associated hnc is diagnosed younger (median age: 54 years) [17] than tobacco-related hnc with excellent five-year survival probability of 79% [18].as such, survivors have the potential to live many active years (even decades) with toxicities of rt including dysphagia.oncologists commonly refer patients for swallowing therapy with a speech language pathologist to lessen risk or severity of radiation-associated dysphagia, but best practice is not established using prospective evidence.some patients are monitored and referred for dysphagia interventions only in response to a swallow problem (reactive therapy), a so-called “wait and see” approach.thus, re-active therapy aims to reverse an already impaired swallow back to its functional pre-morbid state, typically with exercise and mealtime adjustment [19].in contrast, other patients are referred pre-emptively for prophylactic “pro-active” swallowing therapy that aims to prevent or lessen the severity or duration of dysphagia.pro-active interventions are prescribed early – before the swallow problem begins – in an effort to inhibit disuse atrophy and fibrosis [19].pro-active therapy can vary in intensity.some receive proactive therapy focused on keeping them eating to keep muscles active (our pro-active eat group) [20], while others are also prescribed swallowing exercise between meals (our pro-active eat+excercise group) [21].the two broad categories, pro-active versus re-active, as well as the variation in intensity of pro-active therapy (‘eat versus ‘eat+exercise’), represent the most common therapy models in north america [19] and the uk [22].the benefits of swallowing therapy in ideal settings, regardless of its timing and intensity, are suggested by many single institution observational and controlled efficacy trials.two recent systematic reviews (one from the pro-active investigators) showed benefits to various outcomes, including feeding tube dependence, qol, swallow efficiency, function, and swallow physiology from both pro-active (early) and re-active (delayed) therapy [21, 23].specifically, pooled data from efficacy trials suggested lower feeding tube dependence among those who received pro-active therapy with exercise relative to either re-active or lower intensity pro-active therapy (odds ratio [or]: 0.53, 90% ci: 0.29, 0.97, 21).likewise, our meta-analysis identified equally convincing benefit to swallow function from reactive therapy (or: 3.70, 95% ci, 1.53, 8.94) compared to either no therapy or less intensive therapy [21].unfortunately, no review to date has identified the timing (pro-active versus re-active) or intensity (less versus more) most effective when applied in a real-world setting.despite several meritorious efforts using small efficacy trials and even more robust meta-analysis, the decisional dilemma regarding what is the best timing and intensity of behavioural swallowing interventions during rt remains, precluding evidence-based prescription and effective health system resource allocation.the chief dilemma for therapy providers centers on when swallowing therapy should begin.the investigators’ survey [19] of north american speech language pathologists reported an even distribution among these therapies, with 48% of respondents practicing proactive therapy versus 52% who used re-active therapy.clearly clinical equipoise exists and contributes to variation in our practice.the dilemma is further heightened considering the target population comprises rapidly growing numbers of younger hnc survivors who face considerable competing toxicities during rt, of which dysphagia registers the among the most impactful [24].this growing prevalence of hnc survivors with dysphagia makes it even more imperative to identify the most effective therapies, so that healthcare systems can allocate limited resources to optimize functional recovery after curative oncologic treatment.aims and hypotheses to address this evidence gap, the primary aim (aim 1) of the pro-active trial is to compare the effectiveness of pro-active versus re-active swallowing interventions among patients with hnc planned to undergo rt.we hypothesize that at 12-months post radiotherapy the combined pro-active therapies are more effective than re-active therapy (hyp 1.1); and, if so, that more intensive pro-active (eat + exercise) is superior to less intensive pro-active (eat) (hyp 1.2).effectiveness will be measured based on reduced duration of feeding tube dependency, a pragmatic outcome valued by diverse stakeholders including patients, caregivers and clinicians [13, 21, 25, 26].the second aim (aim 2) of the pro-active trial is to compare the relative benefit or harm of proactive to re-active swallowing intervention on all secondary outcomes at both 3- and 12- months post radiotherapy, namely: swallowing impairment, maximum interincisal opening (trismus), swallowing-related qol, symptom burden, self-reported pneumonia, malnutrition and mood disorder, cancer coping strategies, diet level, feeding tube rates, weight loss, hospitalization and/or emergency department (ed) presentation.in addition, the second aim will compare the benefit on reduced duration of feeding tube presence at 3-month post radiotherapy.our hypothesis for this second aim is that the combined pro-active interventions will result in better secondary outcomes at 3- and 12- months following the completion of rt (hyp 2.1).as in our primary aim, for each secondary outcome for which pro-active is superior, dose response of the pro-active arms will be examined hypothesizing the more intensive therapy as superior (hyp 2.2).the third aim (aim 3) of the pro-active trial is to explore the relative benefit or harm of the proactive and re-active swallowing interventions in relevant subgroups according to the following: age, tumor site, prior surgery, radiation dose, chemotherapy use, and prophylactic feeding tube use.our hypothesis for this third aim is that all interventions will be more effective in the following higher risk subgroups: older age; non-hpv related cancer sites; prior surgery; higher radiation dose; chemotherapy (yes); and, prophylactic feeding tube insertion (yes).methods/design overview of trial design this is a multi-center, single-blinded 3-arm behavioural intervention pragmatic rct that will compare the effectiveness of pro-active versus re-active swallowing interventions in patients with hnc receiving rt across eleven sites in canada and the united states.the pro-active trial design was developed using precis2 framework for pragmatic trial methodology and adheres to the funding agency, patient centered outcomes research institute (pcori), methodology standards [27, 28].the research question was determined based on identification of practice variation with clinicians equally committed to each intervention [19, 22], and gap analysis from two recent systematic reviews from the investigative team [21] and others [23] establishing clinical equipoise between re-active and pro-active swallowing intervention.further refinements to the trial protocol, including the selection of secondary outcomes, were made with input from patient and clinical stakeholders following pcori guidelines for stakeholder engagement.the trial protocol is in accordance with the principles of the declaration of helsinki and has been approved by research ethics boards across all sites.the trial was prospectively registered on clinicaltrials.gov (clinicaltrials.gov identifier: nct03455608).any changes that need to be made in the trial protocol will be communicated to all investigators, the ethics committees, and the trial registry.written informed consent will be obtained from each participant.a total of 952 participants with hnc planned for rt who do not currently have dysphagia will be randomized to receive one of 3 swallowing interventions by speech language pathologists:.– arm 1.delayed intervention started promptly if/ when dysphagia is identified (re-active) – arm 2. early low intensity intervention started before rt commences (pro-active eat) – arm 3. early high intensity intervention started before rt commences (pro-active eat+ exercise) the randomization of participants will be stratified by participating site using a centralized, automated and concealed process conducted by the data coordinating center (dcc).participants and therapy providers will not be blinded; however, clinical raters and data analysts will be blinded to the study arm to minimize bias.eligible patients will be randomized in a 1:2:2 allocation to re-active (arm 1, n = 190) and each of the proactive arms (arms 2 and 3, n = 381 per group).all patients will be followed to 12 months post-rt.the main intention to treat analysis will compare duration of feeding tube dependence post-rt.the study schemata is shown in fig. 1.study settings cancer treating centers will be included across eleven north american cities to account for possible regional variation (toronto, houston, boston, madison, new york, london, montreal, baltimore, detroit miami, orlando).the princess margaret cancer centre, part of the university health network (pm/uhn) in toronto, serves as the prime site.in the usa, where patients are as likely to receive treatment at an academic or community facility, we will aim to enroll 50% of participants from both academic and community-based facilities.in canada, where cancer therapy is regionalized at academic teaching facilities, we will enroll all participants from representative academic centers but across two provinces thereby capturing regional variation.study participants eligibility criteria were defined to allow participation of any patient for whom the following decisional dilemma exists: “is there a benefit to proactive swallowing therapy during rt?” in keeping with pragmatic trial standards, the eligibility criteria were operationalized to include any patient who presents to begin rt for hnc with a functional swallow but at high risk for developing post-rt dysphagia: defined as planned rt dose ≥60 gy with treatment volumes including bilateral neck, based on published prediction models for tube-dependent dysphagia [29].our local registries confirm that these criteria should include more than 75% of patients who enter curative (primary or adjuvant) intent rt.as a pragmatic rct, we will limit restrictiveness of eligibility, thereby seeking to include patients who are commonly excluded from efficacy trials.specifically, common exclusions we have avoided for this pragmatic study are: prior surgery or rt; non-squamous cell histology; prior swallowing therapies unrelated to current hnc; and selected hnc sites and stages.we will enroll patients with sufficient fluency in written english, french, spanish or simplified chinese to be able to complete the patient reported outcome (pro) questionnaires and use written therapy materials (i.e., study specific patient hand-outs).eligibility and retention plans are flexible to avoid exclusion of participants based on co-interventions or complications (e.g., pain management strategies, or prophylactic feeding tube use), thereby supporting our planned subgroup analyses for heterogeneous treatment effects (i.e. aim 3).inclusion criteria 1. adults ≥18 years of age diagnosed with head and neck malignancy 2.rt treatment planned for curative intent 3.planned to receive external beam radiotherapy dose ≥60 gy to bilateral neck volumes at participating institution; and 4.sufficient fluency in written english, french, spanish or simplified chinese to be able to complete the study patient reported outcome questionnaires exclusion criteria 1.distant metastasis at enrollment; 2.prior or planned total laryngectomy; 3.moderate/severe dysphagia at enrollment per baseline videofluoroscopy digest grade ≥ 2 (as graded per central laboratory review), 4. previously seen by speech language pathologist for swallowing therapy for the current head and neck cancer 5.diagnosis of second primary non-head and neck cancers in the thorax or the central nervous system at enrollment 6.head and neck radiotherapy for thyroid or cutaneous/skin primary tumors, regardless of neck fields.there will be no purposeful exclusion by age within adult age (18 years of age and older), sex, gender, race, or ethnicity.instead, the composition of age, women, and minorities will reflect the demographic of subjects with hnc who are treated at participating sites.we will adhere to nih standards for maintaining and presenting data on self-declared race and ethnicity including two ethnic categories (hispanic/latino and non-hispanic or latino) and five racial categories (american indian or alaska native, asian, black or african american, native hawaiian or other pacific islander, and white).at each participating site, coordinators will screen tumour board lists and radiation treatment schedules weekly to identify potentially eligible patients.site coordinators will then approach the patient at or after their next standard of care clinic appointment (e.g., with radiation oncology, medical oncology, surgery, or speech language pathology) to obtain consent, after a member of the treatment team has first discussed the trial with the patient.all eligible patients who provide informed consent will be consecutively enrolled to capture a representative spectrum of the target population.a screening log will detail clinicopathologic and demographics of all patients screened, approached, enrolled and randomized to examine the representative nature of the trial, and will be transparently reported in accordance with consolidated standards of reporting trials (consort) standards [30, 31].screening before randomization after giving written informed consent, participants fulfilling all eligibility criteria will be enrolled in the study.participants will only be randomly assigned to the study intervention after screening for no/mild baseline dysphagia using videofluoroscopic swallow (vfs) study digital recordings.the screening step requires independent and blinded central laboratory review of the baseline vfs by trained speech language pathologists.baseline dysphagia is operationally defined as a digest [32] grade of > 2.screen failure information will be captured in the electronic database to ensure transparent reporting according to the consort guidance.
number of words= 2163
[{'rouge-1': {'f': 0.31796656330412804, 'p': 0.8887134502923977,'r': 0.19362030905077265}, 'rouge-2': {'f': 0.18881700429601553, 'p': 0.4160410557184751,'r': 0.1221201413427562}, 'rouge-l': {'f': 0.3262189577510175, 'p': 0.7251724137931035,'r': 0.21044350580781415}}]
-----------------------------------------------------------------------------------------------------------------------------------
p243:
Extractive Summary:
background chronic renal failure (crf) refers to the abnormal renal function and structure persisting for more than 3 months with or without a decrease in the glomerular filtration rate, and its clinical manifestations vary from asymptomatic, laboratory abnormalities to uraemia [1].the global prevalence rate of crf has reached 14.3 %.recently, the incidence of crf, especially terminal-stage renal diseases, has significantly increased, which is a serious threat to human health [1, 2].indeed, chronic renal failure was among the fastest-growing causes of death worldwide and estimated to become the second most common cause of death within the next century in some countries [2].renal interstitial fibrosis is a histological feature of crf and an important predictor of renal function loss in patients [3].it has been demonstrated that post-translational modifications (ptms) can form epigenetic layers that respond to environmental signals and external stimuli, thereby altering the expression of genes involved in crf [4].epigenetic changes, including the importance of ptms in fibrosis, inflammation and immunity related to various kidney diseases, are becoming more important in the development of cfr [5].ptms are an essential part of the protein maturation process, which changes the functions of proteins [6, 7].improvements in the mass spectrometry (ms) technique enhance proteome research and contribute to the recognition of a rich list of ptms [8].protein lysine acetylation is a ubiquitous and reversible ptm.the initial results were mainly through the catalytic regulation of gene transcription and expression in the nucleus by histone acetyltransferase and histone deacetylase [9].in chromatin biology, lysine acetylation was discovered in tubulin and mitochondrial proteins, suggesting that lysine acetylation significantly contributes to cell biology [10].besides lysine acetylation, some new types of ptms, such as lysine malonylation and succinylation, have been identified and play an important role in regulating various eukaryotic and prokaryotic physiological functions [11, 12].lysine crotonylation, a novel protein ptm, was initially found in human cell lines and mouse sperm histone [13].the discovery of lysine crotonylation has attracted extensive attention to the scientific circle.lysine crotonylation has been deeply studied in a short time.lysine crotonylation has been demonstrated first as a potent indicator of active promoters and could be a major signal for controlling male germ cell differentiation [14].wei et al [15] has believed that dna replication may be affected by protein crotonylation, which may result in the inhibition of dna replication, thus affecting the cell cycle.all studies have suggested that lysine crotonylation controls the interpretation of genetic data at the chromatin level and plays a major role in gene expression and cell fate.histone acetylation has been adequately characterised in this paradigm.so, far, studies on histone acetylation have focused on tumours, neuropsychiatric disorders, lupus, cardiovascular diseases, acute lymphoblastic leukaemia, diabetic nephropathy, acute kidney injury (aki) and chronic nephropathy [1, 16–21].according to this, the histone acetylation modification group has a structure similar to that of the lysine crotonylation group [14].recent studies have shown that histone deacetylase inhibitors have protective effects on some experimental models of renal injury [23, 24].histone crotonylation has been observed in renal tissue, suggesting that it contributes to the epigenetic regulation of gene expression during renal injury [25].this study found that lysine crotonylation in renal tissue increased during aki, and crotonate supplementation protected healthy renal tissue from nephrotoxic aki [26].therefore, we speculate that lysine crotonylation significantly contributes to various biological processes and is closely related to the pathogenesis of crf.however, data on the relationship between lysine crotonylation and crf are limited, and the pathogenesis of crf remains unclear.most studies have focused on histone crotonylation and its function, but recent studies have shown that lysine crotonylation also occurs in several non-histone proteins [27].this suggests that non-histone proteins are also widely present in the human body and play a related function, but data on this aspect are still lacking.aki and crf share several pathogenic processes, including inflammation and parenchymal cell death [30].they are considered interconnected syndromes, as crf predisposes affected patients to aki, and aki may accelerate crf progression.thus, lysine crotonylation also can slow the progression of crf and aki.however, we not only found that crotonylation of histone proteins is increased but also found that crotonylation of non-histone proteins is increased and was enriched in cd36 in patients with crf; this is not found in aki.crotonylated non-histone proteins have been characterised in different cell types, such as hela and h1299 cells, and in tissues, such as the lung, kidney, liver, colon, uterus, ovary and brain of mice [32].it is thought to be important in various biological processes, such as rna splicing, gene expression, chromatin organisation, nucleic acid metabolism and cell cycle.however, the requirement of crotonylation for most putative cellular functions has not been formally tested.the functional enrichment of lysine crotonylation in go showed that crotonylated proteins are related to many biological processes, including cellular structural components and cellular molecular binding and pathophysiological processes involved in these biological processes, suggesting that the various interactions related to these biological processes may be regulated by modifying proteins.the functional enrichment of lysine crotonylation in kegg was studied.however, only the b-class scavenger receptor cd36, a kegg pathway, is closely related to renal failure.it showed that crotonylated proteins were enriched in cd36.cd36 is a multifunctional receptor that mediates the binding and cellular uptake of long-chain fatty acids, oxidised lipids and phospholipids, advanced oxidation protein products, thrombospondin and advanced glycation end products and contributes to lipid accumulation, inflammatory signalling, energy reprogramming, apoptosis and kidney fibrosis [33].furthermore, it is expressed in various kidney cells, such as proximal tubular epithelial cells, mesangial cells, podocytes, monocytes and macrophages [32]. to promote crf, cd36 is involved in lipid accumulation, inflammation, energy reprogramming, apoptosis and kidney fibrosis by activating toll-like receptors, na+/k + atpase, nlrp3 inflammasome, pkc-napdh oxidase, scr/lyn/fyn and mitogen-activated protein kinases and tgf-β signalling pathways [32].recent studies have shown that in patients with diabetes, hyperglycemia can increase the expression of cd36, aggravate platelet-mediated inflammation [34], cause apoptosis of renal tubular epithelial cells and accelerate renal tubular degeneration and renal interstitial fibrosis [35].thus, experimental studies have demonstrated that blockade or knockout of cd36 prevents kidney injury.selected patients with crf have shown elevated soluble fibrin plasma levels and enhanced thrombin-induced thrombin generation, which was normalised by cd36 blocking, suggesting that cd36 is a novel therapeutic target for preventing kidney fibrosis [36].the expression and intracellular location of cd36 are regulated by multiple ligands that contribute to gene transcription and ptms.histone crotonylation can promote the expression of genes in the range of increasing substrate availability, but under the condition of promoting inflammation or cell stress, histone crotonylation may decrease the expression of some genes, which are mainly enriched in cd36 [37, 38].we can speculate that histone crotonylation affects the expression of cd36-related genes.in another study, data have shown that crotonylationmodified proteins were reduced in haemodialysis patients because crotonylation may contribute to the recovery of aki.however, patients who maintain haemodialysis are unlikely to restore renal function [39], combined with the role of crotonic acid modification currently being studied in kidney cells.we can speculate that crotonylation also alleviates disease progression in patients with crf and restores some of its functions, whereas in patients with crf, histone and non-histone proteins are higher than those in healthy controls.it can be speculated that in some ways, the body modifies histone and non-histone protein to delay renal fibrosis, further restore renal function and delay the progression of crf.presently, the aetiology of crf is complicated, the clinical manifestations are diversified, the treatment is difficult and the prognosis is of a sort.however, the current treatment is limited.few drugs can inhibit the progression of crf.by further studying the lysine crotonylation reaction in the global proteome of crf, its function in the progression of crf can be further understood.the current therapeutic drugs for posttranslational modification of histone, even if they may affect the expression of multiple genes with different or even opposite functions, can also have certain positive effects under various pathological conditions, including renal injury [40].
number of words= 1303
[{'rouge-1': {'f': 0.29070690211929134, 'p': 0.7630232558139536,'r': 0.17955882352941177}, 'rouge-2': {'f': 0.17079556545616534, 'p': 0.34570093457943923,'r': 0.11341427520235467}, 'rouge-l': {'f': 0.28088188442535167, 'p': 0.5861290322580646,'r': 0.18469534050179212}}]
-----------------------------------------------------------------------------------------------------------------------------------
p244:
Extractive Summary:
introduction hypoxemic respiratory failure requiring mechanical ventilation occurs in approximately 18 per 1000 live births [1], and is associated with increased risk of mortality and morbidity [2].extremely low gestational age neonates (elgans) born at < 28 weeks gestation with hypoxemic respiratory failure frequently experience arterial oxygen desaturations, apneas, or intermittent hypoxia (ih), lasting < 3 min in duration [3].to our knowledge, this is the first study to show low levels of renal ace-2 in the suckling period.these developmentally low levels may account for an inability to mount a compensatory and protective ace-2 response to counteract ace levels.the histopathology results demonstrated that both chronic hyperoxia and neonatal ih are detrimental to the developing kidney, but neonatal ih produced the worst damage.the morphometric analysis supported this finding and a notable increase in kidney size was found in the hyperoxia samples as compared to the controls, possibly due to increased hemorrhage and possible hypertrophy [31].animal models have demonstrated enlarged renal corpuscles and subsequent decreased nephron number in kidneys during periods of hyperoxia [32, 33].previous studies have also shown that neonatal hyperoxia exposure results in impaired nephrogenesis, affecting the nephrogenic zone width and glomerular diameter as well as increased apoptotic cell count [34].there are sparse studies on the impact of neonatal ih on kidney damage and biomarkers of hypertension.a study involving 8-week old mice with obstructive sleep apnea found that ih caused kidney injury accompanied by glomerular hypertrophy, mesangial matrix expansion, increased expression of glomerular growth factors and an increased cellular apoptosis [35].in our study, shorter neonatal ih exposure time with a longer recovery/reoxygenation time resulted in more renal damage than longer exposures with shorter recovery.this may be due to upregulation of mechanisms involved in reperfusion injury.the shorter recovery/reoxygenation time may not be sufficient to initiate those responses.it seems reasonable that a longer ih exposure would require comparatively longer recovery/reoxygenation.the severity of kidney damage was directly correlated with neonatal exposure time with the worst damage seen in the 12 episodes/ day group, where severe hemorrhage and necrosis are demonstrated.the findings in this study prove that hyperoxia and more importantly, neonatal ih can both cause irreversible damage to the kidney structure regardless of recovery/re-oxygenation [31, 36].these are findings are consistent with necrosis that can later lead to the development of chronic kidney disease, supporting our hypothesis.malondialdehyde (mda) is produced by oxidation of polyunsaturated fatty acids [37], and is a major determinant of lipid peroxidation [38].our study showed that mda was significantly higher in the groups recovering from all ih episodes, particularly 8–12 episodes.this finding provides evidence for reperfusion injury and suggests that the immature kidney is highly susceptible to ih-induced lipid peroxidation and cannot sustain more than 6 episodes per day.given the numerous ih episodes reported to occur in preterm infants during the first few weeks of life [4], this may explain, in part, the higher risk for developing aki [39].monitoring mda levels may be a useful biomarker to identify infants at risk.neonatal ih also caused high levels of ang ii and ace, with a plateau seen at 8 episodes per day followed by a decline at 10–12 episodes.this may indicate that 8 ih episodes/day is the maximum number that the developing kidney can sustain.the decline with 8–12 episodes suggest either that the tissue is deteriorating and unable to mount a response, or that there is a bi-phasic response with the shift from predominant hyperoxia (less ih episodes) to predominant hypoxia (more ih episodes).the immunoreactivity results of ace analysis support these findings.ang ii, ace and ace-2 are all part of the raas system, and serve important functions for the preservation of the kidney.while ang ii and ace are involved in vasoconstriction and apoptosis, ace-2 works to counter this effect and balance the system with vasodilation[40–46].ang ii is strongly associated with re-perfusion injury and aki, via reactive oxygen species (ros) production.our data demonstrated a strong correlation with increased ih episodes and renal ang ii and ace production, suggesting renal vasoconstriction and oxidative damage as seen in aki [42, 43, 46].another important finding was that the ace response was more robust in the p7 and p14 animals, while the p21 animals demonstrated a stronger ace-2 response, suggesting that ace-2 is activated with advancing renal development and may be protective, and take precedence over ace.ace-2 has the primary function of degrading ang ii and opposes the effect of ace [37–39].however, this effect may be overridden in neonatal ih as shown in the more mature p14 and p21 rats exposed to 8 ih episodes/day.a study involving ace knockout mice, found that despite unopposed ace and ang ii activity, kidney function and renal development was normal [47].however, in periods of raas activation and kidney stress ace-2 becomes a more vital for preventing kidney damage [45, 46].similar to ace, the efficacy with which the kidney can incite the release of ace-2 during stress reaches a threshold after which it wanes.multiple studies have supported this finding in patients with ckd undergoing dialysis having lower levels of ace-2 compared to pre-dialysis patients [47, 48].the ace-2 pathway may be more effective as reno-protective in aki and early ckd [48–50].the ace/ace-2 ratios were higher with shorter exposure time, peaking with 6 ih episodes/day.longer exposure time resulted in ace-2 overriding ace and resulting in balanced levels, except in the group exposed to 8 ih episodes.studies found that ace/ace-2 ratios were significantly higher in patients with hypertension than in those without [51].other studies in rats showed higher ace mrna compared to ace-2 mrna leading to elevated ace/ace-2.those studies confirm that the ace-ang ii axis was dominant in severe kidney injury [52].this balance of ace and ace-2 is important in maintaining the equilibrium in the kidney, and using these pathways the neonatal kidney appears able to compensate for stress up to a point.however, when threshold is reached, despite subsequent recovery, the damage is irreversible and these pathways are no longer able to function properly, thus leading to ckd.the effects of increasing ih episodes on et-1 were interesting.short term exposure resulted in elevated et-i levels in all groups exposed to 8–12 ih episodes, while long-term exposure resulted in an earlier rise with 6–12 ih episodes.given that et-1 is a potent vasoconstrictor, these findings provide clear support that neonatal ih produces hypertension, with no resolution during the recovery/ reoxygenation phase, suggesting a permanent effect.our findings confirm those of others which showed that et-1 is activated during periods of stress [29, 53– 56] such as with hyperoxia and intermittent hypoxia.et-1 acting via the et receptors is rapidly up-regulated in the kidney by ischemia and has been implicated in renal inflammation and hypertension [57–60].the endothelin system has also been largely implicated as being involved in ckd [55, 56].big et-1 is the precursor to et-1 via proteolytic action, and has minimal biological function, but is able to bind to endothelin receptors with lower affinity [56–58].the concurrent elevation in big et-1 provides further evidence of a vasoconstrictor response, as this precursor is cleaved to form et-1.this finding suggests that neonatal ih induces or promotes big et-1 cleavage.compared to ra and hyperoxia, 8 neonatal ih episodes/ day increased etar and etbr immunoreactivity.etar and etbr both serve as receptors for et-1 but have opposing effects depending on the cell type, tissue type, or physiological situation [58–60].etar has been implicated in vasoconstriction and may play a more active role in causing damage to the kidney when the organ is placed under stress [58, 59, 61–65].etar cause an agonist effect on et-1, and may prevent its degradation [58, 64, 65].conversely etbr may promote et-1 clearance and may be more involved in vascular dilatation [11, 58, 64–66].our findings demonstrated that etar increased with damage caused from hyperoxia and neonatal ih, while etbr declined.higher et-1 and etar with 8–12 neonatal ih episodes/day were consistent with ang ii trends.the increasing severity of damage provide strong evidence that neonatal ih is involved in the pathogenesis of aki in neonates.medication which targets these pathways to promote vasodilation and inhibit vasoconstriction could help prevent aki and subsequent ckd.hypoxia-inducible factor (hif)1α is a transcription factor that regulates the expression of numerous genes and activates various downstream signaling pathways, including erythropoietin production, angiogenesis, energy metabolism, and other related pathways, to facilitate cell adaptation to the anoxic environment [67].studies show that hif1α plays exerts a protective role in acute kidney injury [68].in our study hif1α was higher during recovery/ reoxygenation from hyperoxia, but also during and post neonatal ih, further indicating its renoprotective role.however, studies show that induction of hif1α was associated with reduced apoptosis in rat proximal tubular cells subjected to hypoxia and kidney tissues of mice after renal ischemia-reperfusion injury [69].in our study, apoptosis was indeed higher in the hyperoxia group with lower hif1α.however, this was not the case with exposure to 8 ih episodes, suggesting that frequent ih episodes may override the protective effect.clinical implications the neonatal ih model used in these experiments is clinically relevant.we previously demonstrated that the brief clustered ih pattern is more damaging than dispersed ih episodes [26].using complex mathematical modeling, difiore m, et al. [27] later confirmed our findings in preterm infants.premature neonates, particularly soon after birth, are more vulnerable to the development of aki and ckd [70].stress resulting from hyperoxia and neonatal ih may cause structural damage as well changes in the physiological pathways involved in the function of the kidney.the kidneys appear able to recover shorter numbers of ih episodes/per day due to the longer recovery time between episodes.however, when a threshold is reached due to more frequent ih episodes, resulting in decreased recovery time between episodes, the neonatal kidneys may no longer be able to a mount a compensatory response.this study provided a novel model of assessing the damage caused to the kidney by neonatal ih, which had not been previously assessed in a similar context.conclusions while the study has important clinical implications, there are limitations.for example, the rat model may not accurately reflect apnea experienced by elgans and periviable neonates who may be the most susceptible to the kidney damage [66, 71].these neonates may possibly have different thresholds for compensatory responses.additionally, we did not examine renal histopathology at p7.nevertheless, the most damaging effects were noted in the groups exposed to 8 ih episodes/day, confirming that a critical number of daily ih episodes that may result in irreparable renal damage.long term outcomes such as urinary biomarkers of kidney function and changes in blood pressure was not determined in adult rats, but could be the focus of future studies.our study suggests that the critical number of ih episodes that the immature kidneys can sustain is 6, beyond which irreparable damage may occur.the ang ii and et-1 pathways appear to be highly involved in ihinduced renal damage.therapeutic targeting of these pathways may help decrease the risk for kidney injury in the developing neonate to prevent and/or treat neonatal aki and c
number of words= 1793
[{'rouge-1': {'f': 0.29161870129754996, 'p': 0.8153874538745387,'r': 0.17756123535676252}, 'rouge-2': {'f': 0.1774181204960749, 'p': 0.38481481481481483,'r': 0.11528502930207779}, 'rouge-l': {'f': 0.340593104656977, 'p': 0.6898830409356724,'r': 0.2261119293078056}}]
-----------------------------------------------------------------------------------------------------------------------------------
p245:
Extractive Summary:
rps glomerular class, ifta, interstitial inflammation, arteriolar hyalinosis score, and arteriolar hyalinosis score were determined following standard procedures [24, 25].without knowledge of the clinical outcomes, two experienced nephropathologists (qifeng jiang and yifan zhang) were assigned to independently evaluate and score light microscopy slides.pathological scoring disagreements were resolved by consensus.statistical analyses variables with a normal distribution are presented as the means and standard deviations and were subjected to unpaired student’s t-test between groups, one-way analysis of variance (anova) in multiple groups, or pearson’s test for correlation analysis.nonparametric variables are presented as the medians and interquartile ranges (iqr) and were analysed with the mann–whitney test between groups, kruskal–wallis test in multiple groups, or spearman rank correlation test.categorical variables were expressed as percentages and compared using the pearson χ2 test.the modified arteriosclerosis score cut-off value was analysed using the x-tile plot [28].survival analysis for renal survival or death was performed using kaplan- meier (km) survival curves with log-rank tests and the cox proportional hazards model to identify the potential association between clinical and pathologic variables and outcomes.variables with a significance of p < 0.1 in the univariate models were included in the multivariable models [29].three multivariable models were established, and the first multivariable model (the “clinical model”) included only those clinical variables significant to p < 0.1 in the univariate models.the second multivariable model (the “pathological model”) included only those pathological variables significant to p < 0.1 in the univariate models.the “fully risk-adjustment model” incorporated clinical variables significant at p < 0.05 in the clinical model, as well as pathological variables significant at p < 0.05 in the pathological model.then, the aucs of different risk variables and two models in predicting rs were estimated using time-dependent receiver operating curves (rocs).to estimate median and individual postdiagnosis renal survival probabilities at 1, 3, and 5 years, a nomogram was constructed based on the “fully risk-adjustment model” [20].validation of the prediction model was performed using two parameters: discrimination and calibration [30].discrimination was measured by the concordance index (c-index), whereas calibration was evaluated using a calibration plot.by convention, a concordance index of less than 0.6 indicates poor discrimination, 0.60–0.75 indicates possibly helpful discrimination, and more than 0.75 indicates outstanding discrimination [30].all tests used were two-sided, with a p-value < 0.05 considered statistically significant.all statistical analyses were performed using x-tile software version 3.6.1 (http://tissuearray.org), graphpad prism 7 software (graphpad software) or r version 3.6.3 (foundation for statistical computing, vienna, austria; http://www.rproject.org/).results clinical characteristics at presentation and during followup of 174 patients with biopsy-proven dkd, 37 patients were excluded due to coincidence with other kidney diseases, and 2 patients were excluded due to inadequate tissue samples.finally, 135 patients were enrolled in this study.the mean number of glomeruli for each biopsy specimen was 21.07 ± 9.7.during the 21-month followup (interquartile range, 15–38 months) [31], 10 patients (7.4%) lost connection, 57 patients (42.2%) developed esrd, 5 individuals (3.7%) died before reaching esrd, and 9 individuals (6.7%) died after progression to esrd.the flowchart of the study is shown in fig. 1.the baseline clinical data of 135 dkd patients are listed in table 1.the average age of the enrolled participants at baseline was 52.13 ± 10.42 years, and the majority of patients were male (71.9%), with a bmi of 24.74 ± 3.88 kg/m2.the median duration of diabetes was 7 (3– 11) years, with hba1c of 7.72 ± 3.88%, sbp of 157.12 ± 25.92mmhg, and total cholesterol of 5.89 (4.9–6.81) mmol/l.the median urine protein at enrolment was 4.52 (2.45–7.66) g/24 h, with a median baseline egfr of 45 (29–70) ml/min per 1.73m2.the pathological characteristics of the patients are tabulated in table 2. as depicted in fig. 2, the proportion of severe arteriosclerotic lesions exhibited a correlation with glomerular classification (r = 0.28, p<0.012) and ifta (r = 0.39, p< 0.001) (fig. 3a and b).in glomerular class iv, the proportion of severe arteriosclerosis was higher than in other classes, but there was no significant difference between class iib and class iii.in some cases, the severity of arteriosclerosis of class iii was not more severe than that of class iib (figs. 2 and 3a).ifta is more closely related to arteriosclerosis, and a continuous increase in progressive fibrosis parallels the increase in the proportion of severe arteriosclerosis (fig. 3a and b).figure 4 shows the correlation between the proportion of severe atherosclerosis and clinical variables.the proportion of severe atherosclerosis was significantly negatively correlated with baseline egfr (r = − 0.285, p = 0.001) but significantly positively correlated with urine protein (r = 0.213, p = 0.013), sbp (r = 0.305, p = 0.000), and age (r = 0.221, p = 0.010).univariable and multivariable cox regression analyses were used to explore the association between clinical characteristics and renal survival (esrd or death).only clinical variables that showed significance in the univariate analysis (p < 0.1) were included in the multivariable clinical model.the only clinical variables to remain significant in the multivariable clinical model were baseline egfr, urine proteinuria and albumin (table 3).as shown in table 4, univariate analysis of pathological factors revealed that rps glomerular class (hazard ratio, hr: 2.24, 95% confidence interval, ci: 1.47–3.40), ifta (hr: 2.44, ci: 1.61–3.69), arteriosclerosis score (hr: 2.14, ci: 1.26–3.63) and modified arteriosclerosis score (hr: 4.15, ci: 2.35–7.36) were significantly associated with renal survival.km survival curve for renal survival was performed.higher rps glomerular class (fig. 5a, p = 0.00026), ifta (fig. 5b, p < 0.0001), arteriosclerosis score (fig. 5c, p = 0.0035) and modified arteriosclerosis score (fig. 5d, p < 0.0001) were correlated with shorter rs time in dkd patients.pathological variables with p < 0.05 in univariate analysis were also used to construct two multivariable models.in the pathological model, only rps glomerular class and ifta were statistically significant if rps glomerular class (hr: 1.72, ci: 1.153–2.57), ifta (hr: 1.96, ci: 1.26–3.06) or arteriosclerosis scores (hr: 1.29, ci: 0.73–2.29) were included.however, if the modified arteriosclerosis score (hr: 2.21, ci: 1.18–4.13) was used instead of the arteriosclerosis score in the model, all three variables were statistically significant.in the second multivariable model, clinical features (baseline egfr, urine proteinuria and albumin) and rps glomerular class, ifta or modified arteriosclerosis score were included.the full risk-adjustment model showed that only baseline egfr (hr: 0.97, ci: 0.96–0.98), urine proteinuria (hr: 1.10, ci: 1.04–1.17) and modified arteriosclerosis score (hr: 2.01, ci: 1.10–3.67) were independently associated with rs.
number of words= 1045
[{'rouge-1': {'f': 0.4270349732928467, 'p': 0.8121203438395415,'r': 0.2896776929601357}, 'rouge-2': {'f': 0.28314939256457455, 'p': 0.5010344827586206,'r': 0.1973344651952462}, 'rouge-l': {'f': 0.4191569438876167, 'p': 0.667883597883598,'r': 0.30541666666666667}}]
-----------------------------------------------------------------------------------------------------------------------------------
p246:
Extractive Summary:
background sepsis is a major global cause of high morbidity and mortality for critically ill patients [1] with a continuously increasing incidence [2].besides, sepsis is an enormous burden, accounting for ∼850,000 emergency visits per year and up to 381,000 annual related deaths in the usa [2, 3].sepsis can lead to various complications.for instance, sa-aki is a common and severe complication of sepsis that meets consensus criteria for both sepsis and aki [4], indicating multiple organ dysfunction and significant poor clinical outcomes [5, 6].among critically ill patients with sa-aki, the mortality rates range from 38.2 to 70.2 % [8, 9].nevertheless, so far, no single effective therapy has been reported to change the outcome of sa-aki [10].therefore, early identification of high-risk patients is important for aki prevention [11].accumulating knowledge has highlighted the clinical risk factors, pathobiology, response to treatment, and elements of renal recovery thereby improving the prevention, detection, and treatment of sa-aki [4].participants adult patients (≥ 18 years old) diagnosed with sepsis identified from the international classification of diseases 9th edition (icd-9) code were selected from the mimic-iii v1.4 database.for patients with more than one icu stay, only the first icu admission of each patient was analyzed.finally, the patients were randomly divided into primary (n = 2012) and validation (n = 859) cohorts based on the ratio of 7:3.the following information was extracted directly or calculated using data from the database: age, gender, body mass index (bmi), systemic inflammatory score (sirs), laboratory variables, chronic medical conditions, comorbidities, length of stay in the intensive care unit, the time of aki, administration of drugs.notably, comorbidities and chronic medical conditions were collected based on the recorded icd-9 codes in the mimic-iii database.variables associated with the risk of sa-aki were assessed a priori based on scientific knowledge, clinical importance, and predictors identified in previously published articles [9, 14, 25].definitions and outcomes aki during icu stay was the primary outcome.aki was defined following the kidney disease improving global outcomes (kdgio) criteria [26].vasoactive drugs, diuretic, and aminoglycosides were defined as any vasoactive drugs, diuretic and aminoglycosides use during icu stay for any reason.statistical analysis continuous variables were presented as interquartile ranges (m (p25, p75)) unless indicated otherwise while categorical variables were presented as frequency and proportion of patients in each category.in the primary cohort, the assumption of linearity in the logistic for the continuous variable was assessed and univariate logistic analyses were used to analyze the relationships of relevant variables with sa-aki.all variables with p < 0.05 in the univariate logistic analyses were further assessed by multivariable logistic regression using backward stepwise selection, where the variable with the largest p-value was eliminated at each step until all remaining variables had significant p < 0.05.multicollinearity was evaluated using variance inflation factors and there was no evidence of multicollinearity.nomograms predicting the risk of sa-aki were determined using the independently selected significant variables.while ensuring the stability of prediction performance, a few features were removed to simplify the nomogram [23].the net benefit was calculated by subtracting the proportion of false positives from the proportion of true positives and weighing by the relative harm of foregoing treatment compared to the negative consequences of an unnecessary treatment [30].results characteristics of patients with sa‑aki in total, 2,871 patients with sepsis were recruited and 1, 137 patients (39.6 %) positively tested for sa‑aki.table 1 shows the characteristics of the patient in each cohort.each patient obtains a total point by plus the points of five predictors in nomogram 2.for instance, a patient with mild anemia, the glucose of 10 mmol/l, is using vasoactive drugs, baseline scr of 200 umol/l and 10 days los in icu, total point taken from nomogram 2 of the patient is 38 (5 + 4 + 12 + 6 + 11 = 38), and the risk of sa-aki probability is 60 %.external validation of the nomogram 2 in the validation cohort in the validation cohort, nomogram 2 displayed a cindex of 0.757 (95 % ci 0.724–0.790) for estimation of sa-aki risk.sa-aki has a higher risk of in-hospital death and longer hospital stay than aki caused by other factors [31].this study developed and verified a diagnostic nomogram for predicting sa-aki in critically ill patients.our findings are valuable since the nomogram was established based on a large database of critically ill patients.clinicians may be more accurate in selecting treatment strategies for a higher probability of benefiting from treatments, using a nomogram with clinical factors.the decision curve revealed that nomogram 2 caused a positive net benefit with a threshold probability of between 15 and 80 %.for example, if the threshold probability of a patient is 40 %, the net benefit would be 15 % when nomogram 2 is used to predict aki, hence more benefit than either the treatnone or the treat-all scheme.nevertheless, this nomogram may somewhat help clinicians make reasonable risk judgments and treatment strategies in the absence of high-quality sa-aki prediction tools.conclusions this study developed and verified an aki risk prediction nomogram applied to critically ill patients with sepsis, which may partially help clinicians make reasonable risk decisions and treatment strategies.
number of words= 838
[{'rouge-1': {'f': 0.3877421138209702, 'p': 0.7155223880597015,'r': 0.26592298980747453}, 'rouge-2': {'f': 0.21146284310926436, 'p': 0.34340823970037454,'r': 0.15276643990929706}, 'rouge-l': {'f': 0.34407029406136375, 'p': 0.5667741935483871,'r': 0.24701149425287358}}]
-----------------------------------------------------------------------------------------------------------------------------------
p247:
Extractive Summary:
modifiable risk factors refer to risk factors that can be modified by medical interventions or by individual behavior.clinical variables were also included if they can be modified by medical interventions during perioperative period.extracted data of the included studies were registered on dedicated electronic forms.the forms were piloted over the first 5 included studies for consistency and discrimination.the quality of the included studies was assessed by the newcastle-ottawa scale (nos) [11].the studies were judged on three broad perspectives: the selection of study populations, the comparability of the populations, and ascertainment of exposure and the outcomes of interest for case-control or cohort studies, respectively.a maximum score of 9 reflects the highest quality.no study was excluded because of a low-quality score.two authors (xyz and xjm) performed the data extraction and quality assessment independently.disagreements were settled by discussion involving a third author (hcc) and consensus was reached on all items finally.statistical analysis if a risk factor was reported by at least 2 studies in a consistent manner, we would conducted a metaanalysis.effect size and 95% ci were pooled using a random-effect model with inverse-variance method [12, 13].i2 statistic and cochran’s q test were applied to determine the between-study heterogeneity.a value of i2 of 0–25% represents insignificant heterogeneity, 26–50% low heterogeneity, 51–75% moderate heterogeneity and 76–100% high heterogeneity [14].in addition to the value of i2, we will also consider strength of evidence for the heterogeneity (ci, chisquared test and/or p value) and the size and direction of effect in the analysis [15].p-values on the egger’s test greater than 0.05 and symmetry of the funnel plot determined the absence of publication bias (n ≥ 10) [16].if significant publication bias was noted, duval and tweedie’s trim and fill method was used to acquire adjusted values [17].to minimize heterogeneity, subgroup analyses by diagnostic criteria of aki, duration of evaluation and statistical method were conducted.meta-regression analyses (n ≥ 10) were also used to assess the potentially important covariates that might exert a substantial impact on between-study heterogeneity.sensitivity analyses were performed after excluding 1 study at a time to assess the stability of the results and explore the source of heterogeneity.p < 0.05 was considered statistically significant except where otherwise stated.if data were not available for the meta-analysis or only 1 single study was identified for a given risk factor, these studies were only listed in this systematic review.statistical analyses were performed using stata version 14.0 (statacorp, texas, usa).results literature search and study selection a total of 3273 citations were retrieved after searching pubmed, embase and central database.there were 170 full-text articles assessed for eligibility after screening titles and abstracts.after hand-searching the references of included articles and existing reviews and meta-analyses, 1 reference was added.three articles were excluded due to duplicate data (see supplementary 3, additional file).in total, 67 articles were eligible for inclusion in the systematic review and meta-analysis (see supplementary 4, additional file).full details of the selection process were presented in fig. 1.characteristics of included studies and quality assessment a total of 67 observational studies published between 2001 and 2019 with 28,844 patients were included.the incidence of aki after lt ranged from 3.97% [18] to 71.9% [19].most of the included studies adopted multivariate logistic regression analysis to adjust confounding factors followed by propensity score matching method.the outcome indexes consisted of aki, and rrt due to aki.the rifle, akin, or kdigo criteria were often used in combination with other scales to assess and classify aki.the duration of evaluation varied from 12 h after reperfusion to 3 months after lt.based on the nos, the mean quality score of all included studies was 6.686 (standard deviation = 0.633) (see supplementary 5, additional file).results of meta-analysis incidence of aki after lt overall, the pooled estimated incidence rate of aki after lt was 37.5% (95%ci: 32.3–42.7%, i2 = 99.5%, fig. 2).besides, we further did a subgroup analysis based on different diagnostic criteria.the outcomes indicated that prevalence of aki after lt was 33.5% (rifle), 40.0% (akin), 44.2% (kdigo), and 35.2% (others), respectively.meta-regression showed that the publication year did not significantly affect the incidence rate of aki after lt (p = 0.489, fig. 3).modifiable risk factors of aki after lt we pooled a forest plot for each factor that was described in at least 2 articles (see table 1; details are shown in supplementary 6, additional file).considering the smaller pooled population (< 500) reported for some modifiable factors (cadaveric donor liver graft, intraoperative colloidal use, large postoperative red blood cell [rbc] transfusion, postoperative hypotension), here we only presented the modifiable factors with a relatively large population (> 500) to lower the error of estimates.modifiable factors showing significant associations with aki after lt are presented in fig. 4.all of these factors were classified into the following 4 groups: recipient factors, donor and graft factors, surgical factors, postoperative factors.recipient factors overweight (or = 2.437, 95% ci = 1.629–3.646, i2=%, p = 0.000), preoperative use of diuretic (or = 2.733, 95% ci = 1.302–5.739, i2 = 41.4%, p = 0.008), preoperative anemia (or = 1.621, 95%ci = 1.073–2.449, i2 = 24.5%, p = 0.022) were identified as modifiable risk factors of aki after lt.preoperative hypertension and preoperative hypoalbuminemia were not correlated with aki after lt.donor and graft factors donation after cardiac death (dcd) organ (or = 2.704, 95% ci = 1.938–3.772, i2 = 0.0%, p = 0.000), donor body mass index (bmi) ≥ 30 kg/m2 (or = 2.672, 95% ci = 1.173–6.085, i2 = 57.8%, p = 0.019), abo-incompatible lt (or = 2.761, 95% ci = 1.602–4.759, i2 = 0.0%, p = 0.000), low graft weight to recipient body weight ratio (gw/ rbw) (or = 1.902, 95%ci = 1.013–3.568, i2 = 52.3%, p = 0.045) increased the risk of aki after lt.however, no significant associations were found for long cold ischemia time (cit) and long warm ischemia time (wit).surgical factors intraoperative hypotension (or = 5.582, 95% ci = 3.934– 7.920, i2 = 0.0%, p = 0.000), major bleeding (or = 2.900, 95% ci = 1.495–5.627, i2 = 83.1%, p = 0.002), intraoperative use of vasopressor (or = 2.079, 95% ci = 1.492– 2.899, i2 = 70.3%, p = 0.000), large intraoperative rbc transfusion (or = 3.124, 95% ci = 1.986–4.914, i2 = 72.8%, p = 0.000), postreperfusion syndrome (or = 1.689, 95% ci = 1.275–2.236, i2 = 52.8%, p = 0.000) were associated with an increased risk for aki after lt.nevertheless, no obvious associations were detected with piggyback surgical technique, split lt, venovenous bypass and intraoperative platelet transfusion.postoperative factors postoperative use of vasopressors (or = 2.234, 95% ci = 1.431–3.488, i2 = 75.9%, p = 0.000), overexposure to calcineurin inhibitor (cni) (or = 2.762, 95% ci = 1.737– 4.391, i2 = 0.0%, p = 0.000), cni without mycophenolate mofetil (mmf) (or = 2.087, 95%ci = 1.404–3.103, i2 = 0.0%, p = 0.000), graft dysfunction (or = 3.124, 95%ci = 2.036–4.795, i2 = 0.0%, p = 0.000), infection (or = 3.162, 95%ci = 2.315–4.320, i2 = 0.0%, p = 0.000) were associated with a higher risk for aki after lt.
number of words= 1161
[{'rouge-1': {'f': 0.3081965189117424, 'p': 0.7927272727272727,'r': 0.19128146453089245}, 'rouge-2': {'f': 0.19729332174776562, 'p': 0.4215981735159817,'r': 0.12877862595419848}, 'rouge-l': {'f': 0.32997681272785684, 'p': 0.6574125874125873,'r': 0.22026833631484796}}]
-----------------------------------------------------------------------------------------------------------------------------------
p248:
Extractive Summary:
traditional herbal medicines have been widely used in south korea for a long time.the japanese chaff flower, achyranthes japonica, has been used as complementary medicine for edema and arthritis and to delay a woman’s menstruation without clinical evidence.recent studies have reported the antiinflammatory, pain relief, and antibacterial effects of a. japonica as well as its ability to improve osteoporosis conditions in ovariectomized rats [3–6].however, there has been no report on side effects.therefore, we report a case of ain after ingesting a. japonica.this case report suggests that ingesting a complementary medicine could be a cause of aki leading to chronic kidney disease (ckd) if management is delayed.case presentation a 56-year-old korean woman was admitted for deterioration of renal function.she had suffered from general weakness and nausea for the past month and visited a local clinic.she was recommended to visit the tertiary hospital because of poor renal function.she was a farmer, but she strongly denied any recent exposure to pesticides.she had intermittently taken medicines for arthralgia in both knees.she had been diagnosed with hypertension 3 years previously and was taking 5 mg lercanidipine without change.she had a routine check-up 1 year ago and did not show any abnormalities of renal function at that time.she also denied various infections and other systemic or auto-immune diseases that cause acute tin by a thorough history taking.however, she complained of a 5-kg weight loss in the last 3 months.she did not complain of fever, oliguria, skin rash, or a urine color change at admission.she had not taken nsaids, toxins, or chinese herbal medicines, but had ingested the extract of a. japonica for control of knee pain beginning 4 months ago.her initial vital signs were as follow: blood pressure of 140/80 mmhg, heart rate of 68 beats/min, respiratory rate of 20 breaths/min, and body temperature of 36.1 °c. no remarkable findings were observed in the physical examinations.her complete blood count (cbc), biochemical findings, and arterial blood gas analysis was shown table 1.briefly, her blood urea nitrogen and creatinine were elevated whereas potassium level decreased.there was no eosinophilia on her cbc.a arterial blood gas analysis was compatible with metabolic acidosis and anion gap was normal.c3 and c4 levels were 102 mg/dl (normal, 90–180 mg/dl) and 26.9 mg/dl (normal, 10– 40 mg/dl), respectively.
number of words= 374
[{'rouge-1': {'f': 0.5062355742204292, 'p': 0.733157894736842,'r': 0.3865829145728643}, 'rouge-2': {'f': 0.24714024150965394, 'p': 0.33455026455026454,'r': 0.19594458438287155}, 'rouge-l': {'f': 0.4638274271867457, 'p': 0.6272519083969466,'r': 0.3679591836734694}}]
-----------------------------------------------------------------------------------------------------------------------------------
p249:
Extractive Summary:
following transplantation, patients continue to be considered chronically ill as they have to adhere to lifelong immunosuppressive medication regimes and need to pay close attention to all lifestyle choices in order to avoid aggravation of medication side effects and graft dysfunction [2].video consultations have been emphasized by health authorities as a potential mean to improve access to care and increase quality of care [8–10].studies have shown how video consultations are highly appreciated by patients with chronic illness already using such solutions [15–17], that video consultations can enhance engagement and communication between health care providers and patients [15, 16], and that such solutions have been considered to be effective and convenient [14, 17].some researchers have also argued that video consultations are best suited for consultations where a physical examination of the patient is not required [18, 19] and for settings where an established patient – health care provider relationship is already in place [14].therefore, the aim of the current study was to investigate the perceived benefits and challenges of using video consultations in outpatient renal transplant recipient follow-up from the perspective of patients and health care providers.methods study design and participants the current study explored the patient and health care provider experiences of using video consultations in regular outpatient renal transplant recipient follow-up.the patients could download and access the cisco meeting app solution via their own personal computer (pc), tablet or smartphone.the patients received a letter with information about how to access the solution from their own device, including a personal log-in username and password.patients were also informed about the importance of security precautions and encouraged to make sure they were calling from a private environment where others were not present (i.e., could hear or see them) during the consultations.individual interviews were conducted with patients and health care providers 1 year post final participant inclusion (i.e. april 2020).results participant information seventeen renal transplant recipients, 1 patient in pretransplant phase, and 3 of their health care providers were included in the study and used video consultations in outpatient follow-up.patient participants were enrolled in the study for minimum 12 months (june 2018 – april 2020).patients’ self-reported number of video consultations undertaken during the study period ranged from one to ten. of the 21 participants enrolled in the study, 15 patients and all 3 health care personnel agreed to participate in the post study interviews.of the three patient participants not consenting to participate in the interviews, one had never conducted a video consultation and two did not give a reason for declining. of the patients (n = 15) participating in the interviews, gender was equally represented (female 53%), age was median 53 years old (range 37–64), most were married or cohabitating (12/15, 80%) and all except one were renal transplant recipients.practical execution of the video consultations the alternating between in-person and video consultations was considered an appropriate arrangement by patients as well as the nephrologist.however, some of the patients expressed concerns regarding the procedure, stating that it might be too difficult to follow for people with less technical knowledge/experience, and suggested that the hospital staff could provide more direct “get started” help for those who might need it.one of the patients said: so maybe finding out in advance how much the person knows about the technical things.the latter was possible when the patients had a private room available at work where they could close the door to participate in the consultation.a majority of the patients reported using their smartphone for the video consultations, closely followed by those using tablet or pc.two of the patients bought a new device due to lack of compatibility with the nhn cisco meeting app video consultation solution.(patient 13) the patients and health care providers stated that it had not always been possible to detect the cause of the error, but in many cases there were concrete reasons for the errors.one error that took the team some time to discover, was that the video solution went into “sleep mode” if the patient logged into the video solution too long before the consultation started.one of the health support personnel explained: so we found out lately, the patient has been in the app for too long, so the app has gone to sleep.for example, the patients lost their password or they bought a new telephone or computer and were unable to download the application onto the new device.if i have not been there, and the doctor has not been able to get in touch with the patient, then the doctor must first go to find me, i must come to the consultation room, and then we find the patient's cell phone number, and then i have to call the patient.(health support personnel 1) more than half of the patients described having had to continue the consultation by telephone one or more times, after trying and failing for a while when technical problems occurred.one of the patients stated that every consultation had started with video and ended by telephone: it has actually been so bad that in the end we have decided to do the rest of the consultation by phone.mutual trusting patient – physician relationship, established over many years half of the patients reported having had the same nephrologist, without exception, during their respectively 3– 25 years follow-up at the hospital.three of the patients had had follow-up by other nephrologists for some years, and five of the patients had had other nephrologists only when their usual nephrologist was on vacation or for other reasons unavailable.in that regard, the patients stated that changing the consultation from in-person to video did not represent too much of a change for them as the consultation followed the same structure and covered the same topics whether they were in-person or by video.one patient said: it's the same.i feel as confident and safe with the treatment and what we agree on by video, as what i do when present in the doctor's office.(patient 6) the nephrologist also acknowledged the video consultation as equivalent to the in-person consultation: for the doctor, i would say it is the same.we have our appointment schedule, and that time is set aside for each patient.some of the patients did however express need for re-assurance that they could ask for an inperson consultation if they felt it was necessary.benefits the majority of the patients interviewed stated that they wanted to continue with video consultations, and would also recommend them to others, even if they had experienced a lot of technical problems during the study.and from what i have seen from the patients, who have used video consultation, they are also very satisfied when it actually works.(health support personnel 1) less trips/travel to the hospital the benefit most often reported by patients as well as health care providers was the time saved and that the patients did not have to travel to the hospital for the consultations.one patient said: this has meant that i don’t have to set aside so much time.i have to go all the way to the hospital.some of the patients also mentioned the economical aspect, that money usually spent on travel was saved and also that there was a socio-economic aspect of not needing to leave the workplace to go to the hospital for consultation.some patients also mentioned psychological benefits related to the use of video consultations.for example, one of the patients reported feeling less stressed when conducting the consultation from home, as visits to the hospital usually came with feelings of stress and anxiety.(patient 8) as the interviews were conducted right around the beginning of the covid-19 pandemic situation, several of the patients mentioned that the pandemic situation, with the risk of disease spreading had strengthened their positive attitude towards video consultations.one of the patients said: it is one of the advantages, when such situations occur [covid-19], that you can then still keep in touch with the doctor.(nephrologist) potential challenges and harms some patients said they could not think of any disadvantages to using video consultations, while some described how they thought video consultations could carry the potential for harm.need for physical examination several of the patients mentioned that there was a potential risk of overseeing important signs and symptoms when the consultation was conducted over video and not in-person, particularly related to aspects such as pain, wounds, weight, blood pressure, heart rate and so on.one of the patients said: the downside, of course, is that there may be things that you are not aware of, that the doctor can discover physically, when in-person.(patient 5).the nephrologist also described the potential risks of overseeing important aspects: if there were clinical findings, which in a way, will not be discovered, because you cannot examine the patient.despite the technical challenges experienced the majority of the patients appreciated being able to have video consultations as an alternative to in-person consultations at the hospital.the benefits outweighed the technological shortcomings the patients in the current study described a number of benefits related to using video consultations.time savings was one of the benefits reported, with patients living far away saving a significant amount of time.this is in line with previous studies showing that time spent on travels is unwelcome to patients [14, 17, 24].as renal transplant patients have to have routine follow-up for the rest of their lives, being able to have some of these follow-up consultations (i.e., every second follow-up in the current study) via video appeared to bring a sense of freedom to the patients.traditionally, the routine in-patient consultations at the hospital were set, but with the new alternative, patients could attend the consultation from wherever they wanted, which provided them with a new sense of freedom and flexibility.the patients could even travel around the world if they wanted to, and still be able to have close follow-up from health care providers that they knew and trusted at home.one reason for this might be that many of the patients were experienced technology users.perceived seriousness of the conditions has also been seen to be a key factor influencing patients’ willingness to use electronic consultations, as lower use has been associated with patients in poorer health conditions [26].aspects of human interaction the results from the current study underlined an established, trusting relationship between patient and physician as a prerequisite for the video consultations to be experienced as a good service.the current study further emphasized that not only do the patients need to trust their nephrologist, but the nephrologist must also trust the patients.in the current study, the mutually trusting relationship allowed the patients to trust that they would receive the same high quality of care, whether they met the nephrologist inperson or by video.the patients in this study had to measure physical functions, such as blood pressure and weight, usually measured by health care providers at the hospital, by themselves.as such, the results from the current study showed renal transplant follow-up as suitable for video consultations, particularly when providing patients and health care providers alike with the opportunity to alternate between in-person and video follow-up.the use of video consultations may not be suitable for all types of patients, not even all types of patients post renal transplant.can adequate care be provided should there be the need for a physical examination?individual considerations are required.third, all patients included in the current study were identified and selected by the nephrologist to make sure that the participants were in a stable phase of their disease and also somewhat familiar with technology.finally, the interviews conducted in the current study provided insight into a variety of patient experiences and as such allowed sufficient depth in the analyses, which can be considered a strength.future implications future studies examining the use of video consultations in renal transplant recipient follow-up should aim to include larger patient samples from multiple centers and strive to incorporate more variation in terms of where the patients are in the disease trajectory.this could involve including patients in less stable phases of their disease, if considered appropriate, justifiable and professional sound to do so. furthermore, future options to have health data such as blood pressure, weight, body composition, heart frequency or oxygen saturation transferred by apps directly to the health care team or into a personal health care suite available in advance of the video consultation should also be explored.findings also support the notion that initial small-scale studies, with few selected patients in stable phases of their condition, may lay the groundwork before providing a larger population of patients and health care providers with the option of video consultatio
number of words= 2077
[{'rouge-1': {'f': 0.32895444123453516, 'p': 0.895072886297376,'r': 0.20150557620817844}, 'rouge-2': {'f': 0.22260777781803145, 'p': 0.5202923976608187,'r': 0.1415946071594607}, 'rouge-l': {'f': 0.34599163061706695, 'p': 0.7173988439306358,'r': 0.22796897038081806}}]
-----------------------------------------------------------------------------------------------------------------------------------
p250:
Extractive Summary:
secondly, likely in a degenerated disc, neovascularisation can be accompanied by infiltrating bacteria.by itself, this may irritate the nerve and cause pain symptoms, but it can also stimulate the disc to excrete pro inflammatory factors which worsen pain and may stimulate macrophages to differentiate towards m1.this could result in more m1 and less m2 macrophages and lead to more radicular pain symptoms.in addition, the adjacent endplate might also get involved, which could lead to more irritation of the adjacent nerve.animations in the figure originate from cliparts.zone and are free to use, mri picture is our own.study design this prospective-(longitudinal) observational cohort study is an imaging, histological, immunological and clinical study.both the mri scan and surgery are part of the usual care, in addition to these procedures, this study will draw 3 blood samples, use the rest material from surgery and ask the patient to fill in short pain related questionnaires.the total duration of the study is approximately 2.5 years, of which 1.5 years for inclusion and 1 additional year for the follow up.because lumbar disc herniations occur more often than cervical ones, the inclusion of lumbar patients might be finished after 1 year, while the inclusion of the cervical patients will likely take 1.5 years.patients will be recruited from one of the three inclusion centers in which they are planned for surgery: the spaarne gasthuis haarlem zuid, alrijne hospital leiderdorp, hmc the hague and haga hospital the hague.the analyses and coordination of the study will be done from the leiden university medical center (lumc).patient recruitment will take place during the first (pre-operative) visit with the neurosurgeon, where the patient will be informed about the study.patients that want to participate will be asked to sign an informed consent form (icf) in the week prior to the surgery.afterwards, the participant will fill in a set of online questionnaires in castor edc.this set will contain questionnaires that assess demographic data, pain scores (nrs back/leg pain for lumbar patients and nrs neck/ arm pain for cervical patients), disability scores (odi for lumbar patients and ndi for cervical patients), osteoarthritis (womac) and medication status.furthermore, the patient will receive an mri scan in the weeks before surgery which is part of the usual care.in addition, during surgery, the herniated part of the disc will be dissected and transferred to the lumc for further analyses.the dissected material will be used for three different purposes: one part of the dissected material will be used for bacterial culture, one part for histological analyses, and one part will be snap frozen for later defined analyses.. in addition to the frozen disc samples, this study will also collect blood samples for two purposes: the first is to assess the general inflammation (bse, leukocyte differentiation) and vascular status (lipid and apolipoprotein profile) and the second purpose is for future analysis, for which a sample will be stored in the freezer.only if our hypotheses turn out to be true, the stored blood samples will be used to search for a predictive biomarker for a bacterial infection and/or inflammation of the disc.the exact analysis that will be used to identify the biomarker will be defined in a later stage and will be based on our results from the histological analysis and bacterial cultures.blood will be drawn in the waiting room for surgery from the canula that is placed for the purpose of the surgery.this means that only the donated blood is an additional aspect of the study, injecting the canula is part of the usual care.all participants will be asked to co-operate during the entire follow-up.during follow-up patients will be asked to fill in questionnaires regarding clinical outcome (nrs, odi/ndi & gpe) and medication status at 8 weeks, 16 weeks, 26 weeks and 52 weeks post-surgery.patients will receive emails with a link to the follow-up questionnaires at the above mentioned time points.a timeline of the study is provided in fig. 2.study population population (base) for this study, all patients (18–75 yr.) with 8 or more weeks of radicular pain symptoms, that are eligible for surgery according to a neurosurgeon in the participating hospital, will be asked to participate.eligibility criteria for surgery in participating hospitals are persisting pain symptoms after 8–12 weeks of conservative therapy with an mri verified hnp that compresses the nerve root corresponding to the radicular pain symptoms.patients are eligible for the study if they already planned to undergo surgery for herniated disc and meet the following in and exclusion criteria: inclusion criteria lumbar patients – age 18–75 – a unilateral lumbosacral radicular syndrome, with at least the following criteria: o radicular incitement: radiating pain from (a part of the) dermatome l4, l5 and/or s1 o present for at least 8 weeks – mri verified lumbosacral disc herniation that is corresponding to the side of the symptoms – indication for surgery – informed consent cervical patients – age 18–75 – a unilateral cervical radicular syndrome, with at least the following criteria: o radicular incitement: radiating pain from (a part of the) dermatome c45, c56, c67 and/or c7t1 o present for at least 8 weeks – mri verified cervical disc herniation that is corresponding to the side of the symptoms – indication for surgery – informed consent exclusion criteria lumbar – previous lumbar spinal surgery or chemonucleolysis – paresis of mrc < 4 – history of spinal inflammatory disease – instability that requires surgical fixation – active infection at the time of surgery – usage of anti-biotics in the past six months – epidural steroid injection in the past six months – pregnancy – inadequate knowledge of the dutch language cervical – previous cervical spinal surgery chemonucleolysis – paresis of mrc < 4 – myelopathy as major complaint – history of spinal inflammatory disease – instability that requires surgical fixation – active infection at the time of surgery – usage of anti-biotics in the past six months – epidural steroid injection in the past six months – pregnancy – inadequate knowledge of the dutch language study parameters/endpoints main study parameter/endpoint the main study parameter will be the nrs leg pain for lumbar patients and nrs arm pain for cervical patients.a description of these endpoints and all other parameters used during the study are described below.nrs pain scores – the pain experienced by the patients will be measured by questionnaires that assess leg pain for lumbar patients and arm pain for cervical patients: nrs leg pain, and nrs arm pain.in these validated questionnaires, the patients will display the amount of pain they have experienced in respective locations during the week previously to the visit.the pain intensity will be determined on a scale of 0–10.0 represents ‘no pain’ and 10 represents ‘worst pain imaginable’ [16].nrs will be focusing on multiple aspects of pain: maximum/average intensity, frequency and maximum interference.all nrs pain scores will be measured during baseline (in the week before surgery) and at every follow-up moment (8, 16, 26, 52 weeks).follow-up intervals 8, 26 and 52 were choosen based on our previous studies and 16 weeks was added as an extra time point in between [4].previous test results will not be visible for the patient.in addition to the nrs leg pain, nrs back pain will also be used as an additional outcome measure.also, the nrs arm pain will be accompanies by the nrs neck pain as an additional outcome measure.other patient reported outcome parameters – functionality for estimating functionality of the lumbar patient, the oswestry disability index (odi) will be used.this validated questionnaire contains 10 topics related to the impact of the pain on the patient’s life, with 5 grading’s for each topic.the total will give a score between 0 (no disability) and 50 (maximum disability possible), which will be calculated to a 1–100% score [16].for estimating functionality of the cervical patient, the neck disability index (ndi) will be used, which is an adjusted version of the odi focused on neck pain instead of back pain and is also a validated questionnaire.all functionality scores will be measured during baseline (in the week before surgery) and at every follow-up moment (8, 16, 26, 52 weeks,).previous test results will not be visible for the patient.– recovery in order to estimate the perceived recovery of the patients, the global perceived effect (gpe) questionnaire will be used.the gpe is a widely validated questionnaire in which patients can express their perceived recovery on a 7 point likert scale.on this scale the numbers 1–7 are accompanied by an expression of a state such as ‘fully recovered’ = 7, ‘same as before’= 4 or ‘very bad’ = 1.all recovery scores will be measured at every followup moment (8, 16, 26, 52 weeks,).previous test results will not be visible for the patient.predictive parameters – bacterial infection of the disc bacterial infection in the disc will be verified by a bacterial culture protocol.tissue necessary for the bacterial culture will be harvested from herniated disc tissue that was taken out during surgery.the bacterial culture will be accompanied by a gram stain and methyl blue stain.infection can be distinguished from a contamination by the quantity of colonies on the culture (0–10 is considered as contamination).nevertheless, since it cannot be ruled out that the presence of 0–10 colonies is an infection instead of contamination, additional analysis with 0–10 regarded as infection will also be performed.– disc inflammation disc material harvested during surgery will be stained for the presence of macrophages, for m1 and m2 macrophages separately.evaluation will be done through counting cells and subsequently categorizing them.– modic changes type (1,2 or 3) and severity (< 50% & > 50%) of mc will be scored at baseline on mri.
number of words= 1592
[{'rouge-1': {'f': 0.28423249176734194, 'p': 0.7025088339222616,'r': 0.1781570996978852}, 'rouge-2': {'f': 0.13658184835682896, 'p': 0.2295744680851064,'r': 0.09720677146311972}, 'rouge-l': {'f': 0.27977903848700214, 'p': 0.4852046783625731,'r': 0.19655971479500892}}]
-----------------------------------------------------------------------------------------------------------------------------------
p251:
Extractive Summary:
lambert-eaton myasthenic syndrome (lems) is a rare autoimmune neuromuscular junction dysfunction resulting from antibodies that are generated against voltagegated calcium channels (vgcc) on presynaptic nerve terminals, thereby suppressing the release of neurotransmitters such as acetylcholine [1–3].lems is characterized by limb girdle muscle weakness, easy fatigability, absence of deep tendon reflexes with post-tetanic potentiation, and autonomic alterations, such as dry mouth and erectile dysfunction.a confirmation diagnosis of lems also requires a combination of specific vgcc antibody detection and characteristic electrophysiological results.vgcc-antibody-positivity has been observed in 100 % of lems patients with sclc and 90 % of lems cases without potential malignancy [12].over the past few decades, many symptomatic drug treatments have been tried, including guanidine, pyridazine, 4-aminopyridine and 3,4-diaminopyridine (3,4- dap, amifampridine), the last of which has proven to be the most effective [14].with the exception of 3,4-dap, these drugs have not been studied in clinical trials, but only in small case series.in december 2009, 3,4-dap was approved for the first time in europe and was subsequently recommended as a first-line treatment for lems in 2010 [15].methods literature search a comprehensive search of electronic databases including the cochrane neuromuscular disease group specialized register, the cochrane central register of controlled trials (central), pubmed, web of science, medline and embase was performed for all years up to january 2020.by searching these databases, a total of 122 articles were retrieved and 99 articles were deleted by title and abstract.of the remaining 23 articles, 15 articles were deleted for reasons including no case-control studys, different interventions or insufficient data.the search and select process is shown in fig. 1.the primary outcome indicator was change in muscle strength score (quantitative myasthenia gravis, qmg), or limb muscle strength measured by myometry.the secondary outcome indicator was change in average cmap amplitude at rest.data extraction two reviewers (d.h. and w.m.) independently examined all titles and abstracts retrieved from the literature search of various databases, and evaluated the full text of all potentially related studies.baseline data were collected using standardized tables, including author, year of publication, number of cases, patient age, sex, presence of cancer, and outcome measures (mean, standard deviation).whenever possible, insufficient data was obtained from the study authors.quality assessment the methodological quality of each included article was independently assessed by two authors using the cochrane collaboration’s tool to determine the risk of bias (fig. 2).the risk of bias process includes sequence generation, blinding, allocation concealment, selective reporting, processing of incomplete results data, or any other form of bias [17].the items were rated as “yes” representing a low risk of bias, and “no” representing a high risk of bias.‘unclear’ was used when there was inappropriate information to make a judgment, or when the project was not relevant to the research.statistical analysis due to cross-over studies included in this meta analysis, we pooled data with the generic inverse variance (giv) method, which uses the mean differences (md) and standard error (se) of the mean for the diference between treatment and control.results description of studies the results of the literature search found 8 rcts that used 3, 4-dap to treat lems.however, in two of these rcts, 3tug was used to evaluate the outcome of treatment without the use of qmg or cmap, therefore the two rcts had to be excluded as they did not meet the inclusion criteria.the six eligible rcts included 115 patients with lems treated with oral or intrevenous 3,4-dap or placebo, without any healthy participants.the characteristics of all included studies are shown in table 1.three patients in the first group received an initial dose of 15 mg per day, that was gradually increased up to a daily dose of 80 mg at the end of the 8- day course.the second group increased their daily intake of 30 mg to 75 mg during the 3-day study because of time constraints.qmg score and cmap were recorded as the outcomes [21].patients received a dose of at least 30 mg/day to enter the second part of the study.qmg score and cmap were performed on day 14 [22].the sixth rct was a double-blind, parallel study.qmg score was recorded on day 4 [23].data synthesis primary outcome measure: the score on a muscle strength scale the six included rcts used a muscle strength score or myometric limb measurement as an outcome indicator, and reported significant improvements in muscle strength score, or myometric limb measurement after treatment.therefore, we only compared the overall therapeutic effects of the four rcts that reported a qmg score by observing the change in qmg score from baseline to 3, 4-dap or placebo treatment.a meta-analysis of the secondary outcome cmap showed that the overall cmap amplitude increased significantly in lems patients with 3, 4-dap treatment, compared with placebo treatment.all primary outcome indicators of isometric muscle strength [14], neurological disability score [19] and qmg score [20–23] showed significant improvements after the administration of oral or intravenous 3,4-dap.as sanders pointed out, it may be due to the fact that bulbar, ocular, and distal limb items of the qmg score are less common in lems than in myasthenia gravis.therefore, the change of mean amplitude of cmap appears to be a repeatable and objective secondary outcome for any lems treatment trial.in addition, not all rcts used the same primary outcome (qmg) and secondary outcome (cmap) measures.
number of words= 869
[{'rouge-1': {'f': 0.3403308564610257, 'p': 0.7448768472906404,'r': 0.22054945054945055}, 'rouge-2': {'f': 0.19467380157995187, 'p': 0.3571287128712871,'r': 0.13380638063806383}, 'rouge-l': {'f': 0.32445932132948435, 'p': 0.5847058823529412,'r': 0.2245253863134658}}]
-----------------------------------------------------------------------------------------------------------------------------------
p252:
Extractive Summary:
myelopathy and sensory deficit inaccurately predicted the extent of spinal cord intramedullary t2 signal and the level of savf.this was thought to be due to the unequal distribution of venous outflow channels along the spinal cord, with the lower thoracic region having relatively fewer venous outflow channels compared to the cervical region.this results in venous congestion being transmitted in a caudocranial direction, explaining the more severe myelopathy symptoms correlating with a more distal region of the mri signal [7, 8].this also explains why majority of the savfs involve the thoracolumbar region.similarly, the rate of clinical progression and severity are independent of the length of abnormal spinal cord signal.our patient with high cervical savf from a feeding artery that originated from the meningeal branch of the left vertebral artery was completely lacking brainstem signs as well as the commonly reported subarachnoid hemorrhage or intraparenchymal bleed within the brainstem [9].the actual reason behind this is unclear.one study described five different types of craniocervical junction avfs (dural, radicular, epidural with pial feeders, epidural and perimedullary) based on location of the shunt point.the authors inferred that radicular type is more commonly associated with sah besides other factors such as aneurysm of the feeder artery and anterior spinal artery as the feeder.all these features were not present in our patient [10].this clinical-radiological discrepancy/mismatch may be an important clue to differentiate savfs from other causes of myelopathy.secondly, increased venous hypertension in low-flow savf reflects the underlying dynamic vascular pathophysiology of this disorder.although the underlying aetiology and pathophysiology of savf is not perfectly understood, we know that arterialization of the venous system diminishes the arteriovenous pressure gradient and leads to a decreased drainage of normal spinal veins, resulting in venous congestion [7].therefore, any disruption around this arteriovenous system may result in savfs.in relation to this, we observed 2 unique events among our patients that could have led to the development of myelopathy.one of our patients developed symptoms following hemorrhoidectomy.to our knowledge, acute presentation post-operatively, and in our case, following hemorrhoid surgery, has not been reported.the pathogenesis of hemorrhoids is thought to be due to abnormal distension of the arteriovenous anastomoses within the hemorrhoidal cushions, in which part of the surgical treatment involves the ligation of hemorrhoidal arteries [14, 15].it is still unclear if hemorrhoidectomy could affect the hemodynamics within the inferior vena cava (ivc) and the venous system, resulting in increased venous pressure at the spinal venous plexus contributing to an acute presentation in a patient with a pre-existing subclinical savf.dural savf reported following spinal surgery such as correction of scoliosis and microdiscectomy for radiculopathy, further supports the postulation of an acquired traumatic or inflammatory mechanism [16, 17].another patient of ours developed extensive dilated epidural vessels from c6 to t2 following an incomplete embolization of the left vertebral artery savf.although this occurred after a year, it was not unusual.in majority of the patients, savfs, symptoms are usually slowly progressive over many months to years.elswick and colleagues reported a case of dural savf that developed 5 years after thoracolumbar fusion for scoliosis [16].thirdly, majority of patients with savfs have mris that show either longitudinally extensive intramedullary t2 hyperintense signal, flow voids or cord expansion.the probability of one of these mri changes occurring in a patient is reported to be as high as 100% [18].despite that, these features are frequently missed due to its rare occurrence [19].spinal angiogram is considered the gold standard investigation to confirm savfs [20].however, no imaging modality has perfect sensitivity and specificity.a false negative spinal angiogram may further complicate clinical judgement.this could be due to misinterpretation of images, technical difficulties especially when involving small savf < 1mm or limited spinal segments on angiogram study [3, 13, 20].highly specific angiographic techniques performed by experienced interventional specialists and meticulous analysis of angiographic images are essential for accurate diagnosis and identification of the location of the fistula [3, 4, 20].
number of words= 642
[{'rouge-1': {'f': 0.4188301249660604, 'p': 0.5685835694050991,'r': 0.33151560178306094}, 'rouge-2': {'f': 0.1399097452517335, 'p': 0.1665909090909091,'r': 0.1205952380952381}, 'rouge-l': {'f': 0.33537928645652926, 'p': 0.4047826086956522,'r': 0.28629213483146065}}]
-----------------------------------------------------------------------------------------------------------------------------------
p253:
Extractive Summary:
the proportions of the two subtypes vary in different countries, which is associated with the national income [2].in addition to caffeine, coffee contains other biochemical compounds that affect human health [7].caffeine and phenolic compounds have antioxidant properties and regulate intracellular signaling for growth, proliferation, and apoptosis [8]; they also modulate the gut microbiome [9] and glucose and fat metabolism [10, 11].however, some studies have found consistent protection against stroke in moderate to heavy coffee drinkers [15, 16].habitual coffee drinking is associated with uncontrolled hypertension in older adults with hypertension [20], although a meta-analysis of other cohorts found no association between habitual coffee consumption and hypertension [21].in addition, the sympathomimetic effect of coffee, especially caffeine, may be responsible for an increased risk of atrial fibrillation [22].moreover, confounding factors, including smoking, alcohol consumption, diet, education, and physical activity, interact with coffee regarding the risk of stroke [24].in addition to overall stroke, hemorrhagic stroke and ischemic stroke were subjected to subgroup analyses for risk assessment.methods literature search strategy all relevant articles in english published from january 1, 1990, to may 31, 2020, were identified by searching pubmed, biomed central, medline, and google scholar.the meta-analysis procedure complied with prisma guidelines.inclusion and exclusion of studies the inclusion criteria were as follows: (1) clear definition of stroke diagnosis, (2) clear definition of the quantity of coffee consumed, (3) cohort study published as an original article, case series, or letter to the editor, and (4) publication in english.after ineligible studies were excluded, seven studies were ultimately included.the flow of the selection process is illustrated in fig. 2.data extraction the following data were extracted: name of the first author, year of publication, country and location, study design, and diagnostic criteria for stroke.three investigators (ch bai, ct hong, and l chan) independently reviewed all data, and conflicts were resolved through consensus.two investigators (ch bai and yc fan) independently extracted data from the seven candidate studies.all seven studies enrolled healthy individuals without a history of stroke, and two studies included only female participants.overall stroke, ischemic stroke, and hemorrhagic stroke were identified through self-reports, confirmation of medical records, or a national health care database.most studies categorized caffeine consumption as degrees 3 to 5 points based on the number of cups of coffee per day or week, and only two studies provided yes or no options for regular coffee consumption.considering variations in coffee consumption among studies, we considered results from all degrees of coffee consumers and considered the no-exposure group a reference to determine the hrs.subgroup analyses were conducted to investigate the risk of ischemic and hemorrhagic stroke in coffee drinkers.heterogeneity was unremarkable for both hemorrhagic and ischemic strokes ( i2 = 3.732% for hemorrhagic and 0% for ischemic).no obvious dose-dependent or u-shaped effect (highest protection with moderate amounts of coffee and no protection with the highest amount) was observed under either condition.compared with alzheimer’s disease, the benefit of coffee on parkinson’s disease is more substantial not only for reduced risk in healthy people but also for slowing disease progression in patients [32].coffee also has beneficial effects on cardiovascular disease (cvd), another major vascular occlusive disorder.cvd shares several modifiable risk factors with stroke, such as hypertension, diabetes, hyperlipidemia, smoking, and obesity.the effect of coffee consumption reducing the risk of cvd has been established based on several large-scale cohort studies and pooled metaanalysis.in the present metaanalysis, our findings reveal that for ischemic stroke, the net effect of coffee consumption is beneficial, with a considerable risk reduction of nearly 20%.our findings reveal the stroke-preventive benefit of coffee consumption through a meta-analysis that included prospective, large-scale cohort studies with healthy or stroke-free individuals, and the follow-up spanned decades.
number of words= 603
[{'rouge-1': {'f': 0.4416293840949858, 'p': 0.639023569023569,'r': 0.33740506329113923}, 'rouge-2': {'f': 0.20785841671943045, 'p': 0.2760810810810811,'r': 0.16667194928684628}, 'rouge-l': {'f': 0.3490631293509459, 'p': 0.4842011834319527,'r': 0.2728985507246377}}]
-----------------------------------------------------------------------------------------------------------------------------------
p254:
Extractive Summary:
for this type of lesion, treatment with standard intravenous thrombolysis alone leads to a good clinical outcome in only 17 % of cases, with a death rate as high as 55 % [6].since 2015, several trials assessed the superiority of mechanical thrombectomy in the treatment of ais due large vessel occlusion in the anterior circulation over standard medical care [10–13].nevertheless, the efficacy of endovascular treatment in tl, as well as the choice of emergent carotid stenting is not yet clearly defined.in this study, we report a multicenter experience in evt of ais due to ica occlusion, either in isolation or in the setting of a tl.methods patient selection data collection was carried out from may 2018 to march 2020 at the hub centers for ischemic stroke in bologna (ospedale maggiore carlo alberto pizzardi) and in salerno (ospedali riuniti san giovanni di dio and ruggi d’aragona).we included in this study all patients (n = 51) with isolated ica occlusion (both on an atheromatous basis or due to dissection) or tl.ethical approval was waived by the review boards of our institutions (ospedale maggiore carlo alberto pizzardi and ospedali riuniti san giovanni di dio and ruggi d’aragona) in view of the retrospective nature of the study.all procedures being performed were in accordance with the 1964 helsinki declaration and its later amendments and were part of approved integrated care pathways (pdtai 017 and pdta 234 resolution no. 321 of june 18, 2018).twenty-one patients (41.2 %) were on antiplatelet therapy (asa or asa + clopidogrel) prior to the stroke event.the comparison of patients with successful and unsuccessful intracranial recanalization is reported in table 2.c) lack of post-procedural hemorrhagic transformation: only 2 patients (20 %) among those who had this complication (even though asymptomatic) reached an mrs of 0–2 at 90 days (vs. 78 % of those without hemorrhagic transformation, p = 0.001).notably, patients with favorable outcome were those who had a satisfactory degree of revascularization.the internal carotid artery occlusion study (icaro) showed a higher favorable outcome in patients treated with iv thrombolysis (28.9 %) compared to controls (20.6 %), but an increase in mortality and intracranial bleeding was also observed [16].acute treatment for ischemic stroke has been rapidly evolving over the past 5 years resulting in a dramatic improvement of functional outcome after ischemic stroke in selected patients [19].furthermore, none of these patients developed sich, and only one patient, who died before the follow-up at 3 months, had parenchymal hematoma type 2 (ph2).we found similar results when evaluating the association of post-procedural hemorrage with clinical and angiographic outcome.furthermore, this factor affects the mortality rate, which was 2 % in the first group and 33 % in the second group (p = 0.015).this data is in accordance with a previous study which found a correlation between recanalization and outcome in acute ischemic stroke: recanalization is strongly associated with improved functional outcome and reduced mortality [22, 26].in our study, we analyzed both patients with single ica occlusion and patients who presented with tl, who underwent evt.our data are in accordance with a previous study, which found that the occurrence of recanalization is associated with a 4- to 5-fold increase in the odds of good final functional outcome and a 4- to 5-fold reduction in the odds of death [22].in all 4 cases of death, the pre-treatment nhiss was higher than 12.importantly, early nihss scores has a strong prognostic value for long-term functional outcome after stroke [27, 28], however the strong correlation between nihss and mrs scores does not ensure that the nihss is a valid surrogate endpoint [29].emergency carotid artery stent placement is expected to reopen an extracranial ica occlusion with increase of cerebral blood flow in the affected hemisphere.also, early flow restoration across the tandem lesion in the mca and/or distal ica after extracranial ica stent placement would reverse the ischemic process by stopping the expansion of the ischemic core into the penumbra.the use of stents certainly allows immediate flow restoration but also increases the technical complexity of the procedure.therefore, stent placement led to a good angiographic result also in the tl group, even though these patients have a less favorable prognosis.no acute stent thrombosis was observed, while the occurrence of hemorrhagic transformation was 17 % in patients who received antiplatelet therapy and 21 % in patients in whom it was not administered (p = 0.729).therefore, in our series this therapy was not associated with a greater risk of hemorrhagic transformation.in patients who received pre-treatment antiplatelet therapy, a tici score of 2b-3 was found in 86 %, which was slightly higher than that found in patients in whom pre-treatment antiplatelet therapy was not administered (80 %).among the comorbidities we have considered (diabetes, hypercholesterolemia, hypertension and atrial fibrillation) only atrial fibrillation showed a correlation with worse outcome, with a trend towards significance.it is also relevant to note that 3 out of 4 patients who died before the 3-month follow-up belonged to the group of diabetic patients.we hypothesize that the use of new devices and technical advances that have occurred since the time of the study by nedeltchev et al., such as mechanical clot disruption and mechanical thrombectomy devices, most likely contributed to this difference [35].limitations the present study had several limitations.first, it was observational retrospective, so it was prone to selection and other biases.second, we used multiple different devices for the treatment of tandem occlusions, making it difficult to determine the effect of different thrombectomy techniques.third, we did not have a control group with which to compare outcome rates.last, since the main focus of this study is the treatment technique, we did not analyze perfusion imaging to quantify the ischemic area before revascularization.conclusions our study has shown that evt in the treatment of ais due to ica occlusion (both on an atheromatous basis and due to dissection) is feasible, safe, and effective in determining a good functional outcome at 90 days; in our study the mortality rate was 8 %, which is similar to the one reported in recent studies [35, 37].in our experience, stent deployment has proved to be particularly safe and has led to good angiographic results, additionally therapy with a glycoprotein iib / iiia inhibitor immediately after stent release did not result in a greater risk of hemorrhage.
number of words= 1034
[{'rouge-1': {'f': 0.40269784154055965, 'p': 0.8420588235294117,'r': 0.26462465245597777}, 'rouge-2': {'f': 0.26639133770923856, 'p': 0.5091143911439114,'r': 0.18038961038961038}, 'rouge-l': {'f': 0.4056574713240255, 'p': 0.7345161290322582,'r': 0.2802040816326531}}]
-----------------------------------------------------------------------------------------------------------------------------------
p255:
Extractive Summary:
background asymptomatic carotid artery stenosis (acas) is characterized by extracranial internal carotid artery stenosis in the ipsilateral carotid region, usually without a history of stroke or transient ischemic attack (tia) [1].increasing evidence suggests that cognitive impairment may occur in not just symptomatic patients with a history of cerebral infarction or other cerebrovascular events, but also in those with acas [2].after patients’ baseline data were collated and analysed, 15 healthy volunteers were recruited based on age, sex, and years of education.results in order to reduce the limitations of multiple factors on the study results, 14 patients with unilateral acas (average age, 63.6 years) were included in this study through strict inclusion criteria, and 15 healthy controls (average age, 62.4 years) were matched according to age, sex, years of education, and underlying diseases.in the graph theory analysis, there were no significant differences between the patients and the healthy controls for the node-based network attributes and global parameters after the false discovery rate (fdr) test (table 3), including assortativity (p = .057), hierarchy (p = .080), network efficiency (p = .596), small-worldness (p = .372), or synchronisation (p = .881).the functional connections between the inferior frontal gyrus of the right hemisphere and the left orbital inferior frontal gyrus, hippocampus, superior temporal gyrus, and middle temporal gyrus were significantly low in the acas patients compared with the controls (fig. 1).a previous study observed that patients with acas gave a significantly poor performance on the mmse and the montreal cognitive assessment [14].therefore, we conducted a more detailed and systematic test using computer workstations to determine the specific extent of cognitive impairment in acas patients who had normal mmse scores.this study found a statistically significant decline in short-term word and picture memory ability in acas patients, but not in reasoning and judgment ability, executive function, and reaction speed.notably, calculation ability and spatial imagination ability were well- preserved in these patients.therefore, the ability of short-term memory of words might be predominantly impaired in these asymptomatic patients.a study found a markedly decreased blood oxygenation level-dependent correlation between opposite hemispheres when the roi was predefined on the stenotic side (flipped to the left) in acas patients, suggesting a disruption of interhemispheric connectivity in these patients; [9] however, this finding is controversial because there are some differences between the left and right sides of the brain.it is thought that the brain of patients with acas could reconcile a good deal of short-range connections for segregation, while maintaining a sufficient number of remote connections to ensure processing integration.therefore, the results suggest that patients with carotid artery stenosis have different effects on the affected side, and that hemispheric template results may be more accurate for the global analysis of graph theory.additionally, a reduced efficiency of functional connections in brain networks was observed in patients with acas (fig. 3).in asymptomatic patients, the brain network is damaged and the node response is not significant, which may be slightly related to the illness, but also represents a higher sensitivity of functional connections.the decline of functional connectivity in acas patients was mainly concentrated in the left and right inferior frontal gyri (ifg), temporal lobe, left cingulate gyrus, and hippocampus (fig. 1).our findings suggest that the weak association between these areas and the triangular part of the right inferior frontal gyrus may be directly related to short-term memory decline in patients.we further found that five functional connections between the triangular part of the inferior frontal gyrus (aal no.14) and other brain regions were significantly correlated with short-term memory decline.this finding may suggest that cognitive function may be dominated not only by a single brain region or node, but also by multiple regions or networks.at present, the function of the temporal lobe is not understood fully, but the sense of hearing is one of its main functions.it has been suggested that some visual resolution, as well as memory, language and some motor functions, may depend on the temporal lobe [24] [25].notably, increasing evidence suggests that the hippocampus may be associated with shortterm memory [26], and damage on both sides of the hippocampus may cause short-term memory loss, without affecting long-term memory.these correlations were not significant in normal controls.it may also be related to insufficient samples in this study, and we will continue to conduct longitudinal studies.recent studies have shown that the left supramarginal gyrus is one of the key nodes of the short-termmemory network involved in retaining an abstract representation of serial order information, independently from the content information [27].the decline of functional connections between the right inferior frontal gyrus and the left supramarginal gyrus is related to the impairment of short-term memory (r = −.545, p = .044).this finding suggests that connections in these brain regions may indeed be important for short-term memory.the purpose of this study was to increase the understanding of the mechanism of cognitive impairment by determining the correlation between cognitive impairment in acas and functional connectivity of brain networks, and to help define the criteria for intervention in the treatment of the disease.this study has some limitations.therefore, further studies including more patients with different handdominance and stenosis are needed to confirm our current findings.the alterations in network connections may be an important mechanism underlying the decline of cognitive function in patients with acas.
number of words= 870
[{'rouge-1': {'f': 0.39134773332477785, 'p': 0.8007692307692307,'r': 0.25895027624309397}, 'rouge-2': {'f': 0.24723690942271528, 'p': 0.45626609442060084,'r': 0.1695575221238938}, 'rouge-l': {'f': 0.37211511859943264, 'p': 0.6436434108527131,'r': 0.26170984455958546}}]
-----------------------------------------------------------------------------------------------------------------------------------
p256:
Extractive Summary:
for acute ischaemic stroke within the time windows, iv thrombolysis (intravenous thrombolysis) is a clinically effective and guideline-recommended therapeutic method [1].after intravenous thrombolysis, statins are regularly used as medicine for ischaemic stroke patients during hospitalization [2].however, several studies have shown different conclusions regarding whether intravenous thrombolysis combined with statins is related to functional outcomes in ischaemic stroke patients [3–5].therefore, it is uncertain whether intravenous thrombolysis combined with statins is effective for ischaemic stroke patients.a study showed that high-dose statins could increase the risk of intracerebral haemorrhage in ischaemic stroke patients [9].these findings further raise doubts about the safety of intravenous thrombolysis combined with statins.therefore, the conclusions may not be suitable for asian patients.the present prospective observational cohort study aimed to further explore the relationship between using low-dose statins combined with intravenous thrombolysis and the effects and safety outcomes of asian ischaemic stroke patients.methods participants we consecutively recruited patients with acute ischaemic stroke who received iv thrombolysis in the neurology department of west china hospital, sichuan university.the stroke diagnosis of the patients met the who stroke diagnostic criteria.all patients had a stroke pack (head ct: cta + ctp) examination.the study is a prospective observational cohort design.low-dose statins were defined as atorvastatin 20 mg, simvastatin 10 mg and rosuvastatin 10 mg daily after onset [12].the inclusion criteria were as follows: (1) age over 18 years; (2) the diagnosis of stroke with evidence of neuroimaging (ct or mri); (3) the patients did not receive statins before onset; (4) the patients in the statin group started statins after onset for 7 days or more and (5) iv thrombolysis therapy administration within 4.5 h after stroke onset.the exclusion criteria were as follows: (1) mrs was ≥ 2 before onset; (2) the time between admission and onset was more than 4.5 h; (3) treatment with other doses of statins after the onset of the disease or the use of statins for less than 7 days; (4) stroke associated with trauma or surgery; and (5) intracerebral haemorrhage, subarachnoid haemorrhage, coagulopathy, cancer, cardiac failure, or severe hepatic or renal dysfunction.the study was performed in accordance with the declaration of helsinki and the ethical standards of the institutional and/or national research committee.the study was approved by the ethics committee of west china hospital, sichuan university with approval number 2019 (319).definition of risk factors cardioembolic stroke was proven by definite history, electrocardiography, and cardiac colour ultrasound.the haemorrhage events included intracerebral haemorrhage and gastrointestinal haemorrhage.ich (intracerebral haemorrhage) was proven by ct in the hospital regardless of whether the condition of the patient changed.gastrointestinal haemorrhage included haematemesis and haematochezia.an mrs score of 0–2 was defined as a favourable functional outcome (ffo) at 90 days.a difference in nihss score greater than 4 was defined as an improvement.a nihss score of 0–4 was defined as a mild stroke, a nihss score of 5–15 was defined as a moderate stroke, and a nihss score of 16–40 was defined as a severe stroke.the data included age, sex, blood pressure, history of smoking and drinking, nihss score and mrs score at admission, history of diseases and history of drug use.types of strokes and types of statins were also collected.we recorded laboratory data, including tc (total cholesterol), tg (triglyceride), hdl-c (high-density lipoprotein cholesterol), and ldl-c.the two experienced neurologists who evaluated events and outcomes were blind to the patients’ condition and grouping.a chi-square test was conducted for categorical data and ranked data.to analyse the outcome events, we compared the group differences using the chi-square test.we analysed the effects of risk factors on the outcome events using univariable and multivariable logistic regression methods.safety outcome when we evaluated haemorrhage events in the hospital, we found that the low-dose statin groups had a lower percentage of ich events (p < 0.001) and gastrointestinal haemorrhage(p = 0.003) than the control group (table 1).older age (or = 1.061, p = 0.002), higher nihss at admission (or = 1.122, p < 0.001), higher nihss at 7 days after admission (or = 1.130, p < 0.001) and cardioembolic status (or = 2.779, p = 0.005) were positively related to a higher percentage of death events within 2 years (supplementary table 2).older age (or = 1.061, p = 0.009) and higher nihss at 7 days after admission (or = 1.075, p = 0.006) were still positively related to a higher percentage of death events at 2 years (table 4).subgroup analysis in the subgroup analysis, in mild stroke patients, statin use was only significantly inversely related to ich (or = 0.01) in the hospital and death event rates (or = 0.01) at 2 years.in severe stroke patients, statin use was significantly related to nihss improvement (or = 4.833) and death event rates (or = 0.083) at 2 years.patients not taking these drugs when they have an intracerebral haemorrhage might partly explain the inconsistent results.this finding suggested that statins might be a protective factor against gastrointestinal haemorrhage in acute ischaemic stroke patients with intravenous thrombolysis.the effect was significant for different subgroups of patients.the results were similar to manuel cappellari’s study about the effect of statins on the chance of death in acute ischaemic stroke patients with intravenous thrombolysis over 3 months [5].in conclusion, for acute ischaemic patients after intravenous thrombolysis, the use of low-dose statins was significantly related to nihss improvement, fewer haemorrhage events in the hospital and death events within 2 years.these relationships were significant, especially for moderate stroke patients and noncardioembolic stroke patien
number of words= 896
[{'rouge-1': {'f': 0.46917452114261265, 'p': 0.8653795379537954,'r': 0.321828631138976}, 'rouge-2': {'f': 0.3176207527339299, 'p': 0.5534437086092716,'r': 0.22271966527196654}, 'rouge-l': {'f': 0.43866329374520535, 'p': 0.782,'r': 0.3048284960422164}}]
-----------------------------------------------------------------------------------------------------------------------------------
p257:
Extractive Summary:
background over the last 30 years, stroke has become the leading cause of adult-disability and death in china, which causes huge burden to family and society [1–3].the incidence and morality of stroke has been increasing year by year, and ischemic stroke represents the majority of total strokes [1, 4].intravenous thrombolysis (ivt) has been proven to be an effective way to help patients gain reperfusion and improve the clinical outcomes of ischemic stroke [5–7].however, this treatment is highly time-sensitive.studies have shown that shorter dtn time was associated with lower disability, mortality and better outcomes [8, 9].however, due to the limitations of material resources and manpower, the median dtn time varies in different hospitals.the most common reasons for the time delay were failure to identify the eligible patients timely, requirement to control hypertension aggressively, and in-hospital delays [9].we hypothesized that the joining of a stroke nurse would reduce the time used to response and cut the procrastination in hospital.the role of the stroke nurse was to escort patients suspected to suffer from stroke during the emergency diagnosis and treatment.once the stroke nurse was alerted, she would take initial history, help patients get assessment and imaging examinations and help neurologist on duty to review the indications and contraindications for recombinant tissue plasminogen activator (rt-pa).during the ivt process, the stroke nurse would monitor the patient’s symptoms and vital signs, such as heart rate, respiration and blood pressure.the stroke nurse’s work would be finished when she handed the patient to the inpatient department.we observed 1003 consecutive acute ischemic stroke (ais) patients who arrived within 4.5 h from november 2016 to december 2020 before and after the participation of the stroke nurse.the inclusion criteria were (1) ais symptoms occurred within 4.5 h and received thrombolytic therapy [rt-pa 0.9 mg/kg] ;(2) nihss score ≥ 5 at admission;(3) age over 18 years old;(4) imaging evidence suggested anterior circulation infraction.patients with posterior circulatory ischemia (poci) was excluded because stroke severity was inaccurately assessed by the nihss in those patients and the proportion of those patients was small.detailed information is shown in fig. 1.each patient was assessed by professional neurologists using national institute of health stroke scale (nihss) score at the admission before intravenous (iv) thrombolysis administration, 1 h, 24 h, 72 h and 7 days from admission.a face to face/through phone follow-up assessment was arranged after 3 months using the modified rankin scale (mrs).we also use the results of regression analysis and bootstrap method to create mediation models to explore the significance of mediation effects among the join of stroke nurse, dtn time and 90 days mrs score.th=tlb= ethics approval and consent to participate this study policy was explained detailly and verbal informed consent was obtained from each study patient or their family members, which was approved by the ethics committee of nanjing first hospital, nanjing medical university and conducted in full accordance with the world medical association declaration of helsinki.there are three vital periods of time to evaluate: the onset-to-door time, dtn time and onset-to-needle time.we found that the preintervention group shows shorter onset-to-door time which results in the nondifferential onset-to-needle time.we define the reduction of nihss score ≥ 4 as improvement in neurological functions within 7 days to see if the stroke nurse helps to improve patents’ early functional recovery.there are several limitations in our study.
number of words= 549
[{'rouge-1': {'f': 0.42512541646678453, 'p': 0.5944755244755244,'r': 0.3308695652173913}, 'rouge-2': {'f': 0.1868517461713334, 'p': 0.23842105263157895,'r': 0.15362369337979095}, 'rouge-l': {'f': 0.3326890318094907, 'p': 0.46490445859872614,'r': 0.25902439024390245}}]
-----------------------------------------------------------------------------------------------------------------------------------
p258:
Extractive Summary:
background primary central nervous system lymphoma (pcnsl) is a relatively rare and malignant brain tumour, most frequently presenting as diffuse large b-cell lymphoma (dlbcl), which is characterized pathologically by an angiocentric appearance, with lymphoma and inflammatory cells surrounding small blood vessels [1–3].it is confined to the brain parenchyma, spinal cord, eyes, or leptomeninges but without involvement of systemic disease at the time the patients are diagnosed.its overall incidence is 0.47 per 100,000 people per year in immunocompetent individuals, with a higher incidence in immunosuppressed patients [4].high-dose methotrexate chemotherapy has become the first-line treatment for pcnsl in the last 2 decades because it can cross the blood-brain barrier.patients were divided into two groups according to the diagnostic procedure: a resection group and a stereotactic biopsy group.two neurosurgeons in our department independently performed and recorded detailed preoperative evaluations, including a careful history and a physical examination of all patients.patients underwent brain magnetic resonance imaging (mri) on admission.outcome and follow up patients were followed up after hospitalization and received mri examinations during the follow-up period for the assessment of outcomes.overall survival (os) was calculated from the date of the diagnosis to the date of death or censoring.the mean age of the population was 53.3 ± 14.3 years, the mean kps was 77.6 ± 15.8, and the mean duration of symptoms was 5.8 ± 11.2 months.in the resection group, 23 patients had a single lesion, and 5 patients had multiple lesions.adjuvant therapies all patients were given chemotherapy and/or radiotherapy after resection or biopsy.thirty-two patients (45.7%) received only high-dose methotrexate-based chemotherapy, 10 patients (14.3%) were treated with whole-brain radiation therapy alone, and 28 patients (40%) were given high-dose methotrexate-based chemotherapy with consolidation whole-brain radiation therapy.on single-variable logistic regression, the following factors were significant: age, kps, ieslg score, single tumour, and not involving deep structures.prognosis with a median follow-up of 30 months (range 1–110), the mean os and pfs of all patients were 16.1 months and 6.2 months, respectively.table 3 summarizes the results from univariate analyses of os and pfs.after univariate analysis, multivariable cox regression analysis was performed with the 5 statistically significant variables, which showed that deep structures not involved and resection were favourable prognostic factors for pcnsl (table 4).the clinical characteristics of our patients were consistent with the literature.their median age was 53.3 years, and there was a male predominance with a sex ratio of 1.33:1, which has been previously seen in many previous large series [2, 3, 8, 10].pcnsl mostly presents with neurologic impairment symptoms as the first manifestation rather than common b symptoms (fever, weight loss, night sweats).the mean duration of symptoms to diagnosis was 5.8 months in our study.pcnsl has typical radiological appearances, which often show diffuse homogeneous contrast enhancement and surrounding vasogenic oedema on mri (fig. 3).in addition, pcnsl has markedly higher choline/creatine and choline/n-acetyl aspartate ratios on mrs than other glial tumours [12].in our present study, we also observed that the most common location of pcnsl was the supratentorial location (51.4%), followed by the infratentorial and deep locations (24.3%).this is consistent with a retrospective analysis of the clinical data of 100 patients with pcnsl, which was reviewed and analysed by küker et al [13] at the same time, we found that 65% of pcnsl patients had solitary lesions, and 35% had multiple lesions.all of these clinical characteristics enable us to better differentiate pcnsl from other diseases, including gliomas, metastasis, infections and inflammatory demyelinating disease.a series of studies have been carried out to determine optimal treatment regimens to obtain a better survival benefit for pcnsl patients [3, 10, 14, 15], but there is still controversy.however, bellinzona et al. [15] demonstrated that surgery might have a role in a selected subset of patients presenting with large single space occupying lesions and deteriorating neurological status.however, with the development of modern surgical techniques, especially neuronavigation, fluorescein for tumour visualization and intraoperative neurophysiologic monitoring, which have improved the safety and accuracy of surgery, the postsurgical complication rates for pcnsl have decreased to 0–20%, as the recent literature has reported [10, 14, 17].for example, cloney et al. [17] reported that the complication rate of the resection group (17.2%) was lower than that of the biopsy group (28.2%) through the analysis of 129 patients with pcnsl between 2000 and 2015, and they deemed resection safe for selected patients.furthermore, when patients present with signs of increased intracranial pressure and progressive neurological deficits, they would benefit more from surgery than stereotactic biopsy.weller et al. [18] found that os and pfs were significantly longer in the resection subset than in biopsied patients through a large randomized phase iii study comprising 526 patients.jahr et al. [16] also reported that patients who underwent resection had an insignificant prolongation of os compared with patients who had a biopsy performed.our results obtained in both groups are in accordance with those reported in the literature.there are several limitations of our study.the type of surgery and tumor location are the prognostic factors of pcn
number of words= 819
[{'rouge-1': {'f': 0.5411232383583794, 'p': 0.8450677506775068,'r': 0.39798165137614677}, 'rouge-2': {'f': 0.30027080064330436, 'p': 0.44228260869565217,'r': 0.22729047072330655}, 'rouge-l': {'f': 0.4496250280947981, 'p': 0.7347058823529411,'r': 0.3239325842696629}}]
-----------------------------------------------------------------------------------------------------------------------------------
p259:
Extractive Summary:
background anti-n-methyl-d-aspartate receptor (anti-nmdar) encephalitis is an autoimmune disease associated with serum and/or cerebral spinal fluid (csf) antibodies against functional nmdar [1, 2].patients develop acute or subacute psychiatric symptoms, memory loss, movement disorders, seizures, speech dysfunction and disturbance of consciousness [3].electroencephalography (eeg) can be useful for diagnosing anti-nmdar encephalitis [4–9].extreme delta brush (edb) is a characteristic eeg pattern of anti- nmdar encephalitis [4].the beta/delta power ratio and rhythmic alpha sinusoidal waves in the frontotemporal regions can provide indications of anti-nmdar encephalitis [5–9].here, we report eeg and magnetoencephalography (meg) data on edb from a female patient with psychosis and reduced responsiveness.fluid-attenuated inversion-recovery (flair)- weighted and t2 images were normal (fig. 1g, h).arterial spin labeling showed high blood flow in the right frontal and temporal regions (fig. 1i).on day 4, indirect immunofluorescence technique (iift) results revealed nmdar antibody titers of 1:100 in the csf and 1:1000 in the serum.gynecological sonography was normal.meg data were analyzed by time-frequency analysis, which was performed in each 5-s time window.delta wave activity and beta activity were localized using magnetic source imaging.detailed method involved in time-frequency analysis and magnetic source imaging has been described in our previous study [10–12].time-frequency analysis showed the beta activity varying from 25 to 35 hz (fig. 2c).the magnetic source location showed the beta activity originating from bilateral superior parietal lobes (fig. 2e).however, the delta wave originated from the bilateral superior temporal gyri, the right middle temporal gyrus, the right inferior frontal gyrus, and the left inferior parietal lobe (fig. 2d).after immunosuppression (immunoglobin, methylprednisolone and mycophenolate mofetil) and antiepileptic treatment (oxcarbazepine), the woman improved.
number of words= 261
[{'rouge-1': {'f': 0.53708489877082, 'p': 0.7557142857142858,'r': 0.41657039711191335}, 'rouge-2': {'f': 0.3121792455912948, 'p': 0.42251798561151077,'r': 0.247536231884058}, 'rouge-l': {'f': 0.47774593338497284, 'p': 0.6922222222222223,'r': 0.36473684210526314}}]
-----------------------------------------------------------------------------------------------------------------------------------
p260:
Extractive Summary:
as a technical arm of paho, our who collaborating center was tasked to explore how aslnps are conceptualized and operationalized in literature stemming from the americas region.therefore, the objectives of this scoping review were to: (a) identify community-focused aslnps in the americas across english-language literature; (b) describe their characteristics and scope; (c) identify main enablers and barriers; and (d) offer recommendations for the americas region.methods stemming from the above objectives, a scoping review was chosen as a preliminary assessment of potential size and scope of available english-language literature to map the volume, nature, and features in this given field.similar to a systematic review, it provides analytical transparency and rigor, without attempting to sum up best evidence or formally appraise the quality of research methodology [23].the scoping review presents an overview of a potentially diverse body of literature generating hypotheses rather than testing them [24].we applied arksey and o’malley’s [23] five-stage iterative process: (a) identifying research questions; (b) identifying studies; (c) selecting studies; (d) charting data; and (e) collating, summarizing and reporting results.no ethical approval was required for this type of methodology.to operationalize aslnp, we used the following adapted definition from beal [1]: a structured, established relationship where an academic nursing institution or program and one or more community serving entities agree to cooperate in order to advance primary health care outcomes, resulting in high levels of innovation and effectiveness.proof of formal partnership was operationalized as having any of the following: memorandum of understanding, signed contract, joint funding, measurable goals, or ongoing program evaluation.search strategy the preferred reporting items for systematic reviews and meta-analyses (prisma) guidelines [25] were adhered to in the conduct and reporting of this scoping review.an electronic database search in pubmed, scopus, cinahl, google scholar, and latin american & caribbean health sciences literature (lilacs) was performed using search terms that originated from indexed subject headings, the medical subject headings (mesh) terms, and keywords of relevant studies that recurred repetitively.the following terms, along with the boolean operators ‘and/or’, were used: “universities”, “academic”, “service”, “practice”, “community”, “health care”, “health-service”, “primary health care”, “public-private sector partnerships”, “partner”, “interinstitutional relations”, “schools, nursing”, “education, nursing”, “students, nursing”, “nurses.“ electronic filters for full-text, english language, peer reviewed articles, published from 2010 to january 2020 were applied.a total of 1150 articles were retrieved and duplicates were removed with the use of sciwheel reference manager.the resulting 780 articles were first screened by title/abstract.after excluding 500 articles that did not meet the inclusion criteria, 280 fulltext articles were assessed for eligibility.during screening and eligibility steps, inclusion was determined based on meeting all five of the following content-specific criteria: (a) partnership between an academic nursing institution and another education institution or non-profit organization or professional organization or volunteer group; (b) partnership focuses on community-based or primary health care service/practice (including longterm care and home care); (c) partnership applies service-learning to teach or mentor or improve practice for nursing students or faculty of any program (undergraduate, graduate or professional); (d) partnership is described and/or measured through mutual goals or outcomes or deliverables or indicators; and (e) partnership within paho catchment region (north/central/south america or caribbean region).for a detailed search strategy, see fig. 1 (prisma search strategy).data extraction and quality appraisal to attain validated scoping review processes, two independent investigators identified, selected, appraised, and synthesized all english-language literature.each researcher independently screened title/abstract and full text and carried out data extraction and crosscheck.through an inductive thematic analysis approach, the studies were first coded according to the main concepts addressed.next, codes were grouped into sub-themes, which were eventually grouped into themes.geographic distribution of the studies, along with the frequency and density of the themes, were analyzed.any discrepancies throughout the above steps were resolved by a third investigator to reach consensus.moreover, all selected articles were appraised for type and level of evidence according to the adapted rating system for the hierarchy of evidence [26, 27].in addition, evidence of aslnps was appraised and synthesized in terms of structure, process, and outcomes based on the quality of care approach [28].according to this adapted approach, structural attributes were defined as the physical and organizational components of the setting in which the partnership occurred (i.e. nurse-led clinic).aslnp processes were defined as learning methods, services or outreach provided to individuals, groups or communities (i.e. stroke risk assessment, health literacy).last, aslnp outcomes were defined as the results of those processes (i.e. reduced emergency room visits, increased communication skills, staff retention rates).results a total of 51 articles was included in the final literature review sample, covering a 10-year period from january 2010-january 2020 (supplementary file 1).data were extracted for the following characteristics: author/s and year of publication, countries involved with aslnp, study population/setting/sample, design, level of evidence, framework or model used, aim, type of aslnp (formal, implicit or explicit), main findings (aslnp structure, process, and outcomes), and implications.forty-six studies were conducted in the usa, two in brazil [29, 30], one was a joint study between the usa and haiti [31], another one between canada and colombia [32], and one study was conducted by usa investigators in guatemala [33].no english language studies were identified from other lac countries.the majority of studies (76.5 %) involved multiple stakeholders (i.e. students, faculty, health professionals, preceptors, patients, community members), while 23.5 % focused exclusively on nursing students.most of the studies were conducted in community-based organizations (29.4 %), some in public schools (17.6 %), free standing clinics (11.8 %), multiple settings (11.8 %), universities (9.8 %), va medical centers (7.8 %), community hospital outpatient departments (5.9 %), public health departments (3.9 %), and one in home care (2.0 %).all studies involved a formal academic service-learning nursing partnership (memorandum of understanding, grant, contract or agreement) with 68.6 % of them demonstrating an implicit service-learning component, whereas only 31.4 % explicitly described the servicelearning mission.appraisal of type of study and level of evidence showed a prevalence of experiential and nonresearch evidence (51.0 %) at level vii, followed by descriptive/ qualitative/mixed methods studies (41.2 %) at level vi, and in third place case-control or cohort studies (7.8 %) at level iv.a summary of evidence appraisal is presented in table 1.furthermore, the extracted frameworks and models used by the sampled articles are presented in table 2. out of 51 selected articles, nine articles did not mention use of a framework or model.among the remaining 42 articles, service-learning pedagogy, the future of nursing report, the vanap logic model, cooperative inter- organizational relationships, community-as-a partner, and transcultural nursing/cultural care theory were the most frequently adopted frameworks or models.synthesis of these conceptual frameworks and models revealed the following six focus areas, mapped in fig. 2: (a) communities/populations (26.2 %); (b) nursing (26.2 %); (c) pedagogy (19 %); (d) targeted outreach (14.3 %); (e) ip collaboration (11.9 %); and (f) health determinants (9.5 %).last, synthesis of evidence revealed five aslnp themes, depicted in fig. 3.conceptualizing or implementing innovative academic nursing partnerships was the most frequent aslnp theme (27.45 %), followed by sustaining educational standards and processes - improving academic outcomes (25.5 %), enhancing community services and outcomes (21.57 %), strengthening capacity for collaborative practice and interprofessional education (ipe) in the community (21.57 %) and preparing nurses of the future (11.76 %).the timeline distribution of those five themes by publication year, in relation to the following seminal publications, is presented in fig. 4: (a) institute of medicine “the future of nursing” in 2011 [5]; (b) aacn & aone “task force on academic-practice partnerships: guiding principles” [4] and beal “academic-service partnerships in nursing: an integrative review” in 2012 [1]; (c) community campus partnership for health “position statement on authentic partnerships” in 2013 [20]; and (d) aacn & manatt health “new era report” in 2016 [6].main findings addressing the nature, extent, and range of community focused aslnps that promote primary health care in the americas region are highlighted below, grouped by themes.theme #1: sustaining and improving educational outcomes a total of 13 articles focused on educational quality [30, 37, 39, 44–50, 62, 71, 72].the structure of these asln ps consisted of schools of nursing, municipal health departments, outpatient clinics and health care networks, school districts, community clubs, and local community agencies.within the unique structure of vanap, this strategic alignment of academic and practice goals and resources targeted raising the capacity and capability of students, faculty and staff by providing health services to veterans [50].processes that sustained and improved educational outcomes involved: faculty practice models, faculty and clinical staff serving as preceptors, exchanging faculty and nurse practitioners, conducting health screenings, providing community-based education to elementary students, comprehensive health needs assessments, building trust, and making a long-term commitment.aquadro et al. [45] found that the faculty practice model not only acknowledged the value of practice to academia, but also promoted and drove forward student and faculty teaching, learning and scholarship.success of this model was attributed to being a recognized structure by the university, and allowing for diversity in professional practice and scholarship.several educational quality improvement outcomes were reported [30, 44–47, 50]: (a) decreased preceptor and faculty turnover rates; (b) continuous operation of nurse-managed clinics; (c) cost savings for training and on-boarding of new nurses; (d) sharing of nursing faculty expertise in research and clinical practice; (e) more relevant curriculum for students at all levels; and (f) integration of didactic knowledge and critical thinking skills among nursing students, while providing holistic nursing services.for example, in a study by campbell et al. [47], students performed home safety and cognitive assessments while learning the challenges of nutritional needs of an adult population.theme #2: strengthening capacity for collaborative practice and ipe a total of seven studies enhanced capacity for collaboration and ipe in the community [52, 55–57, 60, 65, 73].these aslnps comprised of colleges of nursing, university health systems, affiliated hospitals, homeless shelters, va patient aligned care teams, community health agencies, and public-school systems.collaborative practice processes involved team-based care, partnering ip student teams with va health professionals, preparing students to collaborate with local partners, providing care for vulnerable, underserved, high-risk, ethnically diverse adolescents.based on the institute of medicine core competencies for ip collaborative practice, a faculty practice with residential juvenile justice services demonstrated how faculty and students served as leaders and experts in care coordination among ancillary providers [55].similarly, by partnering student ip teams with va health professionals the students’ ability to link theory content to care delivery strengthened, resulting in better understanding of veteran population needs [57].other outcomes included [56, 65]: (a) learning and bonding together as an ip team; (b) preventing costly hospital readmissions; and (c) increasing awareness about social determinants of health.
number of words= 1741
[{'rouge-1': {'f': 0.34101491666974104, 'p': 0.8022404371584699,'r': 0.2165281574630946}, 'rouge-2': {'f': 0.2005058594238025, 'p': 0.3932876712328767,'r': 0.13455142231947484}, 'rouge-l': {'f': 0.34845182664756447, 'p': 0.6603614457831326,'r': 0.23666666666666666}}]
-----------------------------------------------------------------------------------------------------------------------------------
p261:
Extractive Summary:
embedding compassion in the system to be adopted by nurse managers as mentors can be a promising undertaking to teach nurses compassion or as cogently coined by burridge et al. [32], ‘compassion literacy’.a compassionate nursing leader plays a critical role here in filling the void and creating a compassionate culture for nurses, who experience compassion vacuum on the job [33].a recent study in a private hospital in pakistan reported an increase on nursing indicators 18–27 months post-intervention as part of their patient experience initiative [34].the improvement was attributed to implementation of an on-job mentorship programme.since little has been written on how to develop, implement, and sustain nursing compassion for improved patient experience outcomes in low- and middle - income countries (lmics), the current study aims s to describe the development of the on-job mentorship programme designed to provide compassionate experience to frontline nursing at a systemic level subsequently leading to enhance patient experience of care.the paper specifically aims to propose mentoring as an efficient technique to develop compassion focused approaches among on-job nurses.however, it is not just this, but the study also highlights mentoring being a type of micromanagement that can empower nursing staff.methods setting established in 1985, aga khan university hospital (akuh, k) is located in karachi, the largest city of pakistan and serves two provinces, sindh and balochistan, with populations of 47.89 million and 12.34 million, respectively.it is a tertiary care hospital with consultant doctors specialized to provide optimal care and treatment to patients with a full range and severity of specialty diseases and conditions.recently, patient experience is one of the six approaches among others, including community and family health and patient safety and quality adopted by akuh, k to facilitate collaborative operations where physicians, nurses, researchers, and teachers work in unison to give patients the best access to care.akuh, k is also joint commission international accredited which focuses on patient safety.through its advanced resources, trained faculty, and technological aid, the institute is creating models for research projects rendering short and long-term impact in the provinces.paediatric service is the largest service line of the hospital having 122 inpatient beds and has the largest number of the nursing services employees (n = 413) providing high quality, multi-specialty care.all major medical and surgical specialties are present to deliver excellent pediatric care for complex health problems.the nursing management team in the service line consisted of a nurse manager as the senior position followed by a specialist, head nurse and then frontline nursing staff (supplementary figure 1).the study was approved for an exemption as quality improvement (qi) study by the ethics review committee of the aga khan university and as per institutional requirements consent was waived (being an exemption).design of the intervention designed as an intervention development study [35], this current programme was part of a larger quality improvement initiative to improve patient experience and was conducted between january 2018 to december 2020 [34].the theory of change (toc) model was utilized to develop a solution to nurture engagement among nurses, facilitate relationship building and impart soft skills thus instilling a satisfactory employee experience for subsequent improved patient experience.toc has been widely used and it is considered to be the basis of monitoring and [36] theory-based evaluations [37–40].according to weiss [41], toc is, “a theory of how and why an initiative works”.a robust toc embodies several elements that makes the model more systematic [36].the nursing mentorship toc comprised preconditions, interventions, outcomes and the final goal.the final impact was to achieve improved compassionate experience of patients through refined skills of compassion in nurses.the preconditions are a necessary requirement, condition or element that should be acknowledged for achieving the desired outcome while to fulfill these preconditions or remove dodges, it is important to have efficient interventions that create a positive difference in outcomes and impact of interest [36].development of the intervention toc model the development of the model took several steps as described below (table 1).conceptualization as the service leadership and director of patient experience of care sought to work on improving patient experience in the service line, a survey was conducted between july–august 2017 with families in the paediatric ward to determine the issues that affected their experience using the child health consumer assessment of providers and systems (hcahps) [42].the form also included items on care from nurses.the survey was completed with 221 families admitted for at least 24 h and age between both to 6 years.the overall results have been reported in another manuscript [34].findings around care from nursing items indicated that 75% of patients reported that nurses always communicated with courtesy and respect, 71% said nurses listened to them carefully and 77% felt nurses explained things in a way they could understand.also, only 27% reported getting help from staff when they pressed the call bell.there was also an open-ended item requesting families to list down the top three factors that affected their experience during stay.about 38% indicated lack of responsiveness from nurses followed by 29% stating ineffective communication.this led to conceptualization of the problem that the staff was disengaged and an intervention to improve patient experience meant engaging the staff via improving their own experience through a ‘compassionate mentorship package’ integrated into their existing supervision system.identification of a framework after a literature review, the core team identified the caring mentorship model [43] as a conceptual framework to address the phenomenon of building relationships through mentorship of nursing staff.this model has developed from two other models of the development of caring nurse-self [44, 45].it depicts three phases: relationship with no connection, surface connection, and shared connection with each containing three levels that a person reflects at: cognitive (task-oriented mentoring), affective (interactive mentoring), and transformative (transformative mentoring).the programme intended to work on encouraging supervisors to become attuned to mentorship roles.the goal of the programme was to enable the head nurses to form a meaningful and reflective relationship with the nurses beyond the cognitive and interactive understanding to improve employee experience leading to enhanced patient experience.table 2 states all the questions addressed in accordance with identified model [43] and the activities incorporated by the nursing mentorship programme in each phase.the team was unable to formally evaluate where the nursing staff stands in terms of mentorship phases.the informal team reflections about the baseline assumption suggests the connection was at the cognitive level with surface connection.there was no human connection and mentorship being limited to only monitoring and completing checklists.hence, the aim was to create opportunities and provide a safe place for the connection to evolve at an ‘affective’ and later ‘transformative’ level.it was important to create a humane culture for nurses to reciprocate a similar habitat for patients.creation of a working group the next step was identifying a well-organized team with the right skills that was designated to work on this task.the service leadership identified the working group to lead the change effort (supplementary table 1).along with the service line chief and director patient experience, members included nurse manager, three nursing specialists, and a research specialist.while the internal group developed technical solutions, external expertise was needed to solidify the compassion angle.therefore, the team reached out to charter for compassion pakistan (henceforth, cfc pakistan – a local initiative inspired by the charter for compassion international) and through shared learning of organizational behavior change and nine skill- based compassionate package, respectively, set in motion a mutually beneficial partnership.the team members from cfc pakistan included a project lead and an associate.meetings of the working group a regular schedule for meetings was carried out to codesign the package.the team members met once a week for the initial 4 months moving to once every 15 days in the next 2 months.the meeting was conducted during working hours every friday and typically lasted for an hour.each meeting held discussions on how to achieve the month’s agenda and meeting minutes were recorded by the nursing lead.certain ground rules were set for these meetings which involved application of an empathetic approach, clarity of roles and responsibilities, valuing inputs from all based on their strengths and focus on outcomes and feasibility.the tasks of the meeting group for design of the implementation of the interventions (frequency, dosage, timing) specifically were divided according to the raci matrix (responsible, accountable, consulted and informed).this matrix proves to be beneficial in organizational context as it allows to identify the roles anticipated for each person and any missing roles for early corrections [46].the explanation of each component is as follows: 1. responsible: the member under this section had the responsibility of completing the task.in the current study, e.g., the tasks of designing supervisory checklists and nursing education services (nes) pediatric modules were assigned to a nursing specialist (ns) who was already performing such duties.she was responsible and owned the whole course of action.2. accountable: the nursing manager (nl) was accountable to have the final approving authority for the work around supervisory checklists and designing of nes paediatric modules completed by the respective members.contrary to the ‘responsible’ team, the accountable one was answerable for the task.3. consulted: the consultants i.e., director patient experience of care, (mr), chief of service line (bh) and compassion specialist (az), provided valuable input on the relevant tasks by engaging in two-way communication.they brought in different perspectives regarding pre-conditions and designing modules.4. informed: the higher leadership roles i.e., the chief nursing officer, dean of school of nursing and midwifery, ceo and others belonged to this group.they did not provide any input, rather kept a track of needs, preconditions and assessments and developments of interventions.a systematic process was adopted to design the model.the first step was understanding the contextual needs and then framing the pre-conditions followed by developing indicators to assess the effectiveness of the model.feasibility was the key consideration of designing the interventions [47].a few principles around different components of feasibility were agreed upon by the working team to guide the process (table 3).needs assessment pain points survey a brief survey was conducted to understand the factors that affected the engagement of the nurses.it included 3 items around factors that affected them personally, the patients and overall service delivery.the survey was meant to be shared by the nursing supervisors with all the nurses as a paper-form and to be completed anonymously.however, only 173 (42%) forms were returned.one reason was that the managers were not able to share the form with staff on night duties.moreover, the staff was spread across 11 different locations and not all of them would have received on time.gathering nurses, distributing the form, and then collecting it was a considerable effort.given the urgency of the matter, the team utilized the forms received for the analysis.the working team also did not want to push harder for additional forms lest the staff should feel coerced.a compiled list of 329 pain points was received from the nursing service coordinator.a thematic analysis following an inductive approach [48] was completed by a behaviour implementation scientist (mr) and a psychology graduate (aah).the pain points were independently coded followed by agreement of the codes in a face-to-face meeting.the analysis revealed three main themes: perception of being overburdened, lack of connection with work, and lack of growth opportunities.perception of being overburdened meant that the nurses viewed their work as being excessive with prolonged procedure, night shifts, call on off days, high expectations or pressure from management, excessive documentation etc.while lack of connection with work holds two main themes: accountability and lack of facilities.lack of facilities included low salary, breaktime issues, one washroom for 30 staff, no medical benefits for their parents, unavailability of instruments, etc.however, accountability catered to different issues that according to the nurses were ethically inappropriate.these involved over criticism from seniors, revenge seeking behaviors of co-workers, work environment, and many more.the last pain point i.e., lack of growth opportunities comprised concerns like insecurity of jobs, no promotion of registered nurses (rns) and no future for the more senior, older staff.as per the answers, ‘perception of being overburdened’ was found to be the most common theme among the responses.the findings from the thematic analysis were shared with the nursing manager and supervisors as a means to establish trustworthiness of the findings.review of existing documents like nes modules, nursing competency checklists led to the realization that few shortcomings had to be addressed to work on patient-centeredness and achieve the final outcome.the existing supervision or performance management system aimed to assess nurses every 6 months based on a checklist for hard skills only.there was also no training for head nurses (supervisors of frontline nurses) to enhance their supervision skills.
number of words= 2075
[{'rouge-1': {'f': 0.30544440166029607, 'p': 0.814927536231884,'r': 0.1879440110142267}, 'rouge-2': {'f': 0.1710829346096208, 'p': 0.3461627906976744,'r': 0.11361799816345272}, 'rouge-l': {'f': 0.2892654788597674, 'p': 0.6187179487179488,'r': 0.18875693673695892}}]
-----------------------------------------------------------------------------------------------------------------------------------
p262:
Extractive Summary:
background competency is defined as a personal trait or set of habits that leads to more effective or superior job performance, and it has five major components: knowledge, skills, selfconcepts and values, traits, and motives [1, 2].motives are initiators that prompt people to do their work, and traits refer to personal qualities [3].self-concepts and values refer to self-identity, which includes a person’s attitudes and self-image, while knowledge and skills are fundamental requirements for a person to perform a certain task or job [3].the responsibilities of military nurses are slightly different from those of their civilian counterparts as the former are indispensable during wars and united nations (un) peacekeeping missions.frequently required to respond to natural disasters or epidemics to save lives [4], they play dual roles as both military officers and nurses [5].the scope of practice for military nurses working in general hospitals includes not only routine nursing tasks but also military missions.the military nursing context is characterised by trauma-centred care to patients of all ages, as well as harsh conditions, which include the potential for physical and psychological harm [6–9].the instability of this healthcare environment and demanding operational requirements increase military nurses’ burden [5].even when working in general hospitals, military nurses require the competencies to be able to thrive in this environment.owing to rapid population ageing and changes in disease presentations, while the healthcare environment has been greatly altered, the requirement for nurses remains the same [10].nurses are expected to possess the abilities to provide comprehensive, quality care.to accomplish this, it is important for nurses to develop nursing competencies, which are the core abilities that are required for fulfilling their roles [11].in other words, nursing competencies are the necessary knowledge, skills, and attitudes nurses must possess to perform their duties in a safe and ethical manner.the american association of colleges of nursing outlines a set of nursing competencies and highlights such areas as ‘patient-centered care, interprofessional teams, patient safety, informatics, critical thinking, cultural sensitivity, and professionalism’ [12].meanwhile, military nursing competencies encompass clinical nursing competency, operational nursing competency, soldier/survival skills, personnel/physical/psychosocial stress, leadership and administrative support, and group integration and identification [13].models have been developed to apply these competencies to workforce performance.the outer layer of the onion model can easily be seen and cultivated, while the inner layer is difficult to evaluate, making training in this aspect also difficult [2].knowledge, skills, self-concepts, values, traits, and motives are divided into three layers, moving from the outer layer towards the core.more specifically, the outer layer contains knowledge and skills, the middle layer of self-identity includes selfconcepts and values, and the core of the onion model encompasses traits and motives.based on the evidence that competencies can be used for translating strategy into job-related performance and individual behaviours, the development and application of a competency model enables the cultivation of a more effective and productive workforce [14].while the literature on general nursing competencies is vast, research focusing on the competencies of military nurses is limited.the onion model is an enriched competency model with different interrelated layers, which can provide a comprehensive perspective of exploring competencies and designing multi-level training.in order to gain a better understanding of military nurses’ competencies, we conducted a qualitative study on the theoretical basis of the onion model, which can provide enriched theoretical guidance for competency-based nursing education and competency building.methods design a qualitative study was carried out using a qualitative content analysis and conducted from april to june 2020 [15].this design provides a contextual description and interpretation of social phenomena [16]—in this case the competencies of military nurses in general hospitals— and facilitates an understanding of related voices, views, and thoughts.the reporting of the study was based on the consolidated criteria for reporting qualitative research (coreq) checklist [17].participants based on the requirements of qualitative studies, a small convenience sample was solicited [18].military nurses from general hospitals in china with experience of participating in military missions such as disaster and public health emergency rescue were included.nurses from 10 military general hospitals were contacted to determine their willingness to participate in this study.written informed consent was obtained from each participant.in total, we interviewed 21 nurses until data saturation was reached.data collection after obtaining institutional permission, telephone or wechat (a popular chinese social media app) conversations were conducted between the research team and military nurses, during which the nurses were introduced to the study objectives and asked if they were willing to participate.willing nurses were then given an explanation of the purpose, significance, and confidentiality of the study and informed that they could withdraw at any time.meanwhile, a formal interview was scheduled at the convenience of the participant.the interview outline was based on the onion model and behavioural event interview framework [1, 2].the following questions were asked, followed by probing questions: (1) background questions including age, gender, qualification, job title, years of working as a military nurse, and name of institution; (2) experience of being deployed in military missions, and the most impressive events—both successful and unsuccessful—during each deployment; and (3) competencies of military nurses.the interviews were conducted in mandarin, digitally recorded and uploaded to an online transcription service.each interview lasted approximately one hour, and notes were taken in each interview.data analysis data were analysed using an inductive approach combined with a deductive approach, and graneheim and lundman’s qualitative content analysis technique was utilised [15].first, using the onion model as the theoretical framework for the interview outline and deductive approach to data analysis might have led to some biases.second, the findings are context- and time-dependent and therefore cannot be generalised to all military nurses.conclusions military nurses play a substantial role in providing quality nursing care to meet the demands of patients and the requirements of the ever-changing healthcare system.existing knowledge of the competencies of military nurses in general hospitals is limited, and qualitative studies of deployment experiences of military nurses reveal that they feel unprepared, necessitating appropriate prior training [7].the present findings contribute to a richer knowledge of nursing competencies for personnel recruitment and competency assessment and building.future studies are necessary to design scientific assessment tools for military nursing competencies and effective vocational courses for leadership and professional development of military nursing teams.furthermore, research exploring how to prepare military nurses to cope with deployment, which will promote the development of a competent, resilient, and prepared nursing workforce, is requir
number of words= 1046
[{'rouge-1': {'f': 0.27463213729812075, 'p': 0.7891780821917809,'r': 0.16624197983501376}, 'rouge-2': {'f': 0.1952397154345809, 'p': 0.47000000000000003,'r': 0.12321100917431194}, 'rouge-l': {'f': 0.2923961491082301, 'p': 0.6782474226804123,'r': 0.18637080867850098}}]
-----------------------------------------------------------------------------------------------------------------------------------
p263:
Extractive Summary:
background nurse and midwifery unit managers (nmums) play pivotal roles in quality patient care, nurse and midwife satisfaction and retention [1].the nmums’ role includes change agent; coach; mentor; finance and human resource manager; clinical expert; educator; quality manager and patient advocate [2, 3].nurses who appraise their practice environments positively, are less likely to report burnout and leave their position, compared to nurses with a negative appraisal of their practice environments [4, 5].furthermore, higher level nurse-reported quality of care, has been associated with work environments where nurses have reported a feeling of being empowered to carry out their work [6].irrespective of the position title or experience that nurses and midwives may possess, leadership is a role that all are expected to fulfil [7, 8].a leader inspires and influences others to act while at the same time directing the way that others act.thus, leadership requires qualities that extend beyond management skills [9].conversely, a manager seeks to meet goals while following organisation rules [9].nmums are expected to carry the responsibilities and exhibit the functions of a leader and a manager simultaneously [10, 11].this involves nmums developing a vision and operationalising nurses and midwives towards this vision.however, being both a leader and a manager can create tensions in the nmums’ role [11].leadership has been viewed as one of the many functions of a manager, despite the fact that others might view management as a role of leadership [11].nevertheless, there is the contention that both management and leadership may not come together in one individual.this is because of the diverse factors in a workplace environment that make it challenging for one individual to be true to both, and that attempting to do so can result in internal conflict [12].hence, because of the diverse motivators and objectives in a dynamic workplace environment, there is a need for leadership training and organisational support to enhance nmums’ performance.in 2016, western health, victoria, australia, initiated a review of the unit manager role including interviews with nmums to inform an organisation wide professional development of this staff group.western health includes several hospitals and a wide range of community based services.these services are provided mainly for the western region of melbourne, which has a catchment population of about 800,000 people [13].one of several information sources to inform this initiative was the nmums themselves.it was considered vital to the success of the overall program that nmums were able to describe their experiences of management and leadership within their roles and relate areas of strength and areas requiring development, for the program to be relevant and responsive.the objectives of this study were: to explore the understanding and experience of nmums regarding their role; to explore what barriers and facilitators nmums identified to achieving the goals of their clinical area; and to explore nmums’ career plans.methods research design the study was guided by naturalistic inquiry using a qualitative descriptive approach [14].there are three main tenets that underpin naturalistic inquiry: the phenomenon should be studied in context; the object of interest should be examined without reference to a priori theoretical frameworks and the researchers’ preconceived assumptions should be explicit; and the research is interpretive [14].i’ve got a three-year-old who is starting to ignore me because i’m not there.i’m gone sometimes before he wakes up.[012] despite the huge burden of responsibilities on the shoulders of the nmums, they were able to listen to and accommodate problems that their staff were facing in their personal lives.as this participant stated: …when they’ve got depression or they’re having family issues or divorces.they need to find someone - they’re very confident that i’m not going to tell anyone about their problems.i treat them exactly the same.[008] discussion this study aimed to understand: the experience of the nmums’ role; the barriers to, and facilitators of, achieving their unit’s goals; and the nmums’ career perspectives.the findings from this study showed that many nmums were nurses and midwives who were appointed to their positions without an orientation or the provision of a mentor.in addition to a lack of preparation for the role, some nmums experienced a lack of role clarity that led to low self-confidence and feeling ill-equipped to carry out the requirements of the position.low selfconfidence on the part of the nmums may have cascading effects on the system [19] which may lead to reduced patient satisfaction and staff productivity [1, 19] as well as staff retention [1, 19].our findings suggest a need to address the nmum preparation as a priority to contribute to staff and patient satisfaction.our study further revealed that targeted professional development programs on subjects such as project management, change management and business case development can enhance the capacity of nmum to deliver effectively on their multifaceted responsibilities.consistent with our findings, previous studies have identified confusion among nmums concerning the boundaries and expectations of their role, worsened by inadequate professional development opportunities relating to leadership and management in australia [6, 20, 21], new zealand [22], ireland [23] and south africa [24].the contemporary role of nmums requires managerial, leadership and clinical skills [25].our study revealed that nmums were worried that their administrative responsibilities were taking more time away from their clinical leadership responsibilities.this reported drift of nmums from one of their core responsibilities because of increased administrative workload has the potential to compromise patient safety unless there is a restructure of roles and responsibilities.in line with our finding, evidence shows that about 65 % of a nmum’s role is largely related to general management activities such as staff management and budgeting, with only 16 % of tasks being clinically related to patient care [25].therefore, it is not surprising that because of this shift in responsibilities, some have advocated that the position of the nmum may be occupied by someone who is not a nurse [26].this view has been espoused based on public management literature [27, 28] which alludes to the idea that any manager can manage any business [26, 27].however, this view overlooks the everyday clinical realities that nmums face daily and the associated complex decisions they make relating to patient and staffing issues such as staff turnover, high part-time employment, skill mix of staff, high bed occupancy and unplanned admissions which can impact on patients’ morbidity and mortality [29].however, if the clinical time of nmum continues to decrease as a result of increased administrative responsibilities, the voices advocating for nonnurses to occupy the position of unit managers may become louder and even become legitimate.a recent qualitative evaluation of an intervention to reduce the administrative burden of nmums in australia via the introduction of a clerical nurse unit manager support officer position, showed that administrative support for nmums improved the capacities of nmums to undertake clinical leadership and to be strategic leaders [30].of note in this study was the self-sacrifices of the nmums.nmums exemplified their personal sacrifices through long hours of work and shouldering some of the personal burdens of nurses and midwives.our study revealed that these extra commitments to their positions by nmums was also affecting their family life in a negative way.this culture of self-sacrifice among nmums is historically embedded within the nursing profession and may lead to burnout, job dissatisfaction and presenteeism [31].a self-sacrificing image of nmums may also discourage prospective recruits from taking up the role of nmums.thus, the detrimental over-exertion and the lack of self-care within the ranks of nmums needs to be addressed to reduce retention challenges.previous studies have linked personal factors such as burnout with intent to leave [32, 33].both theoretical and empirical works support the use of nurses’ intention to leave as a proxy construct for actual turnover [34–36].strengths and limitations a strength of the study is the richness of the data available for analysis and the rigour of the data collection and analysis processes.credibility, confirmability, dependability, and transferability ensure the rigour of a study [37] and in this study credibility was demonstrated through reflexivity, maintaining an audit trail, and the use of detailed descriptions in interpreting the data.methods used in this study to establish confirmability included reflexive journaling, and a clearly identified audit trail.the transparent audit trail detailed the rationale for decisions made throughout the research process to assist establish dependability.transferability was promoted by the large sample size and the description of the context of the study.a limitation of this study is that while it involved nmum from four hospitals, all were from one health service which may limit transferability of findings.conclusions this study of contemporary nmums reveals that there continues to be a lack of investment in the orientation, professional development and support of this critical leadership and management role.effective leadership is critical to the performance of a well-functioning hospital unit and to patient safety.many nmums report feeling ill-equipped, unsure of the expectations of their role, and often overwhelmed with the demands of the position with minimal or no support from systems within the hospital.
number of words= 1465
[{'rouge-1': {'f': 0.32810261445356304, 'p': 0.8320817843866171,'r': 0.2043381389252949}, 'rouge-2': {'f': 0.22287337343694075, 'p': 0.4916417910447761,'r': 0.14409836065573772}, 'rouge-l': {'f': 0.36161668249684964, 'p': 0.7208875739644971,'r': 0.24133956386292835}}]
-----------------------------------------------------------------------------------------------------------------------------------
p264:
Extractive Summary:
background the delphi technique is an established and effective research method with multifaceted applications for health services research.the delphi technique is uniquely designed to explore health issues and topics where minimal information or agreement currently exists, a relatively common situation within nursing practice.secondly, the delphi technique allows for the introduction and integration of viewpoints, opinions, and insights from a wide array of expert stakeholders.however, a recent systematic review highlighted a gap between available methodological guidance and publishing primary research in conducting real-time delphi studies [1, 2].in this paper, we seek to examine the methodological gap in applying real-time delphi methods, by providing a specific case example from a real-time delphi study conducted to develop a self-reporting survey tool to explore pain management practices of australian emergency nursing in critically ill adult patients [3].insight into the procedural challenges and enablers encountered in conducting a real-time delphi study are provided.importantly, key characteristics of the method are presented, followed by the case-based exemplars to illustrate important methodological considerations.reflections from the case are then presented, along with recommendations for future researchers considering the use of a real-time delphi technique approach.overview of the delphi technique the delphi technique was developed in the late 1950s’ by the research and development (rand) corporation [4] as a method for enabling a group of individuals to collectively address a complex problem, through a structured group communication process without bringing participants together physically [5].delphi has value in the healthcare sector, as it is characterised by multidisciplinary teams and hierarchical structures [6].the delphi technique has since become popular with nursing researchers exploring a wide range of topics including role delineation [7–9], priorities for nursing research [10–12], standards of practice [13, 14] and instrument development [15, 16].the four main characteristics of the classic delphi method are anonymity, iteration, controlled feedback and statistical aggregation of group responses [17].data collection within the classic delphi typically includes at least two [18] or three [19] rounds of questionnaires facilitated by a moderator.round one represents what ziglio [20] termed the ‘exploration phase’, in which the topic is fully explored using broad open-ended questions.each following round then becomes part of an ‘evaluation phase’, where results of the previous round, interspersed with controlled feedback from a moderator, are used to frame another set of questions.each round provides an opportunity for expert panel members to respond to and revise their answer in view of the previous responses from other panel members [21].since its introduction, over 20 variations of the classic delphi method have evolved, with researchers modifying the approach to suit their needs.most common delphi versions include modified, decision, policy, internet, and more recently real-time delphi, and have empaneled varying numbers of experts ranging from 6 to 1,142 [22, 23] (table 1).the ubiquitous and interactive capacity of the internet and smart device technology offers benefits that are intimately linked with contemporary research innovations in healthcare [24].two clear limitations of the classic delphi technique were prolonged study durations and high panel member attrition [25].aiming to overcome these issues, gordon and pease [26] developed the concept of an information technology-enabled contemporaneous extension called real-time delphi, to improve speed of the data collection process and syntheses of opinions.conducting a real-time delphi relies on specially designed software to administer the survey; the functionality or capabilities of which can negatively impact on the success of a study.initial thoughts of using technology to facilitate the delphi process emerged as early as 1975 [27].the first specifically designed realtime delphi software was developed in 1998 called professional delphi scan [28], with the first real-time delphi surveys performed and published in the early 2000 s [29].since then, several real-time software-based tools have been developed, often by researchers for the purposes of their study [30–32].however, these have not been evaluated in detail in the literature.in a real-time delphi process, participants are provided with access to an online questionnaire portal for a specific amount of time.on accessing the portal, expert panel members see all their responses to items and the ongoing, hence real-time, anonymised responses from other panel members.the core innovation of real-time delphi studies is the simultaneous calculation and feedback.unlike the classic method, in a real-time delphi participants do not judge at discrete intervals (i.e. rounds), but can change their opinion as often as they like within the set timeframe [33] (fig. 1).method a real-time delphi case exemplar a real-time delphi study was conducted to develop a context specific instrument (i.e. survey) to investigate emergency nurses’ practices in managing acute pain in critically ill adult patients.the following steps were followed in designing and conducting our real-time delphi study: study design, pilot testing, recruiting experts, retention, data analysis and reporting.findings from this study are reported elsewhere [34].the real-time delphi method was selected to: maximise participation from expert panel members geographically separated, minimise the amount of time demanded of experts, enable equal flow of information to and from all members, real-time presentation of results to enable experts to reassess and adjust their opinion, and allow panel members a greater degree of expression [35, 36].prior to commencing the study, a comprehensive literature review was conducted by the research team to generate initial survey items, and used the following questions:  what indicators would signify that acute pain in the critically ill adult patient has or has not been adequately detected?descriptive statistics were then developed in tabular form and scatterplots.survey items that met the above consensus and stability criteria were incorporated into the final survey (table 4).reporting regardless of what delphi study design and approach is adopted, attention to rigour of reporting throughout the process is a vital aspect of research.trustworthiness of the delphi technique has been debated in the general and nursing literature.keeney et al. [36] and powell [84] suggest that the delphi technique should not be judged by psychometric criteria used for more positivist approaches, with several criteria proposed to evaluate trustworthiness of qualitative studies [55, 85–88].a common purpose among criteria is to support trustworthiness by reporting the process of study design and data analysis accurately.in our study, we elected to apply the criteria proposed by lincoln and guba [86], based on four concepts; credibility, transferability, dependability and confirmability.our real time delphi was based on consensus amongst experienced individuals familiar with the phenomena being explored, across emergency nursing, pain management and academia (credibility and confirmability).decisions on development of survey questions was arrived at through a documented and auditable; a processes supported by the surveylet software system (credibility and dependability).the anonymous and continuous process of real-time delphi research fostered honesty and verification of panelist responses, as panelists could provide feedback and ‘member-checking’ without fear of reprisal from their colleagues (credibility) [36].prior to initiating the real-time delphi process, we piloted the survey for its structure, flow, ease of navigation and robustness (transferability).conclusions many papers describe the use of the classic delphi approach in health services research, yet few provide practical advice on the type and process for undertaken such a research design using the real-time delphi method.this article presented a case exemplar of a real-time delphi study and the development of a survey to explore emergency nursing practice.the real-time delphi method can be of great use in a wide range of timesensitive health research issues where divergent opinion or little agreement exists.
number of words= 1198
[{'rouge-1': {'f': 0.24036293946557974, 'p': 0.8286206896551724,'r': 0.14056936647955093}, 'rouge-2': {'f': 0.15011997623112405, 'p': 0.3482608695652174,'r': 0.09568218298555378}, 'rouge-l': {'f': 0.24619933907137923, 'p': 0.6575,'r': 0.15145580589254767}}]
-----------------------------------------------------------------------------------------------------------------------------------
p265:
Extractive Summary:
the condition can be distressing for the individual, their family members and can lead to long-term complications and can result in death [2].as a modern approach to meet the everchanging demand for nursing education, podcasts are now a widely used intervention providing learning resources [21, 22].podcasts can be one-off audio recording or be used to produce a series relating to a theme [21].the study was conducted with a convenience sample of year one undergraduate nursing students (n = 320) from queen’s university belfast in northern ireland during may 2020.the project team worked together to adapt delivery of this co-produced content to a podcast format.the podcast was developed using free software from audacity (https://www.audacityteam.org/), an external microphone and laptop computer.table 1 provides an overview of the running order of the delirium awareness podcast.canvas is the university-wide virtual learning environment for all students at queen’s university belfast and this system is used to support learning and teaching activities.data collection students who wished to participate in this study were informed that they needed to complete the pre and post questionnaires during the same four-week period as they received access to the podcast.a 35-item true-false delirium knowledge questionnaire (dkq) developed by detroyer [26], based on the work of hare [27], was available to students to complete before and after listening to the podcast.these questionnaire weblinks were placed on the same page as the podcast and information sheets about the study.the overall dkq score received by the participant equates to the total of questions answered correctly and this ranges between 0 (lowest) and 35 (highest).the likert scale items included: extremely confident, very confident, slightly confident, and not at all confident.these three items were completed after the 35-item dkq.the final part of data collection related to the posttest phase of the study and this focused on evaluating the effectiveness of podcasting to deliver nursing education.the authors designed a seven-item questionnaire in consultation with current university teaching evaluation guidance and the input of four current undergraduate nursing students at queen’s university belfast.ethics this study received ethical approval in may 2020 by queen’s university belfast, faculty of medicine, health and life sciences research ethics committee (ref: mhls 20_53).participants did not provide verbal or written consent but were informed that they were under no obligation to complete any of the questionnaires.participants gave their consent to complete the questionnaire when they actively accessed the survey web links.consent/recruitment all students (n = 320) received information about this research project via email by an individual independent to the study.it was made clear that their participation in this research was voluntary and would not affect their module grade.participants were advised that the pre-test should be completed immediately before downloading and listening to the podcast, while the post-test completion should ideally take place as soon after listening to the podcast as possible.due to the nature of asynchronous remote learning, it was not possible to determine how soon before or after listening to the podcast that nursing students completed the questionnaires.this enabled the research team to pair the data to carry out specific statistical tests.student participants were required to use their own laptop, computer tablet or mobile phone to complete the surveys.there was no option to complete paper-based questionnaires.data analysis descriptive statistics were used to analyse overall pretest and post-test scores for both the dkq and perceived professional confidence questionnaire to determine the overall impact of the delirium awareness podcast.descriptive statistics were also used for the podcast evaluation which occurred post-test.inferential statistical analysis was undertaken to illuminate pre and post-test differences across the three core sections of the dkq and the three statements regarding perceived professional confidence about delirium.in these two questionnaires, the average differences between participant’s overall scores, before and after listening to the podcast, were also calculated to highlight the extent of improvements in knowledge and confidence.at the time of listening, all participating students had completed two clinical placements, lasting 10 weeks in total, as part of their degree programme.all participants were enrolled as a nursing student on one of the four pathways: adult nursing, mental health nursing, children’s nursing, or learning disability nursing.student perceived professional confidence about delirium was also extremely positive post-podcast and descriptive statistics from this 3-item questionnaire can be viewed in fig. 1.knowledge about delirium the presentation, symptoms, and outcomes of delirium (section a) of the dkq was positively answered by students scoring a mean of 88.34% in the pre-podcast test.these three items improved the most post-test.knowledge improved by an average of 7.78% among student nurses after participation in the podcast (p = 0.007).questions with the weakest averages encompassed those in relation to impaired vision (38.98%), diabetes (14.97%) and those with a family history of dementia (38.70%) being linked to delirium.this items improved the most post-test.student nurse’s knowledge of delirium prevention and management is studied in section c of dkq.on average student scores increased by 12.81% in this section post podcast (p = 0.003).this was statistically significant (p = 0.004).regarding, delirium management, only 20.47% students felt confident before listening to the podcast and this increased to 68.79% feeling they were ‘very’ or ‘extremely’ confident post-podcast.this represented an increase of 48.32% and this was significant (p = 0.004).the final item was around communicating with someone’s next-of kin or care partner about delirium.this finding was also statistically significant (p = 0.002).however, there was no evidence to suggest longer podcasts are associated with poorer outcomes.in terms of this delirium awareness podcast, it would be possible to provide three concise audio recordings (i.e. 20 min each in duration) in relation to; what is delirium, delirium recognition and delirium management as described in table 1.this underpinning constructivist theory is a popular pedagogical approach which has previously been aligned to podcasting in higher education [37].in relation to perceived professional confidence about delirium, nursing student confidence about delirium significantly increased postpodcast.the original delirium awareness can be accessed and shared with other cohorts of nursing via supplementary file 1.while podcasting is clearly an encouraging approach to education little is still known about how to best develop and evaluate podcasts in healthcare education.the apparent lack of supporting research on innovative educational approaches to undergraduate nursing delirium education is surprising given the high prevalence of the condition in hospitals and care homes.
number of words= 1033
[{'rouge-1': {'f': 0.4403057529769315, 'p': 0.8021937321937322,'r': 0.3034241598546776}, 'rouge-2': {'f': 0.2159140341791818, 'p': 0.3442857142857143,'r': 0.1572727272727273}, 'rouge-l': {'f': 0.3817857070615126, 'p': 0.6076344086021506,'r': 0.2783333333333333}}]
-----------------------------------------------------------------------------------------------------------------------------------
p266:
Extractive Summary:
several overlapping aspects were identified, such as engagement in activities, opportunities for involvement in decision-making and the personality of the individual.close caregiver and peer relationships were important thriving requisites for some individuals; however, others indicated that professional and/or social relationships did not greatly impact their experience of thriving, demonstrating the existent variation of individual preferences within the unique care and social context of the nursing home environment [9, 10].likewise, meanings of thriving were explored among a group of australian nursing home residents and were understood as a combination of acceptance, feeling supported, cared for and independent, opportunities to choose relationships, and feeling a sense of home [12].methods participants and setting the study was conducted over a four-week period in march 2018 at a rural nursing home facility providing residential aged care, palliative care, respite care and secure dementia care in victoria, australia.the majority of staff employed at this facility were enrolled nurses (en), however there was a registered nurse (rn) on-site at all times.to practice in the clinical setting, both ens and rns must hold a valid registration with the nursing and midwifery board of australia (nmba) and adhere to the code of conduct, code of ethics, and professional standards for practice [16].the inclusion criteria outlined that eligible persons would (a) be aged 18 years or older; (b) have been working at the nursing home for a minimum of three months; (c) be able to read, speak, and comprehend english; (d) hold a qualification as an rn or en; and (e) be able and willing to provide informed consent.those who were interested in participating were encouraged to contact the nursing home manager to schedule an interview time.the nursing home manager also invited eligible staff to participate who were not present at the meeting.reasons for non-participation were not explored.all participants were provided with verbal and written information outlining the aims and methods of the study.the information statement reiterated that participation was voluntary and that all contributions would remain anonymous.consent was obtained via a signed and dated written consent form which outlined that participants could consent to being interviewed, being audio-recorded, or both.the sample was predominantly female (n = 12), with a mean age of 46.6 years, and between 3 and 40 years of nursing experience (mean, 21.7 years).data collection data were collected through semi-structured interviews which were guided by the aim of the study [17] (see: supplementary file 1 - interview guide).to develop a common understanding of the concept of interest, participants were first asked to describe thriving (e.g. could you tell us about what thriving means to you?).follow up questions were guided by participant responses.all interviews took place in a private room, were scheduled during daytime hours (0900–1630), and were negotiated around the participants’ work schedule and patient care.the interviews lasted between 18 and 41 min and were transcribed by the first author for analysis.transcripts were verified against the audio for validation.data analysis data were analyzed using qualitative content analysis with an inductive approach [18, 19].next, the text was divided into meaning units.meaning units were long or short sections of text that conveyed a single meaning related to the study aim.the results were discussed by all authors resulting in minor adjustments to wording before reaching a final consensus.results the analysis resulted in six sub-categories and three main categories (table 2).thriving to me is something that is positive, thriving in all aspects i suppose.reflecting on personal understandings of thriving reflecting on personal understandings of thriving in the professional context seemed to provide staff with greater insight into how persons in their care thrive in the nursing home environment.in their descriptions of resident thriving, staff commonly compared the situation of persons living in the nursing home to the situation that they themselves would want based on their personal interpretations of thriving.when i am older i am probably just going to stay in my room and binge watch tv all day’ (p8), while another mused, ‘i would hate to come in here and have to share a house with a whole bunch of people that i don’t know’ (p10).for example, physical appearance seemed to be important when observing thriving, as one staff member recalled, ‘some of them can come in in a very frail state, emaciated, not eating.similarly, mental well-being was also assessed through nurses’ observations, as one staff member reflected, ‘you do get residents who come in here and they just, they see this as the end point, they don’t enjoy anything and they don’t want to do anything anymore … if they have made up their minds and they don’t want to continue then it is hard to make them thrive’ (p3).seeing residents participating in activities and having positive interactions with others was described as being indicative of thriving.you find that when people first come in, that is when they are very withdrawn and everything … and then you see them come out.discussion the aim of this study was to explore how staff recognize expressions of thriving among persons living in nursing homes.this was informed through staffs’ reflections on thriving, their knowledge of the individual person, and the overall congruence of these perceptions with their interpretation of the situational and environmental context.nursing home staff are said to be well placed to make clinical assessments and judgements regarding resident experiences as they play a central role in the daily lives of persons residing in nursing homes [9, 11, 20].for instance, familiarity and closeness with residents was said to enhance the potential for identification of expressions of thriving, or changes in thriving.previous research has also emphasized the importance of strong interpersonal relationships from the residents’ perspective as contributing to higher perceived quality of care and feelings of acknowledgment [21].these included aspects such as physical appearance, behavior or even environmental characteristics.nursing home staff described that recognition of thriving extended beyond what could be seen, to encompass what could be felt.importantly, the participants reported differing genders, ages, years of experience and nursing qualifications, allowing for exploration of a range of experiences related to the study aim.these findings are important to consider when measuring, interpreting or comparing staff and resident understandings of thriving, and may be used to inform ongoing development of staff education, clinical assessment tools and person-centred care strategies.
number of words= 1035
[{'rouge-1': {'f': 0.35367737387977044, 'p': 0.8420930232558139,'r': 0.22384615384615386}, 'rouge-2': {'f': 0.2128270929743374, 'p': 0.4298130841121495,'r': 0.14142857142857143}, 'rouge-l': {'f': 0.33253676118409947, 'p': 0.6871875000000001,'r': 0.21933837429111533}}]
-----------------------------------------------------------------------------------------------------------------------------------
p267:
Extractive Summary:
in general, 9% of nurses intended to leave nursing, most of them after only a few years in the profession [3, 4].further contributing to the problem is that the ageing segment of the population is rapidly expanding and consuming more health services with fewer new nurses entering the workforce.as a result, nurses frequently intend to leave their workplace and sometimes the profession.posttraumatic stress disorder (ptsd) was the focus of a study among nurses working in a variety of subspecialties and from 12 different countries [10].theoretical framework the purpose of this study was to understand why hospital nurses remain in their workplace to better inform the development of sustainable work environments.salutogenesis is a resource-oriented field of health research that involves understanding people as unique individuals with different resources and competencies.essential to salutogenic theory is understanding health as a process that exists along a continuum: the health/disease continuum.this is a way of looking at health as a process, as opposed to seeing it as a dichotomy between health and disease.the key concepts are soc, which is the ability to comprehend the whole situation, the capacity to use available resources, and the generalized and specific resistance resources against stress (grrs/srrs) [11, 14].this capacity is a combination of people’s ability to assess and understand the situation they are in, to find meaning in investing the energy needed to move in a health-promoting direction and to manage the situation – that is, comprehensibility, meaningfulness, and manageability.in a swedish context, the level of the three dimensions of soc was shown to vary.manageability was shown to be the weakest and to decrease the total soc, and the meaningfulness dimension was shown to be the strongest [16].dissatisfaction was the most important reason for intention to leave the job (35.5%), and among the dissatisfied, 33.1% intended to leave the nursing profession.push factors included understaffing, emotional exhaustion and poor patient safety.pull factors for staying included positive perceptions of the quality of care, patient safety and performing core nursing activities.the study is qualitative and descriptive in design [18, 19].taking this qualitative approach is appropriate for gathering a maximum amount of information within a particular domain [20].setting and sample the study was conducted at a hospital in western sweden.the hospital includes four speciality areas: emergency medicine, specialist medicine, surgical care and adult psychiatric inpatient care.the interview inclusion criteria were understanding and speaking swedish and at least 5 years of experience working as a nurse.a purposive sampling method was used.the nurses who consented to participate were contacted by telephone by an interviewer to schedule a time for the interview.results data saturation was achieved after twelve nurses were interviewed.the nurses also need to be included in meaningful healthcare teams that are characterized by job satisfaction and humour.job satisfaction and fun at work the nurses see job satisfaction as one of the most meaningful factors for remaining in the workplace for a long time.now, we have a very fine atmosphere.our students say that they’ve never been on a ward where they felt so welcome right away, a basic positive feeling and that’s how we want it.we are able to joke and talk to each other.however, i almost see my colleagues more than i see my family (nurse, psychiatric inpatient care).it is often patients who provide this acknowledgement, either directly or when the nurses have contributed to patients’ improved health.however, it can also be colleagues or relatives who acknowledge the nurse.every day you get this feedback from patients, whatever it may be, it feels like i've done something good.but, it's not always so, there are others too.most of the time i’ve done something good every day/…/i feel good about being seen and acknowledged and seeing and acknowledging others (nurse, surgical outpatient care).the nurses have a great need to feel productive and useful in their work.they can provide selfacknowledgement, especially when patients recover.then, they see how their involvement has contributed to the positive outcomes.a workplace where i thrive and feel i go to easily, that i’m useful, needed in any way.that’s what i get from both patients and staff (nurse, psychiatric inpatient care).i think togetherness gets a little stronger in crisis situations.i actually think that there are probably many institutions that feel that way, that you get a little tighter during crisis situations.togetherness becomes stronger in strained situations (nurse, medical inpatient care).the nurses experience togetherness when they are accepted, involved and part of the work team.having colleagues and supporting each other (nurse, medical inpatient care).these high demands concern both more routine tasks and stressful situations, but the nurses find ways to cope with the different care situations.factors that make the work situation manageable include receiving support from colleagues and managers, being involved and being able to interact with colleagues and other healthcare professionals.a good balance between work and leisure makes the work more manageable.an additional factor is being able to mentally leave work duties behind at the end of the workday.they also express a need to have a manageable workload, that is, to have control over various care situations.using these factors, they can build sustainable strategies for managing different care situations and coping with the work, both physically and mentally, for a long time.manageable workload nurses typically have a high workload and must prioritize tasks if they are to cope with a variety of care situations.having a manageable workload means there is a readiness for unexpected and emergency events.enough, so that both nurses and auxiliaries feel they are coping with the work and that there’s a buffer if something happens.that there are extra staff every day, because there’s usually someone who is absent, children who are sick.so that you have a small buffer to rely on, so that you never have to be understaffed for longer periods (nurse, surgical inpatient care).when the workforce is sufficient and the workload is balanced, there is time for important tasks other than pure physical care or emergency care.this means that nurses have the opportunity to take more time to create care relationships with patients and can perform qualitatively good nursing.this allows them to finish their workday with a higher level of satisfaction.when we’re sufficiently staffed, just enough workload, then i have time for my responsibilities, i have time to deal with things that are hanging over me.that i get a moment to sit and finish, so i can catch up with my duties (nurse, medical outpatient care).the workload can vary during the day and across work shifts.if nurses have opportunities to slow their pace during the day, they can take a break between more stressful tasks, which allows a kind of recovery.this recovery, in turn, can provide extra energy to tackle more strenuous situations.i can work at a fast pace one day because then i know that the next day, maybe, will be a bit calmer, and then i slow down, and i can manage the work.i know that i would have burned out otherwise (nurse, psychiatric inpatient care).you can work under stress temporarily, you can do it, you can push yourself and then you have to find a recovery phase/…/what the recovery phase looks like is individual, you have to find a recovery phase, a bit every day, and then you can handle more stress again.but i have been stressing for 16 years and then suddenly "bang!"and i've been there, i know how it feels and i never want to be there again.i’m an expert at learning to say "stop and no," even if i want to (nurse, surgical outpatient care).variable work and challenging situations nurses’ professional role involves a variety of everyday tasks and challenges.the nurses feel their tasks are positive challenges in that they create variation, which means working as a nurse never becomes monotonous.no days are identical, there are encounters with different people, patients and situations.the nurses need to be able to leave their duties behind and feel they are “finished” with them at the end of the workday.for nurses to develop good manageability, clear leadership is needed and, thus, a clear and strong manager is required.the nurses need varying tasks so that their work is not trivial and monotonous.nurses develop and learn when they face new challenges on a daily basis.valued role and good work the nurses understand that their professional role is of great value owing to the knowledge they have and the tasks they perform, which involve great responsibility.commitment and involvement the nurses feel highly committed to their work, which means they get deeply involved in their patients and try to do that “little extra” for them.according to the nurses, their expertise is also of great value, and this further strengthens their pride.the nurses are proud of and have positive attitudes towards the work they do and its effects.discussion the aim of the present study was to explore and describe factors explaining why hospital nurses remain in the workplace.the nurses clearly describe a care-related driving force that is the source of their great commitment to their work.the nurses try to set limits – an approach they have developed over a long period to prevent work-related stress and ill health.hence, reflection can serve as a sustainability tool, helping nurses develop a “thought respite” and recovery, which can in turn help them control their work situation, provide extra energy to manage workloads and build a defence against stress – all of which can be sustainable in the long term.the working group is an important resource for learning and sharing experiences; the nurse manager is important in that he/she can implement new approaches and strengthen the three soc dimensions – meaningfulness, manageability and comprehensibility – thus allowing nurses to increase their job satisfaction and create a sustainable working life.in this way, nurses can develop experience-based knowledge generated through many years of working with patients and teams at the same workplace.an important limitation is that all the participants were female nurses working in a hospital.this can be achieved by creating work environments where nurses feel pride in their professional roles in an organization that supports collaboration and togetherness with colleagues and leaders.
number of words= 1664
[{'rouge-1': {'f': 0.2751153307771895, 'p': 0.7922222222222222,'r': 0.1664611872146119}, 'rouge-2': {'f': 0.1729773663352276, 'p': 0.3833047210300429,'r': 0.11169046259280412}, 'rouge-l': {'f': 0.285843689244674, 'p': 0.6167625899280575,'r': 0.18603053435114503}}]
-----------------------------------------------------------------------------------------------------------------------------------
p268:
Extractive Summary:
background there are various drivers to promote innovation in clinical education for nursing students.in one jurisdiction, the international health workforce shortage projections [1] were a trigger for innovation.university leaders have increased intakes into baccalaureate nursing programs to meet workforce demand [1].these higher student enrolments are also associated with increased numbers of new graduates, who require transition support upon entering the workforce [2].to further complicate this situation, health agencies are experiencing higher patient acuity and shorter lengths of stay [4], which potentially puts pressure on staff and supervision capacity.for example, some schools employ clinical facilitators (usually external to the host unit) who work with students, usually on a 1:8 ratio, to assist and assess their learning.the ward-based rns are clinical experts while facilitators understand educational best practice to guide ward-based rns help students learn.however, the deu was developed for integrated placements in which students attend placement several days per week over the teaching semester, while the majority of australian nursing students complete block placements, e.g., full time placements for shorter set periods.the clinical facilitator supports rns to engage with students and new graduates assimilating into the clinical setting, providing guidance to rns on appropriate learning activities.participants were aged 17–70 years and included identified stakeholders: undergraduate bachelor of nursing students, new graduates, ward-based rns, e2p facilitators, and academics who convened the students’ clinical courses whilst on placement.the e2p facilitators were provided with information at a scheduled workshop.nurse academics were invited to participate in interviews via email.it had 19 items on a 5-point likert scale (1 = strongly disagree; 5 = strongly agree) focused on students’ perceptions of the qualities and behaviours of the e2p facilitators (14 items) and ward-based rns (5 items) supporting them.within the survey, students were also invited to respond to three open-ended questions about their experience including what the strengths of their facilitators were, what were areas for improvement and other comments they had about the facilitation of the placement.the student comments and new graduate interviews were coded and thematically analysed separately and then the findings were combined to arrive at overall themes from the learner perspective.the interview data from the rns, e2p facilitators and academic nurses were analysed as a group with shared experience of supporting student learning in the workplace.thematic analysis, as described in the previous section, was used.trustworthiness of the findings was confirmed through the selection of participants with experience in the ccem, rich variation in perspectives provided by a relatively diverse sample of participants and use of data saturation for the interview data [21].the results of the data produced by each stakeholder group was then merged to arrive at an understanding of stakeholders’ acceptability of the model.to do this, data was initially presented to the stakeholder reference group, who discussed the data, and together with researchers, identified areas of convergence, divergence, and relatedness in order to produce a more complete understanding [19].study data are held in secure electronic files.the majority were aged 18–25 (56.7%).one hundred and thirteen bachelor of nursing students provided responses to open ended questions on placement facilitation.five new graduates, seven e2p facilitators, four ward-based rns, and three nurse academics participated in interviews.student and new graduate responses scale reliability ranged from 0.77–0.90.for example, students were more likely to report the absence of e2p facilitators in comments such as: ‘more support from the [e2p] facilitators would be good.i think they are too busy with too many students and some more practical hands on with a facilitator would be better’ (sn 86).academics, rns and e2p facilitators thematic analysis of the interviews with ward-based rns, e2p facilitators, and academics revealed three main themes: 1) students’ and new graduates’ integration into the workplace can promote learning; 2) tensions arise in new ways to approach performance assessment; and 3) aligning expectations requires high levels of communication.they become part of the team because you’re not on the floor all the time with nurses [saying] ‘your facilitator can do that with you’…now the unit staff are taking ownership… i think it’s a better experience for [learners] in that they…actually become part of that unit…they have to develop their problem-solving skills in that situation a lot better than the old model where you are problem-solving for them a lot’ (e2p7) ‘it definitely fosters the students to be independent, and to seek their own learning opportunities’ (e2p5) [third years] ‘felt that they were becoming part of the team and that they could troubleshoot and communicate with other staff and felt comfortable about that … (academic nurse (an) 1) however, this was also identified as an area of concern for those students who may not be strong communicators or who are earlier in their career trajectory, such as second year students: ‘… felt that those who weren’t as confident … would have difficulty in asking for help from anyone else but their facilitator, so they were thinking it would be more difficult for the second years’ (an1).but there’s not that in depth asking of questions or delving a little bit more to see whether or not that is the case’ (e2p1) ‘maybe we need to have a bit more discussion on what is expected [regarding] whether you see every student every shift’ (e2p1) processes for communication across the stakeholder groups were negotiated through the reference group activities and discussions.however, further communication appeared to be required.the academics reported feeling frustration as they adjusted to the model.it’s the nurses who are the specialists in that area…so they do the teaching, whereas this role now is more liaising and anyone can do that if they’ve got the skills that they need for that, so i think the role is better’ (e2p3) in summary, the transition to the ccem required alignment of stakeholder expectations and the communication strategies to achieve alignment were inadequate to support the transition.for new graduates, the opportunity to work with multiple nurses with intermittent e2p facilitator support provided structured support and an opportunity to learn about different styles of nursing.academics, e2p facilitators and rns appeared to find the model acceptable, noting that improved communication of expectations is required.they valued the increased independence of more senior students and new graduates in their learning, noting that more junior students may require additional support.these highly ranked behaviours are recognised as important to engage students in workplace learning [22].an unexpected benefit in the assessment process was the development of e2ps as assessors through sharing their perceptions of student performance within their teams.in particular, the role of the ward-based rn was unclear.to improve acceptability, it will be important to build on the foundational relationship between e2p facilitators and ward-based rns.e2p facilitators can work with nursing teams to identify appropriate nursing activities in their specific health service settings.workshops or masterclasses for e2p facilitators can also focus on skills such as coach for rns to support student learning as part of their usual work practices [28].of note, the rns preferred the e2p facilitator to supervise student practice, particularly when the workload was high.limitations there were limitations to this study.merging results of quantitative and qualitative data in a meaningful way can be challenging.this study was purposefully designed with a focus on the same concept, learners in the workplace, which is acknowledged as important in convergent designs [19].using experienced researchers is critical in this design [19].to manage our expectation of a low response, we also considered the open-ended comments and included interviews with new graduates.while the e2p facilitators described their role as supporting ward-based rns to increase their skills in facilitating student learning from practice, continued development of their role in developing their allocated wards as learning units is needed, including the identification of suitable cases and learning strategies, and mastery of the process of continuous feedback on performance to ensure sustained acceptability.the ccem provides an opportunity to gain the benefits for student learning that are found in a deu while providing for block placements, such as student engagement in authentic work as part of a team, and development of work-based pedagogies.given a relatively short implementation timeframe, initial evaluation focused on acceptability as this was critical to sustainability.the rapid transition did contribute to communication deficits that require continued monitoring for improvement.for others interested in this model, we recommend a good communication plan, and attending to formal structures and processes between stakeholder groups.future evaluations will include the appropriateness of the model for specific areas, what elements enhance adoption, fidelity of the model in different settings, as well as cost and sustainabili
number of words= 1398
[{'rouge-1': {'f': 0.3236279683377309, 'p': 0.7366666666666666,'r': 0.20736263736263738}, 'rouge-2': {'f': 0.17261081667550776, 'p': 0.3108026755852843,'r': 0.11948453608247422}, 'rouge-l': {'f': 0.34239792190686535, 'p': 0.6342458100558659,'r': 0.23449511400651465}}]
-----------------------------------------------------------------------------------------------------------------------------------
p269:
Extractive Summary:
background morality is an indispensable part of human life and a subset of practical philosophy looking for the right and wrong and determining good and bad in a collection of behaviors under certain conditions [1].the nursing profession is one of the sciences with abundant illustrative ethical aspects in the past, present, and future [2, 3].since distinguishing the good and bad is in the body of ethics, the moral competency of the nursing profession may be rendered as equal to professional competency [3].nurses ought to possess moral courage to perform on the basis of what is considered ethically right provided personal values and criteria correspond to the accepted healthcare values [4].some studies have demonstrated that moral courage is related to concepts concerning assessment of ethics under certain conditions like sensitivity to justice [9], perception of control on one’s emotions and performance such as emotional self-regulation [10], and selfefficacy [11].empowerment is a process completed by personal values and struggles and also by environmental factors [18].psychological empowerment is an appropriate solution for enabling individuals to cope with mental pressures and work stressors [19].methods design of the study this descriptive cross-sectional study was conducted in 2019.the study population consisted of all nurses employed in khatam-al-anbia hospital and shahid beheshti hospital affiliated to shahid sadoughi university of medical sciences, yazd, iran.a total of 180 participants were selected randomly using sample volume formula with confidence interval of 95%, test power of 80% and the correlation coefficient of 0.18 according to the pilot study.the inclusion criteria were: holding at least a bachelor of science (bs) in nursing, at least 1 year of clinical nursing experience, and inclination for participation.the research instruments were distributed by the researcher in various work shifts and collected after completion.the demographics questionnaire included information on age, gender, employment status, literacy level, marital status, official position, and work experience.the moral courage scale was developed by sekerka et al. [28].thus, the score of each item may range from 3 to 21.the minimum and maximum total scores were 15 and 105, respectively.moreover, the findings demonstrated that the rate of nurses’ psychological empowerment was moderate so that the greatest mean belonged to “competence” and the smallest mean pertained to “confidence” (table 2).the findings indicated a positive significant correlation between “psychological empowerment” and “moral courage and its dimensions” (p < 0.05).besides, there was a significant correlation between moral courage and all dimensions of psychological empowerment except for “confidence” (p < 0.05) (table 3).discussion this study determined the correlation between nurses’ moral courage and psychological empowerment.the findings showed that the participating nurses had a high degree of moral courage.in the present study, the highest score of moral courage belonged to “going beyond compliance” and the lowest score pertained to “multiple values”.some studies reported the greatest amount of moral courage in “moral agency”also organizations must accept moral virtues such as courage and direct organizational culture towards supporting the nurses with moral courage.some of the consequences are the right decision making, the right action, the patient’s safety and comfort, and playing the role of care [16].moreover, another study in egypt, suggested a moderate level of nurses’ psychological empowerment [37].yet, the study by mirkamli et al. reported iranian nurses’ psychological empowerment at a high level [41].such a nurse accepts responsibility with courage in decision-making [18]. to clarify this finding, it may be said that since increasing psychological empowerment may lead to reduced mental pressures and work environment stressors, and enhance the power of decision-making and performing moral behavior by the nursing staff [42–44], it can ultimately result in the creation of moral courage in nurses.the findings showed no significant correlation among demographic variables, moral courage and psychological empowerment except that moral courage was significantly promoted with increasing age and work experience.murray also states that as the work experience of nurses increases, the impact of barriers in the work environment on nurses’ performance decreases and moral courage increases [5].thus, some nurses may have not provided real answers.another limitation was lack of control over intervening variables such as factors affecting personnel’s concentration that might have confounded the results.consequently, organizations and nursing managers are obliged to provide some strategies like changing managerial style in clinical wards, nurses’ contribution to decision-makings, and expanding a suitable organizational culture to move towards promoting nurses’ mental power and its various aspects as far as possible.providing the necessary prerequisites for promoting nurses’ psychological empowerment can lead to increased morally courageous behaviors, ultimately ending in improved nursing quality
number of words= 737
[{'rouge-1': {'f': 0.40779941930800867, 'p': 0.8506122448979592,'r': 0.2681865284974093}, 'rouge-2': {'f': 0.27637235700534224, 'p': 0.5315384615384615,'r': 0.18673151750972763}, 'rouge-l': {'f': 0.39025661214090185, 'p': 0.733716814159292,'r': 0.26582245430809404}}]
-----------------------------------------------------------------------------------------------------------------------------------
p270:
Extractive Summary:
background people with a body mass index (bmi) greater than 30 have a higher risk of a range of illnesses, such as heart disease and type 2 diabetes mellitus (t2dm) [1].in high income countries with an indigenous population who, as a result of colonisation, have been marginalised – such as in the united states, canada, australia and new zealand (nz) – the indigenous people have a higher prevalence of obesity than the dominant european population [2].there has been a call to adopt a group-focused approach to weight loss for indigenous people in america, so as to: “(a) build and reinforce social cohesion and collective efficacy, (b) use the motivating force of friendly competition, and (c) aspire to change local norms and policies through assuring high visibility of alternate behaviors and engaging formal and informal leaders” [6 , p 224].the competition was backed up with pharmacological treatments for cessation, cognitive behavioural treatment delivered via an interactive website and locally-based health providers.participants and sample size three distinctly different geographical regions in nz were selected for recruitment: an urban māori population (palmerston north), a small town/rural māori population (northland) and a pacific island community in a major nz city (auckland).eligibility and exclusion criteria the eligibility criteria were māori or pacific people, aged 16 years of age and above, having a body mass index (bmi) ≥ 30 kg/m2, and being at risk of or having developed t2dm or cardiovascular disease (cvd).they used convenience sampling to find participants, advertising through their existing networks to staff and in their communities.though recruitment of intervention and control participants occurred concurrently, recruitment for control participants was done over an extended time period (four months).this was also publicly visible as was a competition scoreboard displaying the progress of each team.in each region, three cash prizes were offered for: the greatest progress at two months (nz$1000), greatest progress at four months (nz$1000) and greatest progress at six months (nz$3000).progress was based on the number of team members who had lost ≥4 kg weight in the preceding two months plus the number of team members who had lost ≥3 cm in waist circumference during the same period, plus the team’s position on the competition scoreboard, which was calculated by tracking team participation and completion of daily and weekly challenges.other questions at baseline asked about previous use of dieting/weight-loss programmes, demographic characteristics and food security.community services cards enable people on low incomes to receive discounts, for instance on their healthcare and cost of medications.the questionnaire was pilot tested with six māori and pacific people with bmi > 30 from among the researchers’ networks.outcomes the primary outcome was mean weight loss at sixmonth follow-up.baseline weight, waist circumference and bmi were not normally distributed and so non-parametric tests were used to examine differences between groups.to calculate change in eating behaviour a non-parametric related-samples wilcoxon signed rank test was performed to detect if there was a significant difference (p < 0.05) between eating behaviour at baseline versus six months.ethics this study was approved by the nz ministry of health’s northern b health and disability ethics committee (16/ ntb/101).the demographics and baseline anthropomorphic data are given in table 1.the study struggled to recruit participants for the control group, and follow-up of these participants was difficult.an initial 29 control participants were recruited and of those 55% (n = 16) were still in the study at six months.thus, analysis of weight-loss is restricted to the 6- month follow-up data.eating behaviour changes in eating behaviours were not assessed in the control group due to missing data and small sample size.table 4 summarises the change in selected eating behaviours that the intervention was designed to change among the intervention participants.there were trends toward a reduction in waist circumference for the māori and pacific groups in this study.further, the control participants had, on average, a lower bmi and proportionately higher educational levels (found to attenuate weight loss programme attrition [26]).unfortunately, motivation to lose weight was not measured at screening or baseline.the receipt of prizes throughout the competition may have contributed to team members’ motivation to maintain behaviours.chin et al. [28] found that incentivising attendance versus only rewarding weight loss was associated with less attrition and more weight loss, though intervention dose was important regardless of the incentive strategy.research has indicated that financial incentives are effective for assisting people to attend and persist with weight-loss interventions resulting in weight loss [28–30].however, despite including financial incentives in wehi, the desired weight loss was not achieved.burns et al. [30] in their systematic review suggest that an incentive that is contingent on an outcome and is continuous throughout the intervention is more effective than a lottery type reward.the intervention appeared to be effective in changing some dietary behaviours in the short term.other authors have suggested that small changes, especially if triggered by low-cost interventions that remotely facilitate self-monitored or self-monitored with tailored feedback, could produce a significant public health impact if extrapolated over a population [32].future research is needed to identify optimal use of incentives for triggering behavioural change and adherence to weight-loss goals, such as increasing the relative reward for higher impact behavioural change objectives and reduced consumption of fast foods over the reward for less impactful goals, such as drinking more water.this will allow a larger breadth of participants to be able to receive incentives.we did analyse adherence [21] utilising programme data, and we did conduct some qualitative work assessing the acceptability of the intervention to a sub-sample of participants’ and regional co-ordinators’, but that work remains unpublished (those results are contained in a technical report prepared for the funder which is available from the author).group weight-loss competitions have predominantly been researched in workplaces.the involvement of work-based teams in wehi suggests that there could be merit in trialling a version of wehi specifically focused on māori and pacific health workplaces.conclusion given the dearth of previous weight-loss programmes available in many māori communities, and the higher proportion of māori and pacific people with obesity, this study provides initial information useful for designing acceptable and attractive interventions for these high priority groups.the wehi trial was successful at triggering some government recommended dietary changes, such as eating a minimum of two servings of fruit a day and some effect on weight loss was indicated.like the wero stop smoking competition, wehi could be delivered as a health promotion programme with the aim of triggering weight-loss attempts, rather than as a personal health treatment programme.obviously, there is a need to improve retention of participants in the intervention, but this is a global challenge for weightloss programm
number of words= 1087
[{'rouge-1': {'f': 0.30727596561963505, 'p': 0.7669696969696971,'r': 0.19212389380530975}, 'rouge-2': {'f': 0.17719067623401738, 'p': 0.34918781725888326,'r': 0.11871567759078831}, 'rouge-l': {'f': 0.3106280552758687, 'p': 0.6255555555555556,'r': 0.20661202185792352}}]
-----------------------------------------------------------------------------------------------------------------------------------
p271:
Extractive Summary:
introduction severe acute undernutrition affects 18.7 million children worldwide, and moderate acute undernutrition (mam) affects an additional 32.8 million.undernutrition remains one of the most common causes of morbidity and mortality among children throughout the world [1].cardiovascular diseases are one of the commonest medical conditions which are strongly associated with undernutrition [3–5].the prevalence of malnutrition among children with cardiac disease varies according to the population studied.according to okoromah and colleagues, a study done in nigeria, reported a prevalence of undernutrition (90.4%) and severe undernutrition (61.2%).however, there are no published data showing the national prevalence of undernutrition among children with cardiac diseases from africa and ethiopia [6].however, the cause of undernutrition in cardiac patients is multifactorial.every child with cardiac diseases should be screened for growth failure and undernutrition to identify patients at high risk of poor outcomes who might benefit either from medical management or surgical interventions to prevent deterioration of congestive heart failure and improve prognosis [12].methods this hospital-based cross-sectional study was conducted in the pediatric cardiology clinic at university of gondar hospital.premature infants, children with a known genetic disorder, and children with other, non-cd chronic illnesses were excluded.two physicians reviewed the medical records of all participants.socio-demographic, anthropometric, clinical and echocardiographic data were collected in the questionnaire.the who global database on undernutrition recommends a cut-off z score of ≤ − 2 to classify low whz (wasting), low waz (underweight) and low haz (stunting) as moderate undernutrition, and a z score of ≤ − 3 sd to define severe undernutrition.those variables found to be significantly associated with undernutrition were included in the multivariate regression model.acquired heart disease (ahd) was more common than congenital heart disease (69.5% vs 30.1%), and rheumatic heart disease was the most common form of acquired heart disease in the pediatric follow up clinic during the study period.16.4% of participants had pulmonary hypertension, 50.6% had congestive heart failure, and 90.3% had a history of hospitalization.undernutrition was identified in 65.7% of patients, of whom 34.5% had moderate acute undernutrition and 31.2% had severe acute undernutrition.discussion it is well known that undernutrition is common in cardiac patients and related with increased morbidity and mortality.in developing countries like ethiopia where surgical intervention for cardiac disease like congenital heart disease and/or rheumatic heart disease is scarce or unavailable at all, the magnitude of undernutrition is expected to be high [3, 13, 14].stunting which is an indicator of chronic undernutrition was found to be 39.7% and with 14.5% of cases had severe stunting whereas the prevalence of underweight was found to be 54.5%.other studies also showed children with cardiac disease are higher risk of undernutrition compared to those without cardiac disease [5, 9].nyha/modified ross class iii and iv heart failure, cardiac chamber enlargement, and pulmonary hypertension were associated with undernutrition in our study.this is in line with various studies that reported children with advanced congestive heart failure and/or pulmonary hypertension were more likely to be malnourished [8, 9, 18].this association may be explained by congestion of bowel and liver leading to early satiety.second, we were unable to include other variables known to affect nutrition, including prematurity, genetic disorders, and previous dietary interventions.
number of words= 518
[{'rouge-1': {'f': 0.4407468261443391, 'p': 0.6167625899280575,'r': 0.34289048473967687}, 'rouge-2': {'f': 0.21468524671244707, 'p': 0.2793862815884477,'r': 0.17431654676258995}, 'rouge-l': {'f': 0.3321466314641932, 'p': 0.4193975903614458,'r': 0.2749469964664311}}]
-----------------------------------------------------------------------------------------------------------------------------------
p272:
Extractive Summary:
among women of childbearing age, overweight and obesity have been associated with increased risk of non-communicable diseases (ncds), pregnancy complications, caesarean section births, adverse birth outcomes, and infant mortality [8– 11].however, little is known about the role of physical activity and nutrients intake among african women of reproductive age because most reports come from national demographic surveys where physical activity and nutrient intake are not assessed.this may be due to several factors, including limited pre-pregnancy bmi data from clearly designed populations studies in tanzania, as in other low-income settings.inclusion criteria for the study included (i) women aged 15 to 49 years, (ii) who intended to become pregnant within the next 4 years, (iii) were currently not pregnant based on the last normal menstrual period, and (iv) provided written informed consent.a lottery method was used to select one woman randomly from households with more than one woman.a household replacement was considered for those households in which no woman met the inclusion criteria.therefore, the minimum sample size was 1012 women for 80% study power and less than 5% level of significance.face-to-face interviews were conducted for data collection using a standardized questionnaire.study research assistants collected information on participants’ socio-demographic and economic characteristics, lifestyle characteristics such as alcohol use, smoking, dietary intake, and levels of physical.anthropometric measurements, including weight and height, were taken using a calibrated weighing scale to the nearest 100 g and a height board to the nearest cm.outcome variable the outcome in this study was overweight and obesity obtained by computing bmi as weight in kilograms (kg) divided by height in meters (m) squared.a binary outcome variable was generated by combining overweight or obese and compared against women who had normal bmi.moderate to vigorous physical activity was scored as ≥600 met-minutes per week, while sedentary physical activity was scored as < 600 met-minutes per week.total time spent in vigorous physical activity was categorized as ≥75 min per week and < 75 min per week, and moderate physical activity was categorized as ≥150 min per week and < 150 min per week.assessment of macronutrients intake dietary information was assessed using a locally adapted food frequency questionnaire (ffq) used previously in the study area, containing at least 85 foods [25].the akaike information criteria (aic) was used for model selection, whereby the model with the lowest aic was considered as a parsimonious model.tests for trend were conducted for multivariate models for macronutrients.results a total of 1004 women of reproductive age were enrolled in the study.women in the study had a mean age (±sd) of 30.2 (±8.1) years.of these, 31.7% were 35 years or older, 57.9% were either married or cohabiting, and 54.4% had no employment.women who had informal employment had a 14% higher risk of overweight and obesity (95% ci: 1.01–1.29; p = 0.04) compared with those who are not employed.however, other factors, including education, parity and household size, were not significantly associated with overweight or obesity.women who performed moderate to vigorous physical activity of at least 600 met per week had a 21% lower prevalence of overweight and obesity compared with those with a sedentary lifestyle (pr = 0.79; 95% ci 0.63– 0.99; p = 0.04).in comparison, a higher intake of protein from fish and poultry was associated with a lower risk of overweight and obesity.[table 4].discussion our findings show that the combined prevalence of overweight and obesity among women intending to become pregnant within the next 4 years is very high in tanzania.overall, we found that being older, having informal employment and middle to high socioeconomic status were associated with overweight and obesity among women in dar es salaam, tanzania.the study found an association between vigorous physical activity and decreased overall prevalence of overweight and obesity.this is of great concern given most nutritional counselling observed in many antenatal health care services; the emphasis is on maternal weight gain and less on the overweight and obese control (a personal conversation with the health facility providers in the study area).a high prevalence of maternal obesity is also reported in a systematic review and meta-analysis across africa, ranging from 6.5 to 50.7% [28].such a high prevalence of overweight and obesity in women who intend to conceive within the next few years is alarming considering the reported maternal and newborn adverse outcomes associated with high prepregnancy bmi [9, 11].in addition, weight retained during pregnancy is often difficult for women to lose, even for obese women, contributing to increased bmi over time [35].women who were self-employed or under the informal employment sector, such as street vendors, shopkeepers, and tailors, had a higher prevalence of overweight and obesity than women who were unemployed or formally employed.the role of employment status as a determinant of bmi is not clear.more importantly, more affluent households can afford more calories in their diets, having financial power to purchase processed and unhealthy foods, eat fast food from restaurants etc., while also being less likely to be physically active [43].high sugar and beverages consumption above 10% of the total daily energy requirement has increased in recent years, especially in urban settings, including tanzania [50].thus, this calls for immediate attention, given that high sugar intake is associated with non-communicable diseases [51].animal protein and fat intake were not associated with an increased risk of overweight and obesity.however, we cannot ignore the possibility of a recall bias as some respondents may fail to remember foods consumed in the past 30 days.additionally, we utilize a cross-sectional study design which may be affected by confounding.however, we tried to address the confounding effect by adjusted for energy intake and known potential confounders.conclusion the overall prevalence of overweight and obesity among women of reproductive age who intend to conceive within the next 4 years was very high.overweight and obesity were significantly associated with a sedentary lifestyle, wealth, older age, informal employment status, and marital status.high sugar intake was associated with a higher risk of overweight and obesity, while protein consumption from fish and poultry was associated with lower risk.
number of words= 988
[{'rouge-1': {'f': 0.39516281058147584, 'p': 0.7453246753246754,'r': 0.2688527724665392}, 'rouge-2': {'f': 0.22294172110066157, 'p': 0.37293159609120524,'r': 0.1589952153110048}, 'rouge-l': {'f': 0.3592777561631238, 'p': 0.5940963855421686,'r': 0.2575}}]
-----------------------------------------------------------------------------------------------------------------------------------
p273:
Extractive Summary:
important obesogenic factors include culture, home environment, corporate advertising, and parental knowledge [3].because parents can only initiate healthy habits after they understand what those habits entail, parental knowledge has a strong influence on the diets of youth and children residing in the home, on their exercise habits, and on their beverage choices [3, 4].” [9] examples of ssbs include regular soda (not sugarfree), fruit drinks, sports drinks, energy drinks, sweetened waters, and coffee and tea beverages with added sugars.social marketing and public health mass media campaigns have been effective in changing health behaviors, knowledge, and attitudes [11–16], and are considered an evidence-based strategy [17].the program developed and released messages designed to combat ssb consumption by urging oklahomans to ‘rethink your drink,’ and replace ssbs with water.the “rethink your drink” program and messages originated from the nutrition education and obesity prevention branch of the california department of public health [12, 20].we previously reported the results of a 2015 crosssectional study designed to gather information about oklahomans’ knowledge, attitudes, and behaviors concerning ssbs [19].the study reported baseline ssb consumption in oklahoma adults with children living in the home prior to the launch of the ‘shape your future - rethink your drink’ (syf/ryd) health communication program.the current study reports the results of a second cross-sectional study, initiated after the first quarter of the campaign.methods methodology from the 2015 survey, which occurred before the launch of syf/ryd, has been reported in another paper [19], and survey questions were identical for both time periods, using previously validated survey items for ssb consumption [14–16].thesyf/ryd campaign was launched in july 2016, with combined television and cable outlets, digital print, radio, bulletins, and posters.the program is ongoing.data collection after the program launch occurred october 2016 through july 2017 via telephone survey by the sooner survey center at the university of oklahoma hudson college of public health.the population was a random sample of all noninstitutionalized adults in oklahoma with at least one child under the age of 18 years living in the household and with either a cellular or landline telephone.for both questions, respondents could answer in times per day, week, or month.this analysis includes four outcomes related to ssb consumption.the third question was intended to measure knowledge about the consequences of ssb consumption with answers to this statement: ssbs are linked to obesity, diabetes, and heart disease.” if yes, they were asked what the theme, name, or slogan of this program was, and to describe an ad.covariates covariates included gender, three categories of age (< 35 years, 35–54 years, and ≥ 55 years), four categories of race (white, american indian/alaska native, african american and “other”), two levels of education (high school degree or less versus some college /technical school or more), and two self-assessed levels of general health (excellent, very good, or good, versus fair or poor).imputation was used for missing responses in raking dimension variables [22, 23].we used sas version 9.4, for all analyses.survey procedures and sampling weights were used to obtain population-level estimates unless indicated otherwise.pearson chi-square tests were used to examine whether the outcomes varied by the covariates of interest.p-values < 0.05 were considered statistically significant.adjusted odds ratios with confidence intervals are reported.in 2015, prior to the launch of the syf/ryd media program, we surveyed 1118 oklahomans.about two-thirds (69% versus 63%) reported they participated in moderate to intense physical exercise three or more days every week.while most (82% versus 85%) perceived their health status as excellent, very good, or good, only half or less (50% versus 42%) drank the recommended eight cups of water or more daily, and less than one quarter ate the recommended daily three or more servings of fruits (23% versus 20%) and vegetables (19% versus 21%) [28].only one third strongly agreed they could afford to buy healthy foods in 2015 (34%), compared to about half (48%) in 2017.conversely, among those with a high school education or less, the prevalence of daily ssb consumption decreased 31% from 61% in 2015 to 42% in 2017 (p < 0.01).additionally, in those who perceived their health status as excellent, very good, or good, there was a statistically significant 24% decrease in ssb consumption between 2015 (33%) and 2017 (24%).one notable exception was race.although there was not a statistically significant difference within that variable, confirmed exposure among american indian/alaska natives was lower than all other groups (table 1).similarly, there was no difference among those with and without confirmed campaign exposure in plans to limit ssb consumption for their families or perceived ability to substitute water for ssbs for their families.these included high school education or less (aor = 1.33, 95% ci = 1.02, 1.73), perceived health status as fair or poor (aor = 2.02, 95% ci = 1.47, 2.78), and inability to afford healthy foods (aor = 1.33, 95% ci = 1.06, 1.67, table 4).those with less than a high school education experienced a 31% decline in ssb consumption, as did those who reported drinking eight or more cups of water per day.similarly, good or better health status, those consuming fewer than three servings of fruit per day and those less likely to afford healthy food experienced significant declines in ssb consumption following the launch of the syf/ryd campaign.interestingly, this priority population, with significant obesity burden among adults and children [29] also had the highest odds of ssb consumption, and this independent association was consistent across all four ssb outcomes.the larger decrease seen there is likely due to an excise tax and cap on ssb portion sizes which were implemented around the same time [14].a mass media campaign targeting parts of tennessee, kentucky, and virginia in 2017 reported a 3.4% decrease in ssb sales and a 4.1% decrease in soda sales along with an overall increase in self-reported ssb consumption, following the campaign [11].in a meta-analysis using literature published from 1990 through 2014, vargas-garcia and associates found a paucity of research about the association between water consumption and ssb consumption in adults; this study adds to that body of evidence [17].interventions may not lead to immediate changes in ssb consumption, but may have longer lasting benefits.in this study we have unearthed important findings.additionally, about two-thirds understand the link between ssb and heart disease, diabetes, and obesity (67%).understanding factors associated with the consumption of ssbs will lead to more effective strategies and the identification of priority populations.
number of words= 1050
[{'rouge-1': {'f': 0.36661165291322834, 'p': 0.77,'r': 0.24057761732851987}, 'rouge-2': {'f': 0.22023776409321788, 'p': 0.40457249070631973,'r': 0.1513008130081301}, 'rouge-l': {'f': 0.37188059878581736, 'p': 0.68875,'r': 0.25470149253731345}}]
-----------------------------------------------------------------------------------------------------------------------------------
p274:
Extractive Summary:
background undernutrition is an important public health issue particularly for vulnerable groups including children and women of childbearing age especially pregnant mothers [1].undernutrition is a serious global health problem.about 795 million people are undernourished mostly in low and middle-income countries and the problem is most critical during pregnancy [2].globally, undernutrition is contributing to the deaths of 3.5 million mothers and under 5 years of age children each year.an adequate supply of nutrients and oxygen for the mother to her fetus is one of the factors that are critical for fetal survival.the ability of the mother to provide nutrients for her baby depends upon the nutritional status, body size, and body composition of the mother and all of which are being established throughout the life of her fetus [6].undernutrition in pregnant mothers is a key contributor to many problems.it makes the women more susceptible to diseases, more risk of having miscarriages, poor fetal growth, low birth weight, infant morbidity, and mortality [7].evidence showed that the burden of undernutrition among pregnant women is high.the most acceptable explanation for this wide variation is likely to be the fact that variation in the contextual factors of pregnant women’s undernutrition.therefore, the undernutrition of pregnant women needs to be assessed in a specific context to develop effective interventions.population the source population of this study was all pregnant women in konso district.the study population was all pregnant women in the selected kebeles (small administrative unit in ethiopia) of konso district.study variables the dependent variable in this study was undernutrition among pregnant women.results socio-demographic characteristics of the respondents from the total of 527 pregnant women, 501 participated in this study making the response rate 95%.the majority 190 (37.9%) of respondents was using the river as a source of water followed by communal water point (table 3, fig. 3).prevalence of undernutrition among the study participants the result of this study found, 216(43.1%) (95% ci: 38.8%- 47.5) of the pregnant women were undernourished (muac < 23 cm) and those with muac greater than or equal to 23 cm were 285(56.9%) (fig. 4).the mean muac was 22.9 ± 1.4 (sd), the minimum and maximum muac for the study subject was 18 cm and 26 cm respectively.factors associated with undernutrition both bivariable and multivariable logistic regression analyses were employed.in the bivariable analysis variables such as religion, marital status, mothers occupational status, husbands occupational status, average family income, source of water, resource decision making power on the household asset, household food security status, history of stillbirth, knowledge of additional meal during pregnancy, latrine availability, dietary diversity score, and iron supplementation was significantly associated with undernutrition.variables with a p-value of < 0.25 in the bivariable logistic regression analysis were entered into multivariable logistic regression analysis.in multivariable analysis, household food insecurity (aor = 3.1; 95%ci: 2.1–4.6), low dietary diversity score (aor = 4.9; 95%ci: 2.6–9.2), medium dietary diversity score (aor = 2.3; 95%ci: 1.2–4.7), absence of household latrine (aor = 1.8; 95%ci: 1.2–2.6) and having family resource decision making by husband only (aor = 1.7; 95%ci: 1.1–2.6) were significantly associated with undernutrition (table 4).focus group discussion focus group discussion was held with 12 participants in each two fgd from different community groups such as pregnant mothers, husbands of pregnant mothers, mother in-low, and elders.responses were coded and categorized by content with thematic analysis.these were: food restrictions, cultural believe related barriers, food production-related barriers, and food diversification practice-related barriers (table 5).discussion the purpose of this study was to assess the prevalence and associated factors of undernutrition among pregnant women in konso district, southern ethiopia.this study found that four in every ten pregnant women were undernourished.household food security status, dietary diversity, latrine availability, family resource decision-making were significant determinants of undernutrition.food restriction practices, weak nutrition education and malnutrition screening program, the practice of depending on a local alcoholic drink called “cheka”, drought, raindependent farming practices, and low socioeconomic status were identified barriers from qualitative data.the current study found that the prevalence of undernutrition in pregnant women was 43.1% (95% ci: 38.8%- 47.5).the result of this study was higher than those studies conducted in other areas of ethiopia, 31.8% in central refit valley [15], 9.2% in wondogenet district southern ethiopia [16], 21.8% in silte zone southern ethiopia [17], 19.8% in dessie town, northeastern ethiopia [14], 19.5% in eastern ethiopia [13], 24% in humanitarian setting in ethiopia [18], 16.2% in gondar hospital [20] and 14% in gondar town [21].the discrepancy might be due to differences in sociodemographic characteristics, geographical variation, cultural beliefs such as food taboos, poor nutritional intervention programs in the konso district.another possible reason may be due to a difference in the season of studies conducted.this variation might be due to differences in muac cut of value, and the socio-culture distinctions between ethiopia and the other counties.this could be possibly due to that family food shortage usually results in a lack of daily nutritional requirements and poor dietary intake leading to undernutrition of women.the odds of undernutrition among pregnant women with low dietary diversity score were about five times higher when compared with those women with high dietary diversity score (aor = 4.9; 95% ci: 2.6–9.2) and the odds of undernutrition among pregnant women with medium dietary diversity score were two times higher when compared with those women with high dietary diversity score (aor = 2.3; 95% ci: 1.2–4.7).this result is in line with the study conducted in gambella ethiopia [45].this might be because mothers who have a practice of food diversity will get the different nutrients from different diets and this might cause them to be well-nourished than those with less than average dietary diversity score.the reason why it is not allowed to eat the list of food items above is, we believe when the pregnant mother ate these foods, the baby became very big and our fear is the mother will face difficulty in childbirth.” 31 years old fgd participant.this result is in line with the study conducted in tanzania [47].the decision-making power on household assets had also a significant association with pregnant women undernutrition.this might be due to the reason that one part wife only or husband only decision in family resources may affect their communication and if one ignores the idea of others, this may cause the wives not to be supported by their husbands and this may also negatively affect their nutritional habit.on the other hand, males are mobile (moves from place to place for work-related purposes) and can have an opportunity to get a variety of foods but mothers and small kids are restricted to stay at home so that they lack un opportunity to get a variety of food.” fgd participant.limitations of the study this study recognized the following limitations: this study used muac < 23 cm as the cut-off value for undernutrition in pregnant women.some self-reported variables like household food security status and decision-making power on the household assets may be affected by social desirability bias and it was reduced through detailed clarification of the objective before entering into individual interviews.in dietary diversity assessment since 24-h recall data collection (food listing method) used, thorough interview process recall bias is expected (reduced by probing).food security, dietary diversity, latrine availability, family resource decision making, food restriction, weak nutrition education, and malnutrition screening program, the practice of depending on a local alcoholic drink called “cheka”, drought, poor hygiene and sanitation coverage, traditional way of farming and low socio-economic status were identified factors.hence, interventions targeting maternal nutrition education, personal hygiene, and sanitation, encouraging irrigation through working with the agricultural sector to change the traditional way of farming practices and the economic status of the community are recommend
number of words= 1264
[{'rouge-1': {'f': 0.4185987770906128, 'p': 0.8219788918205804,'r': 0.28079881656804734}, 'rouge-2': {'f': 0.2880075319757659, 'p': 0.5276719576719577,'r': 0.19805329385640266}, 'rouge-l': {'f': 0.4639882442892947, 'p': 0.7131535269709544,'r': 0.3438515901060071}}]
-----------------------------------------------------------------------------------------------------------------------------------
p275:
Extractive Summary:
.according to the world bank report, academic performance of students of sub-saharan african countries is less than half of what is expected for their age [10].research evidences also show that academic performance is affected by factors such as the wealth status of the parents, the type of school, parents’ educational status, marital and occupational status of the adolescents [16–19].a single population proportion formula was used to calculate the sample size with the following assumptions; 95% confidence level, 5% margin of error, an estimated magnitude of students’ academic performance of 72.8% taken from a similar study in ethiopia [21], design effect of 2 and 10% non-response rate and the final sample size calculated is 670.the sampling interval was determined by dividing the total number of students in the respective school grade level by the allocated sample size and was found to be five.the questionnaire was pre-tested on 5% of the sample size on adolescent students from schools which were not selected for the actual data collection but no modification has been made.the data were collected by four data collectors and two supervisors after training was given for 2 days on the objective of the study, data collection procedures, anthropometric measurements, the confidentiality of the information and participant rights.of the total respondents, 50.6% were girls.the majority (81.3%) of the parents were currently married.more than one-third (34.8%) of the mothers were merchants, while 42.4% of the fathers were government employees.regarding the wealth index, 23.6%, of the study participants were from the fourth class households (table 1).ci: 4.5, 8.5) were underweight, 9.7% (95% ci: 7.6, 12.2) overweight, 4.1% (95%ci: 2.8, 5.7) obese, and 9.2% (95%ci: 7.2, 11.4) were stunted.the majority (76.4%) of the adolescents spend much their time on the internet for social media purpose and about one-fourth (24.8%) reported drinking alcohol at least once before the study.proportion and predictors of academic performance the mean academic performance of the students was 69.2 ± 11.0 sd (95%the mean score of students from separated parents decreased by 4.7 (β = − 4.7; 95% ci: − 6.7, − 2.7) as compared to students from married parents.being from the first-class wealth index decreased the mean score of students by 9.9 (β = − 9.9; 95% ci: − 12.8, − 7.0).attending private schools increased the average mark score of students by 4.2 (β = 4.2; 95% ci: 2.5, 5.9) compared to their counterparts.in this study, the mean academic score of the students was 69.2 ± 1 (95% ci: 68.3, 70.0%).this difference may be owing to the differences in the students’ assessment techniques, the curriculum and teaching-learning resources availability and accessibility.despite the agreement with these studies, the correlation coefficient in the current study is relatively low.the possible reason might be the small sample size used in the mentioned studies.in this study, the nutritional status measure (baz) is also statistically positively and significantly associated with the academic performance of the students.this result is not in line with another study conducted in north ethiopia where the study reported that there was no statistical association between baz and academic performance [30].this difference might be due to the socio-cultural difference in the study settings.this result is consistent with the results of studies conducted in addis ababa (ethiopia) and ghana [37, 38].this could be due to psycho-social and financial crises caused by separation or divorce the associated parental instability.academic performance of the students from a household of first wealth index or second wealth index class family was decreased students’ when compared to the students from the highest wealth index household families, this finding is in agreement with studies conducted in dessie (northwest ethiopia) and hawa gelan district in southwest ethiopia [24, 39].similarly, another study from goba town in ethiopia depicted that a higher wealth index is associated with better mathematics scores [31].attending private schools increased the mean mark score of students as compared to their counterparts.this finding is in line with a study finding of northwest ethiopia [40].this difference might be attributed to private schools better equipment in a library and laboratory facilities, regular and tight monitoring and evaluation of the teaching and learning process and students who attend private schools are mostly from a well to do families to provide better, adequate and timely nutrition than students of public schools.limitations the study sample consisted of adolescents students in wolaita sodo town secondary schools and therefore, the study results cannot be generalized to other schools elsewhere in ethiopia or other sub-saharan africa or other developing countries.thus, it might be difficult to extrapolate the proportion to the overall adolescent population in the country.we used crosssectional data and the estimate might be better represented if longitudinal follow-up data were used.in this study, only anthropomorphic measurements were used to determine the nutritional status and did not assess the micronutrient status and its possible association with the academic performance of study participants.in the present study, other covariates such as cigarette smoking and time devoted to physical exercise have not been assessed.wolaita sodo town health office should design interventions targeted at improving adolescents’ nutritional status.a school feeding program should be launched particularly for underweight students.schools should give tutorial classes for girl students.further studies to determine the association of nutritional status with school performance by including the micronutrient status data are recommend
number of words= 871
[{'rouge-1': {'f': 0.43844292686709674, 'p': 0.722542372881356,'r': 0.3147033898305085}, 'rouge-2': {'f': 0.24441289242373565, 'p': 0.3702832861189802,'r': 0.18240721102863203}, 'rouge-l': {'f': 0.36701708278580814, 'p': 0.525,'r': 0.2821212121212121}}]
-----------------------------------------------------------------------------------------------------------------------------------
p276:
Extractive Summary:
other functions of the lycii radicis cortex extract include inhibition of ccl4-induced hepatic damage and protection of skin from uvb radiation [13, 14].goji leaves are herbs that are traditionally used in tea and cuisine and has been recognized as a health food.however, the comprehensive profiles of biochemical compounds in goji leaves have only been identified in recent decades.goji leaves contain high amounts of specific flavonoids and phenolic acids, such as chlorogenic acid, quercetin, and rutin [16, 19].comparative studies have demonstrated the differences in compound contents between the leaves of l. babarum and l. chinense, with higher amounts of chlorogenic acid present in the leaves of l. chinense [16].goji plants are susceptible to the goji gall mite, aceria kuko [20], which is a pest that induces yellow-green, bead-like galls in the gall sector of the leaves.severe infection causes the loss of photosynthetic ability and eventually reduces fruit production, and the infected leaves are regarded as waste.pesticides are often used in controlling gall mite-induced damage; however, the application of pesticides is dependent on environmental temperatures and on the growing season in order to maximize effectiveness, yet its impact is still limited [21].pesticide residue is also one of the concerns when using chemicals on leaves.although defoliation of galled leaves is relatively effective in practice, this method is costly and time consuming [22].the infection of galls has been found to induce the biosynthesis of bioactive ingredients, such as flavonoids and phenolic acids, in both plant and gall tissues [23–25], thus, it might be a good idea to take advantage of the gall infection.an excellent example of benefiting from gall infections is the use of infected rhus chinensis mill.in biomedical applications, as it was found to have high antioxidant activities in infected leaf tissues, as well as antiviral, antibacterial, and antitumor function in the gall tissues [26, 27].therefore, the purpose of this study was to estimate the effects of gall infection on the contents of health-related compounds in the leaves of l. chinense.our data show that the contents of polyphenol and the level of chlorogenic acid and rutin were increased in the infected leaves.leaf extracts also exhibited higher antioxidant activities after infection.our results indicate that infected leaves have potential use in pharmacological applications and may possibly be consumed as health food.methods plant material and sample collection goji (l. chinense) seeds were obtained from miaoli district agricultural research and extension station (mdais) in taiwan.plant specimen of local grown l. chinense is available in herbarium of national taiwan university (tai, link: https://tai2.ntu.edu.tw/specimen/specimen.php?taiid=2065 26).dried goji berries (i.e. the fruit of goji from l. barbarum) were purchased from the local market.histology of gall tissues three different developmental stages (initiation, enlarging, and maturation) of gall tissues were collected and were fixed by formalin-acid-alcohol (faa) fixative.after dehydration and paraffin wax infiltration, samples were sectioned using a tissue dissector (leica rm2125 rts, leica, germany) and were subsequently stained with safranin o and fast green.determination of chlorophylls and carotenoid dried samples were ground using liquid nitrogen, and the pigments were extracted with 80% acetone.after centrifugation, the supernatant was used for the measurement of absorbance at 663.6 nm, 646.6 nm, and 440.5 nm.determination of antioxidant capacity the 1,1-diphenyl-2-picrylhydrazyl (dpph) radical scavenging method that was previously reported [31] was modified as follows: 20 mg of sample powder was dissolved in 1 ml of 90% methanol prior to reaction with a methanolic solution of dpph.two hundred μl of extract/dpph solution (1:3) were loaded to 96-well plate and the plate was placed in dark for 90min for reaction.the half maximal inhibitory concentration (ic50) of goji extract on dpph scavenging activity was calculated using a three parameter logistic regression model: statistical analysis statistics were assessed using the student’s t test, and significant differences were presented as *p < 0.05, **p < 0.01, ***p < 0.001.table 1 also reveals the content of rutin in galled leaves and goji berry.principal component analysis (pca) showed the relation of the amount of flavonoids (flv), polyphenols and antioxidant ability (dpph radical scavenging effect) in goji extracts (fig. 6).commercial dried goji fruit were used for comparing the constituents in the leaves and in the goji berry that people usually consume.notably, goji berry are considered to be a source of macular pigments [4], it would be worthy to test if goji leaves is able to contribute more since it contain a higher amount of carotenoid.after testing the antioxidant activity of the extracts from the goji leaves (fig. 5, table 2), our data showed that the antioxidant capacity of leaf extracts was elevated after gall formation.conclusions goji leaves are currently considered as a health food.an examination of the effect of gall infection on the lycium cultivar(s) enriching chlorogenic acid is worthy for determining the application foreground of goji leaves.therefore, the pharmaceutical value of infected goji leaves also need to be carefully evaluat
number of words= 796
[{'rouge-1': {'f': 0.380900102695359, 'p': 0.7380851063829788,'r': 0.25668252080856124}, 'rouge-2': {'f': 0.19152014280498195, 'p': 0.31358974358974356,'r': 0.13785714285714284}, 'rouge-l': {'f': 0.32772810740626623, 'p': 0.5775757575757576,'r': 0.22876777251184835}}]
-----------------------------------------------------------------------------------------------------------------------------------
p277:
Extractive Summary:
for example, vitamin d from cutaneous and dietary sources has been shown to have decreased bioavailability in obese persons and is potentially commandeered by adipose tissue [3]; thiamine metabolism is also impacted in obese persons, leading to a decrease in cellular absorption and an increase in intracellular conservation [4].further nutritional advice is also provided in the form of national ‘dietary guidelines’.few studies have examined the nutritional status of overweight and obese adults.one such study suggested vitamin d, chromium, biotin, thiamine and vitamin c levels are significantly lower in persons who are obese and that insufficiency in these micronutrients has the ability to impact glucose metabolism and cause insulin resistance [8].group allocation was randomised by the supplement supplier, who had no involvement in the trial, and double blinded to reduce bias or interference from the participants and the research assistants.participants attended a briefing session on how to consume the supplements, complete the paperwork and comply with the study protocol.blood samples were then centrifuged at 2500 rpm at 4 °c for 10 min using an eppendorf centrifuge and prepared for storage at -80 °c. micronutrients were analysed systematically, after all the participants had completed the 52-week study.elisa combines antibody binding with enzymatic detection to identify molecules of interest; the result is a colour change that is measured by spectrophotometry at a particular wavelength [10].the baseline data did not meet the assumption of normality for simple linear regressions (or one-sided pearson’s bivariate correlation), so instead a one-side non-parametric bivariate correlation (eg spearman’s) between bmi and each micronutrient was conducted.all statistical analysis was conducted using spss 23.0 (ibm® spss® statistics, new york, ny).results baseline characteristics blood samples, for the 127 overweight and obese participants, were analysed for baseline micronutrient levels.the gender of the study group was mostly female with 73 women versus 54 men.mean ± sem serum values for micronutrients were also correlated with bmi (table 4) and significant associations can be seen in fig. 1a), b), c) and d).as data analysis for vitamin b12 cannot be performed in foodworks, this micronutrient is not in table 5.the data was not normally distributed due to outliers so median values were seen as the most accurate representation and more likely to be applicable to the wider community than mean values.the baseline serum values were compared with the clinical reference intervals for nutrients in australia (aacb) [17]– shown in table 2).vitamin c showed that 96.1% of the sample were in excess of the reference range whereas 100% of subjects did not meet the clinical reference interval for vitamin a; results were considerably less that the reference range of 28–86 μg/dl with the mean sample value only 5.04 ± 0.2 μg/dl.only 28% were within the reference range for folate, with deficiency <3 μg/l, the sample mean was 2.5 ± 0.2 μg/l.sample population serum values for sodium (mean 118.8 ± 0.9 mmol/l), zinc (mean 27.9 ± 1.2 μg/ dl) and calcium (3.4 ± 0.1 mg/dl) levels were all lower than the clinical reference interval.serum magnesium levels were significantly lower than the clinical reference interval with a mean value of 0.7 ± 0.01 mg/dl compared to 1.8–2.6mg/dl.table 3) indicates that recommended nutritional intake from the diet is being met for some micronutrients, but not for others.the study participants were under the reference value for men (8.1 mg) and slightly over the reference value for women (7.9 mg).the rdi for vitamin c is 45 mg daily for 19–70 year old’s; the study participants well exceeded this rdi at 76.9 mg.for vitamin a, retinol equivalents (re) are the reference used.retinol (μg) rdi is 900 re and 700 re for beta carotene (μg).for vitamin d, both men and women were under the ai for both age groups, with dietary intake data showing only 3.3 μg for 19–50 years compared to the recommended 5 μg required daily and 2.8 μg for 51–70 years old compared to 10 μg required.for potassium, males were below the ai of 3800 mg daily with a median value of 3207.4 mg, whereas females exceeded ai of 2800 mg with 3053.1 mg.the zinc rdi for 19–70 years old was met for females but slightly under recommendations for males at 13.6mg instead of 14 mg.however the reverse was true for the 31–70 year age category, with females exceeding the rdi and males only achieving a median value of 385.2mg instead of the recommended 420mg.for table 4) correlation between bmi and serum micronutrients for participants at baseline, significant associations (spearman’s rho) were found for vitamin d (rs = − 0.152, p = 0.044), folate (rs = − 0.176, p = 0.025), potassium (rs = − 0.177, p = 0.023), and magnesium (rs = − 0.206, p = 0.010).however, the line of best fit for folate appears to show a slightly positive association with increased bmi, even though the r-value is negative (fig. 1d).the line of best fit should show a downward trend as bmi increased similar to the other micronutrients, but the slight upward trend appearance may be due to possible bmi outliers skewing the trend line making it appear slightly positive.discussion overweight and obesity is one of the predominant health issues in todays’ global society, having superseded malnutrition in recent years [18, 19].the diet of an overweight or obese individual is typically energy dense and nutrient poor, thus low micronutrient levels may result from inadequate dietary intake and/or alterations in nutrient absorption or metabolism over time [18].this study found a negative correlation (spearman’s rho rs = − 0.152, p = 0.044) between serum vitamin d status and body mass index (refer to table 4 and fig. 1a), with the majority (89%) not reaching the required levels for vitamin d which is 5 μg/day for 19–50 year old australian adults (refer to table 3).most vitamin d is obtained from sun exposure, however dietary intake of vitamin d containing foods is still important.oily fish such as salmon and mackerel, eggs, mushrooms and fortified foods are among the highest vitamin d containing foods [7, 20].for the 19–50 year age group 5 μg is the ai per day, while the mean showed only 3.6 ± 0.3 μg was obtained from dietary sources.vitamin d has also been shown to have decreased bioavailability from cutaneous and dietary sources in overweight and obese populations, as it is potentially sequestered by adipose tissue [21].an australian study by gill, et al. 2014 found a direct correlation between bmi and vitamin d status, indicating that those with a bmi >25 had lower serum vitamin d than those with a bmi < 25.due to the large sample size and the rolling roster of clinical appointments, some participants baseline values were obtained during summer, autumn and winter, which may have resulted in some variability in the findings [23].while seasonal variations are important, it could be argued behavioural changes are more significant.when using the self-reported dietary intake data and comparing the mean values to the nrvs, dietary intake of magnesium was being met.however this did not translate to serum magnesium with all participants being below the clinical reference intervals (table 2).previous studies have linked obesity, and obesity-related metabolic risk factors such as glucose intolerance, cardiovascular disease, dyslipidaemia and insulin resistance, with low serum magnesium [26].although a magnesium-regulating hormone or factor has yet to be described, the effect of vitamin d on serum magnesium concentration has been confirmed in some studies [29].a study by farhanghi, et al. 2009 showed that low baseline concentrations of serum magnesium in obese participants can induce higher renal magnesium retention following vitamin d supplementation [27].current evidence suggests that overweight and obesity alters potassium channel function [30].fruit and vegetables are a major source of dietary potassium, thus a high intake would also be beneficial to ms risk factors such as central adiposity [30].folate and folic acid is found in dark green leafy vegetables, legumes, fortified cereals and foods and has a bioavailability of 50–85% depending on the food and form consumed [32].serum folate levels as shown in table 2), indicated that 72% of participants were below the clinical reference interval and 28% were within normal reference range for serum.cytochrome p450 2e1 is a monooxygenase enzyme that can use folic acid as a substrate [33].calcium intake of the majority of participants was also below the reference value recommendation (table 3).low calcium intake is considered a risk factor for certain disorders, including osteoporosis, hypertension, cancer, insulin resistance, and the metabolic syndrome [35].interestingly, low dietary calcium intake was listed among the risk factors significantly associated with overweight and obesity in a number of published studies [36, 37] and there appears to be a direct link between high dietary calcium levels and increased faecal fat excretion.low dietary calcium can lead to an elevated cytosolic calcium and free ionic calcium in the cytosol plays a significant part in metabolic disorders related to insulin resistance and obesity [40].sodium levels within the body are maintained within a narrow range of 135 to 145 meq/l, and the mechanisms which maintain the plasma sodium concentration in a narrow range are thirst and antidiuretic hormone (arginine vasopressin) release [41].hyponatremia (low sodium) indicates hypotonicity - water excess for the amount of sodium present [41].micronutrients (and macronutrients) have been implicated as an important factor in regulating various metabolic processes and thus playing a role in the aetiology of obesity.if dietary insufficiency is ongoing then deficiency states may present, however this is potentially offset by food fortification of vitamins and minerals in a variety of processed foods and beverages.many industrialized countries have used fortification to prevent deficiencies of vitamins a and d, several b vitamins (thiamine, riboflavin and niacin), iodine and iron [43].one in every three people aged 2 years and over (37% of males and 34% of females) did not meet their requirements for magnesium, however 76% of males and 42% of females aged 2 years and over exceeded the ul for sodium [46].approximately 90% (or 88.9% of males and 93.2% of females) were within reference range values for iron, however 99.2% had less than adequate serum values for zinc.calcium values of the participants were also lower than the clinical reference values with 91.3% having a serum calcium level lower than < 4.64 mg/dl.implications of associations the exact mechanisms that explain the relationship between folate metabolism and obesity are still being established.previous studies have reported people with higher bmi have high erythrocyte folate concentrations, as well as high levels of circulating serum folate oxidation products, but can also have low fasting serum levels [47].a prolonged deficiency in folate can lead to folate deficiency anaemia (megaloblastic) and pancytopenia.neural tube defects are also a concern for the foetus of a pregnant woman with an insufficient folate intake [49].for magnesium and potassium deficiency symptoms are similar and include, fatigue, numbness, tingling, cramps, seizures, personality changes, abnormal heart rhythms, and coronary spasms can occur [52, 53].higher thyroglobulin levels suggest that the thyroid is working harder to compensate for low iodine levels and may be an indication of iodine deficiency [16].including under reporting or not enough detail recorded by participants of food diaries, consumption of multivitamins that weren’t recorded, starting medications that may interfered with the data.by comparing serum micronutrient levels against the clinical reference intervals for australia, it shows that dietary intake affects nutritional status and not just body weight, further highlighting the importance of following dietary recommendations for fruit and vegetables.several hypotheses have been proposed to explain the variability of serum micronutrient levels in this subset of the population, including: how a vitamin or mineral is absorbed, excreted, stored/distributed; whether its sequestered by fat or dispersed in tissue, metabolic processes (catabolic loses, possibly oxidative), increased physiologic requirements, and lower absolute total dietary intake [4, 9].
number of words= 1929
[{'rouge-1': {'f': 0.27820471389563184, 'p': 0.901896551724138,'r': 0.1644689182574645}, 'rouge-2': {'f': 0.180837507669257, 'p': 0.45095238095238094,'r': 0.11309500489715965}, 'rouge-l': {'f': 0.32466440400569685, 'p': 0.7948322147651006,'r': 0.20399503722084367}}]
-----------------------------------------------------------------------------------------------------------------------------------
p278:
Extractive Summary:
background in niger, with only one harvest per year and an annual lean period from june to october, food insecurity is one of the leading causes of acute malnutrition and mortality in children [1–8].while there is a growing body of evidence on the individual efficacy of these products, less emphasis has been placed on how these supplements are perceived and integrated within familial eating practices [15–20].the objective was to provide additional understanding of how nutritional supplements are managed in the home and their acceptability to guide nutritional programming in settings like maradi aimed at reducing the incidence of severe acute malnutrition (sam) [24].the three supplements were a ready to use supplementary food (rusf, supplementary plumpy®), a lipid-based nutrient supplement medium quantity (lns-mq, plumpy’doz®), and improved cornsoy blend for young children also known as super cereal plus (sc+).lns-mq is packed in individual pot of 325 g gr, recommended for 6 months old children at the dosage of 3 spoons per day per child, one pot per week.sc+ is a corn soya blend, for children from 6 to 24 months, packed in 1.5 kg bag.contrary to the two other supplements described, it is consumed like porridge by mixing an appropriate proportion of flour and clean water followed by a cooking time at simmering point from 5 to 10 min and need to be consumed immediately (table 1).perception is defined here as how participants described the products, the impact on their children as well as beliefs and opinions about the products.of the little research to date, ready to use foods appear well accepted in households [25, 26].nevertheless, there has not been a formal documentation of the utilization of sc+.qualitative information can contribute to the design of nutrition interventions that foster the adequate use of products, and promote the development of healthy eating habits [27].the qualitative study was conducted in february 2012, 6 months after the first distribution.two villages in each intervention arm were purposefully selected from the list of villages selected for the overall research based on their diversity in terms of population size, location, access and distance to healthcare facilities and marketplaces.one has the highest population size, was nearest from a health center, market and water access, easily accessible by road and the second had the lowest population size, farthest from a health center, market and water access, and not as easily accessible by road.data from those villages are reported here.available female caregivers of children enrolled in the intervention study were randomly selected from an exhaustive list of participants and invited to participate in focus group discussions (fgd) or in-depth interviews (idi) [28].only one woman per household or compound was eligible.prior to receiving the monthly distribution, caregivers took part in an educational session, focusing on the use of the nutritional supplements and essential nutrition feeding actions adapted to each age group [29].an interview guide (additional file 1) was developed using open-ended and semi-directed questions targeting the themes related to the objectives of the study: preparation and conservation, consumption and sharing practices and perceived impact of the consumption of the nutritional supplements.the recordings were directly transcribed in french by a local translator and then from french to english by another translator.the transcripts were verified by both f.b. and c.m..data focusing on supplementary food (rusf, lns-mq and sc+) were analyzed manually by two researchers.other qualitative data concerning cash transfer were analyzed separately and reported elsewhere [33].thematic content analysis, using both deductive and inductive approaches, was applied to code the transcripts [34–37].results a total of 114 caregivers participated (table 2).“giving the children biscuits protected them against certain illnesses like diarrhea, vomiting and fever”, rusf/cash.“we are very happy with the biskit because it protects our children from malnutrition”, lns-mq/cash.when referring to sc+, consistency, appearance, perceived effects on weight gain and overall nutritional status were also considered.during the interviews, some of the mothers described deviations in the use of the supplement at the household level.sc+.however, although differences in how the nutritional supplements were given to children emerged, when referring to lns-mq and rusf, most of the participants explained that they complied with the daily dose of one packet, most administered it between meals, in the morning.” however, some participants indicated that their children disliked the product, particularly the first few days after introduction.a participant explained, “after preparation, we use it for up to 30 minutes.i waited and offered him to try again and again, until he accepted it.sometimes i buy little cakes and promise to give him after he consumes the porridge”.sc+.from the participant’s perspective, nutritional supplements seem to be shared primarily with siblings in the household, but also with other family members, neighbors and friends.in some households, where men had more than one wife, it was common for them to share the products with other children and even with all members of the household.a participant explained: “honestly, all of the members of the household eat it and some spouses, when you refuse to give them any, they express their displeasure [ …] our wish is to tell you to give us a little more or to give us something else other than gari”.sc+.when referring to lns-mq a participant explained: “...lns-mq/cash.some women also indicated sharing the supplements with close family members who lived in the same compound and/or had many children to feed.feeding them with nutritional supplements, or any other food, was one way of taking care of children and to please them.a participant explained: “for me, in principle i tell myself that the complete contents of the packet must be eaten by the child each day, but if i share it with a child that was not targeted i know deep inside that the product will not work because it was not eaten by the intended child only, so that’s why i never share it with other children”.when referring to sc+ participants indicated: “it’s a fooda participant explained: “really, the” biskit”, in addition to protecting our children from illness, has become a means of independence for the mothers, because as soon as you offer them a “biskit”, they eat it right away, they drink some water and go play, thanks to the biscuit there are no more complaints from children clinging to their mothers’ skirts”.the financial impact was also described in terms of decreasing expenses for medical costs because children were sick less often.this result is important as it is a possible driver for the parents, who perceive positive effects, which may then lead them to give the supplements to the target children to improve health status, and therefore also respecting the instructions.this finding was consistent with findings from other authors who described intrahousehold sharing of rusf and lns-mq among children [9, 26, 38].previous studies and our findings emphasize the importance of considering the sociocultural context of participants for field interventions such as distribution of nutritious foods for specific target groups [41].conclusions the findings showed that the nutritional supplements were perceived to have numerous positive medical, nutritional and physical impacts on the target child.despite sensitization sessions and household support in some intervention groups, sharing nutritional supplements with household members and gifting it to people outside the household were reported and could have reduced the actual amounts of the products consumed by the target children.
number of words= 1198
[{'rouge-1': {'f': 0.31300082188087514, 'p': 0.761304347826087,'r': 0.1969968051118211}, 'rouge-2': {'f': 0.17701080315281165, 'p': 0.34074235807860265,'r': 0.11956035171862511}, 'rouge-l': {'f': 0.31185380988712813, 'p': 0.6203355704697986,'r': 0.20827993254637436}}]
-----------------------------------------------------------------------------------------------------------------------------------
p279:
Extractive Summary:
adolescence period is a critical period of physical growth and development.so far, most of the interventions have either focused on children aged 0–5 years or on pregnant or lactating women.notably, there are different studies which assessed adolescent undernutrition and associated factors in ethiopia.however, they came up with inconsistency and inconclusive findings.methods study design and search strategy a systematic review of eligible articles was conducted using preferred reporting items for systematic reviews and meta-analysis (prisma) guidelines [15].a comprehensive search of articles published in english from january 2000 through november 2017 was made from pub med, scopus, google, google scholar, cochrane library and cinahl.two of the authors made the search independently.articles retrieved from the databases were exported to endnote version x6 to facilitate the article selection process and manage citation.studies published in english and conducted at both facility and community levels were included.before including the studies in the final review, they were assessed for inclusion criteria using the title, abstract and a full review of the studies.when the prevalence or associated factors were not reported, we contacted the authors, and if they did not respond or told us that the required data were not available, we excluded the study from the review.when one population was reported in more than one publication, only the most recent one or with maximum information was included in the review to avoid sample overlapping.according to the who 2006 reference data, adolescent height-for-age, and body mass index (bmi) for age below − 2 standard deviation (sd) is stunting and underweight respectively [16].age, diet diversity score (dds), family size, household food insecurity, residence, sex and water protection were included in the analysis of associated factors for adolescent stunting.quality assessment and data extraction two reviewers independently assessed the quality of the studies by adopting the specific protocol.the criteria proposed in the newcastle-ottawa scale (nos) for nonrandomized studies were used to assess the quality of studies [17].a p-value< 0.05 was used to declare the statistical significance of publication bias.i2 statistics described the total variation across studies.i2 test statistics of < 50, 50–75% and > 75% was declared as low, moderate and high heterogeneity respectively [19].statistical methods and analysis statistical analysis was carried out using stata version 14.initially, data were entered into microsoft excel and then exported to stata version 14 for further analysis.the effect size of the meta-analysis was the prevalence of stunting, underweight and odds ratio of the associated factors.random effect model was used as a method of analysis [12].the findings of the review and meta-analysis were presented using tables, forest plots and odds ratio (or) and 95% confidence intervals (ci).results study searches and selection in the initial search, we found a total of 2100 records from different electronic search databases which include; pub med (595), google (563), google scholar (490), scopus (205), cinahl (150) and cochrane library (97).from this, 250 duplicate records were removed and 1800 records were excluded after screening by title and abstracts.of the 22 studies, 16 and 21studies were used to estimate the pooled prevalence of stunting and underweight respectively.characteristics of the studies and systematic review all the studies included in this review were crosssectional studies.another subgroup analysis showed that adolescent underweight was high in rural areas (41.34, 95%ci: 8.32, 74.37) (figs. 2, 3, 4 5 and 6).six associated factors for adolescent stunting were included in the analysis.heterogeneity wasn’t observed among studies evaluating a residence, family size≥5, unprotected water source for drinking, and food-insecure households.from these associated factors, adolescent age and sex were not statistically significant factors for adolescent stunting but residence, family size≥5, unprotected water source for drinking and food-insecure households were statistically significant factors (table 2).heterogeneity was observed among studies evaluating adolescent age, family size, food-insecure household, sex, latrine availability, diet diversity score (dds), and mother educational status.publication bias we assessed the funnel plot for asymmetry by visual inspection for stunting, underweight and the associated factors.the funnel plot appeared symmetrical and found no publication bias and egger’s test was also computed for stunting, underweight and associated factors similar to the funnel plot, it revealed evidence of no publication bias (figs. 7 and 8) (tables 2 and 3).quality of the studies included in this review was assessed using the newcastle-ottawa scale (nos) for non-randomized studies.studies with > 5 score (out of 10 scores) were classified as good quality studies and those with ≤5 were poor quality studies.in this review, the randomeffects model was used for meta-analysis, considering the likelihood of significant heterogeneity amongst studies.the prevalence of stunting in this review is within the range of the prevalence reported from latin america and caribbean countries (7–43%) [42].food insecurity is one of the underline causes for undernutrition which can result in chronic nutritional problems in adolescent and cause long term negative effects in life [49].residence (rural) and unprotected source of drinking water were other significant associated factors for adolescent stunting.the unprotected source of drinking water is a vehicle for intestinal parasites and other communicable diseases which causes poor nutritional status.improving dietary quality is important for increasing micronutrient intake [50].low who dds reflects inadequate dietary intake which can result in undernutrition [10].this review used a comprehensive search strategy and more than two reviewers were involved in each step of the review process.prisma guideline was strictly followed during the review process and in order to explore the source of heterogeneity, a subgroup analysis was performed.this review, however, has certain limitations like all the studies included were cross-sectional which could affect the temporal relationship between the assessed associated factors and outcome of interest.almost a quarter of ethiopian adolescents were affected by stunting and underweight.large family size, rural residence and unprotected source of drinking water were the associated factors for adolescent stunting.
number of words= 941
[{'rouge-1': {'f': 0.45934758421875654, 'p': 0.8356250000000001,'r': 0.3167270896273917}, 'rouge-2': {'f': 0.3123439875160302, 'p': 0.5370846394984325,'r': 0.22020161290322582}, 'rouge-l': {'f': 0.44011483127890655, 'p': 0.6992134831460675,'r': 0.32112107623318387}}]
-----------------------------------------------------------------------------------------------------------------------------------
p280:
Extractive Summary:
mh predominately occur in the elderly population with a male-to-female ratio around 1:3 [2].a small percentage of mhs close spontaneously, varying between 4.0 and 11.5% [3].if left untreated, the mh size increases over time and severely reduces the visual acuity (va) to less than 20/200 in the majority of cases [4, 5].the pathogenesis of mh formation is not yet fully understood.however, it is generally accepted that anteroposterior traction at the vitreoretinal interface is a major contributor to the development of mh [6].previous studies on the risk of bilateral mh have estimated the risk to be between 7.0 and 16.7% [1, 2, 4, 7–10].some studies have investigated changes at the vitreoretinal interface and showed that foveal or complete of mh formation [7, 11–13].patients with mh often ask for information about the risk of developing mh in their fellow eye, and selected patients with a predicted high risk may require regular follow-up examinations and early surgical intervention.the aims of the study were to determine the risk of developing bilateral mh, and to investigate oct-based vitreoretinal interface- and intraretinal abnormalities associated with mh formation.methods study design and participants this retrospective, observational study was conducted at the department of ophthalmology at stavanger university hospital in norway.stavanger university hospital is the only referral hospital for vitreoretinal surgery in rogaland county and serves a population of approximately 450,000 inhabitants.we categorised the patients into two groups: a bilateral group comprising subjects who subsequently developed mh in the fellow eye, and a unilateral group with subjects who did not develop mh in the fellow eye during follow-up.the study was approved by the regional committee for medical and health research ethics (2018/954 rec west, norway) and followed the tenets of the declaration of helsinki.written informed consent was sent to all living patients, and the opportunity to decline study participation was offered.background parameters and optical coherence tomography imaging the following patient characteristics were retrieved from the electronic medical records: sex, date of birth, duration of symptoms, laterality, va in logmar, date of surgery, and date of death if deceased.the scanning protocol used for sd-oct was a macula 3d scan, 512 × 128 (6 × 6 mm, spacing 47 μm) centred on the macula, and for the ss-oct a macula 3d scan, 512 × 256 (7 × 7 mm, spacing 23 μm) centred on the macula.two patients had been operated for a mh in their fellow eye prior to 2008.a total of 12 patients subsequently developed a mh in their fellow eye and were enrolled in the bilateral group.the male-to-female ratio in the bilateral group was 1:5 and 1:1.9 in the unilateral group (p = 0.35, fisher’s exact test).the median observational time was 54 months (range, 3–138 months).in the bilateral group, the median time interval between the diagnosis of the first and the second mh was 17 months (range, 5–83 months), and 75% of the patients developed the mh in their fellow eye within 32 months.in the period until mh development in the fellow eye, two patients in the bilateral group underwent cataract surgery.table 1 summarises the baseline demographics and oct features of the two groups.figure 3 demonstrates the development of mh in a patient with foveal pvd.the presence of vmt and erm in the fellow eye was not significantly different between the two groups.the presence of ord had a sensitivity of 41.7% (95% ci, 19.3–68.0%) and specificity of 93.4% (95% ci, 89.0–96.1%) in detecting subsequent mh formation.there were no statistically significant differences regarding the presence of intraretinal splits and foveolar detachment.among the patients with ord in the fellow eye, 27.8% (95% ci, 12.5–50.9%) subsequently developed a mh.still, we cannot fully exclude the risk that some patients have moved out of our catchment area or been referred elsewhere with a mh in the fellow eye.kay et al. reported a significantly higher frequency of mh among family members of patients with bilateral mhs, which may indicate a genetic predisposition in some individuals [20].in contrast, all five eyes with ord in the study by choi et al. developed a mh.nevertheless, many of our patients in the unilateral group had retinal abnormalities in the fellow eye.this is in accordance with the findings of chhablani et al. and kumagai et al., reporting that retinal abnormalities and vitreofoveal interface changes are more common in fellow eyes of patients with mh than in a matched healthy population [15, 25].in norway, patients need a referral from a health care professional to access specialised hospital departments, which may explain some of the delay from onset of symptoms to treatment.the present study has several limitations including its retrospective design and a relatively small sample size.a longitudinal study design with repeated oct examinations could have revealed other transient retinal abnormalities and vitreoretinal interface changes.conclusion our study provides useful information when counselling patients with mh.
number of words= 793
[{'rouge-1': {'f': 0.4129171630337367, 'p': 0.7719607843137255,'r': 0.2818343195266272}, 'rouge-2': {'f': 0.25315934592620365, 'p': 0.43220472440944885,'r': 0.1790047393364929}, 'rouge-l': {'f': 0.33337380586928134, 'p': 0.559051094890511,'r': 0.23750000000000002}}]
-----------------------------------------------------------------------------------------------------------------------------------
p281:
Extractive Summary:
since then, it has been used as a refractive surgical procedure to correct the large refractive errors associated with aphakia, high myopia, and keratoconus [1–6].among these patients, lenticular removal is required for cataract surgery or lenticular opacity.all ekps were performed between 1991 and 1992.cases were excluded if the duration of followup was less than 3 months, or if the keratometric data had not been measured.of the 16 patients, three were excluded due to lack of follow-up, and another three were excluded due to the absence of keratometric values.the ekpl was removed uneventfully.in the 1990s, before ekp and 6 months after ekp, all patients underwent an ophthalmic examination including corneal k value, refractive error measurements using an auto-kerato-refractometer (atlas; carl zeiss meditec, dublin, ca), and manual refraction.a total of 10 cases were available.four female (40%) and six male (60%) patients were analyzed.mean age at ekp was 24.2 years ±10.6 years (range, 5 years – 48 years).the mean age was 42.9 ± 12.7 years (range, 18 ~ 66 years) when the cases underwent removal procedure.the mean of spherical equivalent refraction (se) of high myopia cases was − 21.4 d ± 7.0 (range, − 12.75 – -30.75 d, n = 7) preoperatively, and − 4.64 d ± 5.26 (range, − 15.0 – + 1.25 d, n = 6) postoperatively.in the aphakia case, the preoperative manifest refraction was + 10.0–1.00 × 90, and postoperative manifest refraction was + 7.0–1.5 × 180. compared with post-ekp (mean se, − 3.2 d – ± 2.4, n = 3), excessive myopic shift was observed at pre- ekpl removal (mean se, − 20.4 d ± 6.4, n = 3).after ekpl removal, mean refractive power of the cornea (km) revealed a tendency to increase from 43.8 d ± 3.4 (range, 36.62–50.75, n = 9) at pre-removal to 46.6 d ± 6.1 (range, 33.15–57.5, n = 10) at 6 months post-removal.out of nine cases, six cases showed corneal steepening and three cases revealed corneal flattening.contrary to prediction, corneal flattening was observed in two cases (cases 8 and 10) who underwent ekp for high myopia correction.when the keratometric readings of pre-epikeratoplasty and post-lenticular removal were paired and compared within the same case, the average difference was 5.1 d ± 4.0 (n = 8).all five patients who underwent lenticular removal due to graft opacity showed reduced corneal opacity (fig. 1).topographic changes after removal of lenticule table 3 presents an analysis of the topographies of six cases’ taken at pre-removal, and at one, six and twelve months after removal.depending on the different types of refractive errors to be corrected, the lenticule is shaped as a plus lens for aphakic hyperopia or a minus lens for myopia.however, the central cornea steepened in the two high myopic cases (fig. 2b, d).center corneal irregular astigmatism (ira) within 3 mm also showed little change over time after removal, except for case 7, who developed keratoectasia (1 month vs. 6–12 months; 4.25 d ± 1.4 vs. 4.45 d ± 1.31, n = 4).the central corneal thickness decreased significantly from 707 μm ± 156.5 to 519 μm ± 93.3 after ekpl removal (n = 6).likely keratoconus, inferior corneal thinning, and protrusion were clearly visible on slit-lamp examination (fig. 4e-h).the cornea remained stable without recurrence, and the photophobia disappeared (fig. 4k).however, most studies presented that cornea was not reversible with a removal of epikeratophakia (table 4).bleckmann et al. reported that fixation of the epikeratophakia lenticules led to a 2 to 3.5 d reduction in the k value after the removal of the corneal transplants in two cases (one high myopic eye after 13 years, the other aphakia eye after 15 years), and shin yj et al. also reported that epk led to an increase in corneal refractive power in three myopia cases (table 4) [10, 16].whether this patient had innate keratoconus or if ectatic changes were complicated after lenticular removal could not be accurately discriminated because there was no initial topography of the cornea.when observing the rapid progression as the lenticule was removed, it is possible that the lenticule mechanically pressed the cornea like a hard contact lens, and the progression was slowed during that time.the trephine thickness of ekp is usually 180 μm [6].considering that post-lasik ectasia can occur in percent tissue thickness alteration ≥40% or a low residual stromal bed (≤ 300 μm), [20] thin cornea (≤ 480 μm) or high myopia that may have a weak tensile strength of the collagen fibrils would be risk factors for post-ectatic changes.thereafter, the peripheral edge of the lenticule can be easily lifted off with hard grasping of the lenticule using tooth-forceps.rk involves a vertical incision, whereas both lasik and ekp require a sloping incision.therefore, epithelial ingrowth is observed on the endothelial surface in the perforation [21].due to the limitations of the retrospective design of this study, it was difficult to collect all data, especially in the 90s.
number of words= 806
[{'rouge-1': {'f': 0.378239392516432, 'p': 0.7643231441048035,'r': 0.2512998859749145}, 'rouge-2': {'f': 0.23511077308079195, 'p': 0.42526315789473684,'r': 0.16246575342465752}, 'rouge-l': {'f': 0.3736198599686337, 'p': 0.6569565217391304,'r': 0.26103773584905665}}]
-----------------------------------------------------------------------------------------------------------------------------------
p282:
Extractive Summary:
background pathologic myopia is defined as a refractive error of − 6.0 diopters or worse spherical equivalent, accompanied by the characteristic degenerative changes in the sclera, choroid, and retinal pigment epithelium, with severe visual impairment [1, 2].the asian population displays a higher prevalence of pathologic myopia, ranging from 0.9 to 3.1%, compared to other regions of the world.pathologic myopia has been reported to be the major cause of visual impairment or low vision in 12 to 27% of the asian population [3–5].the complications associated with pathologic myopia include posterior staphyloma, myopic maculopathy, myopic choroidal neovascularization (cnv) [1].myopic cnv is one of the most common complications associated with the aforementioned condition that may cause severe visual impairment [6].currently, therapy involving the intravitreal injection of anti-vascular endothelial growth factors (vegfs), such as bevacizumab, ranibizumab and aflibercept, is widely used for the treatment of myopic cnv [10–17].recently, aflibercept, a novel recombinant fusion protein binding all isoforms of vegf, was approved for the treatment of cnv secondary to pathologic myopia, following the well-tolerated and effective results demonstrated by the myrror study [18].traditionally, the diagnosis of myopic cnv relied on fundus fluorescein angiography, indocyanine green angiography, and optical coherence tomography (oct).optical coherence tomography angiography (octa) is a recent, noninvasive method without dye injection, which provides a layered image to observe the different shapes and sizes of cnv [19–21].the ability of octa to detect the morphological features of myopic cnv have been reported by few studies [22].the patients underwent treatment during the time period from august 2015 to june 2020 in the department of ophthalmology, kaohsiung veteran general hospital, taiwan.the study program was reviewed and approved by the institutional review board of kaohsiung veterans general hospital (ksvgh21-ct1–17).the exclusion criteria included the following: prior treatments for cnv, including pdt and thermal laser photocoagulation; history of intraocular surgery, except cataract surgery; extrafoveal cnv; cnv secondary to ocular pathology other than pathologic myopia, such as age-related macular degeneration, choroiditis, angioid streaks, or trauma; and hereditary diseases in the eye under investigation or the contralateral eye [23].complete ophthalmic examinations were performed at the baseline and all subsequent visits, which included snellen bcva (converted to the logarithm of the minimum angle of resolution; logmar), slit-lamp biomicroscopy, tonometry, fundus examination, fluorescein angiography, and sd-oct (rtvue xr avanti with angiovue, optovue, inc., fremont, ca).the values pertaining to the selected cnv areas were recorded, according to the selected size of cnv.the values pertaining to the flow area of cnv were automatically measured from the flow signals detected within the selected area.intravitreal injections of aflibercept 2 mg were administered under aseptic conditions using a 30- gauge needle, 3.5mm or 4mm from the limbus.retreatment using aflibercept was performed on the basis of at least one of following observations: an increase in the central foveal thickness (cft) of more than 50 μm between successive examinations, new or persistent cystic retinal changes, subretinal fluid, and new or persistent cnv on oct or hemorrhage.continuous variables with normal distribution were described as mean and standard deviation, and continuous variables with non-normal distribution were described as median and interquartile range (iqr).an analysis of the normality of independent variables in the different groups (using the shapiro-wilk test) revealed that the results of the current study do not follow a normal distribution.paired data were analyzed using the wilcoxon signed-rank and two-way anova (friedman) tests.the significance of the differences between the values pertaining to the study groups were evaluated through further analysis.a significance level of 5% was adopted for the decision-making in statistical tests.the mean duration of follow-up was 16.6 months.the study observed that myopic cnv resolved in 10 of the 21 patients (47.6%) after one aflibercept injection.furthermore, among the 21 patients under study, nine (42.9%) received two aflibercept injections and two (9.5%) received three aflibercept injections during the follow-up (mean of two aflibercept injections per patient).in addition, among the 11 patients who required more than one injection, 10 patients received the second injection within 3 months after the primary injection and only a single patient received a third injection 6 months after the second injection (fig. 1).the greatest improvement in visual acuity within the first 3 months after the initial aflibercept injection, and the bcva remained stable 12 months after the injection.the improvement in median cft was greater in the younger group (< 50 years), compared to the older group (≥ 50 years) (− 52 μm against − 30 μm, respectively; p = 0.038) (table 3).indeed, some studies report that myopic cnv in older patients could manifest simultaneous amd and high myopia, resulting in poor natural outcomes.cheng et al. used oct b-scan and octa to perform quantitative analysis and monitor the therapeutic effects of intravitreal ranibizumab injection (0.5 mg/0.05 ml) (lucentis; genentech, inc., south san francisco, ca) in myopic cnv [19].the octa revealed significant attenuation of the capillaries and small caliber feeder vessels after intravitreal ranibizumab injection.nine in twelve patients displayed decrease in the selected cnv flow area at 3 months after aflibercept injection.the decrease of selected cnv area and flow area after aflibercept injection is consistent with the results reported by cheng et al., wherein myopic cnv was treated using intravitreal ranibizumab injection [19].octa could be considered as a useful tool that can be employed to identify the different cnv patterns and detect cnv activity, predict treatment response, and monitor the need for repeated treatments in patients with myopic cnv.conclusion in conclusion, the administration of a single injection of intravitreal aflibercept 2.0 mg at the baseline in patients with myopic cnv showed effective results.
number of words= 912
[{'rouge-1': {'f': 0.4311026328528023, 'p': 0.8280071174377224,'r': 0.29141372141372146}, 'rouge-2': {'f': 0.2374242443878252, 'p': 0.4057142857142857,'r': 0.16781477627471386}, 'rouge-l': {'f': 0.3692467101267361, 'p': 0.6977372262773722,'r': 0.2510526315789474}}]
-----------------------------------------------------------------------------------------------------------------------------------
p283:
Extractive Summary:
background pathologic myopia is defined as a refractive error of − 6.0 diopters or worse spherical equivalent, accompanied by the characteristic degenerative changes in the sclera, choroid, and retinal pigment epithelium, with severe visual impairment [1, 2].the asian population displays a higher prevalence of pathologic myopia, ranging from 0.9 to 3.1%, compared to other regions of the world.pathologic myopia has been reported to be the major cause of visual impairment or low vision in 12 to 27% of the asian population [3–5].the complications associated with pathologic myopia include posterior staphyloma, myopic maculopathy, myopic choroidal neovascularization (cnv) [1].myopic cnv is one of the most common complications associated with the aforementioned condition that may cause severe visual impairment [6].currently, therapy involving the intravitreal injection of anti-vascular endothelial growth factors (vegfs), such as bevacizumab, ranibizumab and aflibercept, is widely used for the treatment of myopic cnv [10–17].recently, aflibercept, a novel recombinant fusion protein binding all isoforms of vegf, was approved for the treatment of cnv secondary to pathologic myopia, following the well-tolerated and effective results demonstrated by the myrror study [18].traditionally, the diagnosis of myopic cnv relied on fundus fluorescein angiography, indocyanine green angiography, and optical coherence tomography (oct).optical coherence tomography angiography (octa) is a recent, noninvasive method without dye injection, which provides a layered image to observe the different shapes and sizes of cnv [19–21].the ability of octa to detect the morphological features of myopic cnv have been reported by few studies [22].the patients underwent treatment during the time period from august 2015 to june 2020 in the department of ophthalmology, kaohsiung veteran general hospital, taiwan.the study program was reviewed and approved by the institutional review board of kaohsiung veterans general hospital (ksvgh21-ct1–17).the exclusion criteria included the following: prior treatments for cnv, including pdt and thermal laser photocoagulation; history of intraocular surgery, except cataract surgery; extrafoveal cnv; cnv secondary to ocular pathology other than pathologic myopia, such as age-related macular degeneration, choroiditis, angioid streaks, or trauma; and hereditary diseases in the eye under investigation or the contralateral eye [23].complete ophthalmic examinations were performed at the baseline and all subsequent visits, which included snellen bcva (converted to the logarithm of the minimum angle of resolution; logmar), slit-lamp biomicroscopy, tonometry, fundus examination, fluorescein angiography, and sd-oct (rtvue xr avanti with angiovue, optovue, inc., fremont, ca).the values pertaining to the selected cnv areas were recorded, according to the selected size of cnv.the values pertaining to the flow area of cnv were automatically measured from the flow signals detected within the selected area.intravitreal injections of aflibercept 2 mg were administered under aseptic conditions using a 30- gauge needle, 3.5mm or 4mm from the limbus.retreatment using aflibercept was performed on the basis of at least one of following observations: an increase in the central foveal thickness (cft) of more than 50 μm between successive examinations, new or persistent cystic retinal changes, subretinal fluid, and new or persistent cnv on oct or hemorrhage.continuous variables with normal distribution were described as mean and standard deviation, and continuous variables with non-normal distribution were described as median and interquartile range (iqr).an analysis of the normality of independent variables in the different groups (using the shapiro-wilk test) revealed that the results of the current study do not follow a normal distribution.paired data were analyzed using the wilcoxon signed-rank and two-way anova (friedman) tests.the significance of the differences between the values pertaining to the study groups were evaluated through further analysis.a significance level of 5% was adopted for the decision-making in statistical tests.the mean duration of follow-up was 16.6 months.the study observed that myopic cnv resolved in 10 of the 21 patients (47.6%) after one aflibercept injection.furthermore, among the 21 patients under study, nine (42.9%) received two aflibercept injections and two (9.5%) received three aflibercept injections during the follow-up (mean of two aflibercept injections per patient).in addition, among the 11 patients who required more than one injection, 10 patients received the second injection within 3 months after the primary injection and only a single patient received a third injection 6 months after the second injection (fig. 1).the greatest improvement in visual acuity within the first 3 months after the initial aflibercept injection, and the bcva remained stable 12 months after the injection.the improvement in median cft was greater in the younger group (< 50 years), compared to the older group (≥ 50 years) (− 52 μm against − 30 μm, respectively; p = 0.038) (table 3).indeed, some studies report that myopic cnv in older patients could manifest simultaneous amd and high myopia, resulting in poor natural outcomes.cheng et al. used oct b-scan and octa to perform quantitative analysis and monitor the therapeutic effects of intravitreal ranibizumab injection (0.5 mg/0.05 ml) (lucentis; genentech, inc., south san francisco, ca) in myopic cnv [19].the octa revealed significant attenuation of the capillaries and small caliber feeder vessels after intravitreal ranibizumab injection.nine in twelve patients displayed decrease in the selected cnv flow area at 3 months after aflibercept injection.the decrease of selected cnv area and flow area after aflibercept injection is consistent with the results reported by cheng et al., wherein myopic cnv was treated using intravitreal ranibizumab injection [19].octa could be considered as a useful tool that can be employed to identify the different cnv patterns and detect cnv activity, predict treatment response, and monitor the need for repeated treatments in patients with myopic cnv.conclusion in conclusion, the administration of a single injection of intravitreal aflibercept 2.0 mg at the baseline in patients with myopic cnv showed effective results.
number of words= 912
[{'rouge-1': {'f': 0.4311026328528023, 'p': 0.8280071174377224,'r': 0.29141372141372146}, 'rouge-2': {'f': 0.2374242443878252, 'p': 0.4057142857142857,'r': 0.16781477627471386}, 'rouge-l': {'f': 0.3692467101267361, 'p': 0.6977372262773722,'r': 0.2510526315789474}}]
-----------------------------------------------------------------------------------------------------------------------------------
p284:
Extractive Summary:
summary statement pachychoroid phenotypes were identified in myopic eyes with nontractional serous foveal detachment and were divided two groups: myopic central serous chorioretinopathy and myopic pachychoroid neovasculopathy.among various complications associated with high myopia, foveal detachment is one of the clinical manifestations that inevitably lead to visual impairment.pachychoroid diseases share common characteristics such as a thick choroid, dilated outer choroidal vessel (pachyvessel), inner choroidal attenuation, and a history of central serous chorioretinopathy (csc); drusen, pigmentary abnormality, and geographic atrophy are absent [7].the pathogenesis of pachychoroid diseases is unknown, but pachychoroid features, such as choroidal congestion and hyperpermeability, are generally associated with the focal disruption of retinal pigment epithelium (rpe) and bruch’s membrane, which can lead to subretinal fluid (srf) and cnv [8].the purpose of this report was to describe myopic nontractional foveal detachment with features mainly associated with pachychoroid diseases.the eyes with history of ocular inflammation, history of vitreoretinal surgery, history of ocular trauma or glaucoma were excluded.oct imaging by raster scan was conducted for the existence of srf, intraretinal fluid, retinal pigment epithelial layer, subfoveal choroidal thickness, choroidal vessel and macular hole to exclude.in this study, we utilized oct scans to define pachyvessel as the large choroidal vessel occupying a significant portion of the choroid and manifesting as the attenuation of choriocapillaris and sattler’s layer and dilated haller’s layer beneath srf [8].the presence of choroidal vascular hyperpermeability and punctate hyperfluorescent spots was independently evaluated by two examiners (yks, khb), both of whom were masked to diagnosis and oct results.eyes with nontractional foveal detachment in high myopia were categorized into two groups according to the presence of type 1 cnv.group1 was defined as the presence of srf without evidence of cnv on icga and octa.localized serous detachments of the neurosensory retina with focal rpe detachment or diffuse rpe abnormality were found on oct, but indistinct or diffuse leakage or nonspecific finding on fa and no evidence of cnv on multimodal imaging were noted in group1.the best-corrected visual acuity was converted into logarithm of the minimal angle of resolution (logmar) units prior to the analysis.t-test and wilcoxon rank sum test were conducted after the normality test respectively.demographic and clinical characteristics of all patients were shown in table 1.the mean follow-up period was 37.3 ± 31.9 months (range, 1 to 99 months).mean logmar visual acuity at baseline and at the final visit was 0.22 ± 0.67 and 0.20 ± 0.60, respectively.three eyes had refractive surgery history, and 3 eyes had cataract surgery history at first visit.the mean age was 48.9 ± 11.3 years for myopic csc group and 51.1 ± 9.5 years for myopic pnv group.the logmar visual acuities of myopic csc and myopic pnv groups at baseline were 0.26 ± 0.76 and 0.18 ± 0.60, respectively.mean total follow-up period in myopic csc and myopic pnv groups from baseline to the final visit was 35.0 ± 35.0 and 39.9 ± 28.1 months, respectively.five out of 8 eyes in myopic csc group and all 7 eyes in myopic pnv group received treatment (intravitreal anti-vegf (vascular endothelial growth factor) injections and/or photodynamic therapy (pdt)).in the myopic pnv group, anti-vegf injection was the first-line therapy.in the myopic csc group, we applied half-fluence pdt as the first-line treatment in patients with foveal detachment accompanied by choroidal hyperpermeability on icga and who showed recent visual deterioration.in all other patients, we monitored the condition without intervention.two of three untreated eyes in myopic csc had no significant change and the last one had a spontaneous resolution of srf (tables 1, 2).in the myopic pnv group, 5 eyes (83.3%) showed choroidal vascular hyperpermeability, and 5 eyes (83.3%) showed punctate hyperfluorescent spots.in typical myopic cnv, fa reveals well-defined hyperfluorescence in early phases and leakage of fluorescein dye during the late phases, and oct typically delineates a highly reflective area above the retinal pigment epithelium, namely subretinal hyper-reflective material, with minimal srf.the relationship between dome-shaped macula and serous foveal detachment in eyes without typical myopic cnv was reported [13].the morphologic changes in choroidal vascular structure were reported according to presence of posterior staphyloma [14].however, the main focus of our report was to present serous non-tractional foveal detachment in highly myopic eyes with pachychoroid features and to classify them by presence of type 1 cnv.however, all included eyes also showed common characteristics of pachychoroid features.on the other hand, a combination of ranibizumab and pdt was superior to monotherapy for the treatment of non-myopic polypoidal choroidal vasculopathy in improving bcva and achieving complete polyp regression [23].however, further controlled study is required to establish the treatment principles in these groups of patients.if there was an incomplete resolution of srf, we tried pdt in the area of choroidal hyperpermeability and srf.according to previous studies, the primary cause of srf development was not only choroidal vascular permeability, but also mechanical and vascular damage to the choroid by excessive scleral bulging [25].we provide new data on angiographic findings on octa and present our perspectives in diagnosing and treating myopic csc and myopic pnv.in this study, we used ‘myopic csc’ and ‘myopic pnv’ that demonstrate pachychoroid phenotypes.these may result in nontractional serous foveal detachment.in general, these are responsive to anti-vegf treatment and/or pdt, but shows limited response.despite of small series, our treatment experience broadens the perspective on disease entity overlapping between high myopia and pachychoroid spectrum disea
number of words= 877
[{'rouge-1': {'f': 0.34968358192679366, 'p': 0.8293582887700535,'r': 0.22154749199573107}, 'rouge-2': {'f': 0.21678306938186556, 'p': 0.4409677419354839,'r': 0.14371794871794874}, 'rouge-l': {'f': 0.387356179662901, 'p': 0.7855963302752293,'r': 0.2570503597122302}}]
-----------------------------------------------------------------------------------------------------------------------------------
p285:
Extractive Summary:
myopia progression over time could result in high myopia, which is related to some irreversible blinding complications, such as retinal detachment, myopic macular degeneration, and glaucoma [4].as such, international myopia institute (imi) recently introduces the concept of premyopia, which is defined as se between − 0.50 d and + 0.75 d, to guide research on myopia prevention [7].several cohort studies have evaluated factors associated with myopia incidence and progression rate among children.previous studies showed that due to the influence of accommodation, noncycloplegic assessment of refractive errors in children overestimates myopia and results in a high error rate for emmetropic and hyperopic refractive errors [10, 11].however, measuring se by cycloplegic refraction in preschool children is limited by poor cooperation and long examination time.also, some studies on myopia onset have shown stronger correlations between myopia and the ratio al/corneal radius of curvature (cr) than with al or cr alone [15, 16].we hypothesized that both premyopia and myopia in preschool children could be predicted by ocular biometric parameters, such as al and al/cr.in the present study, we aimed to examine the associations between al and al/cr at baseline and future se in preschool children and identify factors for predicting premyopia and myopia.verbal consent was obtained from all children right before the examination.examinations and questionnaire a comprehensive examination of all participants was performed by a team composed of 4 junior ophthalmologists, 2 senior ophthalmologists, and 4 optometrists using similar protocols as described elsewhere [20].basic information, including name, gender, and birth date, was obtained from each kindergarten’s principal and was checked during the examination.basic examinations, including anthropometric parameters, visual acuity, anterior segment, and fundus examination, refraction without cycloplegia, stereoacuity test, ocular alignment and motility, and ocular biometric parameters, were performed in the setting of each kindergarten.children with suspected or confirmed eye problems were referred to senior ophthalmologists and underwent further examinations.the right eye was tested first.all ocular biometric parameters, including al, the greatest corneal radius of curvature (cr1), and the lowest corneal radius of curvature (cr2), were performed five times.a comprehensive questionnaire was self-administered by legal guardians at baseline, including parental myopia, outdoor time and near activities time [21, 22].in accordance with the definition in imi [7], myopia was defined as se ≤ − 0.50 d, and premyopia was defined as − 0.50 d < se ≤ + 0.75 d. cr was calculated as the average of cr1 and cr2.parental myopia represented the number of myopic parents, including zero, one, and two myopic parents.table 2 showed characteristics of children in 2 years’ follow-up.there were 0.30 d and 5.79 d shifts towards myopia with every 1 unit increase in the al and al/cr, respectively.after adjusting for other covariates, higher baseline al (hazard ratio (hr) = 2.875, for the middle quartile compared with the bottom quartile; hr = 4.916, for the top quartile compared with the bottom quartile; both p < 0.001), higher baseline al/cr (hr = 1.702, for the middle quartile compared with the bottom quartile, p = 0.022; hr = 2.979, for the top quartile compared with the bottom quartile, p < 0.001) and two myopic parents (hr = 1.756, compared with no myopic parents, p = 0.001) were statistically significantly associated with incident myopia and premyopia in future.however, baseline height (p = 0.093), weight (p = 0.652), cp (p = 0.259), and only one myopic parent (p = 0.590) had no significance.al, al/cr, and number of myopic parents each result in points from 0 to 63 (al), 0 to 100 (al/cr), and 0 to 25 (number of myopic parents), and the total points range from 0 to 160.individual risk score is obtained by summing each separate risk factor.by the sum of the risk score, we might estimate the probability of 1-year and 2-year survival, which was the probability of not developing pre- or myopia.the survival curves for incident myopia and premyopia in different al and al/cr groups are shown in fig. 2.the incidence of myopia and premyopia were highest when baseline al > 22.33mm (75th percentile) and al/cr > 2.85 (75th percentile) compared to the other two groups.in our study, the agerelated increase was statistically significant in both al and al/cr, consistent with the shenzhen crosssectional study, which showed al and al/cr increased from 3 to 6 years of age (al from 22.19 ± 0.65mm to 22.63 ± 0.63 mm, al/cr from 2.84 ± 0.06 to 2.91 ± 0.07) [18].meanwhile, our study found a flattening in cr over 2 years, consistent with previous research on the refractive development of 3- to 6-year-old children in shanghai [24].it has been reported that early achievement of emmetropia is a risk factor for subsequent progression to myopia [23].particularly, before the onset of myopia, eyes showed an accelerated axial elongation pattern and refractive error for several years.the acceleration of axial elongation is faster than the one that occurs after the onset of myopia [31, 32].these results indicated the importance of myopia prevention in the stage just before the onset of myopia.imi has made it clear that premyopia is a refractive state of an eye of ≤ + 0.75 d and > − 0.50 d in children, which could be considered as a framework for research on myopia prevention.the national eye institutefunded collaborative longitudinal evaluation of ethnicity and refractive error (cleere) study assessed the ability of 13 candidate risk factors to predict the onset of myopia and found that se was the single best predictive factor [33].moreover, it is more difficult to collect cycloplegic refraction data than collect ocular biometric parameter values in preschoolers, suggesting that ocular biometric parameter values might be a more realistic alternative data source for myopia prediction in preschoolers.al, al/cr have been found to highly correlate with noncycloplegic se in preschool children, according to crosssectional studies [16, 18].also, a 1.5-year follow-up among school-age children in china indicated that although there were confounding factors, the use of the current al/cr as a precursor indicator for myopia onset is undeniable [34].therefore, baseline al and al/cr could be used to monitor refractive development and the tendency to develop myopia.we found that higher al value (al > 21.42 mm) and higher al/cr (al/cr > 2.77) at baseline were the independent factors most strongly associated with incident myopia and premyopia.due to the lack of complete questionnaire data in baseline and middle grades, we retrospectively checked the questionnaire from graduation grades.there were no statistically significant between difference two groups of children (p = 0.586, p = 0.498).conversely, ye s et al. found that personal anthropometry values, such as height and weight, remained independently related to refraction among schoolchildren aged 6– 15 years old in tianjin, china [38].we infer that the absence of a relationship between anthropometric measures and se may be due to the difference in the study population that varies in corneal and lens powers, which compensate for al growth, although the eye grows in harmony with the body.while, some studies indicated that lens power also played an essential role in refractive development [32].children with higher risk should be provided with preventive advice and monitored closely for the onset of myopia so that anti-myopia therapies can be applied in time [39, 40].
number of words= 1181
[{'rouge-1': {'f': 0.4147781939777915, 'p': 0.7947956403269756,'r': 0.28060965954077594}, 'rouge-2': {'f': 0.23082665469429164, 'p': 0.3924043715846995,'r': 0.16350237717908084}, 'rouge-l': {'f': 0.41083304737490156, 'p': 0.6599999999999999,'r': 0.29823984526112185}}]
-----------------------------------------------------------------------------------------------------------------------------------
p286:
Extractive Summary:
over 3 million adults in the united states are visually impaired or blind, and up to 80 million have eye diseases that may ultimately lead to blindness [1].we used the national inpatient sample (nis) database for the year 2017, and hypothesized that hospitalized patients with svi/b would have higher in-hospital mortality, less likely to be discharged home following hospitalization, have longer hospital stays, and greater hospital charges compared to those without svi/b.in fact, research has shown positive associations between obesity and development of debilitating eye conditions such as cataracts, retinal vein occlusion, and age-related macular degeneration [10].study population, patient and hospital characteristics, and o utcomes all patients ≥ 18 years of age were included in the sample.supplementary table a with icd-10 codes provides specific descriptions of categories for each level of svi/b.severe vision impairment has been defined as individuals with visual acuity worse than 6/60, and blindness as those with visual acuity worse than 3/60 [12].for adults, obesity is defined as having a body mass index (bmi) of 30 or greater [13].the primary clinical outcome was mortality during hospitalization; secondary outcomes were total hospital charges which represent the amount a hospital bills for each individual case [16], length of stay (los), and disposition after hospitalization.this is most often home, but not infrequently can be elsewhere including venues such as other hospitals, inpatient hospice, inpatient rehabilitation facilities, and nursing homes [17].statistical analyses comparisons were examined between patients with and without svi/b using pearson’s χ2 tests and one-way analysis of variance to test categorical and continuous variables.analyses were also carried out within the svi/ b patient cohort assessing those with and without obesity.the primary and secondary outcomes were adjusted for all of the patient demographics and hospital characteristics shown in table 1, as well as the cci and select specific comorbidities described in table 2.binary outcomes under logistic regression analyses (in-hospital mortality and discharge disposition) were studied.demographic data is shown in table 1 and compares patients with and without svi/b.patients with svi/b were older (mean age ± sem: 66.4 ± 0.24 vs. 57.9 ± 0.09 years, p < 0.01), less likely to be female (50 % vs. 57.7 %, p < 0.01), and a higher proportion were insured by medicare (75.7 % vs. 49.2 %, p < 0.01).patient clinical and resource utilization outcomes table 3 shows adult patients with svi/b had higher rates of mortality compared to those without svi/b (3.9 % vs. 2.2 %; p < 0.01).patients with svi/ b had a lower odds to be discharged to home after hospital discharge (adjusted odds ratio {aor} =0.54, [confidence interval (ci) 0.51–0.58]; p < 0.01).total hospital charges were not significantly different (adjusted mean difference {amd} = $247 ci [-$2,474-2,929]; p = 0.85) between groups, but los was longer (amd = 0.5 days ci [0.3–0.7]; p < 0.01) for those with svi/b.table 4 shows that obese status was not associated with an altered los, odds of mortality, or likelihood of being discharged to home following the hospitalization.while crewe or choi investigated hospitalized patients with blindness, neither study included hospitalized patients with severe vision impairment as was done in our study.the study found that patients with severe vision loss had longer los, more readmissions, and higher hospital costs compared to patients without vision loss.though our study also found that patients with svi/b had longer los, there were not significantly higher hospital charges compared to those without this disability.lack of variance in charges accrued over the protracted time span, and this may be linked to taheri’s observations that los attributable to the last portion of the hospitalization does not significantly contribute to hospital costs [23].though we cannot be certain why svi/b patients were less often discharged home after hospital discharge, it is not unreasonable to speculate that difficulty complying with post-discharge plans and therapies, either real or imagined by the inpatient care team, may have contributed to the decision.while homecare services can be excellent, patients with vision impairments or other disabilities may need more support after discharge necessitating some time in subacute facilities before transitioning back to their homes.however, in contrast to zizza, our study did not find longer lengths of stay in patients with obesity compared to those without obesity [29].the prevalence of obesity may be higher among visually impaired people compared to those without vision impairment, and populations with other disabilities [30, 31].there are ongoing efforts trying to routinize nutritional counseling for all obese patients while they are hospitalized [33].it is possible that under-coding for svi/b and obesity may have occurred.hospital-based healthcare providers should recognize this vulnerability and consider how to optimally care for and serve this group of patient
number of words= 767
[{'rouge-1': {'f': 0.5591102522870137, 'p': 0.8278947368421052,'r': 0.422078239608802}, 'rouge-2': {'f': 0.39728879873953776, 'p': 0.5739577836411609,'r': 0.30378212974296204}, 'rouge-l': {'f': 0.532958259911464, 'p': 0.7301941747572815,'r': 0.4196143958868895}}]
-----------------------------------------------------------------------------------------------------------------------------------
p287:
Extractive Summary:
damage to the outer retinal structure or development to foveal detachment and a full-thickness macular hole in the advanced stage can cause visual impairment in mtm, and surgical intervention is recommended to promote anatomical reattachment of the retina and visual recovery [2–5].retinoschisis can occur in the fovea and extrafovea and in different locations of the intraretinal neural layers due to different dominant pathological factors.recently, several studies have indicated that the progression rate of mtm varies, that paravascular abnormities and paravascular inner retinoschisis may be associated with the pathogenesis of foveoschisis, and that the severity of retinoschisis in mtm can affect its progression regardless of whether cataract surgery is performed [12–17].however, these studies have focused little on identifying the evolution of retinoschisis prominent in different locations caused by different initiation factors, on the role of inner retinoschisis at paravascular arcades and ilm detachment in the progression of mtm, or on the comparison of the baseline factors affecting visual acuity at follow-up in large series.the eyes were classified into 5 groups based on the size and the location of the outer retinoschisis as proposed by shimada et al. [15].the baseline characteristics of all patients are listed in table 1.changes in morphological characteristics by oct there were 5, 23, 8, 42 and 35 eyes in stages s0, s1, s2, s3, and s4 of mtm, respectively, at the last visit.the progression rate in eyes with outer schisis most prominently located in the fovea or staphyloma (51.6 %) was significantly higher than that in eyes with outer schisis located in the vascular arcades (20.9 %) (p = 0.0011).changes in bcva and predictive factors for visual acuity logmar bcva at the last follow-up was worse than that at baseline in all patients (p = 0.0372), in patients who were older than 65 years, in patients without dsm, and in patients with defects in the ez (p = 0.0416, p = 0.0494, and p = 0.0130, respectively) (table 3).in the multiple linear regression analysis, logmar bcva (p < 0.0001) and a defect in the ez at baseline (p < 0.0001) were significantly correlated with logmar bcva at the last follow-up (table 4).discussion our study found that 49/113 eyes (43.4 %) with mtm progressed over the 2-year follow-up period, and eyes with retinoschisis prominently located in the fovea or posterior staphyloma were prone to progression.the different progression rates may be related to the difference in age, the status of vitreomacular traction, the involved location and area of schisis or the follow-up period in these studies.shinohara et al. [20] suggested that posterior staphyloma may act as the main cause of retinoschisis located within the area of the posterior staphyloma by ultrawide-field swept-source oct, and outer and inner retinoschisis located in vascular arcades may be caused mainly by vitreous adhesion in the blood vessels of the retina and the tractional force of the retinal arterioles [8, 21].in our series, almost all cases of inner schisis were confined to the paravascular area adjacent to the superior or inferior temporal vascular arcades, which may result from tangential traction of the retinal arterioles.additionally, we found that the stage of schisis had no significant influence on the risk of progression.shimada et al. [15] and cheng et al. [16] suggested that the status of s4 eyes was unstable and had a high risk for deterioration.cai et al. [17] also believed that as long as the schisis involved the fovea, regardless of whether it extended to the entire macula, the possibility of progression was relatively high.in foveoschisis, müller cells and astrocytes proliferate to produce tangential stress to separate the intraretinal tissues [10].damage to ez is not uncommon in patients with retinoschisis.sayanagi et al. [24, 25] reported that the incidence of ez defects in foveoschisis is between 29 and 38 %.cheng et al. [16] reported that 6/14 (42.9 %) patients with retinoschisis who had ≥ 2 lines of vision loss in their natural courses had ez disruption at the first visit.we speculate that some elderly patients had a long course of schisis, and the baseline ez was not intact or even absent, the photoreceptors were damaged severely over time, which subsequently caused poor vision.second, the exertion of centrifugal vertical and tangential forces may cause an increase in the height and expansion of the extent of retinoschisis, respectively.in future studies, if we can increase the sample size, quantify the extent of the schisis, supplement the criteria for the progression of mtm, and further explore the equivalence of the impact of the expansion of the extent and the increase in height of the schisis on the progression of mtm, it may be of some significance to investigate its pathogenesis.conclusions in summary, mtm had a high progression rate during the follow-up.mtm progression was related to the location of retinoschisis.vitreomacular traction may play an important role in the natural course of mtm.
number of words= 801
[{'rouge-1': {'f': 0.22136378736548512, 'p': 0.41343434343434343,'r': 0.15114558472553702}, 'rouge-2': {'f': 0.08994099423331749, 'p': 0.10553299492385787,'r': 0.07836320191158902}, 'rouge-l': {'f': 0.22071915434380776, 'p': 0.3097260273972603,'r': 0.17144927536231885}}]
-----------------------------------------------------------------------------------------------------------------------------------
p288:
Extractive Summary:
furthermore, other causes of painful ophthalmoplegia including tumors, vasculitis, basal meningitis, sarcoidosis, migraine, cranial herpes zoster, giant cell arteritis, myositis, pseudotumor, and thyroid ophthalmopathy were excluded.case notes were reviewed to obtain data on patient demographics (age, sex), previous medical history (hypertension, diabetes mellitus, or cancer), symptoms (recorded as orbital pain, diplopia, ptosis, visual loss, and facial numbness), the involved cranial nerves, laboratory results, neuroimaging, the symptom-resolution interval, and treatment.at the first visit, the patients underwent ophthalmologic assessments including slit-lamp examination, intraocular pressure measurements, ocular motor limitation assessment, and fundus photography.in addition, the patients were examined the hematological tests and cerebrospinal studies.results a total of eleven patients (night males and two females) were included in this study and the mean follow-up time was 201.27 ± 31.36 days after the initial visit.all eleven patients had orbital or periorbital pain as the first symptom followed by diplopia or ptosis.sixth cn palsy was most common (eight cases, 72.73 %), followed by fourth and sixth cn palsy (five cases, 45.45 %, respectively), optic nerve (on) involvement (two cases, 18.18 %), and trigeminal nerve and facial nerve palsy (one case, 9.09 %, respectively).the mean duration of recovery was 26.91 ± 24.35 days (range, 8–73 days) (table 2).six patients received oral mpd at 50 mg/day and five patients received iv mpd at 1 g/day.the laboratory test results showed an elevated erythrocyte sedimentation rate (esr) in three patients and no patients had elevated c-reactive protein (crp) values.three patients with elevated esr had a paralysis of all third, fourth and sixth cns.in two patients with optic neuritis as on involvement, one had fourth cn palsy at the first visit with a history of facial nerve palsy before four months.on swelling was observed three days later, and iv mpd at 1 g/day was administered.after five weeks, decreased visual acuity was observed and iv mpd at 1 g/day was injected.the first patient recovered completely after treatment, but the second patient did not recover from visual impairment.of all the patients, two patients relapsed, and one patient had recurrence of visual impairment at 5 weeks interval.the other patient had a history of treatment for ths five years prior according to the medical record and was included as a relapsed patient.discussion this retrospective study of eleven cases of ths based on ichd-3 beta criteria showed that the clinical manifestations were variable.the sixth cn (72.73 %) was most commonly involved, followed by the third (45.45 %), and fourth cn (45.45 %).previous studies reported a high rate of third cn (78–91 %) involvement [6, 7], however, curone et al. reported a low rate of third cn (39 %) involvement [8].as such, cn involvement has been variably reported in each study.the difference in treatment dose was determined in consideration of the patient’s age, the severity of their symptoms, and underlying diseases.after the initiation of mpd treatment, pain resolved within two days in all patients, and ocular motor limitation gradually improved.in the patients in this study, the recovery period varied from one to eight weeks, similar to the previous report.an accurate diagnosis is important for quick and appropriate treatment, but many diseases must be considered in the differential diagnosis of painful ophthalmoplegia.ths patients have a good prognosis but recurrences occur in about 21–50 % of the cases over an interval of months to years [3, 7, 10].however, no patients with non-elevated esr had paralysis of all third, fourth, and sixth cns.
number of words= 563
[{'rouge-1': {'f': 0.5105211109552532, 'p': 0.8231380753138076,'r': 0.37}, 'rouge-2': {'f': 0.3290993384712057, 'p': 0.5069747899159665,'r': 0.24362270450751253}, 'rouge-l': {'f': 0.490044159457988, 'p': 0.6950000000000001,'r': 0.37844155844155847}}]
-----------------------------------------------------------------------------------------------------------------------------------
p289:
Extractive Summary:
if the retina is detached, the retina is not supplied with nutrients and the function of photoreceptors gradually decreases [6].te risk factors for low functional and anatomic outcomes after primary rd surgery are as follows: more than 6days of visual loss, macular involvement and the size of detachment area, and independency from surgical procedures (buckling or vitrectomy) chosen to repair the detachment [4].it is known that usage of steroids may infuence the infammatory and proliferative components of proliferative vitreoretinopathy (pvr), by reducing the breakdown of the blood-retinal barrier [8–10].te most widely accepted theory is that frequent rubbing of the eye in ad patients might lead to traction and retinal tear of the anterior vitreous, resulting in rd.one study has reported that the incidence of rd is about 8 times higher in people with frequent eye rubbing [12].moreover, it has been noted that rd associated with ad is more frequent in bilateral involvement than rd associated with congenital anomaly or infammation, or high myopia.atopic rd is also associated with a higher risk of surgical failure due to pvr [13].pvr is believed to be the leading cause of failure of rd surgery with accounts for 75% of retinal redetachment surgeries [14]..althoughthe risk of rd in ad patients is known to be high, there have been few studies on clinical characteristics of atopic rd.especially, the risk of developing rd after cataract surgery in ad has not been reported yet.terefore, the objective of this study was to investigate the frequency and clinical characteristics of rd in ad patients in a single institution after a relatively long follow-up period.methods medical records of 2258 patients who underwent rd surgery in seoul st.mary’s hospital (seoul, republic of korea) between 2008 and 2018 were retrospectively reviewed.a total of 61 (2.7%) patients had ad.tey were assigned to the experimental group.patients with rd due to trauma, exudate, or macular hole were excluded from this study.ad was diagnosed by a dermatologist based on the ‘diagnostic criteria for korean atopic dermatitis’.all patients analyzed in this study underwent a complete preoperative evaluation, which included a comprehensive history taking, best-corrected visual acuity (bcva) measurement, slit-lamp microbioscopy, fundus examination with a contact wide angle viewing lens (superquad 160, volk) that could evaluate range, type, and location of retinal break, optical coherence tomography to confrm macular involvement, and presence of proliferative vitreoretinopathy (pvr) (those classifed as grade 3 or above according to the classic classifcation of the retina society) [13].optical coherence tomography (oct) was done using a swept-source oct device (dri triton, topcon, tokyo, japan).break types had six categories; retinal dialysis, horseshoe tear, retinal tear (as tear except horseshoe tear), hole with lattice, hole without lattice, and unknown (if the type of break was unclear).te location of the break had fve categories; superotemporal, inferotemporal, superonasal, inferonasal, and unknown.bcva and refraction (rk-f1®, canon, tokyo, japan) were compared before surgery refraction was excluded if it occurred in a pseudophakic state.in eyes with a history of cataract surgery, the mean period between the cataract surgery and the diagnosis of rd was recorded.te range of the area with rd involvement was defned as the number of quadrants containing rd.in all patients, the surgeon decided the surgical procedure by considering fundus and lens status, age, location of tear, and pvr.te surgery was performed using the procedure of silicone sponge (506-silicone; labtician, oakville, on, canada) buckling, 23 or 25 gauge pars plana vitrectomy (accurus surgical system; alcon laboratories, tx, usa), or both.additional sub-retinal drainage, air-gas exchange, silicone oil implantation, and laser photocoagulation were performed if necessary.to evaluate anatomical and functional outcomes, postoperative bcva and recurrence rate of rd were investigated at 6 months and the last visit after surgery.in subjects with rd for both eyes, each eye was recorded separately.tose two subjects were excluded from analysis.statistical analysis logarithm of the minimum angle of resolution (log mar) was used for va analysis.va values for counting fngers, hand movements, and light perception were then assigned ratios of 0.01, 0.001, 0.0005, respectively.a mann-whitney u test was used to compare demographics, surgical prognosis and refractive status between the two groups.chi-square test was used to compare initial surgery method and demographics.all statistical analyses were performed using spss statistical software for windows, version 25.0 (spss, chicago, il, usa).demographic characteristics of study participants are summarized in table 1. of these 61 patients with ad, 13 had bilateral rd.results are shown in (table 3).in this study, ffteen (21%) eyes of atopic patients received cataract surgery and rd surgery at the same time.postoperative log mar va was 0.80 for the atopic group, which was signifcantly (p value=0.05) poorer than that (0.34) for control group.in the control group, breaks were mainly in the superotemporal area (50%).te type of retinal breaks was also analyzed (table  7).(p value 0.002, table 8).discussion characteristics of rd associated with atopic dermatitis were analyzed in this study.although some studies have already reported the characteristics of atopic rd, this is the frst report that compares characteristics of atopic rd with those of non-atopic rd.in korea, the prevalence of atopic dermatitis is 2.2% in the total population, 6.9% in those under 18years of age, 0.9% of those over 18years of age [15].and in both groups the number of male patients was 1.7 times more than that of female patients.likewise, a previous study has reported bimodal distribution of rd [16].bilateral rd accounts for 5-10% of total rd in a previous study [2].retinal dialysis was found in 16% of the atopy group, which was signifcantly higher than that (2%) of the control group.
number of words= 911
[{'rouge-1': {'f': 0.3846835103964233, 'p': 0.7715503875968992,'r': 0.25621399176954734}, 'rouge-2': {'f': 0.19402821133636902, 'p': 0.32680933852140076,'r': 0.13797116374871266}, 'rouge-l': {'f': 0.34358482564118875, 'p': 0.6192957746478873,'r': 0.237741935483871}}]
-----------------------------------------------------------------------------------------------------------------------------------
p290:
Extractive Summary:
disabled individuals appear to have poorer oral health than their non-disabled counterparts [4].a systematic review on the oral health status reported that children and adolescents with intellectual disabilities have poorer oral health (higher level of dental plaque, worse gingival status, and fewer decayed and filled permanent teeth) compared to their counterparts [5].according to the global burden of disease, untreated dental caries in permanent teeth is the most prevalent oral health problem [6], and with over 530 million children suffering from primary tooth caries [3].people with a disability may have worse oral health than those without disabilities; this might not only cause physical problems, but it can also have a far-reaching impact since poor oral health can hurt self-esteem, quality of life, and general health [10].moreover, oral diseases and conditions have serious health and economic burden, particularly among school-age children and adolescents [11].healthcare-seeking behavior is, any action taken by individuals who believe that they have a health problem or believe that they are ill [12].it comprises activities carried out to maintain good health, to prevent ill health, as well as any departure from a good state of health [13].oral healthcare-seeking seems to be inextricably linked to the demand for oral healthcare services[14].thus, the demand for oral healthcare services is often associated with an individual’s choice about which service to access and when and where to access healthcare services [15].hence, the purpose of this study was to assess the dental health problems and treatmentseeking behavior of special needs school students in in amhara regional state, ethiopia.methods study area and period a cross-sectional study was conducted from november 2020 to april 2021 in eight special needs schools in the amhara regional state, ethiopia: gondar, dessie, debre markos, and bahir dar.in the study area, 696 disabled students are attending special needs schools (gondar = 170, dessie = 179, bahir dar = 237, and debre markos = 110).population the study participants were disabled students attending special needs education special in amhara region, ethiopia.sample size and sampling procedure the sample size was calculated using a single population proportion formula; considering 50% (since no previous study found in ethiopia) proportion of students who have a dental health problem, d (the permissible margin of error 5%), zα/2 (the value of the standard normal curve score corresponding to the given confidence interval = 1.96) corresponding to 95% confidence level, and 15% non-response rate.self‑reported oral health problems and dental care‑seeking behavior of the total study participant, 204 (46.1%, 95% ci: 41.4%, 50.7%) reported oral health problems.more than half (53.7%) of the participants followed the dental care treatment courses until recovery.of them, place of residence, grade level, religious affiliation, years lived with disability, and knowledge of dental health-related risk behaviors were statistically significant factors associated with dental health problems.students affiliated with the islamic religion were 2.38 times more likely to report dental health problems compared to those affiliated with orthodox christians (aor = 2.38, 95% ci: (1.07, 5.32)).students who knew at least one dental healthrelated risky behavior were 2.31 more likely to report dental health problems compared to their counterparts (aor = 2.31, 95% ci: (1.40, 3.80)).factors associated with dental treatment‑seeking behavior the multivariable analysis showed that living in dessie town, being hearing impaired, and having prior information about dental health problems were statistically significant factors associated with dental treatment-seeking behavior.discussion this study was aimed to assess the dental health problems and treatment-seeking behavior of special needs school students in amhara regional state, ethiopia.due to this, students with disabilities depend on others to achieve and maintain good oral health [21].this reflects that ongoing coaching and reinforcement from caregivers is critical in improving special needs students’ oral care practices and reducing the occurrence of oral health problems [22].in this study living in bahir dar and debre markos cities, and increased years lived with disability were associated with lower dental health problems.this means individuals who know dental health-related risky behaviors may not adhere to them appropriately.this may be due to knowledge is neither sufficient nor necessary to trigger a behavioral change [23].meeting the oral health treatment requirements for children with intellectual, emotional, or physical disabilities can be a difficult task for their caregivers and health professionals [24].children with special needs, oral health needs are competing with already burdensome chronic health conditions [25].oral health treatment-seeking and decision about what type of treatment to receive could be said to depend on the recommendations of family and friends and the cost of services [16].similarly, a study on oral health care services utilization among children in lagos, nigeria, found that children with disabilities did not adequately use dental facilities [25].children’s treatment-seeking behavior was attributed to family’s low commitment to their dental care.a national survey of children with special health care needs in the usa from 2005 to 2011 found that children with special health care needs are experiencing greater unmet dental needs and are receiving less help coordinating care services [26].the analysis of factors associated with dental treatment- seeking behavior revealed that children living in dessie town, hearing impaired, and who had prior information about dental health problems had higher dental treatment-seeking behavior.a cross-sectional study on the oral health-seeking behavior of different population groups in nigeria found that geographic location and socio-economic status group have a negative regression coefficient to the demand for treatment in the dental clinics.specifically, the most frequently reported barriers to use dental care among disabled peoples were the cost of treatment, lack of preparation for dental care of the disabled persons, the inadequacy of dental facilities for the disabled, and lack of adaptation of the access routes to the health care facilities and dental offices [27].this is the first study on the dental problem and treatment- seeking practice of special needs school students in ethiopia.this means we have used language translators for hearing-impaired participants.therefore, schools and centers for special needs students should develop and implement oral hygiene programs focusing on screening, prevention, and treatment of oral health problems to reduce the impact of dental diseases.policymakers, health professionals, and other concerned bodies should emphasize oral health care as a major component of the overall wellbeing of children with special needs.
number of words= 1016
[{'rouge-1': {'f': 0.4221877829642623, 'p': 0.8249668874172185,'r': 0.28368322399250234}, 'rouge-2': {'f': 0.25978782822288654, 'p': 0.4620265780730897,'r': 0.1806941838649156}, 'rouge-l': {'f': 0.40094933856663206, 'p': 0.6773619631901842,'r': 0.28475054229934926}}]
-----------------------------------------------------------------------------------------------------------------------------------
p291:
Extractive Summary:
background implant treatment is becoming an increasingly popular choice for patients.a study showed that the implant survival rate of implant supported fixed prostheses (single crowns) was satisfactory [1].as shown by a systematic review, 12.7% of implant supported fixed prostheses are affected by loosening after an average of 5 years [4].accordingly, pure (screwless) implant systems, such as conical connection systems that are fixed by only friction, have been developed.the implant-abutment interface of the conical connection system is mostly morse taper connected with cold-welding [8, 9], thus eliminating the prosthetic complications associated with screws [10].the implant-abutment connections were usually less than 1.5° morse tapers with an internal cone [8].the abutment is fixed only by means of friction.moreover, cold-welding provides a well-closed abutment-implant interface, which is conducive to plaque control and may reduce the incidence of biological complications [11].compared to screw-based systems, locking-taper connections are more stable and can better resist lateral and axial forces [8, 12].many studies and systematic reviews [13, 14] have assessed the complication rates of implant-supported fixed prostheses.however, most of these earlier studies focused on butt-joint screw- type implant-abutment connection systems, while relatively few have examined conical connection systems [15, 16].the present study was a long-term retrospective study that aimed to assess the correlations between the cumulative prosthetic complication-free rate of fixed prostheses supported by locking-taper implants and various relevant factors, such as patient age, patient sex, prosthesis position, jaw position, restoration type (single crown (sc) or fixed partial prosthesis crown (fdp)), and the prosthetic materials that were used.the marginal bone loss (mbl) and implant survival rate were also investigated.methods study design and sample the current work was a long-term retrospective clinical study with an average follow-up of 5 years.this article has been reported using a statement from strobe (strengthening the reporting of observational studies in epidemiology) as closely as possible.the exclusion criteria were as follows: 1.6. patients for whom a sinus floor elevation procedure was used.specific definitions of the complications are presented in table 1.both iacs and gpcs are most commonly used at our hospital, while non-precious metal porcelain and zirconia crowns are seldomly used.the implant-abutment connections were 1.5° locking tapers with an internal cone.preparation of the implant sites was performed according to the standard procedures for the bicon system.bone healing took over 6 months, and the prosthetic procedure was initiated thereafter.all the impressions were taken using polyether silicone rubber (type of rubber type, 3 m, st.paul, mn, us).iacs or gpcs were used, and the prosthesis type was an sc (single crown) or an fdp (fixed dental prosthesis).integrated abutment crowns (iacs) are a typical and fully retrievable type of restoration that is supported by locking-taper implants.iacs are metal-resin crowns made from a titanium base and a composite resin veneer of ceramage (type of veneer resin, shofu, jp).ceramage(type of veneer resin, shofu, jp) is a fiber-reinforced composite resin with filler particles that has a higher strength than conventional resin [17].the gpcs were cemented with glass ionomer cement.once the crown was in place, its occlusion was thoroughly checked and adjusted, and then the crown was glazed (gpcs) or finely polished (iacs).these complications were recorded as endpoint events.implant failures in this study were defined as the failures occurring after prosthetic loading, and other failures occuring before prosthetic loading were not included.implant failures in this study included peri-implantitis, progressive bone loss and implant body fracture.the distance from the most coronal bone to the margin of the implant neck was measured.the process was repeated by the same observer for three times, and the final result was the average of the three measurements.data management and statistics a database was prepared using microsoft excel.the populations of the different variable groups were assessed, and the data were then transferred to graphpad prism (version 7.0) for statistical analysis.kaplan–meier estimators were used to obtain survival curves (restoration based), from which the cumulative complication-free rates were calculated (95% ci).log-rank and gehan- breslow-wilcoxon tests were used to assess whether there were significant differences in the survival curves between the different groups (p < 0.05 represented a significant difference).the average age of the patients was 43.07 years (restoration-based).however, prosthetic complications, such as abutment loosening and abutment fracture, were presented by gpcs.similarly, it has been previously reported that prosthetic complications occur more frequently in the molar region because of the mechanical force conditions in that area [34].biological complications and the survival rate require further prospective and longterm follow-up studies.significant differences were observed between the different prosthetic materials, placement positions and sexes.
number of words= 739
[{'rouge-1': {'f': 0.49536417021851353, 'p': 0.7311570247933885,'r': 0.37456852791878176}, 'rouge-2': {'f': 0.23356937666282657, 'p': 0.3186187845303867,'r': 0.18435832274459973}, 'rouge-l': {'f': 0.45272745220434985, 'p': 0.6421649484536083,'r': 0.3495969773299748}}]
-----------------------------------------------------------------------------------------------------------------------------------
p292:
Extractive Summary:
importantly, the prevalence of untreated dental caries has increased [1, 2].the plausibility of systemic consequences from untreated dental caries and mechanistic role of the associated oral microbial-inflammatory process in these associations requires further inquiry through human and animal studies.such factors would include diseases [11] and medications [12] that result in reduced saliva production, adhesin expression in s.mutans for collagen binding [13–15], dysbiosis of the oral microbiota [16, 17], genetic factors that predispose to dental caries and share common mechanistic underpinnings with systemic diseases [18].our current understanding of metabolic disease-dental caries associations and use of animal models [20–27] can serve to expand understanding of associations between dental caries and other systemic diseases.this scoping review compiled and evaluated recent evidence from animal and clinical human studies that assessed associations between dental caries and systemic diseases and potential mechanisms for such associations.search strategy the following search strategy in pubmed utilized both keyword terms in the title and abstract fields as well as medical subject headings (mesh) to identify possible qualifying articles: (((((“dental caries“[mesh terms]) or caries[title/abstract]) or carious lesions[title/ abstract]) or carious lesion[title/abstract])) and (((((((((((((((((((((((((((((((((((((((“neoplasms“[m esh terms]) or cancer[title/abstract]) or metabolic syndrome[title/abstract]) or “metabolic syndrome“[mesh terms]) or obesity[title/abstract]) or “obesity“[mesh terms]) or cardiovascular diseases[title/abstract]) or cardiovascular disease[title/ abstract]) or “cardiovascular diseases“[mesh terms]) or myocardial infarction[title/abstract]) or heart disease[title/abstract]) or heart diseases[title/ abstract]) or diabetes[title/abstract]) or “diabetes mellitus“[mesh terms]) or atherosclerosis[title/ abstract]) or cerebrovascular disease[title/abstract]) or cerebrovascular diseases[title/abstract]) or “cerebrovascular disorders“[mesh terms]) or asthma[title/abstract]) or “asthma“[mesh terms]) or pneumonia[title/abstract]) or “pneumonia“[mesh terms]) or chronic obstructive pulmonary disease[title/abstract]) or “pulmonary disease, chronic obstructive“[mesh terms]) or allergies[title/abstract]) or “hypersensitivity“[mesh terms]) or “respiratory tract diseases“[mesh terms]) or arthritis[title/ abstract]) or “arthritis, rheumatoid“[mesh terms]) or alzheimer disease[title/abstract]) or “alzheimer’s disease” [mesh terms]) or dementia[title/abstract]) or “dementia“[mesh terms]) or “inflammatory bowel diseases“[mesh terms]) or crohn disease[title/ abstract]) or “osteoporosis“[mesh terms]) or osteoporosis[title/abstract]) or “joint diseases“[mesh terms]) or systemic[title/abstract]).this search was translated and updated for embase and cochrane central register of controlled trials accordingly.data filtering the results obtained using search strategy described above were deduplicated and further managed in an online workflow management system for scientific reviews (https:// www.covid ence.org/).after removal of duplicates, titles were examined by one author (as) and articles unrelated to dental caries were removed.for retained articles, after title-based filtering, their eligibility was assessed by abstract-based filtering by two authors (as and fas).articles that were published in potentially predatory journals were also removed if the corresponding journal was listed in beall’s list (https:// beall slist.net/) and was not listed in the directory of open access journals (https:// doaj.org/).in addition, infective endocarditis was removed from our search criteria as it has been extensively reviewed previously.clinical human and animal studies were included where associations between dental caries and a systemic disease were explored or a potential mechanism was elucidated and they did not meet any of the aforementioned exclusion criteria.using the filtering criteria above and after full-text screening by two authors (as and fas) studies were included in the summary tables and additional studies were included in the review to support evidence.if one author agreed to inclusion after full-text screening, the corresponding article was included.results after deduplication, the initial search yielded 4817 results.relatively, more studies on metabolic diseases (type i diabetes, type ii diabetes, obesity and metabolic syndrome) were included in this review (40 total, 32 human and 8 animal) when compared to evidence found in other disease groups.these findings suggest that specialists in pediatric dentistry and general practitioners who regularly treat children are more informed about appropriate dental care for children with congenital heart disease.immunohistochemically, sections of atherosclerotic plaque from injured group showed macrophage invasion in the tunica adventitia of aorta and upregulation of tlr4.specifically, it was found that serotype f strain omz175 of s. mutans has this capability [91].the rationale behind such an approach is that the likelihood of developing infective endocarditis due to a bacteremia from dental procedure is significantly lower than bacteremia from routine at-home toothbrushing and flossing.furthermore, it is not clear if antibiotic prophylaxis prior to dental procedures will prevent all potential for infective endocarditis secondary to dental procedures.in this scenario, the risk-benefit analysis appears to be of low benefit and high risk, taking into account the potential for antibiotic resistance.while caution must be exercised, there are exceptions and it is thought that patients in certain high-risk categories may benefit from antibiotic prophylaxis [93].outcomes other than caries were also studied, including salivary composition, microbiology and periodontal status.hegde et al. found that caries active participants who were diabetic demonstrated significantly reduced salivary calcium and significantly increased alkaline phosphatase when compared to caries active non-diabetic participants [34].reduced salivary ph and higher lactobacilli count are crucial factors for demineralization of teeth and exacerbation of dental caries [38].there were reduction in volume of pulpal connective tissue and enamel and dentin, along with excessive wear of enamel [20, 21, 25, 27].carious lesions positively correlated with gingivitis and periodontitis [23, 26].a study showed significant weight gain in children when teeth with severe dental caries and pulpal involvement were extracted [98].various factors including access and attitude to dental care, socio-economic status, maternal education, oral habits, diet, biological and microbiological factors interact in caries etiopathogenesis [99].an important implication of the mixed results observed for caries association with systemic conditions likely results in lack of reliable, reproducible risk prediction tools for dental caries [101].cherkasov et al. found an increased relative abundance of veillonella from dental biofilm in caries-affected children when compared to caries-free children with asthma [69].however, it may be argued that since mutations in ameloblastin (ambn) and other dental development genes are related to dental developmental defects [113], clinical information on absence of dental developmental defects may be considered when evaluating and reporting genetic association of dental development genes and asthma.peker et al. noted lower caries experience (dmf-t) in cf patients, and suggested this could be related to frequent use of antibiotics [67].salivary factors studied were not significantly different between the groups.however, human studies on salivary parameters in cf patients do not consistently show low ph and higher caries severity and systematic review on this data has shown limitations in study design and high risk of bias [118].similar results in an adult cohort has been shown previously [119].the presence of these factors creates a conducive environment for accelerated caries activity.results showed that serotype k of s. mutans was able to evade host response in peripheral blood due to variation in glucose surface side chains.in the study by kojima et al., significantly more ibd patients showed cnm encoding s. mutans (serotype k or f).delwel et al. raised an important point about composite nature of dmft index, wherein caries experience (decayed component of dmft) should be evaluated separately to assess caries burden and for statistical comparison between groups [130].in a case-control study of participants with adhd, paszynska et al. found significantly higher bmi in test group and significantly higher icdas 5 and 6 scores (teeth with advanced caries) in the primary dentition.they also found that increased intake of sugary foods and drinks were significantly higher in adhd group[81].these diseases included ra, chronic renal diseases and systemic lupus erythematosus.dmft/dmfs were associated with ra as assessed by disease activity score and serologic markers.although evidence of the association between sle, ra and dental caries is limited, risk stratification of patients in consultation with rheumatologist may facilitate preventive dental care.the oral cavity has evolved with a symbiotic and diverse microbiota which serves under some circumstances as a safeguard against numerous environmental challenges [136].conditions that disrupt this balance include breach in mucosal defenses and acquisition of pathogenic species, or pathogenic traits by certain commensal microbiota.examples of acute local infections that occur secondary to breach of mucosal barriers and/or colonization by pathogenic microbiota include dental abscess and lymphadenopathy.this study is a scoping review and provides an overview of available evidence on the topic of associations between dental caries and systemic diseases.it has a wider scope and does not limit the analysis to one systemic disease.interesting animal studies were noted that could generate clinical hypotheses and further investigations in rodent models for cardiovascular injury and hyperglycemia.also, an association between hyperglycemia and dental caries was consistently noted in animal studies.
number of words= 1348
[{'rouge-1': {'f': 0.31124566896263833, 'p': 0.8300000000000001,'r': 0.1915351812366738}, 'rouge-2': {'f': 0.20482737371098864, 'p': 0.45839285714285716,'r': 0.13187766714082505}, 'rouge-l': {'f': 0.2944168062894703, 'p': 0.7063636363636363,'r': 0.18596385542168675}}]
-----------------------------------------------------------------------------------------------------------------------------------
p293:
Extractive Summary:
those orthodontic appliances were assembled appropriately on the buccal side of dentition and are shown in fig. 1.rotation movement was decomposed into three planes of motion (sagittal, frontal and occlusal), and the corresponding directions (posterior, medial and mesial) were predetermined as positive.the buccal cusps of the mandibular posterior teeth and the edge of anterior teeth were lined virtually as buccal lines, while the lingual cusps of mandibular posterior teeth and the cingulum of anterior teeth were connected as lingual lines.the movement of the lingual line and buccal line was recorded.results displacements of anchorage teeth the results of 46 teeth in terms of force component distribution, displacement and rotation movements in three directions are shown in table 2.regarding the force distribution of 46 teeth, very similar results were found for the 0.017 × 0.025-in.and 0.019 × 0.025-in.the force components of the x-axis, y-axis and z-axis were 1.36 n, 0.31 n and 0.45 n, respectively.compared with the rectangular wire and round wire groups, the ribbon wire group had the lowest force component in three directions, and the force of the 0.025 × 0.017-in.group in the x-axis, y-axis and z-axis was 1.31 n, 0.29 n, 0.43 n, respectively.the round wire group displayed the highest x-axis force and z-axis force (1.39 n and 0.48 n, respectively).for the rectangular wire group, with increasing wire size, there was a descending trend in the displacement in the y-axis and z-axis.the displacement of the 0.017 × 0.025-in.wire was 9.13 μm.among the three groups, ribbon wire (0.025 × 0.017-in. and 0.025 × 0.019-in.) exhibited the lowest displacement in the x-axis (12.61 μm and 12.77 μm, respectively) and z-axis (8.99 μm and 9.06 μm, respectively).however, the 0.025 × 0.017-in.ribbon wire showed the highest y-axis displacement.the φ 0.018-in.round wire also showed higher z-axis displacement (9.44 μm) and y-axis displacement (5.96 μm) than φ 0.020-in.round wire (9.20 μm and 4.40 μm, respectively).in the ribbon wire group, the frontal rotation was higher than that in the rectangular and round wire groups, especially the 0.025 × 0.017-in.group, which exhibited 0.042° of rotation.in the round wire group, φ 0.020-in.wire, where the sagittal, frontal and occlusal rotation of φ 0.020-in.von mises stress distribution of the mandibular pdl figure 2 illustrates the buccal view and occlusal view of the pdl von mises stress distribution.in the posterior zone, the pdl of the first molar endured the most stress, especially in the distal root, and stress decreased gradually in the mesial direction.in the anterior zone, there was an increasing trend in stress from the canine tooth towards the central incisor.as shown in fig. 2, there was a very slight difference caused by the cross-sectional area in the pdl stress distribution among the study groups, and the maximum stress was was in the range of 9.87–11.06 kpa.displacements of mandible dentition the detailed displacements (x-axis, y-axis and z-axis) of mandible teeth after the application of 1.5 n retraction force is shown in fig. 3.in the x-axis direction, all the teeth were inclined mesially due to the retracting force.as the cross-sectional area enlarged, the displacement of mandibular dentition in the x-axis was reduced in ribbon wire group and rectangular wire group and round wire group.in the y-axis direction, the posterior teeth were tipped to the lingual side, whereas the anterior teeth exhibited a labial-oriented motion.in the z-axis direction, the first molar moved in an upward direction, but the central incisor had an opposite trend, indicating that the anterior teeth might intrude vertically since the posterior teeth tilted inward.the largest vertical displacement among mandible dentition was exhibited by the mandibular first molar, where the ribbon wire group exhibited the least vertical movement and the round wire group showed the highest displacement.displacement of the buccal line and lingual line the displacements of the buccal line and lingual line in different groups are shown in fig. 4 (red refers to the buccal line, and blue refers to the lingual line).from the observation of the curve contour, the tendencies of the buccal line and lingual line in different arch wire groups showed some similarity.the displacement was dramatically increased in the first molar region, followed by a slight decrease in the premolar zone before subsequently rising in the anterior teeth region.in the rectangular wire group (0.017 × 0.025-in. and 0.019 × 0.025-in.), the maximum displacements of the buccal line and lingual line were 15.98 μm and 15.91 μm, respectively.the movement of the first molar region was intermediate between the ribbon arch group and the round wire group.in the ribbon arch group, the lingual line displacement in the first molar region was 0.008 mm, which was also the lowest among the corresponding results of all the studied groups.notably, the values of the 0.025 × 0.017-in.in the 0.025 × 0.019-in.group, the displacements ranged from the first molar to the central incisor of the buccal line, and the lingual line was lowest among the studied groups.in the round wire group, the displacement ranging from the first molar to the central incisor of the buccal line and lingual line was higher than those of the rectangular and ribbon arch groups.the movement induced by the φ 0.020-in.previous studies also revealed the relatively high occurrence of root resorption in the distal root of the mandibular first molar in orthodontic treatment [29].
number of words= 868
[{'rouge-1': {'f': 0.4978557082216028, 'p': 0.8381564245810056,'r': 0.3540909090909091}, 'rouge-2': {'f': 0.34968987825257125, 'p': 0.5657983193277312,'r': 0.25304033092037226}, 'rouge-l': {'f': 0.5058111159664769, 'p': 0.6589570552147239,'r': 0.4104255319148936}}]
-----------------------------------------------------------------------------------------------------------------------------------
p294:
Extractive Summary:
background denture stomatitis is a common clinical condition occurring in removable denture wearers.it is characterized by inflammation, erythema and edema of denture bearing mucosa which may accompanied by pain or burning [1].although, many studies have been published on prevalence and etiology of denture stomatitis, the conclusion is still controversial [2, 3, 5, 6].intra-oral examination performed by examiners who were trained and calibrated in workshops hold by an oral medicine specialist to diagnose denture stomatitis.for a lesion to be classified as denture stomatitis, there must be visible inflammatory changes under the denture [21] and patient-reported discomfort in the absence of inflammation was not recorded as denture stomatitis.in terms of income, wsi was categorized into two groups: lower and higher than median.to determine cigarette, hookah and opium consumption, a structured questionnaire about frequency of use and years of smoking was used.we classified cigarette smoking as current and former smoking; participants who had smoked more than 100 cigarettes in lifetime and are still smoking considered as current smoker and who quit smoking prior to admission considered as former smoker.duration of cigarette smoking in current smokers divided into four quartiles: ≤ 11 year, 12–24 year, 25–34 year and ≥ 35 year.opium consumption was defined as self-report of opium use at least once per week for 6 months prior to admission, and hookah smoker was defined as active smoking of tobacco using a hookah over the past 30 days [19].questionnaires were verified in the persian [18].statistical analyses data of the present study were analyzed by stata 14.0 (stata corp, college station, tx) software.thereafter, they were sequentially entered into models according to their hypothesized strengths of association with denture stomatitis.to achieve this goal, separate models at bivariate level were run to acquire variables associated with denture stomatitis.also, due to lack of any internal validation study for evaluation of the self-reporting of opium consumption, utilizing of the results of an external validation study [23] and estimation based on the current condition [19] to determine the bias parameters and applying of simple bias analysis to correct the exposure measurement error were inevitable [22].prevalence of denture stomatitis in all removable complete denture wearers was 21.6% (table 1).there are significant differences regarding gender, cigarette smoking, opium consumption and history of hypertension among denture stomatitis and non-denture stomatitis participants.in the crude regression model, the odds of denture stomatitis among current cigarette smokers and opium users were about 2.12 and 1.33 times greater than those of noncigarette smokers and non-opium users respectively (or; 2.12, 95% ci 1.63–2.76 and or; 1.33, 95% ci 1.05–1.69 respectively).as the results classified by quartile of cigarette smoking duration, dose–response rises were shown, with the highest odds ratios for denture stomatitis in the 4th quartile (adjusted or; 2.09, 95% ci 1.29–3.37).also, we investigated the interaction between opium consumption and cigarette smoking and denture stomatitis and the results showed that participants who were opium consumer and cigarette smoker at the same time had no higher odds of denture stomatitis (adjusted or: 1.65, 95% ci 0.85–3.22).according to this analysis, percent biases, were − 7% for association between denture stomatitis and opium consumption, demonstrating that the odds ratio increased or, in other words, there was 7% error towards null prior to controlling for this bias.discussion the aim of the present cross-sectional study was to investigate the association of denture stomatitis prevalence with cigarette, hookah and opium consumption in participants of rafsanjan cohort study, a district in southeastern iran with a high rate of opium consumption.the present study would provide basic data for better understanding of distribution and association of denture stomatitis and cigarette, hookah and opium consumption as essential information for prevention, early diagnosis, treatment and also for better dental health services.according to our findings, the prevalence of denture stomatitis in all removable complete denture wearers was 21.6% which was in accordance with similar previous studies [4, 24].shulman et al. found that cigarette smokers had increased or of denture stomatitis [5].in addition, our findings on dose– response relationships between the cigarette smoking and odds of denture stomatitis strengthened the conclusion that cigarette smoking were directly associated with an increased odds of denture stomatitis.our finding indicated that the prevalence of denture stomatitis was 0.81 times lower in opium users compared to non-opium users (not statistically significant).therefore, this result suggests that opium consumption has no effect on denture stomatitis.interestingly, our additional analysis indicated that the interaction between opium consumption and cigarette smoking was also not associated with higher odds of denture stomatitis.according to the findings of the present work, contrary to opium users, denture stomatitis were more prevalent in hookah smokers versus non-hookah smokers which are consistent with those of the previous studies [10, 11] although they were not statistically significant.for example we are not able to determine if participants started smoking before or after the onset of denture stomatitis, accordingly, it is suggested that this relationship be reconsidered in the follow-up phase of this prospective study.abnet et al. in agreement with this study showed a high rate of sensitivity of opium consumption among residents of northern cities of iran [23].these findings may shed a more light in the field of causative association of denture stomatitis prevalence with opium consumption or tobacco smoking as determining factors for updating dental health services and education.also, we failed to find any association between hookah smoking and denture stomatitis.
number of words= 882
[{'rouge-1': {'f': 0.427851754307188, 'p': 0.8636507936507936,'r': 0.28436227224008576}, 'rouge-2': {'f': 0.2848387254668944, 'p': 0.5321513944223107,'r': 0.19446351931330474}, 'rouge-l': {'f': 0.38432228266389107, 'p': 0.6920472440944883,'r': 0.266029776674938}}]
-----------------------------------------------------------------------------------------------------------------------------------
p295:
Extractive Summary:
the interview duration ranged from 15 to 25 min.at the end of every interview, the interviewer and the observer reports were subjected to data analyses (table 1).subsequently, the main report, which was discussed with the expert panel, was written.after the first round of interviews, one of the 15 participants faced minor difficulties in understanding what we truly needed him to understand in items two, three, and four.adjustments in the items, which measured patient satisfaction with rd retention, support, and stability, were made by adding elaboration.the second round of interviews included 13 participants (7 males).nine of the 18 rds were in the upper jaw.the participants were wearing 5 cds (upper and lower), seven rpds with metal framework, and one acrylic rpd.interviews revealed that all participants were able to understand the items.data were analyzed, and the main report was formulated.the expert panel held a meeting to discuss the results of the interviews, and a decision was made to make no additional major changes, except for bolding the items’ keywords to increase their readability.the last round of interviews included 15 participants (7 males) wearing 21 rds.the participants were wearing 6 cds (upper and lower), seven rpds with a metal framework and two acrylic rpds.twelve of the 21 rds were in the upper jaw.the second- and third-round confirmed that all the participants comprehended all items, and both questionnaires were self-administered instruments with good face validity.step three (field test) two hundred thirty-five questionnaires were distributed to 133 participants with a mean age of 65.at the time of examination, the mean usage of the rds was 21.5 months (table 2).the correlation matrix between the items (table 3), bartlett’s sphericity test and kaiser–meyer–olkin test (kmo) were used to estimate whether the dataset was suitable for factor analysis.the correlation matrix between the items was acceptable, not very high or very low, with a determinant equal to 0.005.bartlett’s test results were statistically significant.the kmo results for the upper jaw and lower jaw questionnaires were 0.87, which is considered meritorious according to some researchers [20].therefore, it was confirmed that the dataset is suitable for factor analyses.eight linear components were identified for every questionnaire (table 4).the analysis of the dataset revealed that for every questionnaire, only one factor could be retained.the retained factor had an eigenvector over 1.0 (table 4).the factor explains 60.95% and 63.06 of the upper jaw and lower jaw questionnaires’ total variance, respectively.the component matrix shows that the items’ loadings onto the extracted factor were high, > 0.6 (table 5).therefore, the one-factor model was reliable [21, 22].subsequently, the items’ scores could be summed to form an upper jaw index and a lower jaw index for measuring patient satisfaction with their rd [22, 23].cronbach’s alpha (α) was used to assess the internal consistency of the questionnaires [22].cronbach’s alpha for the upper jaw questionnaire was α = 0.91.for the lower jaw questionnaire, it was α = 0.92 (table 6).this reflects excellent instruments internal consistency [24].eighty-seven participants (47 male, 54%) wearing 102 rds (52 upper jaws) completed the questionnaires again.the rds were nine acrylic rds, 44 rds with metal framework and 49 complete dentures.the participants answered the second questionnaire one week after filling out the first questionnaire.the icc (test–retest) values ranged from 0.72 to 0.95 (table 7).therefore, the level of reliability can be considered “moderate” to “excellent”.[25] discussion the arabic version of the upper jaw questionnaire and the lower jaw questionnaire to measure patient satisfaction with rds proved reliable and valid.factor analysis identified a single factor with an eigenvalue above 1.the items in every questionnaire shared the same cluster and could be summed to form an upper jaw index and lower jaw index that reflected the level of patient satisfaction with their rds.this cluster contained eight items covering the following domains: speaking, eating, appearance, cleanability, rpd movement in three directions (retention, stability and support) and overall patient satisfaction.the two indexes proved to be reliable with excellent internal consistency.quality control and quality assurance processes were applied throughout the study steps.the first level was the interview observer.the second level was the study coordinator, who compared the interviewer report with the observer report.he documented any discrepancy between the reports before performing the data analyses and drafting the main report.the third level was the expert panel, which continually monitored the study until the end by holding regular meetings with the investigators to track progress and assess how much the investigators adhered to the given study guidelines.this layering of control helps improve the quality of the study and strengthen the confidence in the study conclusion.however, the study had several limitations.it was not a multicenter study, and all data were collected from patients treated at ajman university.covid-19-related shutdowns reduced the number of participants and interrupted the study flow.the authors discarded the questionnaires that were returned from the participants who were asked to complete the questionnaires again if they sent it after 5 weeks of completing the first questionnaire.at the end of the second round of interviews, all 13 participants expressed a full understanding of the items.according to previous studies [26, 27], to reach data saturation in qualitative studies, 12 is considered the minimum sample size.nevertheless, a third round with 15 participants was performed to emphasize the results that both questionnaires were self-administered instruments with good face validity.no additional interviews after the third round were conducted because the participants stopped adding new contributions to the existing findings and holding more interviews was considered repetitious of comments and themes [28].the plan was to collect 300 questionnaires.unfortunately, the covid-19-related shutdowns forced the researcher to close the study at a sample size of 235.a sample size of 200 should be considered fair [29].moreover, some researchers believe that a sample size between 100 and 200 can be good enough if there are communalities after extraction in the 0.5 range [30].others stated that a sample size of 100 is not recommended, unless the loading is greater than 0.51 [30, 31].in the current study, all items had a loading ≥ 6 (table 5).therefore, the study sample size can be considered acceptable.many studies have been conducted to determine the different aspects of patient satisfaction with rds.these aspects can be categorized into two groups.the first group is rpd-related aspects, such as fit, chewing, speech, appearance, denture cleanliness, occluding teeth, distribution of chewing forces, type of rpd and rpd location (upper or lower jaw) [32–36].the second is patient-related aspects, such as the patient’s personality, socioeconomic status, emotional status, expectations, patient-doctor relationship, prior experience with rpds, age, sex, natural tooth problems, oral cavity status and general health status [36–39].
number of words= 1081
[{'rouge-1': {'f': 0.47283868557376496, 'p': 0.8274999999999999,'r': 0.33098191214470285}, 'rouge-2': {'f': 0.2871996100788477, 'p': 0.46849624060150374,'r': 0.20706896551724138}, 'rouge-l': {'f': 0.42779145553952835, 'p': 0.7386390532544378,'r': 0.3010838445807771}}]
-----------------------------------------------------------------------------------------------------------------------------------
p296:
Extractive Summary:
group d: the dentin was conditioned (dentin conditioner, gc corporation, japan) for 20 s and then rinsed, and a fast-setting glass ionomer restoration, shade a1 (fuji ix gp, gc corporation, japan), was added in 4 mm increments, followed by a topcoat (g-plus, gc corporation, japan), which was light cured for 20 s with the same led polymerization lamp.the color was recorded for all specimens directly after applying the tested materials.all specimens underwent aging using a suntest aging device (suntest cps + , atlas material testing technology gmbh, linsengericht, germany) with a xenon arc lamp (150,000 lx, wavelength > 370 nm) for 24 h in water at 37 °c according to en iso 7491, and the color was recorded [30].color measurement color was recorded by a spectrophotometer (x-rite sp62, x-rite gmbh, cologne, germany).the device was adjusted on a d-65 standard illuminant and calibrated once daily on both white and black calibration tiles.each specimen was dried and centered over the 4 mm diameter aperture of the device, and the mean of 4 recordings was taken for each reading.three readings were recorded for each specimen as follows: 1 = baseline, 2 = directly after the application of the tested material, and 3 = after suntest aging.the l*, a*, and b* values (l* = lightness, + a* = red, − a* = green, + b* = yellow, − b* = blue) were measured to calculate δl, δa, and δb.then, the extent of the color change (δe) was assessed using the following equation [31]: outcome the primary endpoint of this study was to identify and assess the ability of the factor “material” to mask the color change produced by the application of sdf on primary dentin.the secondary endpoint of this study was to determine the direction of this color change in an attempt to better understand the masking effect and hence provide better esthetic outcomes.statistical analysis statistical analysis was conducted with spss 24 (spss, ibm, usa).a proposed sample size of 52 primary molars (13 in each group) was considered sufficient to detect an effect size of f = 0.4 with a power of 70% with a significance level of 5% [32].the means and standard deviations were calculated.samples were numbered and placed in identical containers by one operator and allocated to the different e = (l1 − l2)2 + (a1 − a2)2 + (b1 − b2)2 groups by a computer-generated program to create a balanced block randomization with an allocation ratio of 1:1 using random block sizes of 4 and 8 [33], while the implementation of the experiment itself was done by another operator.the primary endpoint was analyzed using multivariate analysis of variance (manova) to test the effect of the factor “material”.the scheffé post hoc test was applied for multiple comparisons among the different treatment groups.the first comparison was among all the tested groups to evaluate the differences between t0-t1, while the second compared the tested groups to evaluate the differences between t1-t2.statistical significance was considered at p < 0.05 for all tests.in addition, a descriptive statistic including the means and standard deviations for the values l*, a*, and b* was used to interpret the direction of color change in different groups at different time points (t0, t1, and t2) to fulfill the secondary outcome.results the primary analysis included all samples that were randomized.manova revealed the significant influence of the factor “material” (p < 0.001).sdf was reported to cause an obvious color change compared to the color of carious dentin (δe = 17.2) when observed by the naked eye.as observed in table 1, the δl results from t0-t1 revealed a color shift that was significantly different among all treatment groups (0.001 < p < 0.008) except groups c and d (p = 0.28).after suntest aging, this difference was significant among all groups, including groups c and d (p = 0.04).regarding the red/green scale (δa), sdf resulted in a color shift that was significantly different for groups b (p = 0.001) and c (p = 0.001).moreover, group b showed a significant difference in color shift in comparison to group d (p = 0.001).a significant difference in the color shift was also observed between groups c and d (p = 0.001).suntest aging produced the same results, with the exception that the color shift of group b was significantly different from that of group c (p = 0.001) instead of group d (p = 0.64).with respect to the blue/yellow scale (δb), the color shifts of all tested groups varied significantly from one another (with significance ranging from p < 0.001 to p < 0.04).after aging, there was no difference in color shift between group d and either group a (p = 0.67) or group b (p = 0.10).the secondary analysis also included all randomized samples.to interpret the origin and direction of the color change, l*, a*, and b* values were calculated descriptively and summarized in table 2.the l* values suggest that directly after application of the tested materials, sdf resulted in a tooth color that was darker than the color of carious dentin, while all treatment materials (ki, cmp, and gi) resulted in a lighter tooth color, with cmp resulting in the lightest, followed by gi and ki.aging decreased the l* values in all the groups (shift to darker colors) compared to the values directly after material application; cmp was the only material that maintained a value corresponding to a color lighter than the tooth baseline color.the a* values of all groups decreased from t0 to t1, meaning a reduction in yellow or even a color shift towards green for the ki groups.this result became more accentuated in the sdf and gi groups but subsided in the cmp and ki groups with aging.however, ki resulted in the greenest values.immediately after the application of sdf, group a showed a decrease in the b* value towards a less yellow color, and this value decreased even more from t1 to t2.in contrast, the b-values of ki and gi increased towards yellow from t0 to t1, and this yellow color subsided from t1 to t2.regarding cmp, the b* value decreased directly after its application and then increased with aging to give a yellow color, which was less observable than the baseline reading.discussion the covid-19 pandemic has drawn the attention of dental healthcare workers to the existing gap in the dental infection control standards implemented to prevent the transmission of airborne pathogens.thus, sdf application is now recommended as one of the most important alternative nonaerosolizing management of dental caries [5, 6].it supports an immediate and likely long-standing need to reduce aerosol-generating procedures in the dental therapy to minimize patient-to-patient transmission of sars-cov-2, to protect dental health care workers from harm, and to address in the long term a movement toward minimizing aerosol-generating procedures in dentistry, as required by raskin et al. [5].
number of words= 1128
[{'rouge-1': {'f': 0.42144872337635114, 'p': 0.7980453257790368,'r': 0.2863299663299663}, 'rouge-2': {'f': 0.2535577098006601, 'p': 0.4364772727272727,'r': 0.17867733782645323}, 'rouge-l': {'f': 0.4224239253001041, 'p': 0.6585416666666666,'r': 0.31093816631130067}}]
-----------------------------------------------------------------------------------------------------------------------------------
p297:
Extractive Summary:
background the instrumentation aims to shape the canals to facilitate cleaning and obturation, preventing disease progression and promoting healing.their performance and safety have always been a subject of interest among practitioners in the presence of anatomical challenges.canal curvature is a parameter that can challenge the performance of a file; hence respecting the anatomy of a canal in the presence of curvature is a desired characteristic of any rotary instrument considering the root canal morphology [1, 2].the oneshape (os; micro méga, besançon, france) is a single-file canal preparation system made of conventional niti alloy that works on full clock rotation.this system was introduced to the market after two reciprocating single-file systems named waveone (dentsply maillefer, ballaigues, switzerland) and reciproc (vdw munich, germany) promising results.the os file has a tip size of 25 and a constant taper of 0.06 with different cross-sections along its length, changing from s-shaped to concave triangular shape near the tip.this system requires only one file working in a clockwise rotation to prepare the canals up to the apical size of 25.these features have decreased the preparation time by os compared to other single and full sequence file systems [3, 4].bürklein et al. found that os required less time in comparison with another single file and a full sequence system to prepare canals in extracted teeth although the clinical significance this difference is questionable [3].the file has features such as anti breakage control (abc) and asymmetric file design, which is claimed by the manufacture to increase the safety of the system [5].hero shaper (hs; micro méga, besançon, france) is a full-sequence system introduced as a modification of hero 642 in terms of helix pitch and helix angle with a shortened handle.hs files have a positive rake angle, large inner core, and abc, incorporated into the design to increase the files’ efficacy and safety.the system consists of six files with tip sizes of 20, 25, and 30 and is grouped in tapers of 0.4 and 0.6.the manufacturer suggests three protocols, namely yellow, red, and blue, based on the canal’s anatomy to be instrumented.a sufficient amount of studies are available on this system to make it a proper baseline for the evaluation of rotary instruments [6–8].revo-s (rs; micro-mega sa, besançon, france) is another full-sequence system introduced after hs by the same company, which consists of three files with a constant apical size of 25.the main feature of this system is the asymmetry in the cross-section or offset mass of rotation [9].this feature is claimed to enhance the negotiation of curved canals due to the files’ increased flexibility [10].the manufacturer also claims that the file’s helical design facilitates debris movement away from the apex because of the increased available volume of space between the file and the canal’s surface.this asymmetric design was not incorporated in revo-sc2.claimed by the manufacture, this file’s asymmetry balances the forces and ensures the instrument’s guidance up to the apical region.like hs, this system also benefits from variable pitch angle.although hs and rs might be considered traditional systems compared to the latest rotary instruments, especially the new wave of single-file systems.these are the products of constant changes and improvements in niti instruments’ designs over a short period by the same manufacturer.evaluation of these systems’ behaviour may give us some insights into the effect of changes on our understanding of different designs and sequences of rotary files on their shaping ability.therefore, this study aimed to evaluate three niti files’ shaping ability regarding their centricity in simulated l-shaped canals.methods sample size sample size was calculated with assumption of alpha = 0.05, power = 80% maximum difference of means (d) = 0.05 and pooled standard deviation (s) = 0.04 (4) and 3 levels(k = 3) by this formula n = 2s2(z 1− /2+z1− )2 (d)2 , n = k − 1 .so the sample size in each group approximately was 16.simulated canals forty-eight simulated l-shaped canals were used in this study (endo training block-s; dentsply maillefer, ballaigues, switzerland, with 0.02 taper, 0.15 mm apical diameter, 17 mm length, and 40 curvature).the patency of the canals was confirmed by passing a size 10 k-file (micro méga, besançon, france) just beyond the apex, and the unity in the angles and length of the curvatures were confirmed before distribution of the blocks by taking pictures of the samples on a photography stand.after assuring that the samples are standard, they were randomly divided into three groups (n = 16 canals/group) and were numbered.all canals were injected with black ink (parker quink, parker, france) to obtain a clear pre-instrumentation image (fig. 1).the canals were photographed using a digital camera (sony alpha dslr-a100 camera with dslra100 macro lens, sony, japan) on a fixed stand with constant settings.all the canals were rinsed with saline before and after instrumentation prior to ink injection.instrumentation of l‑shaped canals a new instrument was used for each canal in all groups.glyde-prep (dentsply maillefer, ballaigues, switzerland) was used as a lubricant before using each instrument, and saline was used for irrigation during preparation.the canals were instrumented using the protocols suggested by the manufacturer described in the following sections without glide path preparation or additional use of hand files except for recapitulation with a size 10 k-file.group a the os file (tip size, 25; apical taper, 0.06) was used in a full clockwise rotation generated by an x-smart motor (dentsply maillefer, ballaigues, switzerland), and the speed and torque were adjusted to 400 rpm and 4 ncm.the files were used in a slight pecking motion according to the manufacturer’s instructions.the flutes of the instrument were cleaned after each retrieval of the file from the simulated canal.
number of words= 931
[{'rouge-1': {'f': 0.3432339548592039, 'p': 0.6715325670498085,'r': 0.2305316973415133}, 'rouge-2': {'f': 0.16231547059790097, 'p': 0.25461538461538463,'r': 0.11912998976458547}, 'rouge-l': {'f': 0.29040825319188784, 'p': 0.5218518518518518,'r': 0.20118279569892475}}]
-----------------------------------------------------------------------------------------------------------------------------------
p298:
Extractive Summary:
background traditional views hold that one’s prior experience of dental treatment plays a key role in shaping dental fear/anxiety [1–3].clinical evidence supports the notion that fear of dental treatment is closely associated with previous negative treatment experience [4–7].a recent cross-sectional study reported that the experience of past dental appointments may influence patients’ intentions of future appointments, highlighting the importance of patients’ evaluation and anticipation for their intentions of dental attendance [8].notably, patients’ anticipation of fear and pain towards coming treatment may not reliably reflect their actual experience of treatment.individuals may expect a stimulus to be more painful than what they actually perceived [9, 10].moreover, individuals may ‘overestimate’ the fear of pain of the dental treatment that they who have not experienced, compared to those they have experienced in the past [11].such an ‘overestimation’ of fear of pain may be associated with trait dental anxiety [11].critically, fear and anxiety are not only associated with pain but also avoidance of dental treatment.around 15% to 20% of the adult population avoid or delay visiting dentists [12–14], which results in a ‘vicious cycle’ of oral health [12].while individuals may magnify their fear of the treatment that they have not received [11], it has remained unclear if individuals also show an intention of avoidance (ia) towards the treatment that they have not experienced (i.e., non-experienced treatment).the current study aims to investigate the association between fear/ia of experienced and non-experienced conditions of dental treatment in adults.because dental treatment consists of a variety of procedures, which show different anxiety-stimulating effects [15], fear and ia of 12 conditions about common dental procedures were investigated in the study.among these procedures, some of them have been widely investigated for their association with anxiety and pain during treatment, such as extraction of wisdom tooth [16–18] and endodontic treatment [9].notably, even for a non-invasive procedure of regular treatment (e.g., restoration and dental scaling), patients’ anxiety was markedly associated with their treatment experience.for example, in patients receiving dental scaling, increased pain was associated with increased dental anxiety [19].in patients receiving restorative procedures, higher dental fear was associated with increased pain during treatment [20].here, three major hypotheses were tested: hypothesis 1: based on the previous findings of overestimation about fear of dental pain [11], it is hypothesized that fear/ia ratings are higher for the non-experienced vs. the experienced conditions of treatment.hypothesis 2a: due to the close relationship between fear and avoidance [13], it is hypothesized that one’s fear of experienced treatment is positively correlated with not only the ia of experienced treatment but also the ia of non-experienced treatment.therefore, it is hypothesized that individual scores of trait dental anxiety are positively correlated with their fear/ia of non-experienced treatment.hypothesis 3: different dental procedures have different anxiety-stimulating effects [15].some dental procedures may show a greater magnification factor (i.e., a greater discrepancy in the fear/ia of nonexperienced vs. experienced conditions).it is hypothesized that individuals would magnify fear/ia to a lesser degree for the conditions that more people have experienced (i.e., with a higher prevalence).methods participants study samples (n = 402) were recruited independently from two sites for the current study: 201 participants from a local community recruited via advertisement and 201 dental patients from the outpatient clinic of taipei veterans general hospital (table 1).the inclusion criteria were: (a) aged between 20 and 90 years and (b) having an ability to verbally communicate with the experimenters.the exclusion criteria were: (a) having a history of major physical or psychiatric disorders and (b) feeling stressed for answering the questions (which are related to the negative experience of dental treatment).the participants provided written informed consent, approved by the institutional review board of national yang-ming university (id: ym106095e) and taipei veterans general hospital (id: 2018-12-003ac) before all the assessment started.in addition, to assess the test–retest reliability of the questions of dental treatment experience, 26 healthy adults were recruited to complete the questionnaire of dental treatment experience twice at a onemonth interval.this group of participants also provided written informed consent, approved by the institutional review board of taipei veterans general hospital (id: 2013-12-002ac).experimental procedure dental treatment experience the questionnaire for dental treatment experience was customized with descriptions about 12 conditions of common dental procedures (table 2), based on the previous study [22] and an earlier study [11] that adopted a set of selected dental procedures.to ensure that the conditions are common to most patients, the procedures about complicated orofacial surgery or orthodontic therapy were excluded.trait dental anxiety was assessed using the chinese version [18] of the index of dental anxiety and fear (idaf-4c+) [23].the idaf-4c+ consists of eight questions, which assess the physiological, emotional, cognitive, and behavioral aspects of dental anxiety and fear [23].previous findings based on an australian population norm revealed that the idaf-4c+ score was associated with the avoidance of dentists and pain and anxiety related to dental visits [24].the score was also associated with the distress related to anxiety-stimulating dental procedures, such as the postoperative pain of wisdom teeth extraction [18].for each condition of dental treatment, the participants were asked to indicate (a) whether they have experienced that condition of treatment in the past (i.e., history of treatment, ht), the degree of (b) fear of the condition (fear), and (c) intention of avoidance of the condition (ia), respectively, according to the following instruction: ‘if you have previously experienced a condition about the treatment, please rate the degree of fear and intention of avoidance about that condition, by recalling your prior experience about it.’ ‘if you have never experienced a condition about the treatment, please rate the degree of fear and intention of avoidance about that condition, by anticipating what you would feel about it, according to what you know about the treatment.’ all the ratings were scored based on a 10-point numerical rating scale, ranging from 1 (the least degree of fear/ ia) to 10 (the maximal degree of fear/ia).the scores of ht, fear of experienced treatment (expfear), ia of experienced treatment (expia), fear of non-experienced treatment (nexpfear), and ia of non-experienced treatment (nexpia), were calculated by including the ratings from all the 12 conditions, according to the following methods: the conditions that subjects have experienced were indexed by the value ‘1’ and those they have not experienced were indexed by the value ‘0’.clinical implications based on the novel findings presented here, three aspects for further considerations are suggested for clinical practice.a. individuals with a higher trait dental anxiety may tend to show the fear of the procedures that they have not experienced.therefore, the assessment of dental anxiety will be helpful for predicting patients’ dental-visiting experience.therefore, dentists may pay attention if a negative impression about the treatment has been cast on patients, leading to their fear and avoidance.c. finally, recent evidence has gradually disclosed the biological mechanisms of dental fear, including its association with genetic variations [34] and brain activation [22, 35].the biological mechanisms underlying fear/avoidance towards non-experienced stimuli would require further investigation.conclusion the novel findings suggest that individuals may develop a high degree of fear and intention to avoidance toward the conditions of dental treatment they have not experienced.individual variations in trait dental anxiety play a key role in the fear of non-experienced treatme
number of words= 1185
[{'rouge-1': {'f': 0.3718958035589101, 'p': 0.9223206751054853,'r': 0.23290322580645162}, 'rouge-2': {'f': 0.2562046632124353, 'p': 0.5700000000000001,'r': 0.16523809523809524}, 'rouge-l': {'f': 0.3861417114113492, 'p': 0.8045132743362833,'r': 0.25403547671840354}}]
-----------------------------------------------------------------------------------------------------------------------------------
p299:
Extractive Summary:
background in orthodontic treatment with premolar extraction, sliding and closing loops are two major techniques to close the space.although sliding mechanics is widely used in the clinic with the advantages of simplified mechanics, increased patient comfort and reduced chair time [1, 2], loop mechanics is still thought to be more efficient in controlling tooth movement patterns [3].orthodontic loops are frictionless, and all the generated force will be fully expressed against the brackets and finally to teeth after deactivation [4].some clinicians still prefer loop mechanics to sliding mechanics [5, 6].closing loops with different configurations such as t loops, teardrop loops, l loops, and mushroom loops are used in the clinic, and they need individual adjustments according to the clinical experience of orthodontists [4, 7].the load deflection ratio (l/d), vertical force and moment-to-force ratio (m/f) are three important indexes for the evaluation of different loops, and the m/f ratio is the most critical index in loop mechanics.as reported in previous experimental and analytical studies, the m/f ratio varied with the wire material, cross section, height, width and configuration of loops [10–14].preactivation methods such as gable bends and vertical steps diversified the m/f ratio [15–18].the m/f ratio was also reported to change with the distance of activation [19, 20].double key loop (dkl) is a special method advocated by john parker to close space with straight wire appliance [21].normally, dkl is composed of two key holes, vertical loops at the mesial and distal interproximal positions of canines.loading at the distal end, distal key and additional ligation between keys are the three major loading types of dkl, which provide advantages of flexible force system and effective control of anterior torque [22].retrospective clinical studies by dr.kim and chen reported the high efficiency of dkl in the vertical and torque control of upper anterior teeth [23, 24].clinicians are interested in the mechanical properties of dkl, and several relevant studies have tried to explore precise control methods for dkl.dobranszki used photoelastic models to compare the force response of teeth subjected to dkl and confirmed that vertical force on anterior teeth varied with the loading types [25].tábitha used finite element method to investigate the force and deformation of dkl, but no detailed m/f ratio results were provided [26].to provide a preliminary guide for the application of dkl, finite element method was used in this study to explore the effect of wire material, loading types and preactivation angle on the mechanical force system of dkl.methods archwire between the upper lateral incisor and second premolar with a single key loop and a double key loop was established in finite element analysis software ansys workbench 17.0 (ansys, usa).the cross-section of the archwire was 0.019 × 0.025 inch rectangular, and the configuration of the key loop is shown in fig. 1.the height and width of the key loop were 6 mm and 4 mm, respectively.key loops were at the mesial and distal interproximal positions of the canine.the archwire was divided into several parts, and bonded contacts were added between the parts to calculate the force and moments at specific positions.automatic meshing was finished in workbench, and 174,830 nodes and 108,039 elements were attained (fig. 2a).wire material was defined as stainless steel (ss) with an elastic property of yang’s modulus 168 gpa, poisson’s ratio 0.3, and titaniummolybdenum alloy (tma) with yang’s modulus 66 gpa, poisson’s ratio 0.3[5, 27].to simulate the activation of key loops, the mesial end was fixed in six degrees of freedom (three displacements along and three rotations around the three orthogonal axes).the vertical (z axis) displacement of the distal end was set as 0 mm to simulate the sliding of the archwire through the bracket slot of the second premolar.force was applied at different positions to simulate three types of loading (fig. 3).for type-1 loading, horizontal force was applied to the distal end.for type-2 loading, force was applied between the distal key and the tube of the second molar.for type-3 loading, a spring was added between mesial and distal loops to simulate preactivation, and then retraction force was applied as in type 2.stiffness of the spring was set as 15 n/mm.preliminary experiments verified that preloading of 7.1 n, 14.2 n and 21.3 n in spring generated upward bending of the distal wire in ss dkl to preactivation angles (θ) of 5°, 10° and 15° (fig. 2b).when the distal end was constrained in the vertical component, simulating the engagement of preactivated dkl in premolar brackets, the residual tension in the spring was 1.86 n, 3.71 n and 5.57 n, respectively.for tma wire, preloading of 6 n, 12.4 n, 18.9 n and 25.3 n was applied accordingly to generate preactivation angles of dkl of 5°, 10°, 15° and 20°, and the residual tension after engagement was 0.97 n, 2.07 n, 3.18 n and 4.28 n, respectively.the mesial displacement of the distal end for each preactivation angle was calculated and set as the neutral position.to achieve similar torque control on anterior teeth with the same preactivation angle, tma dkl could provide 2.5 times the longer distance of activation at equal retraction force or provide approximately 60% lower magnitude of force and moment at the same distance of activation compared with ss dkl.there could be several combinations of loop materials, preactivation angles and activation distances.these data were instructive for clinicians to make a good decision.traditionally, the optimal force was 3.1 n for upper anterior teeth and 2.6 n for lower anterior teeth [30, 31].however, the latest research suggests that force with lower magnitude could be optimal for bodily orthodontic movement [32].activation of 0.019 × 0.025 inch ss dkl up to 1 mm in type-3 required force that was quite heavy.unsurprisingly, this heavy force was not realized by clinicians and called for evidence from experimental studies.if tested in actual measurements, the use of ss dkl for space closure should be reconsidered.tma dkl of the same size required approximately 40% of the force of ss dkl for the same loading condition.changing the dkl material into tma is a good choice, or the cross-section and configuration should be optimized for ss dkl.conclusions the force system of dkl changed with loading type, preactivation angle and wire material.type-2 loading of dkl showed a higher l/d ratio than type-1 loading.type-3 loading showed the highest m/f ratio with a similar l/d ratio of a single key loop.the m/f ratio of dkl on anterior teeth in type-3 loading increases with the preactivation angle.the m/f ratio of dkl in type-3 loading increased in the process of deactivation, and the m/f ratio at a certain distance of activation depended mainly on the preactivation angle instead of the wire material.
number of words= 1094
[{'rouge-1': {'f': 0.4027785701217026, 'p': 0.9221400778210116,'r': 0.25766066838046275}, 'rouge-2': {'f': 0.2661343533578741, 'p': 0.5504687500000001,'r': 0.17548885077186965}, 'rouge-l': {'f': 0.4009656104520198, 'p': 0.7786614173228346,'r': 0.27}}]
-----------------------------------------------------------------------------------------------------------------------------------
p300:
Extractive Summary:
spiritual care meets the needs of life meaning, self-worth, expressing oneself, and faith support [2].a recent concept analysis of spiritual care suggested that spiritual care includes healing presence, therapeutic use of self, intuitive sense, exploration of the spiritual perspective, patient-centeredness, meaning-centered therapeutic intervention, and creation of a spiritually nurturing environment [3].spiritual care competence (scc) refers to an ability to assess and identify patient spiritual needs and implement appropriate interventions to promote patient spiritual health.spiritual care for cancer patients can improve negative mental states such as anxiety and depression and relieve fatigue and pain.and nurses with negative attitudes towards death had difficulty providing spiritual care [22, 23].thus, it is essential to assess the effects of attitude towards death on nurse spiritual care competence to provide a means to improve nurse spiritual care competence.methods aim the aim of this study was to establish oncology nurse spiritual care competence and its relationship with the attitude toward death and personal characteristics.3. to identify factors that affect nurse spiritual care competence.spiritual care competence scale van leeuwen et al. [24] developed the spiritual care competence scale (sccs), a self-reporting scale to measure nurse spiritual care competence.the questionnaire contains 27 items, consisting of six core domains of spiritual care-related nursing competencies (assessment and implementation of spiritual care, professionalization and improving quality of spiritual care, personal support and patient counseling, referral to professionals, attitude towards patient’s spirituality and communication).results demographics and professional experience of 326 participants, 95.4% (n = 311) were women.a little more than half of the nurses had more than six years of working experience (63.8%) and had experience in caring for terminally ill patients (77.0%).forty-three percent (n = 139) had never participated in the care of a family member or friend.table 1 presents additional demographic information.the total score of sccs-c was 61.12 ± 16.10, ranging from 22 to 110.the highest score on competence was for communication, 3.64 ± 1.00, and the participants had the lowest score for competence on referral to professionals, 2.27 ± 0.93.attitude towards death table 2 lists the scores for the subscales of dap-r that described participant attitudes towards death.the highest score on dap-r was for neutral acceptance, 3.83 ± 0.62, and the lowest score was escape acceptance, 2.64 ± 0.82.the participants had scores of 2.87 ± 0.70 on fear of death, 2.77 ± 0.67 on death avoidance, and 2.77 ± 0.67 on approach acceptance.relationship between scc and attitude towards death the spearman correlation (table 3) showed that sccs was related with fear of death, death avoidance, neutral acceptance, and approach acceptance, with the correlation coefficients ranging from 0.109 to 0.198.this result indicates that the scc is weakly and positively correlated to the attitude towards death.factors associated with scc table 4 shows the results of regression analysis.the linear regression equations fit the data well (f = 4.985, p < .001), which explained 18.1% of the variance.the absolute value of the standardized regression coefficient showed a significant association with the dependent variable.discussion the situation of oncology nurse scc the total score of sccs-c of oncology nurses in guangzhou was 61.12 ± 16.10, which was at a medium level and slightly lower than the score reported by yang et al. [15].the difference in scores may be related to differences in the nurse populations.moreover, the scc scores in our study were significantly lower than those reported by petersen et al. [27], who investigated 112 pediatric oncology nurses (98.30 ± 14.05) scores on sccs [27], and attard et al. [28] who investigated nurses (105.73 ± 14.053) and midwives (104.37 ± 8.999).the reason for this difference may be due to differences in religious, cultural, educational, and spiritual aspects.spiritual care is highly valued abroad, and theoretical training and clinical practice in spiritual caring for terminally ill patients are more mature external to china.at present, there is a lack of a unified spiritual concept, systematic intervention, and an independent care model [4, 30, 31].chinese clinical nurses have a different understanding of spirituality.some even consider spirituality a matter of personal privacy or they equate it with religion [32, 33].therefore, it is necessary to define the connotation of spirituality and spiritual care, to increase nurse recognition of spirituality, to raise their recognition of spiritual distress, and to promote the implementation of spiritual care.the results showed that nurses were confident about their communication with patients on spiritual issues, and nurses respected patient spirituality.we found that nurses received the lowest score on referral to professional, which was different from the results of azarsa et al. [36].these factors hinder nurses from referring patients to other professionals or organizations for more effective help.in addition, government should provide appropriate support.the influencing factors of oncology nurse scc we found that, compared with oncology internal medicine and other departments, the scc of oncology surgical nurses was relatively high.this result indicated that spiritual care training could enhance scc.thus, the curriculum and learning requirements are lacking in china.in our study, the natural acceptance had the highest score, which was similar to other reports [43, 44].in addition, we found that attitude towards death was positively associated with nurse scc.oncology nurses held an objective and positive acceptance of death, which was akin to results reported by du et al. and han et al. [43, 45].investigators have pointed out that a negative view of death affects nurse cognition of spiritual and spiritual care, and spiritual care cognition is an important factor that affects nurse scc [13, 46–48].we found a positive correlation between the death approach acceptance and nurse scc (p < 0.001).nurses who had approach acceptance (an attitude that accepts death optimistically and regards death as part of life or as a way to a happy next life) had higher spiritual care competence.the reason for this relationship may be that nurses with a heart-felt positive acceptance of death can better detect the spiritual distress of cancer patients and provide spiritual care.furthermore, we found that the escape acceptance was negatively correlated with nurse scc (p < 0.05).we found that the death attitude of oncology nurses was not optimistic.although most participants had natural acceptance, there were still some negative factors such as death fear and escape.nursing administrators should recognize the death attitude of oncology nurses, promote oncology nurses to approach death by improving their self-acceptance level, and construct effective death and spiritual education models.strengths and limitations the strengths of our study are the following factors.third, a study of a chinese population was needed because of important differences in culture and health care systems between mainland china and elsewhere and because of the ever-increasing importance of spiritual care.some limitations of our study need to be acknowledged.conclusion oncology nurses showed different levels of competence in various dimensions of spiritual care.implication for practice because of inadequate spiritual care competence in china, patient spiritual issues may be ignored or underrecognized, especially for cancer patients who have more spiritual issues that other patients.
number of words= 1133
[{'rouge-1': {'f': 0.27264722808363706, 'p': 0.8922222222222222,'r': 0.16090909090909092}, 'rouge-2': {'f': 0.20174994499410262, 'p': 0.5550746268656717,'r': 0.12327868852459017}, 'rouge-l': {'f': 0.28398403792006804, 'p': 0.7155696202531645,'r': 0.17714285714285716}}]
-----------------------------------------------------------------------------------------------------------------------------------
p301:
Extractive Summary:
preferences, needs and values [2].full information-provision is, however, not always achieved; nor may it always be desirable.more specifically, patients are not always explicitly informed about the incurability of their illness [5, 6], or about all available treatment options and potential side effects [5–7].doctors sometimes omit the option of no anti-cancer therapy [5–7], and the possibly negative effects of treatment on condition and social functioning [5, 8].although the lack of full information-provision can limit informed decision-making, some patients are also reluctant to receive all information [9, 10], especially as their illness progresses [11, 12].in order to inform patients without overwhelming them, we need to shift our focus beyond current practices and preferences, to a deeper understanding of the challenges oncologists and patients perceive in information-provision (about disease status, available treatment options, and side effects), and possible strategies to address these challenges.moreover, specific ways to facilitate information-provision need to be explored.promising techniques include communication strategies based on placebo-effect principles, such as positive expectations and empathy.positive expectations can decrease patients’ pain perceptions [13, 14], while empathic behaviors (e.g., reassurance, attentive silence) can decrease anxiety and increase recall [15–19].recruitment and procedures patients we followed principles of purposeful sampling, aiming a variety in participants’ age, disease characteristics, geographical location, education, cultural background.patients contacted the research team, who provided more information via telephone and checked inclusion criteria.topic list a topic list was created (additional file 1), in collaboration with patient-representatives.the topic guide focused on i) the challenges of informing patients about treatment options, aims and side-effects; ii) strategies how these challenges can be overcome; iii) the possible facilitating role of empathy and positive expectations in reducing these challenges.outcomes patients sociodemographic/ disease characteristics were assessed.the audio-recorded interviews were transcribed verbatim, and personal identifiers removed.data-analysis was part of a cyclical process of data collection and analysis.data collection was stopped when data saturation was achieved.analysis process analysis was performed using thematic analysis [20].firstly, two researchers (lv, mm) (re)read the transcripts to familiarize themselves with the interview data and independently wrote a memo of the most striking findings of each interview (step 1).these memos were subsequently compared and discussed, and initial codes were given to relevant interview fragments (step 2).using further discussion of memos and initial codes, the two researchers generated themes, which were displayed in a draft figure (step 3).themes from the analysis of oncologists’ and patients’ data, three main themes were identified, concerning challenges in the discussion of disease status, treatment options, and side effects.oncologists identified challenges, but considered informing patients a core task.challenge 1: how to handle unrealistic disease (status) beliefs oncologists mentioned the challenge of handling patients’ unrealistic disease beliefs.this focused on patients not expecting or accepting the disease’s incurability, and having overly optimistic/pessimistic expectations.strategy chosen by oncologists: informing & accepting to address this challenge, oncologists try to provide information – especially concerning the disease’s incurability – and to keep an open dialogue, actively exploring patients’ perceptions.simultaneously, they seem to accept – temporary – denial of the incurability of the disease.if necessary, e.g. with disease progression or if decision-making is hampered, this may be re-discussed.what we often see in clinical practice is that as a coping mechanism people say “i hear you [that the disease is incurable, red], but i don’t want to accept it yet (…).so i’ll say (…) you need to know this to make well-informed decisions, but we don’t need to keep discussing that you’re dying.(4005).patients’ preferences: informing patients stressed the importance of being clearly informed about their disease status; the incurable status is devastating but essential to know.you’ve got two children; so you don’t want to hear it [that the disease is incurable, red]; nobody wants to hear that.but well [it needs to be said clearly, ed]; they cannot keep avoiding it.(3013).challenge 2: how to approach discussing available treatment options a second challenge involves discussing treatments options, specifically whether to discuss all options or a selection, and whether to include the option of no anticancer therapy.i generally make a selection.i think most people benefit from your advice; that’s why they’re there.i generally discuss the most effective treatment (…).if there are two equal options, then you discuss the pros and cons of the different options.(id 4028).patients’ preferences: medical expertise and selection is trusted patients often expressed faith in oncologists’ medical expertise and appreciated them selecting treatment options.most did not want to carry sole responsibility for decision-making, arguing that they lacked medical expertise.i don’t know about all the treatment options there are so i trust that the oncologist will give me the best treatment (…).he will discuss it with the team; he confers with other oncologists and doctors.the option of no anti-cancer treatment was explicitly discussed where the burdens of anti-cancer treatment might outweigh the gains (e.g., with older patients, limited prognosis, sole option of chemotherapy).patients’ preferences: option of no‑anti‑cancer treatment is a sensitive issue patients’ – sometimes strongly expressed – views varied on whether this option should be discussed.i do think we should have a choice.most oncologists mentioned common and alarming side effects (more would be overwhelming); a few mentioned all possible side effects (to inform the patient fully and protect against later complaints); some always mentioned specific side effects (e.g. nausea).oncologists considered it important for patients ultimately to be fully informed – possibly through specialized nurses/written information – to make well-informed decisions.
number of words= 880
[{'rouge-1': {'f': 0.374666653834502, 'p': 0.7226717557251909,'r': 0.2528877005347594}, 'rouge-2': {'f': 0.18018857361495164, 'p': 0.28839080459770117,'r': 0.13102783725910067}, 'rouge-l': {'f': 0.3444753472339563, 'p': 0.5861290322580646,'r': 0.24391304347826087}}]
-----------------------------------------------------------------------------------------------------------------------------------
p302:
Extractive Summary:
…they’ll seem like yeahit’s a good ideabutthen they, it’s not as important as everything else that they’ve got going on.so they’re the kind of people that it’s not as important.i have had visits with people who have had the opposite.so one lady who her mother was told at the age of 70 well that’s the end for her, well she had some sort of treatment i can’t recall what it was but something, she lived for another 20 years.so that woman who was in her 90 s refuses advanced care directives because she believes that she would be signing her own death warrant.that happens a fair bit.acp rn b concern about the impact of major illness on family members was associated with willingness to uptake of acp.all acp rns reported similarly that "they were prepared to take that step and make it easier for other family members …they saw this is a good idea and they would do it…." acp rn d for the nurses that acp rns worked with, their beliefs, knowledge, behaviour and attitudes about acp impacted the implementation of acp in routine practice.some were more aware of and accepting of acp."there was a little bit of resistance.one (commu nity) rn stated her views on it and felt that acds were so final.she did indicate that she doesn’t really agree with them (acd and acp) and she doesn’t hand them (the acp flyer) out to everyone and get them to do it."acp rn c while all acp rns described some degree of resistance in the implementation of the acp service among the existing staff, others were committed to implementation of the acp service as a routine practice.acp rn b further appreciated a community nurse’s commitment to acp."i had one male nurse in particular who is absolutely brilliant and he gets the prize for the most referrals given to me at the end.most nurses don’t think about it.you constantly have to ask next time you see mrs such and such can you please ask (if they want the acp nurse to visit)." acp rn b enablers and barriers related to acp rns the acp rns shared what worked well and what challenged them in delivering nacp service, and these included; 1) the training program, and weekly debriefing and troubleshooting meetings, 2) the referral process for the nacp service in community settings, 3) everyone loved the conversation card, 4) let it take its course in a series of conversations during the acp process, and 5) is hospital right place to do acp?firstly, all acp rns stated that the training program enabled them to gain the knowledge and skills to deliver the nacp service.the acp rns also mentioned gaining their confidence as they implemented the acp service.i have now completed 3 days of acp/acd training and am starting to feel more confident as my knowledge increases."acp rn b "after completing the acp training and getting to talk to some patients about it, i have become more confident in discussing this topic with patients and their families." acp rn a central to supporting the acp rns in sustaining the acp practice was a weekly debriefing and troubleshooting meeting with the acp experts (sj, so) in the research team.this enabled the exploration of issues around service delivery as they arose and consistency of advice about challenging cases.“the weekly meetings were fantastic that was absolutely necessary.…absolutely.(we) are doing the same job but it’s chalk and cheese as well so it was really good to hear how she was going… particularly because as i say it’s not been done before so we were really feeling our way to make it work, ... it was good to be able to touch base once a week to say this is where i’m at challenging cases, absolutely.” acp rn b. secondly, the acp rns mentioned that the referral process to the acp service in community setting was a challenge as it was dependant on the existing staff’s belief and attitudes about acp."the first nurse was one of the ones who would not refer and when the second nurse happened to mention to me ’oh this gentleman has said that he’d be really good and he’s said yes’, she (the first nurse) actually turned around and said ’you can’t harass that man, he’s got enough going on, he’s got cancer…." acp rn b thirdly, the acp rns commented how useful the acp conversation card was to initiate the conversations and to capture iterative patient conversations."the patients have absolutely loved it (conversation card).as soon as it’s all complete and everything, they fold it up and straight in their purse it goes." acp rn a acp rn a further explained that other health professionals such as social workers accepted the acp conversation card well, saying “they love it, absolutely love it.all think it’s awesome and i’m surprised we haven’t got something like sooner, so thumbs up for that one, everybody absolutely loves it.” the number and duration of conversations needed for the acp process surprised the acp rns.the acp rns realised it was important to let it take its course and to let them (patients) reflect their life and transcend.the sequence or topic of conversations with the clients/patients for acp were similar among all acp rns.in general, the first conversation was to build rapport with the patients and introduce acp to the patients with explanation about the elements of acp such as acd, substitute decision-makers, enduring guardians, power of attorney, and will.in the second conversation, they explored values and their preference of care using the conversation card.although there were some patients who required more than three conversations, it was usually at the third conversation where they would finalise the acd with all the signatures which would make the acd legally binding.the conversations were spaced around 1 to 2 weeks apart to allow them to talk with family members, reflect on their situations, and experience gerotranscendence during a series of conversations.this study were not a senior or specialised personnel (e.g. clinical nurse consultant (cnc), clinical nurse specialist), but were general rns who should be available at various settings.this study adds new evidence that acp facilitators do not have to be a clinical specialist.however, the acp facilitators in previous studies all received additional training which varied from a 2-day training [5] to a full-day workshop [7].it is not specified what trainings were provided in other studies [3, 4].although miller et al. [4] provided some insight about the training workshop, there is no detailed report on what the training entailed.this study provided the detailed a 5-day training program in table 2.this study demonstrated that purposely designed face-to-face small group training, supported by on-line resources, was sufficient to equip rns to deliver an acp service.use of validated resources, such as the nsw health acp flyer and workbook or standard recording formats, such as the conversation card, facilitated consistent delivery of acp.
number of words= 1146
[{'rouge-1': {'f': 0.31227899624012356, 'p': 0.6745627376425856,'r': 0.20316582914572864}, 'rouge-2': {'f': 0.14903428448808725, 'p': 0.2417557251908397,'r': 0.10772003352891871}, 'rouge-l': {'f': 0.27381523006769054, 'p': 0.4889189189189189,'r': 0.19015503875968992}}]
-----------------------------------------------------------------------------------------------------------------------------------
p303:
Extractive Summary:
background advance care planning (acp) is a discussion of: care options; patient beliefs, values and preferences; mental and physical prognoses; and care decisions not restricted to goals of care and resuscitation directives [1, 2].acp can occur over multiple interactions with healthcare providers as patient beliefs, values and preferences may change with time [3, 4].physician-led discussions of acp with older adults (> 65 years old) are associated with increased quality of life and mood, longer survival, decreased use of non-beneficial medical care near death, decreased expenditures, enhanced goal-consistent care, and positive family outcomes [5, 6].canadians want more information about acp from their healthcare providers [7].a structured literature review determined that the prevalence of general practitionerled acp conversations with older adults is approximately 21% globally [1].primary care providers have mixed perceptions of acp and experience challenges in implementing this activity into practice.a national survey of canadian primary care providers (pcps) reported that pcps are very confident and willing to have acp conversations with their patients, yet reported low participation rates [9].the same survey also identified key barriers to successful acp implementation in primary care such as insufficient time and busy clinical schedules, lack of acp knowledge or training, and patient end-of-life care literacy [10].despite these mixed perceptions, primary care is an ideal healthcare setting to initiate acp conversations.the serious illness care program (sicp), developed at ariadne labs in boston, massachusetts, is a communication intervention developed to identify oncology patients with a high risk of death in the next year and to train oncologists in having acp conversations using the structured serious illness conversation guide (sicg) [11].the sicp has also been used in primary care settings with evidence of acceptance and benefit [12].apart from quality improvement initiatives in two primary care clinics affiliated with the brigham and women’s hospital in boston [12].sicp implementation has not been evaluated in primary care settings extensively.as a result, there is an impetus to evaluate sicp implementation in canadian primary care settings.however, addressing the need for serious illness communication training among pcps is only one of the strategies that will be needed to foster the implementation of acp in primary care settings.the normalization process theory (npt) is a theory of action that is used for understanding and evaluating how complex interventions are implemented and embedded in the everyday work of health care [13].npt can assist with explaining success or failure of implementation, and highlight the specific constructs that are impeding implementation [14].the core constructs are coherence (work that defines and organizes the objects of a practice); cognitive participation (work that defines and organizes the enrolment of participants in a practice); collective action (work that defines and organizes the enacting of a practice); and reflexive monitoring (work that defines and organizes the knowledge upon which appraisal of a practice is founded) [13].as part of a quality improvement initiative, our goal was to explore the perceptions of clinicians in an interprofessional academic family practice regarding implementing acp into routine care through using an adapted sicp and sicg.participants attended a training session consisting of group-based discussions about the principles of serious illness communication, followed by simulated conversations with standardized patients using the sicg, and receipt of real-time observation and feedback from trainers.the primary care cases were those adapted by the program sicp developers for primary care [12].trainers had attended training themselves previously, from the team who developed the sicp.upon completion, participants were asked to complete pre-and post-training self-assessment surveys using likert scale responses (1 = not at all skilled, 5 = extremely skilled) to evaluate their perception of their skills (additional file 1).surveys and interviews were used to measure pcp perceptions of acp and the sicp in primary care, as per the npt framework.we chose npt to guide the evaluation because it is a theory of implementation that focuses on people and their behaviours both individually and collectively as part of a social system, [14] and it can provide empiric evidence of where there are gaps in implementation and progress over time [16, 17].the constructs of npt align well to implementing the multi-step emotionally laden process of acp in a complex interprofessional environment such as primary care.we felt that npt, and specifically the measurement tool nomad, would be the most informative approach to evaluating implementation that would highlight specific areas for improvement and importantly, where additional attention would not be needed.we asked about perceptions of the sicg, how it was being used and if not used what were the barriers, and how patients could be best identified for initiating conversations.the research ethics board required that we obtain informed consent from participating patients and substitute decision makers.fifty-four percent lacked confidence in their colleagues’ abilities in conducting acp, and 57% lacked confidence in their patients’ abilities to engage with them when discussing acp.
number of words= 790
[{'rouge-1': {'f': 0.4085893888767555, 'p': 0.6856462585034013,'r': 0.291001221001221}, 'rouge-2': {'f': 0.1809646032446782, 'p': 0.26112627986348125,'r': 0.13845965770171148}, 'rouge-l': {'f': 0.36826436071649166, 'p': 0.520261780104712,'r': 0.28500000000000003}}]
-----------------------------------------------------------------------------------------------------------------------------------
p304:
Extractive Summary:
over the last two decades, the who advocated that palliative care should be considered as a public health issue, with calls for earlier integration of palliative care within the wider healthcare system to improve access and availability [1, 4, 5].this earlier or ‘integrated’ model of palliative care enables palliative care professionals to build relationships and become increasingly responsive to the need of patients and their families [7].a recent scoping review of the literature noted that thirteen international studies had been undertaken between 2003 and 2019 and concluded that the public had poor knowledge and misconceptions about palliative care [29].methods design a two phase, explanatory, sequential mixed methods design based on the taxonomy of creswell and plano clark [33] was utilised, including a cross-sectional survey and qualitative interviews/focus groups with members of the public (fig. 1).participants participants for the survey included a random representative sample of adults from the northern ireland population aged 18 years and over; selected from a database of addresses, where interviewers selected one adult at random for face to face completion of the survey at each address using the ‘next birthday’ method.this is an efficient method for selecting representative respondents within a household unit [34].participation was voluntary.following completion of the survey, participants were asked if they would like to participate in the second, qualitative phase.those who agreed to contribute had their contact information collated (separately to the survey responses).willing participants were contacted via telephone.uk/nilt/).the nilt uses a random sample with 1200 adults completing the questionnaire each year, ensuring the socio-economic and demographic characteristics of the sample are representative of the total population [36].this survey is run on a modular format and four modules are included each year.questions investigating palliative care were based on palliative care knowledge scale (packs) [37, 38], a 13-item instrument, and true/ false format that assesses goals, timing and what constitutes the palliative care team and systems related issues [37, 38].seven yes/no items on participants’ prior knowledge of palliative care were also included within the nilt survey.participant’s sociodemographic characteristics such as age, gender, religion, education level, marital status and income etc.are collected as part of the nilt annual survey (see supplementary file 1).face-to-face questionnaires were carried out using computer assisted personal interviewing, and there was a further self-completion questionnaire which respondents are asked to fill in on a tablet, or on paper.in phase two, data was collected from october 2018 to july 2019 by dm, kc, eb and smcc (postdoctoral researchers and/or specialist practitioners).the researcher was not known to participants prior to the data collection.the interview schedule was informed by the literature and quantitative phase of the study (see supplementary file 2).while focus groups were offered, the majority of participants wanted to undertake interviews.data were collected in a neutral public place, the participant’s home or via telephone, and were audio recorded and field notes taken.the data collection tools were piloted with academics and nilt data collectors prior to implementation.data analysis in phase one, the number of correct responses were tallied, and scores ranged from 0 (lowest knowledge) to 13 (highest knowledge).“i don’t know”/ “not sure” responses were coded as incorrect when calculating total packs scores.all survey data were analysed in spss v 25.0.a factor analysis was undertaken on packs and the factor structure was found to be acceptable.linear regression was used to identify demographic variables that impacted on the packs score and explored further using inferential statistics.further analyses were undertaken on appropriate variables; t tests were conducted to compare packs scores across gender.an analysis of variance (anova) was conducted to analyse how age, country of birth, marital status, household income, and qualifications performed on the packs.phase two data were stored and managed through nvivo software (v13).qualitative data were transcribed, and any identifying information was removed.transcripts were analysed using thematic analysis [39] which involved a six-step process: familiarisation, generating codes, searching for themes, reviewing themes, defining and naming themes, and producing a report.data analysis was done by two authors (ob and lh-t) and discussed with a third author (fh) to enhance credibility and trustworthiness.integration integration was evident through data transformation between phase one and two [40], and merging in the results and discussion [33].the data from phase one informed the development of the interview schedule utilised in phase two, and the results from both phases were analysed in parallel, and integrated using data matrix and weaving the thread techniques, and presented thematically throughout the results and discussion.results description of the sample a total of 2161 people were contacted, 1201 of whom completed the northern ireland life and times survey (response rate 56%), representative of the demographic profile in ni.the participants were aged between 18 and 95 years (mean: 61 yrs).the largest proportion of the population, 17.7% (n = 210) were aged between 45 and 54 years.factor loadings and communalities are reported for each item (see supplementary file 3).findings from thematic synthesis five overarching themes were identified through the merging and integration of qualitative and quantitative data; three which examined public awareness, knowledge, and perceptions of palliative care and two identified strategies to raise awareness: “prior knowledge of palliative care”; “variable understanding of palliative care”; “promoting public awareness of palliative care”; “shortcomings in current palliative care information and provision”; and "future strategies to improve understanding of palliative care".a fifth of participants (20.1%) indicated that they had previously heard about palliative care from either tv, newspapers or social media, with a further 9% acknowledging that they had heard the term from another source including school/university, word of mouth, a friend/relative, or a medical practitioner.for 13.8% of participants, the term palliative care was familiar, but they were either unsure or could not remember where they had heard the term.many of the respondents acknowledged that palliative care was complex and specialised.this highlighted that although there was relatively high awareness of the term palliative care, most respondents had very limited understanding of the term.promoting public awareness of palliative care in phase two, respondents were asked to give their views of the barriers and facilitators to promoting public awareness of palliative care.personal experience shaped many participants’ views on palliative care and potentially their misconceptions, which is also a common thread in the literature [11, 15, 27].this is supported by previous literature internationally, which repeatedly reports a public perception of palliative care provided at the very end of life [11, 15, 26].these studies have consistently demonstrated that palliative care awareness among the general public is variable and that demographic factors have a significant impact.according to collins [9] one of the potential contributing factors to this is the general public’s attitude and level of knowledge about palliative care.without open conversations and readily available information, such experiences provided a framework upon which to articulate and base their understandings of what constitutes palliative care.this highlights issues that collins et al. [9] consider as an ‘image/language problem’ for palliative care.such misconceptions should not be dismissed as they may deter people from accessing services in the future.study limitations whilst this study adopted a mixed methods approach with phase one based on a random sample representative of the total northern ireland population [36], the structured framework of the nilt survey did not allow to fully explore why people think the way they do and only provides a snap shot of attitudes for that period in time.conclusions in conclusion, this population based mixed methods study builds upon previous research in this area, indicating widespread misunderstandings of the concept palliative care.understanding is derived from limited ad hoc personal experiences focusing on the end of life and not the holistic palliative care journey.
number of words= 1252
[{'rouge-1': {'f': 0.37317358772967457, 'p': 0.7502325581395348,'r': 0.24835365853658536}, 'rouge-2': {'f': 0.20576962101385216, 'p': 0.3557142857142857,'r': 0.14475209763539282}, 'rouge-l': {'f': 0.3292817360019101, 'p': 0.5529268292682927,'r': 0.23445182724252492}}]
-----------------------------------------------------------------------------------------------------------------------------------
p305:
Extractive Summary:
the most important risk factors include ovarian polycytosis, estrogenic hormonal therapy, early menarche, late menopause, tamoxifen therapy for breast cancer treatment, anovulatory cycles and obesity.the primary purpose of this study is to analyze the possible role of anastrozole, in elderly patients with ec who are ineligible for surgical treatment, in terms of safety, efficacy, and clinical benefit.secondarily, this study evaluates quality of life modifications, toxicity and tolerability of the drug in this high risk group of patients.from 2015 all consecutive patients age older of 75 with diagnosis of type i ec referred to our hospital underwent full clinical examination and trans-vaginal ultrasound.four months after starting the therapy, all patients underwent ultrasound examination and abdominal ct in order to evaluate the response to the therapy, than every 6 months.in order to evaluate the quality of life in each of the patients who followed the anastrozole treatment, the authors administered the european organization for research and treatment (eortc)- core quality of life questionnaire(qlqc)30 questionnaire at different times: before starting the treatment, and every six months until discontinuation.results eight patients, according to inclusion criteria, were enrolled in the present study between 2015 and 2018, while ten women were excluded, as shown in the flow chart (fig. 1).the response to the therapy, counted such as any reduction in endometrial thickness, after twelve months was reported.the endometrial thickness had a mean of 10.74 ± 4.68 mm, with a reduction of 9.25 ± 4.77 mm (relative reduction rate of 44 %).the maximal endometrial thickness was 18 mm and the minimum was 5 mm (table 2).partial response was observed in seven patients (87.5 %, 7/8), with a reduction in self reported symptoms such as pain, vaginal bleeding, and vaginal discomfort for all patients.in one patient no endometrial thickness reduction was achieved during the follow up (12.5 %), and vaginal bleeding, pain and vaginal discomfort persisted, without any disease progression.the average follow-up evaluation was 18.5 ± 5.2 months for all patients.no deaths related to endometrial cancer, disease progression/relapse or side effects from therapy were reported.toxicity data were available for all patients included in the present trial.occasional nausea (4/8 patients), muscle/joint aches and bone pain (5/8 patients), and fatigue (5/8 patients) during the first month of the treatment were the only side effects reported.no grade 3–4 toxicity was highlighted.the global health status increased significantly 12 months after drug prescription (p = 0.002) (fig. 2a).however, fatigue increased significantly at six months (p = 0.011) (fig. 2b).we also reported a significant decrease in pain (p = 0.038) at 12 months compared to the baseline (fig. 2c).no statistically significant changes from the baseline and follow-up evaluations were reported for other items of qlq-c30.discussion our preliminary trial shows that anastrozole can be a valid palliative therapy in the treatment of ec in elderly women, which not only improves the quality of life, but also helps them to relieve the disease-related symptoms.in fact, this treatment demonstrated a positive clinical response in 87.5 % of patients with an endometrial thickness reduction.the review of gao et al., suggested that the use of aromatase inhibitors appears to be a potential active therapy in endometrial cancer also in early stages [20].the mean reduction reported by the authors was 4.5 mm in those patients with previous breast cancer treated with anastrozole and tamoxifen, while in our patients the reduction of endometrial thickness was 9.25 mm only with anastrozole.these difference can be related to a proliferative action of tamoxifen on endometrial tissue.the improvement of the quality of sleep at six and twelve months was reported by most of patients with psychological and physical benefits.surprisingly we reported a significant increase of fatigue declared; we explained this result as related not to the side effects of anastrozole, but, more likely, to the advancing age of patients.unchanged values reported in the other items of questionnaire are probably related to the early stage of the disease.nowadays progestin therapy by ius system (mirena) seems to be effective in the treatment of ec and in the decrease of malignant progression [28].nevertheless, in those patients with reduced life expectancy, the primary goal of palliative therapy of ec is to reach an acceptable control of cancer symptoms, good tolerance of the therapy and an adequate quality of life: anastrozole could have all these characteristi
number of words= 706
[{'rouge-1': {'f': 0.46072673386154156, 'p': 0.7795588235294117,'r': 0.3269906790945406}, 'rouge-2': {'f': 0.23214500523012552, 'p': 0.3541328413284133,'r': 0.1726666666666667}, 'rouge-l': {'f': 0.44769038314289794, 'p': 0.6559872611464967,'r': 0.3397947214076246}}]
-----------------------------------------------------------------------------------------------------------------------------------
p306:
Extractive Summary:
background in january 2019, the patient autonomy act of taiwan (hereafter, “the act”), the first of its kind in asia, officially went into effect, allowing taiwanese citizens to establish a legally binding advance decision (ad).an ad enables someone to refuse specified medical treatment in the future when they may lack the capacity to consent to or refuse medical treatment [1].according to the act, a patient must complete an advance care planning (acp) consultation with a medical team before documenting their wishes with a formal ad.observations suggest that taiwan citizens had a strong desire for self-determination in endof- life medical decisions but were limited by insufficient knowledge of medical treatments, clinical scenarios, education, and legal literacy.there is a great deal of clinical knowledge and legal terms that have to be explained and presented in a way that the general public can understand.in the past, medical options have typically been written on handout forms provided to patients with a short conversation between medical teams and patients before patients made their choices.a previous neuroscience study has shown that video decision tools enable patients to be more informed and confident about their medical decision making, because the visual cortex under video-watching prompts the brain in decision making [8].we developed a virtual reality (vr) video in 2017 to supplement the standard way of decision making before acp and to determine more realistic and accurate perceptions of patients as they explore their values and preferences for end-of-life treatment.virtual reality (vr) is a computer technology that provides users with a simulated and immersive experience.vr may make it easier for users to understand aspects of life-sustaining treatment (lst), and to arouse emotions that will make their decision making more aligned with their true values.in this study, we examined whether vr video can help users make end-of-life decisions and clarify their preferences by comparing their choices for end-of-life medical treatment before and after experiencing the vr video and collecting their feedback on the research experience.methods participants this study had a total of 160 participants, 40 of whom were randomly assigned to the control group and 120 of whom were randomly assigned to the intervention group with the vr video.design the language used in the handout and questionnaires was traditional chinese.participants who were unable to decide whether to use treatments if they fell under specific clinical conditions, or were unconscious, or unable to clearly express their wishes could choose “uncertain.participants were asked to read a handout published by hospice care foundation of taiwan with clear information including the introduction of the act and an illustration of the legal process for making an effective ad to help the participants understand their fundamental right to make medical choices for themselves.technical expertise was provided by a commercial company that specializes in vr techniques.data were organized using spss 22 and analysis was performed using r software (version 3.6.0) and r package “exact2x2” (cite: fay mp (2010).“two-sided exact tests and matching confidence intervals for discrete data.dnr means the “do not rescue form,” a form signed by a patient or their closest relatives including the options about refusal of cpr and lst during the predying status under severe illness or injury.preference for treatment options figure 2 illustrates the control group’s change in the percentage of individual preference for the five medical treatments before and after participants read the handout.this trend was not observed in the statistical analysis of the control group.to my knowledge, this study represents the first decision tool for acp using vr technology.meanwhile, preference for not using these medical treatments had the opposite trend after watching the videos.the findings of our research were consistent with previous research that aimed at enriching patient understanding of worsening health states and informing their decision making with the use of a video decision tool [6].end-of-life decision making has never been an easy task for people, especially in asian cultures where it is taboo to talk about issues of death and palliative care.the hospice palliative care ordinance of taiwan (hpcot) was passed in 2000, with the aim to promote hospice palliative care and dying with dignity and to respect the wishes of patients with terminal illness and their right to personally decide about medical treatment.although asian culture still regards talking about death as taboo, hpcot has had a significant impact on dnr rates (fig. 5) [18].in response to this, we set a storyline for our video, and the scene changed from the icu to a hospice home care setting from the first-person perspective.the way doctors spoke in the film followed the disease progression with the option of hospice care.according to the feedback, the vr video significantly helped equip users with a better understanding of medical scenarios.in addition, during development we collected professional opinions from medical staff as well as members of an interdisciplinary team including psychologists and senior social workers.the traditional way for presenting acp information to patients includes verbally communicating the given scenario between the medical team and patients [22].acp involves far more than merely establishing an ad for certain treatments; additional factors include family dynamics, emotional response, and the values of patients.
number of words= 845
[{'rouge-1': {'f': 0.4557538782909587, 'p': 0.7337681159420291,'r': 0.3305233219567691}, 'rouge-2': {'f': 0.1918322105538533, 'p': 0.2705813953488372,'r': 0.1485876993166287}, 'rouge-l': {'f': 0.40337557404366525, 'p': 0.632130177514793,'r': 0.2961904761904762}}]
-----------------------------------------------------------------------------------------------------------------------------------
p307:
Extractive Summary:
we sought to gauge physicians and nurses’ self-reported perspectives and current practices on addressing spiritual care.provision of spiritual care has also been known to influence medical treatment intensity at the end of life [5].a systematic review on discussing spirituality in a medical setting demonstrated that patients desire to discuss spiritual concerns, but medical teams infrequently provide spiritual care; however, life-threatening illness can make a physician more likely to provide spiritual care [6].spiritual care has received increased attention in taiwanese inpatient settings in recent years, with the first private hospice in taiwan founded with a christian orientation and the first public hospice at national taiwan university hospital with a mahayana buddhist orientation in the 1990’s [1, 8].clinical buddhist chaplains (cbc’s) remain a mainstay of taiwanese hospice care, and often provide spiritual guidance to patients and families during life-limiting illness [1, 9].of note, while the term “spiritual care” in mandarin chinese may hold an additional connotation of death acceptance and subsequent peace of mind, the term itself serves a similar function in chinese as it does in english for purposes of hospital care [10].hospice and palliative care physicians in the national taiwan university hospital are primarily family medicine physicians, who receive mandatory hospice training in taiwan; spiritual care, including learning the role of cbc’s, is a mandatory topic in their training curriculum [12].for nurses working in hospice, more initiatives in spiritual care training and research have been undertaken across taiwanese institutions [8, 13].in taiwan, caring for patients with advanced illness has been influenced by sociocultural taboos surrounding speaking the truth of adverse health circumstances openly [14–16].based on taiwanese end of life care literature suggesting 1) great interest in spiritual themes in inpatient care and 2) difficulties in candid discussion of death and dying due to sociocultural norms, interviewing physicians and nurses about their experiences in spiritual care can yield valuable information about how spiritual care is perceived and practiced in the inpatient hospice setting and what challenges healthcare staff may face by involving themselves in spiritual care [15–18].methods we recruited a total of 20 participants through purposive sampling during weekday working hours within the palliative care unit physician and nurse working area of a single tertiary care hospital (table 1).we identified physicians and nurses who had worked in hospice care for at least 1 year at national taiwan university hospital (ntuh); we excluded those participating as study personnel for concerns of biasing interviews.irb approval was granted from national taiwan university hospital [irb number 201805065rinc, approved 6/8/ 18], and all individuals signed a written consent form before participation.interviews were conducted in july 2018 by zt, a medical student with a degree in religion and psychology with prior experience in qualitative research unaffiliated with ntuh outside of the study.prior to initiation of interviews, she was observed and given feedback by the supervising faculty members of the study practicing and recording 3 mock interviews with family medicine residents who had worked in the hospice ward.demographic information was obtained from participants immediately prior to the interview.interviews lasted an average of 20 min and were semi-structured in nature, where participants responded in an open-ended manner to a series of four set questions with encouragement to elaborate if applicable and verbal probes to share experiences [19].interviewees were given a written copy of the questions as a guide (fig. 1).interviews were conducted in mandarin chinese and recorded using a portable recording device.the attending physician members of the study did not have access to interview transcripts or recordings for house staff confidentiality.interviews were transcribed in chinese using dictation software, proofread and edited by a staff research assistant, then translated into english by the second student assistant.the interviewer was present at every part of the transcription and translation process and was available to answer questions and resolve discrepancies where present.data analysis we employed a qualitative design with semi-structured interviews and a directed approach to content analysis of the interviews.directed content analysis is an approach in qualitative methodology which validates or expands on an existing theoretical framework or theory and differs from conventional content analysis in its usage of pre-determined broad categories for analysis [19].interview content was organized using corpus tool software.we employed prolonged engagement (zt), peer review, clarifying researcher bias, a second coder, and development of a coding system derived from the interview questions (fig. 1) for data reliability and validity [20].“in clinical settings, we aren’t able to tell patients where they’ll go [after death] for sure.“a lot of patients would think that they won’t get better, they’re just dragging the time out, just spending money, spending family’s money, spending family’s time and energy, and becoming a really heavy burden.so that patient didn’t really talk much.challenges to providing spiritual care participants listed the relative emotional, physical, and social status of the patient, especially altered consciousness, as one of the greatest challenges to effectively providing spiritual care for their patients.” an emphatically and frequently cited challenge to providing spiritual care was providers’ lack of preparedness to deliver spiritual care.while literature on hospice care in taiwan has made mention of spiritual themes at the end of life, to our knowledge, this is one of the first studies which focuses on the topic of spiritual care at the end of life from taiwanese hospice physician and nurse perspectives.we developed a preliminary understanding of taiwanese spiritual care in hospice care from the perspective of physicians and nurses through a directed content analysis of semistructured interviews.regarding how providers define spiritual care, we found that our participants frequently noted that spiritual care was difficult to provide a concrete definition for.lastly, in response to being asked about challenges to spiritual care, our participants reported the general state of the patient and providers’ lack of preparedness to be barriers to provision of spiritual care.combined with providers’ self-perceived lack of preparedness to address spiritual concerns, this paucity of consistent, discrete definitions of spiritual care may stem from both a lack of professional training in spiritual care and a lack of clear definitions and roles for spiritual care on a more global scale.studies in taiwanese inpatient settings have defined spiritual care with elements of existentialism, religiosity, and personalized care, with documentation in the literature of spiritual care curricula developed specifically for nursing staff in taiwan using such elements; presence of such curricula may vary greatly from institution to institution [1, 25–27].this emphasis on relationship loss as a key cause of spiritual distress parallels a qualitative study in the midwestern united states which interviewed dying cancer patients about their experiences of illness; however, unlike in this american study, our hospice healthcare staff did not consistently mention relationships to god as an essential part of spiritual care [29].our results from our population of mostly nonreligious healthcare providers are surprisingly compatible with a study surveying jordanian muslim nurses, whom despite their overt religious identification reported providing spiritual care that was more existential rather than religious in nature and infrequently made explicit references to god [30].while the emphasis which providers placed on communication and lack of candid discussion indicates that truth-telling may influence spiritual care provision, providers did not specify whether challenges arose from talking to patient or to family; rather, they referenced the difficulty of having conversations about end of life care in general.lastly, we did not separate perspectives of physicians from nurses during content analysis but treated them as a whole.despite more extensive literature on initiatives in spiritual competency for nurses than physicians in taiwan, the ramifications of these training differences for physician and nurse perspectives on spiritual care did not stand out in the present study and this represents another potential limitation [8, 11, 13, 26].conclusions using a qualitative methodological approach, we identified several themes from taiwanese physicians and nurses’ self-reported practices of spiritual care in a single academic institution’s palliative care unit.
number of words= 1291
[{'rouge-1': {'f': 0.3235121267583579, 'p': 0.8365198237885463,'r': 0.2005326331582896}, 'rouge-2': {'f': 0.18113742342207576, 'p': 0.36646017699115047,'r': 0.12030030030030031}, 'rouge-l': {'f': 0.29392517128129003, 'p': 0.6196183206106871,'r': 0.19265758091993185}}]
-----------------------------------------------------------------------------------------------------------------------------------
p308:
Extractive Summary:
according to sweden’s national clinical cancercare guidelines, all significant others should be offered bereavement support regardless of where or how their loved one died.grief when the deceased has died following to a chronic illness, the bereaved have time to anticipate and prepare for the death.although most people experience normal grief and cope well.research has shown that, after 6 months, it can be distinguished from complicated grief [6, 10].complicated grief is characterized by continuing severe distress and dysfunctional thoughts, feelings or behaviours related to the loss.a feeling of yearning persists and includes a strong desire to be with the deceased, along with an inability or refusal to accept his or her death, preoccupation with thoughts about the deceased, and a habit of keeping reminders close.complicating factors are rumination about the death and unrelenting bitterness or anger.complicated grief and bereavement have also been associated with increased physical morbidity such as cardiovascular events, pulmonary embolism, and acute coronary syndromes [5].a good death the aim of bereavement support is to ease the process of grieving and strengthen the ability of the significant others to cope [7].’s [12] questions for retrospectively determining whether the deceased had an appropriate death.seeking information about the significant other’s preferences for the care of a terminally ill loved one offers the same measure of dignity [9, 13].yet significant others can struggle following the death of a loved one if they have a desire to talk to somebody and do not know where to get help [21].aoun, breen, white, rumbold and kellehear [22] found that informal support from family, friends or funeral providers was perceived as very helpful (92–88.3%: n = 678).aim the aim of this study is to describe significant others’ experiences of participation in bereavement groups.methods setting the study setting was a county in northern sweden with about 130,000 inhabitants in an area of 49,935 km2 (< 1% of the total swedish population and 2.7 inhabitants per km2).accordingly, within that time frame, the social workers and clergy members or deacons working at the hospital extended an invitation to the bereavement group by telephone.the bereavement groups followed the same rationale recommended by the swedish church, with an underlying theory of potentially positive results through the verbalization and re-exposure to grief in a safe group context.counsellors for the groups included a nurse from the palliative home-care team, a social worker, and a clergy member or deacon.all had several years of clinical experience meeting with and counselling the bereaved significant others of palliative-care patients.at the same time, the swedish church also provided bereavement groups with the same concept to bereaved persons, independent of the cause of death.data collection this study is part of a larger project that aimed to analyze quantitatively the effects of bereavement groups on grief, anxiety and depression [17].the wording for the question was as follows: “if you participated in the bereavement group, what role has it had in your processing of grief?the total data set resulted in a word count of 3652 words, divided into 158 meaning units.questionnaires with this open question were completed 5 weeks post-participation and 1 year later as a follow-up.data analysis reading the written comments made the researchers consider how to analyze them.the 6s seemed to be a relevant tool, in this concept evaluating the bereaved persons’ feelings of a good death [8, 9].self-image in regard to self-image, significant others described their experiences of participating in bereavement groups as a positive form of support.a, whose age was between 61 and 70 years, commented 5 weeks post-participation: i [now] better understand that those who do not have someone in their own household to share their grief with have much more difficulty.they seemed unable to move on from their grief 5 weeks post-participation: i still have a hard time and i cry a lot, but i guess that’s the way i am (mrs.b, age 61–70 years).over time, the follow-up answers illustrated that the groups gave the participants a feeling of commonality but that grief persisted.c, age 61–70, wrote: however, grief goes up and down and it needs to do so.the bereavement group supported them in regaining a sense of harmony within themselves by being co-actors in the group.e, age 41–50).these comparisons were mostly positive, but some did not want to share their grief in this setting.f, age 61–70, wrote 5 weeks post-participation: i do not want to discuss my grief with unknown people; i process it by myself and with my relatives.possibly smaller groups – maximum of five significant others.i, age 51– 60, illustrated this: i do not know how only the bereavement group has influenced my grief.starting with adequate medication was helpful; mrs.k, age 61–70, wrote 5 weeks postparticipation: it [the bereavement-group] had a great impact, where each one could tell and cry about the trauma that everyone had experienced.it was of great value to me, but the grief is still just as big (mrs.l, age 61–70).the bereavement group’s structure of five meetings over time was designed for passing through the different phases of grieving up to the death, at the time of death, and the period following the death and the funeral 5 weeks post-participation.i began healing and gained insight on how others experienced their grief (mrs.m, age 61–70).participation in bereavement groups gave significant others strategies to help them realize that life goes on and grief reaches closure, although it is always present.mrs.the bereavement groups were considered very valuable for providing a context for grief.however, the answers also indicated the high value placed on the earlier close contact with the palliative home-care team.thank you for being there.you do an incredible job.thank you for being there (mrs.discussion care planning for grieving significant others is as important as the care of the dying person.the 6s model, therefore, was found to be a well-functioning tool for analyzing grief among significant others.however, not all significant others used the bereavement groups as a catalyst to affirm their self-image.identifying those individuals who are likely to benefit from bereavement services should be part of the palliative-care process before the death of their loved one.other findings have shown, for example, that psycho-educational group interventions for significant others before death can increase competence for caregiving and preparedness for the loved one’s end of life [32].followup support to some significant others may need to be continued for a longer time.to reveal one’s inner feelings in a group can be difficult and may require self-determination.research shows that being an ageing couple can imply that friendships outside the relationship are lacking.however, this study’s findings illustrate that the bereavement groups had a positive influence on social relationships close to the death of the deceased.significant others experienced that participation as providing a sense of coherence and understanding that they could share with others in the same situation in connection to their loved one’s death, as confirmed by blackburn and bulsara [19].members’ relationships with the deceased influenced the group’s responses; in oliver et al.’s study, spouses offered significantly more support to each other.however, the findings in this study did not seem to indicate that the symptoms of physical needs were alleviated.rather, the benefit seemed to be psychological.findings from the present study demonstrate that bereavement groups were a source of a feeling of synthesis in the retrospect of the loved ones’ death.findings from rural australia also confirm this.the earlier study by näppä et al. [17] revealed that living a large geographic distance from the groups could be a reason for not participating in bereavement groups.in the internet age, a way to manage time and distances might be the use of online services for bereavement.this already exists but is, in general, directed to specific groups, for example, bereaved parents after the loss of a child [33, 34] or bereaved children, for example, as through canteen [38], a website that has a page for children with parents suffering from cancer.members in the groups benefit from opportunities to give, not just receive, assistance in the form of support, understanding, comfort, and suggestions about how to go on in life.the relationship that developed earlier seemed to be more valuable, yet they participated in the bereavement groups.therefore, it is important to be clear regarding the setup of work by the team while the care is on-going.implementing some kind of program, for example, the one described by holm et al. [32] with three sessions by health professionals involving significant others in palliative caregiving, can be a way to support the lives of significant others following the death of a loved one.methodological considerations using the 6s for analysis may be controversial since it was developed with the goal of having a good death.our belief is that, if the significant others experienced that their loved ones had had a good death, their own experience of grief could be connected to the 6s’s.studying the effectiveness of bereavement support should align with the actual settings to the greatest extent possible [42].the aim of the bereavement groups studied in this research was to provide a sense of the loved one having had a good death.an advantage of participating in such groups was that they helped for the significant others to capture and share expressions for enhancing their self-image, self-determination and their needs for synthesis and summation.significant others may remain in a state of complicated grief.to identify these persons, a routine of planned contacts with the bereaved should begin before the death of the loved one in a controlled way and be followed up later than 6 months post-death.the findings of this study have provided valuable knowledge about the needs of bereavement support for significant others participating in bereavement groups and have implications for health care and social welfar
number of words= 1597
[{'rouge-1': {'f': 0.3667095517638668, 'p': 0.8512500000000001,'r': 0.2336904761904762}, 'rouge-2': {'f': 0.21678878579862268, 'p': 0.428974358974359,'r': 0.1450446694460989}, 'rouge-l': {'f': 0.35820801838276445, 'p': 0.6895652173913043,'r': 0.24194570135746607}}]
-----------------------------------------------------------------------------------------------------------------------------------
p309:
Extractive Summary:
us hospice is for people who have a prognosis of 6 months or less [9], while uk patients can access hospice care at any stage of life-limiting illness [10].what is important is hospice is a philosophy of care that, like palliative care, focuses on care not cure [6].in the study location (uk), over 200 hospice units exist, collectively supporting around 200,000 patients annually, free at the point of service.they receive an average of 32% of funding from the government, with the rest coming from fundraising [11].services are delivered through inpatient units, outpatient and day care centers, programs (e.g. living well, bereavement), and clinics (e.g. breathlessness clinic), and hospice@home services, with the combination of services unit specific.uk hospitals also offer palliative care; some have bespoke inpatient palliative care units, while others offer services to outpatients via a variety of clinics similar to those offered by hospices.our study focuses on specialist palliative care services delivered either through hospice or hospital-based bespoke palliative care services.recently, palliative care providers have striven to expand and reach consensus regarding key quality of care indicators.in the uk, projects include ‘listening differently to users’ [12] and ‘every moment counts’ [13].well-known surveys include the palliative care outcome scales [14] aimed at patients, the sparc questionnaire which is an early screening tool that ascertains palliative care needs [15], and the carer support needs assessment tool [16] which aims to assess carer support needs.examples of us initiatives include the measuring what matters program [17] and the cahps hospice survey [18].there are also some commendable attempts to validate international measures, including the impa ct project spanning five european countries [3].such initiatives focus on quality measurement and assessment techniques to drive service improvements [17].however, major gaps in evaluation remain.most of the available instruments comprise short satisfaction surveys.though important, metrics fail to capture overall user experience and provide little information about how people felt.some outcome measures are prone to floor and ceiling effects [19], and high misresponse levels [20].many measures are disease or location specific [9].systematic reviews reveal no survey that incorporates all areas important to palliative care patients and their families [21, 22] with many evaluations failing to take place within clinical practice [23] and often failing to detail systematic processes for developing quality indicators [24].currently, there is substantial variation in the quality of palliative care that people receive [1, 4, 25], and many recent calls for more research into ways of evaluating it [3].well-documented data capture difficulties when researching palliative care also hinder research.difficulties include variable definitions of key issues [26], a hesitancy to discuss mortality [27], and a tendency for patients to formulate perceptions of palliative care solely on the basis of the interpersonal relationships they have with care providers which often leads to agreement that their care is superb [28].additionally, high participant attrition and a prevailing view that many hospice patients are too vulnerable to participate in research [8] has led to much research relying solely on retrospective views of family members [22], which of course misses key inputs from patients.capturing patient perspectives is pivotal for evaluating quality [29] and patient-centeredness.despite considerable debate regarding what exactly comprises patientcentred care [30] and certainly the concept is not without its critics [31], patient-centeredness is part of the larger transition from paternalistic professional and passive patient to a more collaborative partnership, both in the clinical encounter and more recently to include organisational structures, cultures, leadership, and processes [32, 33].people experience healthcare services holistically, so research should incorporate the whole range of service dimensions that may impact that experience.taking the argument further, a shift in focus from ‘patient- centred’ to ‘person-centred’ care is not just about semantics.person-centred care is “understanding what is important for the individual as a person, not just a patient with a condition.this understanding facilitates discussions and shared decisions about personalised care planning and management” [34].acknowledging that a universal definition of person-centred care is elusive, the health foundation [30] offers a person-centred care framework that 1) affords people dignity, compassion and respect; 2) is co-ordinated; 3) is personalised to fit around individual needs; and 4) is enabling in that that the traditional balance of power shifts from the professional being paternalistic to a more collaborative partnership.additionally, person-centred care includes families and caregivers [31], which is important because family dynamics strongly influence individual experiences of palliative care [35].thus, being person-centred is about focusing care on the needs of the person rather than the needs of the service [36].to shape the service around user needs, an in-depth understanding of these needs is required.this concept is at the heart of service design, which provides the techniques required for shaping services in a way that ensures they are personcentred, systematic, and collaborative [37].it is on these foundations, i.e., the need to develop a novel way of understanding and shaping specialist palliative care services, using a methodology that is personcentred, that captures overall user experiences, irrespective of the nature of their illness and where they receive their care (hospital, hospice, at home), and that uncovers real opportunities for service improvement, that the trajectory touchpoint technique (ttt) was designed.the aim of this paper is to introduce the ttt to palliative care research.first, we explain the underlying theories and concepts used to design the ttt.we then detail its application across multiple palliative care settings with a variety of palliative care service users.we define users as patients and their families.finally, we evaluate the benefits of the new technique.methods to design our new ttt, we used design science research (dsr) as our overarching method.dsr is a process that focuses on creating innovative artefacts that solve organisational problems [38].we followed the well-established dsr process [39], which incorporates three major phases: design, apply, evaluate.the design phase includes setting objectives and actual artefact design.the dsr process then moves to application, where the artefact in used real settings.these two steps – design and application – are detailed in our methods section here.the final dsr stage is evaluation.we evaluate our artefact – the ttt – against our objectives in the results section of the paper, and against alternative methodologies in our discussion.solution objectives our preceding discussion outlined three major issues to consider when developing our objectives: the limitations of existing palliative care evaluation methods; specific data capture difficulties that beset palliative care research; and the need for a person-centred methodology able to capture the full service experience.consequently, our objectives are that our new methodology should: 1. be user friendly for all palliative care service users irrespective of illness type or place of care.2. systematically capture the lived experiences of service users, including subjective and abstract dimensions.3. enhance understanding of what ‘good palliative care’ means from the service user’s perspective.4. though innovative, be practical and functional from the researcher’s perspective.5. uncover feasible opportunities for improvement to palliative care services.kernel theories new artefacts start with kernel theories and current available methods and use these as building blocks to design better artefacts [39].evaluation of potential kernel theories from the service design and service systems literature led us to utilize six different concepts as our building blocks in designing the ttt, which we now explain.service blueprinting service blueprints entail breaking down services into their logical components, establishing different steps in service processes, and examining the execution of these steps in order to map a service at multiple levels [40].our service blueprint of ‘blue hospice’, our first collaborating organisation, comprised service user actions (e.g., accessing different information channels); service user-provider contact (with doctors, nurses, counsellors); support processes (e.g., admission processes) and the physical evidence that impacts the service experience.in sum, service blueprinting forces service designers to consider countless tangible and intangible elements that impact service experiences.servicescapes often overlooked in health, the physical, sensorial, and social dimensions of services are key issues, yet little research investigates the impact of servicescapes on the psychological and social needs of patients, as well as their coping resources [41, 42].servicescapes incorporate three major dimensions: ambient (e.g., cleanliness, olfaction, noise, furniture, visual attractiveness), design (e.g., architecture, comfort, signage), and social.servicescapes have recently been recognized as extremely pertinent to people at end-of-life [43].touchpoints touchpoints represent any point of contact between a service user and any aspect of the service [44].touchpoints are also “clusters of experiential elements that foster product or service experiences” [45].hence our touchpoints incorporate issues central to palliative care, such as quality-of-life, spirituality, dignity, and ability to cope with grief; acknowledging that palliative care experiences comprise cognitive and emotional elements as well as physical, sensorial, symbolic, and social [42].we wanted participants to discuss those touchpoints that they felt were important, hence a crucial part of the ttt is that people are free to select those touchpoints that resonate with them.nevertheless we had to begin the process, so we identified as many potential touchpoints as possible, from the sources in table 1.journey mapping the ttt examines the flow and processes involved in accessing different palliative care services.as such, it considers the uniqueness of individual journeys [10], which are often nonlinear and incorporate different locations (hospital, home, and hospice).this process, known as patient journey mapping, is reflected in the ttt together with emotional and physical touchpoints [42, 46].consequently, the ttt blends these two different approaches to incorporate opportunities to explore different experience dimensions.the starting point for a service journey is when the user needs the service, not when they first come into contact with the provider.in order to understand access to palliative care, our methodology needed to capture the point at which the patient needed palliative care, through to the present day.for some, the present day is days from death, for others it is a state of bereavement.using the journey concept, we grouped the touchpoints into 7 categories, each representative of one stage or major aspect of the palliative care journey experience.the seven sets of touchpoints are detailed in fig. 1.rich picture methodology we used rich pictures in the form of cartoons to depict each potential touchpoint.rich pictures enable people to explore their subconscious [47] and can act as an aide-memoir to help patients to construct their stories [48].increasingly used in health research, rich pictures are a systems thinking tool useful to explore complex phenomena [49], and can reveal issues missed when alternative methods are used [50].in the first iteration of the ttt we used real photographs and cartoon drawings.however, during early testing of the ttt it became clear that people far preferred the cartoons, hence the images in fig. 1.we printed each set of touchpoint images onto large laminated cards and uploaded them onto tablets.what is important is that respondents are free to use all, none, or some of the images.experienced based co-design (ebcd) designed for healthcare services in the uk, ebcd uses various qualitative techniques (interviews, observations, group discussions) to identify emotionally significant key elements – or touchpoints - of a healthcare service.a video then shares key findings with a group of staff and patients to explore ways to improve the care pathway.central to ebcd is its qualitative focus on storytelling which allows for rich insights into experiences, ultimately enabling codesign and service innovation [51].
number of words= 1830
[{'rouge-1': {'f': 0.31035562201832584, 'p': 0.7750147492625368,'r': 0.1940269849507006}, 'rouge-2': {'f': 0.17619048035747387, 'p': 0.34514792899408286,'r': 0.11828660436137073}, 'rouge-l': {'f': 0.291733752340198, 'p': 0.6205617977528091,'r': 0.19068965517241382}}]
-----------------------------------------------------------------------------------------------------------------------------------
p310:
Extractive Summary:
background epilepsy is one of the most common neurological disorders, globally as well as in thailand.the prevalence rate of epilepsy in thailand was approximately 7.2 per 1000 population [1].anti-seizure medications (asms) are the mainstay of treatment.the drugs have various effect and adverse effect profiles, as well as mechanisms of actions.among other things, they cause cognitive impairment [2].additionally, vitamin d deficiency is another known adverse effect of asms [3] essentially, 90% of the vitamin d in the human body is synthesized in the skin following exposure to sunlight, particularly to ultraviolet b with a wavelength of 209–305 nm, which is most abundant in sunlight between 10 am and 2 pm.substrates in the skin are then converted to vitamin d3 (cholecalciferol).foods such as cod liver oil, milk fat, butter, animal liver, and egg yolk are another source of vitamin d, in the form of vitamin d2 (ergocalciferol).both vitamin d3 and vitamin d2 are metabolized in the liver and converted by 25-hydroxylases into calcidiol (25[oh]d), a prehormone that is the major circulating form of vitamin d and is used to determine an individual’s vitamin status.circulating 25(oh) d is eventually metabolized in the kidneys to a more biologically active form known as calcitriol (1,25(oh)2d), which serves various functions such as increasing calcium and phosphorus absorption in the intestines, inhibiting the secretion of parathyroid hormone, and modulating the formation and development of bones and teeth [4].the prevalence rate of vitamin d deficiency in epileptic patients in the united states is reportedly 11.9% in adults and 25% in children aged between 3 and 17 years [7, 8].we enrolled 138 pediatric epilepsy patients aged 1–20 years at the neurology pediatric clinic at phramongkutklao hospital from april 2018 to january 2019.patients that had abnormal kidney or liver function, received vitamin d supplements, or were on a ketogenic diet containing vitamin d were excluded.if developmental delay was suspected, patients under 6 years of age were assessed with mullen scales of early learning (msel), and the children older than 6 years of age were tested with the wechsler intelligence scale for children-third edition.thus, 71% (n = 98) of the patients had hypovitaminosis d. demographic characteristics the demographic characteristics of all 138 pediatric epilepsy patients are shown in table 1.the most commonly used newer asm was levetiracetam (80 patients), followed by topiramate (53 patients).seventeen and 36 patients received a low (less than 5 mg/kg/day) and high (5 mg/ kg/day or more) dose of topiramate, respectively.multivariate logistic regression analysis adjusted for other risk factors such as gender, puberty status, duration of sun exposure, monotherapy drug, enzyme-inducing asms, enzyme-inhibiting asms, newer asms, and more than 2 years of asm use suggested that puberty status and asm type were significantly related to hypovitaminosis d. specifically, patients who had undergone puberty had a 5.4-times (95% confidence interval 1.9–15.7) higher risk of hypovitaminosis d than pre-pubescent patients.however, when we considered both vitamin d deficiency and vitamin d insufficiency together as hypovitaminosis d, our results indicated that approximately two-thirds of pediatric epilepsy patients had hypovitaminosis d, even though thailand is located in the tropical zone.interestingly, 53 of the 138 patients (38%) were receiving an enzyme-inhibiting asm (valproate).we found that more patients in the high dose group had hypovitaminosis d than in the low dose group (80.6 and 58.8%, respectively), although this difference was not statistically significant (p = 0.092, fig. 2).conclusion our study shows a relatively high prevalence rate of vitamin d deficiency and vitamin d insufficiency of 23.2 and 47.8%, respectively, even though thailand is located in the tropical zone.the key risk factors of hypovitaminosis d are puberty status and non-enzymeinhibiting asm use.our findings suggest that clinicians who treat pediatric epilepsy patients, especially those entering puberty and being prescribed non-enzyme-inhibiting asms should consider regular monitoring of vitamin d adequacy.
number of words= 623
[{'rouge-1': {'f': 0.44846675886557086, 'p': 0.6834751773049645,'r': 0.33371951219512197}, 'rouge-2': {'f': 0.21456675891805577, 'p': 0.2977580071174377,'r': 0.16770992366412213}, 'rouge-l': {'f': 0.40053792972819396, 'p': 0.5525581395348838,'r': 0.3141176470588235}}]
-----------------------------------------------------------------------------------------------------------------------------------
p311:
Extractive Summary:
background the public health burden of undernutrition remains heavy and widespread, especially in low-income countries like nepal, where rates are still very high, particularly in certain subgroups of the population [1].the prevalence of wasting in children under 5 years of age is estimated at 6.1% in the mountains, 6.4% in the hills, but 12.2% in the terai [3].the authors cited several factors beyond economic isolation as potentially contributing to the high stunting rates observed in the mountains, including differences in agricultural production and micronutrient deficiencies (zinc and iron).intermediate and underlying factors associated with stunting include the duration of breastfeeding, low size of baby at birth, low socio-economic status, economic isolation, and low and unpredictable rainfall.few studies have examined metrics of political will and quality of policy implementation relevant to nutrition, and their potential to support actions targeting factors associated with stunting.the concept of good governance (defined as “the effective implementation of national policies”) provides a platform for questioning the long menu of institutional changes and capacity building initiatives that are important and essential for development [10].good governance for nutrition entails four key elements including efficiency, accountability, transparency, and participation [11–13].most studies on nutrition governance have focused on evidence at a national or global level and the role of governance, focused on nutrition improvements, at the sub-national level has received limited attention.a critical gap remains in understanding the effectiveness of nutrition governance with a focus on the role of capacity, resources, information, coordination, and collaboration in achieving nutrition gains at the sub-national level [14].the paper is organized as follows: we start by presenting the study design for the two surveys used in this analysis.then the generalized estimating equation (gee) model and multilevel model methods are described, before presenting results, including findings from regression analyses for both statistical approaches.we end the paper with discussion of the findings, articulation of study limitations, and recommendations for further research.measuring the quality of nutrition governance measuring the quality of ‘governance’ is challenging.there is no consensus on a single theoretical definition of governance, thus it is difficult to determine what to measure.yet there is need for governments to know if they are on track in improving governance systems in order to assess impact at the grassroots level and/or identfiy areas for improvement.to address this gap, several metrics have been proposed, including the world governance index [15] and the hunger and nutrition commitment index (hanci) [16] among others, each with advantages and disadvantages.they collected data from focal point personnel in 30 municipalities and cities and created a single score out of four domains: 1) nutrition policies and programs, 2) organizational structure and resources for nutrition, 3) efficiency, and 4) accountability, transparency and participation.cities and municipalities with a score above the median were defined as communities with “good governance”, that is, a governance structure supportive of nutrition.furthermore, these good governance communities also had significantly lower underweight and stunting levels compared to those communities with lower scores (or poor governance structures).while these findings are in line with those of previous authors’ [18], the results obtained were based on bivariate analyses, thus were not robust because they did not account for possible confounding factors.more recently, namirembe et al. [19] developed a nutrition governance index (ngi) that captures the effectiveness of nutrition governance at a sub-national level.this novel metric is an aggregate of five domains, as follows: i) understanding of nutrition and responsibilities by policy implementers, ii) collaboration, iii) financial resources, iv) capacity, and v) institutional support.nutrition knowledge may arise from scholarly pursuits or community and societal engagements, supported by government and non-governmental initiatives.examples of these include the usaid-funded suaahara project and the multi-sector nutrition plan (msnp) [24], which sought to improve mother’s infant and young child feeding knowledge and behaviors, provide nutritional counseling and raise awareness regarding better health and nutrition practices, among other objectives to fight malnutrition in nepal.the suaahara project was initiated in twenty districts of nepal from 2011 to 2016.the msnp is a collaborative multi-national partnership spearheaded by the government of nepal to improve maternal and child nutrition and reduce chronic malnutrition, largely through evidence-based nutrition interventions [30].statistical analysis the statistical analysis focuses on two child anthropometric measures: height-for-age z-score (haz) and weight-for-height z-score (whz).age squared was included to account for any non-linearity in the relationship between age and outcome.mother-specific variables included duration of formal schooling, bmi and age.after adjusting for other known predictors of stunting and wasting, we found that this relationship was positive for children over 2 years of age.they found that approximately 6% of total variance and 22% of explained variance in heightfor- age z-scores occurred between districts [43].the multilevel approach revealed that ngi explained a nonnegligible amount of variation in haz and whz, which underscores the fundamental role that good governance plays in promoting child nutrition and growth.average child’s weight was 11.20 kg (95% cl: 10.97–11.43).the prevalence of stunting and wasting was estimated at 34.0% (95% cl: 33–35%) and 14.0% (95% cl: 13–15%) respectively.seventy-two percent of the households were food secure in 2014; slightly more households were food secure in 2016 (79.10%).distance to the market and child’s diet diversity were negatively associated with haz and whz.the ngi score was aggregated from its five constituent domains.collaboration, financial resources and support domains were significantly and positively associated with haz in older children as was the knowledge domain for whz.results from the multilevel models are shown in table 4. because haz variation across vdcs was small (fig. 1a and b) and the vdc-level analysis must be based on a small set of observations (21 vdcs), we investigated vdc-level effects using only one community-level variable, ngi.interpretation of results was restricted to model 4 for both outcomes as it was the best-fit or most improved model based on possessing the smallest aic estimate.this means that adjusting for ngi together with its interaction with age, improved model fit for both outcomes.from model 1, we calculated a significant icc of 0.07 (95% ci, 0.04, 0.12) for haz and an icc of 0.17 (95% ci, 0.10, 0.28) for whz.this indicates that 7 and 17% of the overall variances in haz and whz were accounted for by variations across vdcs.when the ngi was zero, the average estimated haz for all vdcs was -1.49 and the average estimated whz was -0.75.the covariance estimates for both the intercept and residual were significantly different from zero.this implies that average haz differed considerably across vdcs (intercept = 0.116, p < 0.001) and there was more variation in haz among children within vdcs (residual = 1.555, p < 0.0001) as reflected in the low icc estimate.no reported fever, maternal education and maternal bmi were all positively associated with haz and whz.the covariance parameters for the intercept in model 4 were unchanged compared to previous conditional models for the same outcome.this minimal decrease in variance meant that ngi and its cross-level interaction with age, explained only a small portion of the between-vdc variations in haz and whz.specifically, 19% (0.116– 0.094/0.116) of the explainable variation in mean haz and 42% (0.199–0.115/0.199) of the explainable variat
number of words= 1165
[{'rouge-1': {'f': 0.38692115745867656, 'p': 0.8063344051446946,'r': 0.25452860596293314}, 'rouge-2': {'f': 0.23242152426660057, 'p': 0.4280645161290323,'r': 0.1595161290322581}, 'rouge-l': {'f': 0.3803284729976581, 'p': 0.6689847715736041,'r': 0.2656882255389718}}]
-----------------------------------------------------------------------------------------------------------------------------------
p312:
Extractive Summary:
millions of children require hospitalization, and some endure lifelong disabilities that could negatively affect their stages of development.teenagers between 15 to 19 have an increased risk due to increased exposure to hazards and risk-taking behaviours.gender also plays a role in the risk of fatal and nonfatal injuries among children [7].further understanding of the epidemiology of child injuries will facilitate efforts in prevention and guide future research to understand the magnitude of the problem.methods search strategy and study selection: a literature search was performed in january 2021 using scopus, medline, and web of science for any study published in english between january 2000 and december 2020.we used search limits for source type, document type, year, language, country, and age.the search strategy for each database is detailed in supplementary file 1.additional records were identified from reference lists of selected articles, saudi digital library, and search engines.one reviewer (h.a) reviewed titles and abstracts to assess relevance and collected relevant titles and abstracts for full text assessment.deduplication was performed using microsoft excel.full text manuscripts were reviewed and evaluated against the eligibility criteria.eligibility criteria the databases were searched from april to may 2021 to identify articles published between january 1st, 2000, to december 31st, 2020.the timeframe was chosen because medical documentation and research in saudi arabia became mature only in the last two decades.the steps we followed for study selection are shown in (fig. 1). to be included in this review, a study had to meet the following criteria: 1) children (between 0 to 18 years of age.).2) the publication date is between 2000 and 2020.3) published in english.data extraction and synthesis the primary outcome was the type and the cause of childhood injuries and their distribution among age groups, gender, and regions.the strobe checklist includes 22 items distributed as 1 for abstract, 2 for introduction, 9 for the methods, 5 for the results, and 4 for the discussion, and 1 for funding.disagreements were resolved by consensus.results the initial search identified 3,384 studies.after screening titles and abstracts, 3,330 publications were excluded for being irrelevant or duplicates.the full text of 54 studies was assessed for eligibility, where 18 did not meet the inclusion criteria and were excluded.as for the remaining studies, five were conducted in the eastern region, four, three, and one were conducted in the western region, southern region, and northern region, respectively.two studies were conducted in several regions of the country.five studies focused on fractures, five on burns, four on mvc injuries, four on oral injuries, and seven examined poisoning.head injuries were most likely to be associated with mvc followed by falls [18].furthermore, blunt injuries caused by falls and mvc were the leading cause of deaths in overall injuries [21].in fact, the mechanism of injury was found to be a significant predictor of extended length of stay with higher odds for mvc and burns (or: 16.2, or: 14.5, respectively, p < 0.001) as opposed to falls [22].the study, which was conducted in the eastern region, reported that most cases occurred in the sea (74.5%) at night (56.9%) with no lifeguard present (92.2%).the majority of reported fractures occurred among males (72.8%).the ratio of male to female was 2.68:1.the leading cause of fracture injuries was falling at home (37.9%), followed by mvc (21.5%).other causes for fractures included door slams, direct hits, and pedestrian injuries.on the other hand, nonaccidental fractures were more common in preschoolers, followed by infants [32].two studies showed that most burns (60%) occurred among toddlers and preschoolers [33, 34].front seated children accounted for 46% of the cases with a higher rate of isolated head, neck, or facial injuries than back seated children (51.2% vs. 25%, p = 0.01) [39].more than half of injured children (53.8%) were back seated without seatbelts or car seats, while 9.1% were driving [41].poisoning and toxicological exposure studies on childhood poisoning and toxicological exposure included 2,550 children (table 6) [46–52].further, age was found to be a significant predictor of hospital admission (or: 1.19, p < 0.05) [48].discussion this scoping review suggests that falls and mvc are the leading causes of injuries in the kingdom.similarly, a global study conducted in low-and middle-income countries found fall and mvc to be the most common mechanism of childhood injuries [53].however, they remain a neglected focus in the kingdom.therefore, further investment in public health interventions to reduce falls and mvc is warranted to reduce their burden on population health.these results are consistent with the public data obtained from the cdc, stating that males accounted for 58.4% of unintentional injuries that occurred between 2001 and 2019 in the united states [55].one of the pillars of this vision is to extend life expectancy from 74 to 80 and improve quality of life.our research describes the magnitude of the problem and highlights the need for intersectoral interventions.one of the vision’s objectives is to promote traffic safety which could potentially aid in mvc prevention.however, we acknowledge that our review has several limitations.third, most studies were retrospective chart review studies which can be more susceptible to bias.fourth, except for one study, the primary focus of the included publications was unintentional injuries, which emphasizes the need to extend the current research in saudi arabia to nonaccidental childhood injuries.
number of words= 857
[{'rouge-1': {'f': 0.4091089390173249, 'p': 0.7692753623188406,'r': 0.2786486486486487}, 'rouge-2': {'f': 0.21496607176055027, 'p': 0.35363636363636364,'r': 0.15441558441558442}, 'rouge-l': {'f': 0.39528720290897734, 'p': 0.6389655172413793,'r': 0.2861572052401747}}]
-----------------------------------------------------------------------------------------------------------------------------------
p313:
Extractive Summary:
background in the literature, only 10 case of cecal epidermoid cysts (cecs) have been reported [1, 2], with its pathogenesis remaining unclear.cec can be congenital or acquired in origin.in the congenital form, it is thought to develop from ectodermal implantation during embryogenesis and development, whereas in the acquired form, it is thought to develop from epithelial implantation secondary to previous trauma or surgery [3, 4].additionally, epidermoid cysts are more commonly found in the mediastinum, head and neck, sacrococcygeal area, central nervous system, and gonads [3].in the present case of congenital cec, heterotopic epithelial tissue was located in the subserosal layer, which is suggestive of ectodermal implantation during intrauterine rotation of the gut.additionally, mucin-producing ciliated stratified epithelium was identified in this case and this epithelium resembled a bronchial epithelium.this finding could suggest a possible pathomechanism which may be related to the bi-differentiation of heterotopic ectodermal inclusion tissue.we hypothesized that ectodermal tissue was sequestrated in the subserosal layer of the cecum during the rotation of the gut back into the abdominal cavity, wherein these tissues differentiated into both squamous and mucinproducing ciliated epithelium.a previous report suggested that the most likely explanation for cec development and its sharing of the cecal muscular wall is the result of aberrant embryonic ectodermal implantation during embryogenesis [9].our present case showed a subserosal cyst composed of squamous mucosa and submucosal connective tissue, which supports the aforementioned proposed mechanism.furthermore, immunohistochemical phenotypes of cecal and cystic mucosal epithelium were different; colonic mucosa was reactive to intestinal mucin muc2, while the mucosa lining the cyst wall was reactive to gastric and bronchial mucin muc5ac.when we evaluated the serial radiographs in this case, the rlq intraluminal gas, which initially did not appear until after birth, continuously expanded.these findings suggest that the cyst was connected to the intestinal lumen, even though the macroscopic examination revealed no connection between two.on microscopic examination, there was one focus of the junction between the cecal and squamous epithelium which was located in the cecal muscularis propria, suggesting a microscopic fistula.thus, we assumed that the two lumens may have been connected through the microfistula.in conclusion, considering that epidermoid cysts can develop in the cecum during the perinatal period, the management of a fetal abdominal mass should be tailored individually.moreover, the possibility of cec should be considered in the differential diagnosis of subserosal cysts in the cecal area, and the cyst should be completely removed to achieve a good prognosis
number of words= 402
[{'rouge-1': {'f': 0.43914945726121013, 'p': 0.7433333333333334,'r': 0.31162679425837325}, 'rouge-2': {'f': 0.2335052254134957, 'p': 0.35859060402684567,'r': 0.17311750599520384}, 'rouge-l': {'f': 0.3931245771312585, 'p': 0.580204081632653,'r': 0.29727272727272724}}]
-----------------------------------------------------------------------------------------------------------------------------------
p314:
Extractive Summary:
introduction the coronavirus disease (covid-19) caused by novel severe acute respiratory syndrome coronavirus 2 (sarscov- 2) has been globally spreading since the end of december 2019.historically when looking at previous coronavirus pandemics caused by sars-associated coronavirus (sars-cov) and middle eastern respiratory syndrome coronavirus (mers-cov), there were reported cases of spontaneous abortions, preterm births, low birth weight, and neonatal respiratory infections [1].this was hypothesized to be a result of increased inflammatory activity associated with the first and third trimesters and a period of overall decreased immune activity associated with the second trimester and in the newborn period [2, 3].at the beginning of the covid-19 pandemic, this prior experience from pandemics caused by the same family of viruses, raised the concern that pregnant women and newborns could be at high risk for sars-cov-2 related complications.early reports of pregnant women infected with sarscov- 2 in the third trimester raised concern for increased risk of premature delivery [4], while large cohorts of pregnant women in the uk suggested that they are not at increased risk of spontaneous abortion or preterm birth but do have higher rates of cesarean section [5–7].neonates stand additional risk of infection by possible intrauterine transmission, intrapartum infection after exposure to maternal infected secretions or feces around the time of birth and postpartum infection from infected mother.although the majority of reports have not shown any strong evidence of vertical transmission of sars-cov-2 [17–19], there have been some anecdotal cases with possible vertical transmission based on neonatal nasopharyngeal rt-pcr swab or presence of igg and igm antibodies for sars-cov-2 [20–22].additionally, given the incubation period, there is the consideration of whether neonates will develop lateonset infections or sequelae after hospital discharge in those born to mothers with active infection.we sought to review the outcomes of infants born to mothers with active perinatal covid-19 infection during initial hospital stay and up to 2 months of age.methods study design and participants we established a database to prospectively enroll all neonates born to mothers with active perinatal covid-19 infections presenting to our network of two neonatal intensive care units (nicus) starting in april 2020.active perinatal sars-cov-2 infection was defined as maternal infection diagnosed between 14 days prior to and up to 72 h after delivery.in keeping with national practice, active infection was defined by detection of sars-cov-2 by reverse transcription polymerase chain reaction (rt-pcr) on nasopharyngeal (np) swabs.initially, testing for mothers was symptoms-based between april and may 2020; thereafter, all mothers were uniformly screened with np swabs on admission for delivery with the option to optout unless symptomatic.if the mother was confirmed positive for sars-cov-2 or was a person under investigation (pui) at time of delivery, the neonate would undergo testing at 24 and 48 h of life.only infants of confirmed sars-cov-2 positive mothers were included in the database.all data were collected by chart review and stored in a secure hippa compliant database.data collection & follow-up we collected several maternal and neonatal variables, which included demographics, maternal symptoms, treatment, mode of delivery, neonatal symptoms, treatment, mode of feeding, and lab data when available.mothers and any family members present were required to wear surgical face masks during deliveries.practice for symptomatic sars-cov-2 positive mothers remained unchanged.when rooming-in with mother, the neonate was kept in an isolette at least 6 ft away from mother’s bed, and mothers were instructed to wear masks and to perform hand hygiene prior to and after taking care of the infant (including breastfeeding).breastmilk feeding (directly or expressed) was always encouraged after counseling mother of unknown but likely exceptionally low risk of transmission by breastmilk.neonates were bathed soon after birth and underwent testing with np rt-pcr swabs at 24 and 48 h of life while those in the newborn nursery or rooming-in generally only received testing at 24 h of life prior to discharge.given the possible increased risk of aerosol generation, if neonatal sars-cov-2 status was unknown or positive and they required non-invasive respiratory support greater than 2 l per minute of high flow nasal cannula, they were intubated with hepa filter placed in expiratory limb of ventilator circuit.regarding discharge practices, when possible, infants were discharged to a family member with a proven negative sars-cov-2 test and families were educated to isolate from mother for 10 days from her positive test.results between april 2020 to april 2021, there were 2367 deliveries, of which 67 mothers tested positive for sarscov- 2 via np rt-pcr in the perinatal period.of these women, 70 neonates were identified, with 65 single, 1 twin, and 1 triplet gestation live-born deliveries.mean maternal age was 26.7 ± 6.4 years old.forty-two (63%) mothers were confirmed covid-19 positive at the time of delivery, the remainder were persons under investigation (puis) at time of delivery and later confirmed to be positive.overall, median time from diagnosis to delivery was 1 day (iqr, 0 to 1.75 days), with a positive number indicating that the test was obtained before birth.the remaining were tested due to symptoms that included fever, upper or lower respiratory symptoms, anosmia or ageusia, or recent known sars-cov-2 contact.of the 13 symptomatic mothers, 6 received specific treatment for covid-19.three received remdesivir, two of them also received dexamethasone, and of those, one received convalescent plasma therapy as well due to severe disease.in addition to the standard drying and stimulation during resuscitation at delivery, 21 (32%) neonates needed supplemental oxygen delivered, 20 (31%) received positive pressure ventilation, and 6 (9%) were intubated.all neonates requiring supplemental oxygen or additional respiratory support were negative for sars-cov-2.two (3%) were tested on day of life 0 (day of birth), 65 (93%) were tested on day of life (dol) 1, 49 (70%) on dol 2, 5 (7%) on dol 3, 1 (2%) on dol 4, and 1 (2%) on dol 9. of these, 67 (97%) neonates tested negative.of the two who tested positive, one (neonate a) was positive on dol 1 with repeat testing on dol 2 being negative, and the other (neonate b) was positive on dol 2 after a negative test on dol 1.neither neonates required additional resuscitation efforts other than drying and stimulation and did not have any oxygen requirements during hospital stay (length of stays were 2 and 3 days).additionally, in our study, the neonates who tested positive were found to be asymptomatic at all follow-up periods up to 2 months.of the patients who received breast milk, none tested positive for sars-cov-2 during hospitalization and remained asymptomatic during follow-up.however, it would be difficult to differentiate if the transmission were vertical or horizontal if the neonates tested positive after the immediate post-natal period.
number of words= 1083
[{'rouge-1': {'f': 0.2888671875, 'p': 0.8500000000000001,'r': 0.174}, 'rouge-2': {'f': 0.1733431062647789, 'p': 0.38543624161073825,'r': 0.11181494661921709}, 'rouge-l': {'f': 0.32330409023584367, 'p': 0.7366666666666666,'r': 0.20709677419354838}}]
-----------------------------------------------------------------------------------------------------------------------------------
p315:
Extractive Summary:
background henoch-schönlein purpura (hsp), also called iga vasculitis, is a type of vasculitis in which small blood vessels accumulate deposits of immunoglobulin a1.hsp is more common in children [1] and is characterized by non-thrombocytopenic purpura, abdominal pain, joint swelling and pain, and hematuria or proteinuria [2].renal involvement is an important determinant of longterm prognosis [3–5].in our clinical experience, we found that many patients and families do not accept ctx therapy, mainly due to its gonadal toxicity.at the 12-month-follow-up, the patient had no recurrence of the skin rash or abdominal pain, normal clinical status and renal function, no hypertension, a urinary protein level of 8 mg/kg/day, 45 red blood cells (rbcs) per high power field (hpf), and no relevant adverse events (table 1).at the 8-month-followup, the patient had no recurrence of the skin rash, no detectable urinary protein, no relevant adverse events, normal clinical status and renal function, no hypertension (table 1).patient 4 in november 2015, an 11-year-old girl was admitted to our hospital for leg purpura, bilateral swelling of the wrists and knees, microscopic hematuria, and proteinuria (25 mg/kg/day).at the 32- month-follow-up, she patient had no recurrence of skin rash, no detectable urinary protein, no relevant adverse events, normal clinical status and renal function, and no hypertension (table 1).patient 5 in september 2015, a 12-year-old boy was admitted to our hospital for leg purpura, microscopic hematuria, and proteinuria (34 mg/kg/day).although this patient weighed 45 kg, his parents requested use of a lower dose of lef, and this lower dose subsequently appeared to be effective.steroid therapy was quickly discontinued in 1 patient who experienced increased intraocular pressure.although nephritis is the most serious long-term complication of iga vasculitis, limited data are available regarding the best therapy.corticosteroids are currently a first-line treatment for hspn, and this approach leads to remission in most patients.however, some patients exhibit steroid dependence or steroid resistance, and require immunosuppressant therapy in combination with a corticosteroid.the share recommended treatments for children with moderate iga vasculitis with nephritis include oral prednisolone and/or pulsed methylprednisolone as a first-line treatment, and aza, mmf or intravenous ctx as a first- or second-line treatment [9].due to the lack of evidence-based data for treatment of iga vasculitis with nephritis and the similarities of this condition with primary iga nephropathy, the kdigo guidelines proposed the same treatment for both diseases.thus, they recommend oral or intravenous corticosteroids plus oral or intravenous ctx for initial therapy, but provided no recommendations for maintain therapy [10], and did not recommend mmf or aza.clinicians who strictly follow the kdigo guidelines for the treatment of hspn may face the risk of delayed initiation of effective treatment and an increased risk of chronic kidney disease over the long term.mmf (which selectively suppresses the proliferation of t and b cells) is another commonly used immunosuppressant.the adverse events from mmf therapy are less severe than those from ctx, but the high-cost of mmf restricts its use in low-medium income countries.cysa (which decreases lymphocyte function) is another commonly used immunosuppressant.cysa can be very effective in pediatric patients with severe hspn; however, administration can be difficult because it has poor water solubility and close monitoring of the blood concentration is necessary.cys a may also cause severe adverse events [19].azathioprine, a prodrug of 6-mercaptopurine, has antileukemic, anti-inflammatory, and immunosuppressive properties, and is also often used in the treatment of iga vasculitis with nephritis [9, 13].previous studies reported administration of tacrolimus and rituximab for treatment of hspn [20, 21], but there is limited evidence of their efficacy and further research is required for confirmation.lef is an affordable medication that is associated with few adverse events.research in adults with hspn reported that lef in combination with a corticosteroid may lead to a slower deterioration of gfr than corticosteroid monotherapy [16].aza was not availble in our hospital, and we had limited experience with aza and cysa.lef is commonly used for juvenile idiopathic arthritis and lupus nephritis in our center, and we found no obvious adverse effects, suggesting suitability for our patients.lef is an immunosuppressive agent that disrupts t and b cell function via inhibition of dihydroorotate dehydrogenase (the rate-limiting enzyme of de novo pyrimidine nucleotide biosynthesis) and also inhibits several tyrosine kinase signaling molecules involved in immune function [22].lef is currently a common treatment for autoimmune diseases.there is evidence that lef is effective in treating iga nephropathy, and nephropathy caused by systemic vasculitis and systemic lupus erythematosus [14, 15].in this study, we prescribed lef in combination with a corticosteroid to successfully treat 5 children who had iga vasculitis with nephritis.data from randomized controlled trials (rcts) supporting the use of lef as a treatment for iga vasculitis with nephritis are currently lacking.thus, we need to study more patients using a well-designed rct to compare lef with conventional agents (i.e., a corticosteroid and a standard immunosuppressive agent) to confirm the effectiveness and safety of lef therapy.nonetheless, our examination of 5 children who had iga vasculitis with nephritis indicated that lef in combination with a corticosteroid achieved complete or nearly complete renal remission, and that none of the patients experienced severe adverse even
number of words= 839
[{'rouge-1': {'f': 0.38408778541782224, 'p': 0.8005936073059361,'r': 0.252648401826484}, 'rouge-2': {'f': 0.1996784039067018, 'p': 0.3498165137614679,'r': 0.13971428571428574}, 'rouge-l': {'f': 0.36929258343408006, 'p': 0.6457575757575758,'r': 0.25858560794044666}}]
-----------------------------------------------------------------------------------------------------------------------------------
p316:
Extractive Summary:
background clinical practice guidelines (cpgs) are statements to guide health providers and patients [1].however, implementation of cpgs with insufficient quality or inappropriate contents may mislead clinicians [6, 7].when implementing cpgs in everyday clinical practice, users should pay attention to the content and local adaptations of the guidelines and their quality [8, 9].so far, it has been widely used and recognized in the quality assessment of cpgs [16, 17].recently, the number of pediatric cpgs grew substantially.previous quality assessments of pediatric cpgs are out of date [21, 22] or only focus on a certain field [23, 24].search strategy the following search engines and databases were systematically searched, pubmed (pubmed.gov), medlive (guide.medlive.cn), guidelines international network (gin; g-i-n.net), national institute for health and care excellence (nice; nice.org.uk), and world health organization (who; who.int).the searching terms included pediatric restriction, “child (m, for mesh)” or “child, preschool (m)” or “infant (m)” or “adolescent (m)” or “infant, newborn (m)” or “child* (* for wildcard)” or “pediat*” or “paediat*after cross-checking the selected cpgs and extracting data, the two researchers reached a consensus.the first step was to select cpgs that potentially met the eligibility criteria by screening titles and abstracts.the data extraction procedure collected the following parameters: published year, country or region of origin (divided into developing and developed countries or regions according to the list of world trade organization (wto), version 2019), organization or group responsible for cpg development (individual, few persons, or small teams were excluded), applied population, and field of focus (based on the international classification of diseases 11th revision, icd-11; released by who on june 18, 2018).the evidence-based cpgs were defined by the health and medicine division of the american national academies as “statements that include recommendations intended to optimize patient care and are informed by a systematic review of evidence and an assessment of the benefits and harms of alternative care options” [29].in the first round, each reviewer was required to independently assess ten randomly selected cpgs.the assessment was considered complete after we finished at least threeround tests and achieved an icc value no less than 0.85 in each item.the agree ii consists of 23 key items in 6 domains to capture different dimensions of cpg quality, which include scope and purpose (items 1–3), stakeholder involvement (items 4–6), rigor of development (items 7– 14), clarity of presentation (items 15–17), applicability (18–21), and editorial independence (items 22–23) [11, 18].each item is assigned a score from 1 (strongly disagree, when no given information is relevant) to 7 (strongly agree, when full criteria of the item are met).the more criteria that are met, the higher the scores are given.if the overall assessment scores given by these two reviewers differed by 1 point, the lower score was assigned; if it varied by 2 points, the average scores were assigned; and if it differed by ≥3 points, the reviewers reviewed for agreement [17]. to reach the recommended level, cpgs had to achieved overall assessment scores of 6 and 7 (above 80% of 7 scores).taking into account the criteria considered in the assessment process, if the cpg had serious issues in one of the domains, it would be downgraded one level [17, 31].to ensure the validity and reliability of the assessment, after the overall assessment procedure, 10% of the assessments were randomly selected by a senior experienced reviewer and re-assessed.additionally, the overall quality scores of cpgs in different fields, organizations, groups, countries, or regions were summarized and ranked.only variables with at least 3 cpgs were given a ranking.in the screening process, 1474 records were excluded (712 records were not cpgs, 154 records were duplicates, and 608 records did not focus on pediatrics).after including 22 records from references and citations and after excluding 484 records (168 records were not cpgs, 23 records were duplicates, 58 records did not publish in 2017 to 2019, and 235 records did not focus on pediatrics) in the full-text analysis, a total of 216 pediatric cpgs were used.among these cpgs, 71.3% were compiled by developed countries or regions; 85.65% of them were through organizations or groups.three-quarters of included cpgs used evidence-based methods to develop cpgs, while the other one quarter did not.table 1. shows the characteristics of included pediatric cpgs.in the six domains assessment, the “clarity of presentation” domain achieved the highest mean score of 66.77%.cpgs compiled by developed countries or regions and under organizations or groups achieved higher scores in different domains.evidence-based cpgs achieved a significantly higher score lead in nearly all domains, overall assessment scores (p < 0.001), and recommendation levels (p < 0.001) compared to non-evidenced cpgs.the score of overall cpgs and subgroups are presented in table 2.the scores in each domain of different recommendation levels are summarized in fig. 2.the cpgs that achieved lower recommendation levels were insufficient in “applicability” and “rigor of development”.additionally, the score of cpgs in different fields (supplemental table 1.), organizations or groups (supplemental table 2.), and countries or regions (supplemental table 3.) were summarized and ranked.the cpgs related to the circulatory system, digestive system, and general fields (e.g., screening and diagnosis) achieved higher overall assessment scores.for different countries or regions’ comparisons, cpgs developed by the u.k., australia, and italy had better quality.influential factors the multi-factor linear regression was used to explore the association between scores in each domain and the characteristics of cpgs.discussion overall guideline quality previous studies assessing quality assessment of pediatric cpgs are outdated or only focused on a specific field [21–24].after assessment with agree ii, they showed that the cpgs achieved an overall mean score of 55%, which is lower than the present study.however, there were still few cpgs that reached a high-quality level.moreover, the overall quality score was still inadequately compared to the quality evaluation for other recent cpgs focused on adults.most of the studies that focused on adult cpgs reported a mean overall agree ii scores of 4.77–5.97 in 7 points (68.21–85.35%), and 8.2– 50.0% of them could reach the “recommend” level [33– 35].quality of domains compared with other studies using the agree ii assessment, the present study also revealed that “applicability” and “rigor of development” domains had poorer quality [21, 22, 35, 36].the “applicability” domain mainly focuses on the barriers and facilitators to apply the cpg [18].the study of boluyt et al. was a great example of adopting cpgs [22].they conducted a systematic review of cpgs and assessed the quality and applicability of the cpgs.thus, the potential conflicts of interests in cpg development should be disclosed and reviewed carefully.independent committees should also be engaged for evaluation and management [18, 40].furthermore, a small team might lack the skills or training in developing cpgs as compared with large organizations or groups [20].several studies suggested cpgs developed in regions with different economic development statuses might influent the quality of cpgs [22, 43].however, as there were nuances in many healthcare systems worldwide that might preclude the direct deployment of international cpgs, agencies should consider cpg adaptations for their institutions.thus, we conducted strict training and test assessment procedures.agree-ii scores are dependent upon reporting, while some cpg committees may comply with the requirements but do not ultimately report.we suggest health providers should closely follow new versions of well-developed tools for the appraisal of cpgs.conclusions in conclusion, the quality of the pediatric cpgs was rarely excellent.besides that, the quality and applicability of a cpg should be evaluated before its applicati
number of words= 1212
[{'rouge-1': {'f': 0.3643781918696563, 'p': 0.8217730496453901,'r': 0.23408668730650156}, 'rouge-2': {'f': 0.20710095040268794, 'p': 0.3938434163701068,'r': 0.1404879938032533}, 'rouge-l': {'f': 0.36421175101803377, 'p': 0.6632203389830509,'r': 0.2510344827586207}}]
-----------------------------------------------------------------------------------------------------------------------------------
p317:
Extractive Summary:
introduction household food insecurity (fi) — a household’s experience of inadequate or insecure access to sufficient, safe and nutritious food because of income or finances — is a major public health problem [1, 2].families affirming one or both of the first 2 items of the 18-item u.s.the first item measures uncertainty about having enough food and the second item measures uncertainty about exhausting their food supply.however, the 2 items on the hvs™ are from the adult module in the 18-item hfssm, which may not apply to children [9].the nutrition screening tool for every preschooler (nutristep®) is a valid and reliable 17-item parentcompleted questionnaire developed in canadian children addressing multiple domains of nutrition risk [10, 11].a single item addressing fi may be a useful child-specific screening tool.the 17-item nutristep® has an area under the curve of 84.6% compared with a dietitiancompleted assessment [10, 11].for the purpose of this study, children were included if they had complete data on both the hvs™ and nutristep® questionnaires.all methods were performed in accordance with the relevant guidelines and regulations and approved by research ethics boards at the hospital for sick children and st.parents completed the hvs™, the first 2 items of the 18-item hfssm; this brief 2-item measure has 97% sensitivity and 83% specificity for identifying marginal food security, using the hfssm as the criterion measure [8].by definition, families affirming one or both items were classified as having marginal food security.we included “rarely” as an affirmative response as we reasoned that families struggling to meet their needs, may either choose not to disclose or select “rarely” due to stigma or shame [13].families affirming this single item were classified as having marginal food security.convergent construct validity was tested by comparing the multiple logistic regression models for the 1-item nutristep® with those of the 2-item hvs™ evaluating the associations of these fi measures with variables considered predictors of marginal food security (family income, maternal education and parent employment status) adjusting for covariates.results participants of 1753 children recruited to the target kids!cohort, 1174 children had complete data on the hvs™ and nutristep® questionnaires.compared with families not at risk for food insecurity, a higher proportion of children in families at risk for food insecurity had a lower family income, a mother with high school education or less, 1 or both parents were unemployed, and were from a single-parent family (table 1).marginal household food security is associated with poor educational outcomes and emotional and behavioural problems in children, as well as maternal major depression and anxiety [3–5].in addition, detection of marginal food security is an appropriate target for a clinical screening tool.while no previous study has examined this single item on the nutristep® as a food security screen, others have examined the validity of a single item measure of food security.they identified one child item (“in the last 12 months, were there times when it was not possible to feed the children a healthy meal because there was not enough money?while healthcare providers recognize the importance of identifying poverty in clinical settings, they identify time constraints and multiple competing demands as barriers to integrating social needs screening into healthcare [16].however, the importance of identification of caregiver needs and priorities, and referral to appropriate community supports have been highlighted [17, 18].furthermore, caregivers experiencing food insecurity report feeling ashamed or embarrassed in reporting fi and that health care provider empathy, concern and empowerment can mitigate these challenges [13].
number of words= 566
[{'rouge-1': {'f': 0.4027490538020967, 'p': 0.7366666666666666,'r': 0.2771307300509338}, 'rouge-2': {'f': 0.21922693078289796, 'p': 0.3557142857142857,'r': 0.15843537414965986}, 'rouge-l': {'f': 0.39055611407290386, 'p': 0.6027868852459017,'r': 0.28885521885521886}}]
-----------------------------------------------------------------------------------------------------------------------------------
p318:
Extractive Summary:
background the notorious group of pathogens, enterococcus faecium, staphylococcus aureus, klebsiella pneumoniae, escherichia coli, acinetobacter baumannii, pseudomonas aeruginosa and enterobacter spp., named “eskapeec” owing to their high resistances to multiple antimicrobial agents, have recently aroused global concern [1, 2].currently, the incidence of bloodstream infection (bsi) caused by eskapeec has increased rapidly [3], and eskapeec bsi brought about worse outcomes [3], longer hospital stays, higher economic costs [4–6], and increased mortality [4–6].in addition, effective antimicrobial agents against eskapeec strains were limited due to the growth of resistance to multiple antibiotics in these bacterial species.therefore, a wide understanding of the main clinical characteristics of eskapeec bsi among hospitalized children was crucial for physicians to early recognition and select proper empirical therapy.so far, there was no data concerning the bsi caused by eskapeec in pediatric populations.thus, we sought to investigate the epidemiology, clinical characteristics among hospitalized children with eskapeec bsi.simultaneously, we also assessed risk factors, and clinical outcomes of them.methods study population hospitalized children who were diagnosed with bsi based on a positive blood culture between 2016 and 2018 at the west china second university hospital of sichuan university and aged under 14 years old were enrolled retrospectively after obtaining ethics approval.the exclusion criteria were listed next (fig. 1): (1) polymicrobial infections;(2) diagnosed with fungal bsi; (3) incomplete clinical data.data collection this study obtained the following data from electronic medical records, including age, gender, previous hospitalization (within 1 month), history of surgery and/ or trauma (within 3 months), records of previous antibiotic use (within 1 month), underlying diseases, nosocomial infection or not, symptoms, microbiology data (microorganisms and resistance to antimicrobial agents), likely source of infection, blood products transfusion, pediatric intensive care unit (picu) admission, invasive operation (indwelling gastric tube, central venous catheter, urinary catheter, mechanical ventilation), empirical antibiotics therapy, length of hospital stay and patients’ clinical outcomes.meanwhile, the additional laboratory results within the first 24 h of admission were collected: the blood routine, c-reactive protein, alanine aminotransferase (alt) and aspartate aminotransferase (ast).this study was analyzed through three parts.finally, the 174 patients with eskapeec bsi according to patients’ final condition at discharge were divided into survivor and non-survivor groups to investigate the risk factors closely connected with mortality.definitions bsi was defined as the causative bacteria isolated in the blood cultures and clinical manifestations in accordance with sepsis syndrome [18].results two hundred fifty-two hospitalized pediatric patients were diagnosed with laboratory-confirmed bsi between 2016 and 2018 at the west china second hospital, sichuan university.six of the 174 (3.4%) patients lacked resistance data; 124 (73.8%) were mdr eskapeec and 44 (26.2%) were non-mdr eskapeec.microbiology as table 1 showed, the two leading eskapeec pathogens were escherichia coli (26.8%,61/228), klebsiella pneumoniae (20.2%,46/228), followed by enterococcus faecium (12.7%, 29/228), staphylococcus aureus (12.7%, 29/228), acinetobacter baumannii (2.6%, 6/228), pseudomonas aeruginosa (1.3%, 3/228) and enterobacter spp.regarding underlying disease, a greater proportion of premature and/or low birth weight (31.6% vs 9.3%, p = 0.001), tumor diseases (16.7% vs 0%, p < 0.001) was presented in patients with eska peec bsi.compared with non-eskapeec bsi, patients with bsi caused by eskapeec had increased percentages of previous surgery and/ or trauma (13.8% vs 3.7%, p = 0.042), nosocomial infection (32.2% vs 7.4%, p < 0.001), more source of urinary tract infection (14.9% vs 1.9%, p = 0.009), but lower rate of previous antibiotic use (25.9% vs 50.0%, p = 0.001), less source of lung infection (17.8% vs 42.6%, p < 0.001), intracranial infection (1.1% vs 16.7%, p < 0.001) and lower levels of platelet count (median, 209 [111–323] vs 341 [168–439], p < 0.001), c-reactive protein (median mg/dl,15.0 [3.0–85.3] vs 26.5 [10.0–116.6], p = 0.017).in terms of empiric antimicrobial treatment and outcomes of all cases, we found that 50 of the 228 patients (21.9%) were treated inappropriately: 46 (27.4%, 46/168) were in eskapeec bsi patient group and 4 (7.4%,4/54) were in non-eskapeec bsi group.patients with eska peec bsi had received inappropriate empiric antibiotics treatment significantly more often (27.4% vs 7.4%, p = 0.003) (table 2).patients with bsi due to eskapeec had longer hospital stay (median days, 20.5[10.0–31.0] vs 14.0[8.8–23.0], p = 0.023) compared with those with non-eskapeec bsi.comparison of mdr eskapeec and non-mdr eskapeec bsi the 168 patients with resistance data were divided into mdr eskapeec and non-mdr eskapeec bsi groups.the differences of the main characteristics between the 2 groups were showed in table 3.compared with non-mdr eskapeec bsi, patients with mdr-eskapeec bsi were less likely to have a history of antibiotic use within 1 month (21.8% vs 43.2%, p = 0.006), source of skin or soft tissue infection (6.5% vs 27.3%, p < 0.001).furthermore, the median level of platelet count was significantly lower in mdreskapeec group with bsi than that in non-mdr eska peec group (median,188[100–302] vs 271[173–413], p = 0.004).in multivariate analysis, the independent risk factor for mrd-eskapeec bsi was nosocomial infection (or = 3.314, p = 0.037), while the skin or soft tissue infection (or = 0.245, p = 0.011) was a protective predictor of mrd eskapeec bsi.hospital stay in mdr-eskapeec bsi group was longer than that in non-mdr eskapeec bsi group (median days, 24.0 [13.0–36.0] vs 14.5 [9.0–27.3], p = 0.006) (table 3).whereas, no significant differences were ascertained in mods, septic shock, mechanical ventilation, picu admission, mortality between mdr and non-mdr eskapeec bsi groups (all p > 0.05).the overall mortality rate of these patients was 14.4% (25/174), and no significant difference was ascertained regarding mortality between mrd-eskapeec and non-mrd eskapeec bsi groups (13.7% vs. 11.4%, p = 0.692) (table 3).(3) the overall mortality rate in patients with eskapeec bsi was 14.4% (25/174), and no significant difference was ascertained in mortality rate between mrd-eskapeec and non-mrd eskapeec bsi groups (13.7% vs. 11.4%, p = 0.692).marta b, et al. in 2013 have revealed that solid-organ transplant patients who had previous antibiotic use and septic shock were more likely to develop drug-resistant eskapeec bacteremia [15].to explore the possible influence of mrd pathogens for the outcome of patients with eskapeec bsi, patients’ characteristics were systematically evaluated.the overall mortality rate among children with eskapeec bsi was 14.4% (25/174), and no significant difference was ascertained in mortality between mrd-eskapeec and non-mrd eskapeec bsi patients (13.7% vs. 11.4%, p = 0.692), which was in accordance with previous studies [16, 17].
number of words= 1031
[{'rouge-1': {'f': 0.41277565570617925, 'p': 0.8564406779661018,'r': 0.2719147084421236}, 'rouge-2': {'f': 0.28406992478836895, 'p': 0.5461904761904761,'r': 0.19195121951219513}, 'rouge-l': {'f': 0.41282002610784935, 'p': 0.7211627906976745,'r': 0.2891780821917808}}]
-----------------------------------------------------------------------------------------------------------------------------------
p319:
Extractive Summary:
antibiotics may lead to increased weight gain via reduction in clinical and subclinical infections, through reducing inflammation, or through alteration of gut microbiome [7, 11].azithromycin may affect growth and nutrition outcomes via such pathways, contributing to overall observed reductions in mortality.they were therefore limited by the number of clusters randomized and by lack of longitudinal measurements.here, we report linear and ponderal growth in children receiving a single oral dose of azithromycin compared to placebo with longitudinal measurements.methods study design this study was a 1:1 individually randomized placebocontrolled trial comparing single dose azithromycin (20mg/kg) to matching placebo.this study was registered at clinicaltrials.gov before recruitment of the first participant (clinicaltrials.gov nct03676751, registered 19/09/2018).nouna is the capital of the province of kossi and has approximately 20,000 residents.ethics this study was reviewed and approved by the institutional review board at the university of california, san francisco and the comité national d’ethique pour la recherche (national ethics committee of burkina faso) in ouagadougou, burkina faso.eligibility and recruitment study staff visited households with children under 5 years of age known to be resident and informed caregivers about the study.participants were evaluated at baseline, 14 days after enrollment, and 6 months after enrollment.at baseline, caregivers completed a short baseline questionnaire that included mother’s age and educational status and the child’s breastfeeding status (supplemental material).the allocation sequence was generated through unrestricted randomization by the study biostatisticians with no blocking or stratification.the randomization sequence was implemented by assigning a letter to azithromycin and placebo and assigning each child a study identification number that was linked to the treatment letter in the study’s mobile application.height measurements were recorded in triplicate and the median measurement was used for analysis.muac measurements were taken at the midpoint between the tip of the elbow and tip of the shoulder of the left arm using a standard muac tape.weight-for-height (whz), weight-for-age (waz), and height-for-age (haz) z-scores were calculated based on 2006 who standards [18].sample size considerations the sample size for the trial was based on the primary outcome, which was shannon’s diversity index of the gut microbiome.we estimated the detectable difference for anthropometric outcomes given this fixed design.assuming a 2-sided alpha of 5%, and assuming the empirical standard deviations observed in the trial for anthropometry outcomes, we estimated that 225 children per arm would have at 80% power to detect a difference of 0.8 g/kg/d in weight velocity at 14 days (sd = 3.1) and a difference of 0.1 g/ kg/d at 6 months (sd = 0.5).weight gain velocity was analyzed using a linear regression with a term for the randomized treatment assignment as the sole predictor, without adjustment for baseline weight as baseline weight is included in the outcome calculation.each outcome was analyzed using a linear regression model with terms for the randomized treatment assignment and the baseline value of the outcome.as all outcomes were pre-specified, no adjustment for multiple comparisons was made.at baseline, 18 and 9% of children randomized to azithromycin and placebo, respectively, were classified as underweight based on waz, 17 and 13% were stunted based on haz, 11 and 6% were wasted based on whz, and 5 and 3% were wasted based on muac (table 1).demographics and anthropometric indices were similar among children who were and were not lost to follow-up (supplemental table 1).adverse events at 14 days have been previously reported and were most often gastrointestinal in nature [20].rate of change in height was also similar between the two groups at 6months (mean difference 0.02mm/day, 95% ci − 0.02 to 0.06mm/day, p = 0.57).possible mechanisms for short-term increases in weight gain following azithromycin have been hypothesized to include reducing clinical or subclinical infection or via alteration of the microbiome [10].short-term increases in weight gain may confer some benefit against infection in children that prevents mortality, or a reduction in infection may both increase weight gain and reduce mortality, or the two may be independent.we were unable to assess specific mechanisms for the effect of azithromycin on weight gain in the present analysis.given complex relationships between infection and malnutrition, children with concurrent infection that is treated by antibiotic administration may experience increased growth catch-up as they recover from their illness.these results must be considered in the context of several limitations.given the limited sample size of the trial, we did not have statistical power to conduct age-based subgroup analyses.such analyses may be useful for better understanding heterogeneity in the effect of azithromycin on growth during different periods of early childhood.children in the azithromycin arm had a higher prevalence of underweight at baseline compared to those in the placebo arm, indicated some moderate chance imbalances in weight at baseline.the 29 children lost by day 14 had similar anthropometry to those who remained, though children who were lost to follow-up by 6 months were slightly more stunted, underweight, and wasted at enrollment than those who were not lost to follow-up.finally, this study was conducted in a peri-urban burkina faso where childhood undernutrition and infection are common.conclusion short-term increases in weight as documented in this study may be protective against adverse health outcomes and partially explain any benefit of mass azithromycin distribution for childhood survival.
number of words= 850
[{'rouge-1': {'f': 0.3886402154689495, 'p': 0.7525396825396826,'r': 0.26196428571428576}, 'rouge-2': {'f': 0.20985230857597706, 'p': 0.3528685258964144,'r': 0.1493296089385475}, 'rouge-l': {'f': 0.3492918952173499, 'p': 0.5838888888888889,'r': 0.2491767554479419}}]
-----------------------------------------------------------------------------------------------------------------------------------
p320:
Extractive Summary:
thus, we could develop new strategies to mitigate sunitinib-induced cardiac injuries by recapitulating reversible cardiac injuries and elucidating the molecular mechanisms driving cardiac events in preclinical models.we previously demonstrated that many cancer drugs, including sunitinib, significantly disrupt the sarcomeres in human induced pluripotent stem cell-derived cardiomyocytes (ipsc-cms), and the extent of disruption could help predict the occurrence of drug-induced cardiotoxicity [6].therefore, in the present study, we investigated whether the reversibility of sunitinibinduced cardiac events in clinical settings can be mimicked in-vitro by analyzing the sarcomere structure in ipsc-cms before and after sunitinib washout.in addition, we attempted to identify the molecular pathway involved in sunitinib-induced sarcomere disruption in ipsc-cms.the organization of cardiac sarcomeres is strictly regulated by the balance between synthesis, assembly, and sarcomere degradation, to maintain the physiological states of contraction and relaxation [7].treatment with xmu-mp-1, a small molecule inhibitor of kinases mst1/2 (homologs of hippo), has been shown to preserve cardiac function following pressure overload in mice [11].we also analyzed the sarcomere structure after simultaneous simulation with sunitinib and xmu-mp-1 to confirm whether the inhibition of the hippo pathway has a beneficial effect on sunitinib-induced cardiac injury.methods materials all reagents and compounds were purchased from either fujifilm wako pure chemical (osaka, japan), thermo fisher scientific (waltham, ma, usa), or sigma-aldrich (st.compound preparation and stimulation after the ipsc-cms were cultured for 5–6 d, they were stimulated with sunitinib with or without xmu-mp-1 for 72 h. both sunitinib and xmu-mp-1 were dissolved in 100% (v/v) dimethyl sulfoxide (dmso).the compounds were added to the wells of a 96-well plate to obtain the final concentrations 0.3 μm, 1, μm, or 3 μm for either sunitinib or xmu-mp-1.to analyze yap1 nuclear localization, six image fields per well were acquired using the 4× objective lens.for quantification of the yap1 nuclear localization, immunofluorescence intensity of yap1 in the cytoplasm and nuclei, respectively, were measured.then, cells having a significantly more intense signal of yap1 in the nuclei than in the cytoplasm were defined as cells showing nuclear localization of yap1.seventy-two hours after stimulation with sunitinib, the hsi values of groups treated with 0.3 μm, 1 μm, and 3 μm sunitinib were 64.3, 13.7, and 1.1% of the control group, respectively.this result suggests that sunitinib caused sarcomere disruption in ipsc-cms.next, we analyzed whether ipsc-cms had the ability to spontaneously recover from sunitinib-induced sarcomere disruption by analyzing sarcomere morphology following stimulation with sunitinib and subsequent drug washout.immediately after 72 h of sunitinib stimulation, sarcomeric z lines were observed as dot-like staining images indicating sarcomere disruption in most ipsc-cms, but after 72 h of drug washout, the dot-like staining images partially changed to stripe-like staining images, and after 144 h of washout, the dot-like staining images almost disappeared and almost completely changed to normal stripe-like staining images (fig. 2b).the hsi values of groups treated with 0.3 μm, 1 μm, and 3 μm sunitinib 72 h after drug washout were 86.0, 69.3, and 16.7% of the control group, respectively.we next investigated the molecular pathways involved in the disruption or recovery of sarcomere structure following sunitinib-mediated injury in ipsc-cms.to further examine the involvement of the hippo pathway in the disruption and/or recovery process, we modulated the pathway with a small molecule kinase inhibitor, xmu-mp-1, with the expectation that this could recapitulate spontaneous recovery of cardiac sarcomeres.we added either 1 μm or 3 μm sunitinib with or without 0.3 μm or 1 μm xmu-mp-1 for 72 h. xmu-mp-1 suppressed sunitinib-induced sarcomere disruption in a concentration-dependent manner (fig. 4a and b).on the other hand, the hsi of the group treated with 3 μm sunitinib alone was 3.3% compared to the control group.indeed, the pharmacological inhibition of pdgf causes the redistribution of yap1 from the nucleus to the cytosol and downregulates yap1 target genes [19].further studies using various vegf or pdgf inhibitors would clarify the relationship between the degree of reduction in nuclear localization of yap1 and the extent of sarcomere injury.since this washout period resulted in the complete recovery of disrupted sarcomeres, the increased nuclear localization of yap1 may be related to the recovery process of injured sarcomeres in ipsc-cms.therefore, in addition to the hippo pathway, other molecular pathways may also be involved in the spontaneous recovery of cardiac sarcomeres.although their functional significance in the disease setting is still unknown, the modulation of each pathway using specific inhibitors may also affect the sarcomere structure, even after stimulation with sunitinib.importantly, a small molecule inhibitor of the hippo pathway, xmu-mp-1, suppressed the sunitinib-induced disruption of cardiac sarcomeres.furthermore, the clarification of the molecular mechanism underlying the recovery of cardiac sarcomeres from a sunitinib-induced disruption would enable discover and development of novel drug targets for the treatment of sunitinib-druginduced cardiotoxicity.this study also indicates that the hippo pathway is involved in the process of sunitinib-induced disruption of sarcomere and its recovery after drug washout (fig. 5 a, b and c).additionally, inhibition of the hippo pathway with xmu-mp-1 suppressed the sunitinib-induced disruption of sarcomere.
number of words= 814
[{'rouge-1': {'f': 0.4851005451410135, 'p': 0.8451677852348993,'r': 0.34017543859649124}, 'rouge-2': {'f': 0.2845129498941342, 'p': 0.4605723905723906,'r': 0.2058313817330211}, 'rouge-l': {'f': 0.4148485066110654, 'p': 0.6521917808219178,'r': 0.30415977961432505}}]
-----------------------------------------------------------------------------------------------------------------------------------
p321:
Extractive Summary:
introduction doxorubicin (dox) is an antibiotic anthracycline that was first isolated in 1960 and has been used for the past 30 years to treat cancer patients [1].dox has several side effects, such as cardiac and renal toxicity.dox-mediated nephropathy is caused by glomeruli destruction and tubule damage [2, 3].dox-induced nephrotoxicity has been studied much less than other anthracycline-related side effects.dox induces nephropathy via a complex mechanism.free radicals, lipid peroxidation, and decreased antioxidant enzyme activity are among the most likely primary mediators in developing nephrotic syndrome [3, 4].inflammation also plays a significant role in dox-induced kidney damage via the effects of topical cytokines and other cytotoxic factors [2].dox can also cause proteins to be excreted in the urine by destroying the nephrons directly.dox-induced nephropathy is a classic model of renal failure in rats.damage to the filtration barrier is the primary cause of protein excretion due to renal failure [2].dox is responsible for this filtration barrier damage [5, 6].thus, administering drugs with anti-inflammatory and antioxidant effects could mitigate the side effects of dox.alamandine is a new member of the reninangiotensin system (ras) family that protects the cardiovascular system and kidney functions [7].it differs from ang (1–7) in only one amino acid in the n-terminal region.it exerts its effects by binding to the mas-related g-protein coupled receptor of the type d (mrgd) receptor[8–10].the renal actions of ang (1–7 (might be related to reductions in oxidative stress and the activation of anti-apoptotic pathways [11].lu et al. showed that ang (1–7) protects kidneys from injuries caused by hypoxia by reducing oxidative stress, fibrosis, and inflammation [11].also, mori et al. used a model of diabetic nephropathy to show that ang (1–7) can protect the kidneys by reducing inflammation, oxidative stress, and lipotoxicity [12].in addition, it was recently reported that ang (1–7) reduces dox-induced cardiotoxicity in rats through antioxidant mechanisms[13].in a study on an animal model of sepsis induction by polysaccharides in c57bl6/j mice, plasma and tissue levels of interleukin-1b (il-1b) and tumor necrosis factor alpha (tnfα) increased.to the best of our knowledge, no prior studies have examined the effects of alamandine on renal function and dox-induced nephrotoxicity.many experimental and clinical reports have highlighted the protective role of ang (1–7) in renal hemodynamics and functions under different conditions [18].in this study, several biomarkers of renal injury were measured in dox-treated rats and alamandine/dox coadministrated rats.these biomarkers included blood urea nitrogen (bun); creatinine; creatinine clearance; serum urea; albumin; and pro-inflammatory cytokines, including il-1β and il-6, pro-inflammatory transcription factor nuclear kappa b (nf-κb), pro-fibrotic proteins transforming growth factor-β (tgf-β), and renal antioxidant enzymes [superoxide dismutase (sod), glutathione peroxidase (gpx), and malondialdehyde (mda)].histological and immunohistochemistry analyses were also performed.materials and methods materials this experiment was performed on 35 male sprague- dawley rats (180–220 g) obtained from the experimental animal centre of fasa university of medical sciences.all procedures followed relevant guidelines and regulations regarding the care and use of animals for the experimental procedures.the procedures were also approved by the committee of animal care of the fasa university of medical sciences (ir.fums.rec.1397.014) in compliance with the arrive guidelines [20].all animals were acclimatized under the controlled standard conditions of 12 h light/12 h dark cycles, at a temperature of 20–22 °c, standard pellet diets, and water ad libitum for one week before the start of experiments.experimental group design after one week of acclimatization to cages, the rats were randomly divided into five groups: 1.these changes in urinary albumin were significantly (p < 0.001) reduced when alamandine was administered (50 mg/kg/day).dox administration increased serum bun (p < 0.001) and serum creatinine (p < 0.001) while significantly decreasing urine creatinine (p = 0.033).co-treatment with alamandine significantly reduced serum bun (p = 0.017) and serum creatinine (p = 0.005) compared to the dox treatment.the results also revealed that creatinine clearance levels were lower in the dox group than in the control group (p = 0.003).rats co-treated with alamandine exhibited an insignificant increase in creatinine clearance compared to the dox group (p = 0.535).effect of different treatments on inflammatory cytokines and nf-kb dox administration increased the serum levels of il-1β (p = 0.005), il-6 (p < 0.001) and nf-κb (p = 0.011).insignificant reductions were observed in il-1β and nf-b levels (p = 0.439 and p = 0.091, respectively).tgf-β levels in the kidney and urine were higher in the dox group than in the control group (p < 0.001).effects of different treatments on renal oxidative stress markers dox administration markedly increased kidney mda (p = 0.005).alamandine co-therapy insignificantly diminished these differences (p = 0.554 for kidney sod and p = 0.999 for gpx activity) (table 3).immunohistological study all immunohistochemical sections of kidney tissues are presented in fig. 1.the minimum expressions of il-6, il-1, p53, and nf-κb were seen in the control group.moreover, the expression levels of p53 were negligible in all groups.intense staining of il-6, il-1, and nf-κb was observed in the dox group (p < 0.01).however, the expression of pro-inflammatory cytokines (il-1, il-6, and nf-κb) was lower in the alamandine + dox treatment group than in the dox group.histopathological study all hematoxylin and eosin (h&e) stained kidney sections from different experimental groups were evaluated histologically.the kidney sections of the alamandine group did not show any significant histopathological changes however, the histopathological evaluation of the kidneys of dox-treated animals showed severe tubular degeneration, tubular cast and moderate tubular necrosis.discussion dox is used to treat multiple solid tumors but has severe adverse effects on the kidneys [25, 26].the mechanisms by which dox causes glomerular toxicity have are not yet fully understood.the conversion of dox to its semiquinone is thought to play an essential role in its nephrotoxic actions[29].semiquinone is unstable under aerobic conditions and, therefore, reacts with molecular oxygen to form superoxide anion radicals[30].as the number of primary free radicals increases, locally infiltrating neutrophils and active mesenchymal glomerular cells generate more free radicals that damage kidney tissues [31].as a result, renal function is compromised following dox administration—in turn, serum urea and creatinine concentrations increase while serum albumin, urea, and creatinine clearance decrease, ultimately leading to extreme proteinuria [33, 34].in the current research, we used a cumulative dose of dox injections to induce an experimental nephrotic syndrome model.this model was characterized by albuminuria, hypoalbuminemia, increased serum levels of bun and creatinine (two significant indicators of renal function), and decreased creatinine clearance) indicator of gfr), all of which are associated with increased oxidative stress and inflammatory factors.the toxic effects of dox administration observed in the current study are consistent with the findings of previous studies in which renal function parameters, including serum urea and creatinine, increased [35, 36].however, administering almandine before, during, and after dox injections yielded improvements in renal function parameters, including attenuated serum urea, creatinine, albumin, and creatinine clearance.our results showed that alamandine co-therapy restored the levels of ros end products (mda) and rospreventing enzymes (sod and gpx) in the kidneys compared to the control group.the results presented in this research are consistent with those of a previous study indicating that alamandine increased antioxidant protein expression in ventricles exposed to i/r injuries [41].in this study, alamandine alone increased il-1.alamandine treatment reduced these pathological lesions.however, further studies must be done to elucidate the downstream events triggered by alamandine/mrgd receptors that modulate this protective effect.
number of words= 1194
[{'rouge-1': {'f': 0.2796067959132569, 'p': 0.7506282722513089,'r': 0.17180109631949883}, 'rouge-2': {'f': 0.15363264026661277, 'p': 0.2963157894736842,'r': 0.10369905956112854}, 'rouge-l': {'f': 0.2850827937353471, 'p': 0.5559154929577466,'r': 0.1916931216931217}}]
-----------------------------------------------------------------------------------------------------------------------------------
p322:
Extractive Summary:
background substandard and falsified (sf) medicines are a wellestablished threat to public health [1–4].world health organization (who) defines ‘substandard’ medicines which are often termed as ‘out of specification’ medicines as authorized medical products that fail to meet either their quality standards or specifications, or both.on the other hand, falsified medicines are those that deliberately/ fraudulently misrepresent their identity, composition or source [5].in general, the problem of substandard medicines, as defined by newton et al. [6, 7], has been overshadowed by the focus on falsified medicines [8–12].indeed, the proportion of substandard medicines in circulation is difficult to ascertain because of inadequate reporting.the maximum number of samples collected from each shop was three.visual inspection each sample was given a unique code after the shipment was received.details of the packaging condition and label information were noted carefully.observations included the packaging and labeling, and physical appearance of the tablet (size, shape, color, etc.) according to the who guideline and the international pharmaceutical federation (fip) checklist for visual inspection of medicines [28–30].authenticity investigation and legitimacy verification for the authenticity investigation of the products and legitimacy verification of the manufacturers, a detailed questionnaire was sent to each manufacturer and regulatory authority of the manufacturing country.sample questionnaire for product authentication and legitimacy verification are presented in supplemental file s1 and supplemental file s2.the registration status of all products as stated on the product packaging was recorded, and included on a questionnaire sent to the importing country to confirm the registration of the product and manufacturer (sample registration verification form is presented as supplemental file s3).laboratory analysis pioglitazone hydrochloride as a reference standard, benzophenone as an internal standard, methanol, acetonitrile, ammonium acetate, potassium chloride and other chemicals of reagent grade were procured from the wako pure chemical industries ltd.japan.hydrochloric acid was purchased from nacalai tesque inc. and acetic acid from alfa aesar.analysis of the sample was done by highperformance liquid chromatography (hplc) according to the modified and validated jp (japanese pharmacopoeia) protocol [34, 35], using a shimadzu prominence hplc equipped with a phenomenex gemini nx c18 column (150 × 4.6mm) and a uv-photodiode array detector (spd-20a/20av series).the dissolution medium was prepared by mixing 50 ml of 0.2 mol/l hydrochloric acid and 150 ml of potassium chloride solution, adding water to make 1000 ml, and adjusting to ph 2.0 with 5 mol/l hydrochloric acid.drug release studies were carried out according to the united states pharmacopoeia (usp) type ii dissolution apparatus paddle method.among the n = 32 online sites visited, all were in breach of japanese regulations in some respect [37].site observation results of online pharmacies are summarized in table 2.authenticity, legitimacy investigation, and registration verification the response rate to our questionnaire (supplemental file s1 and supplemental file s2) was very low, but the manufacturers who replied confirmed their products to be genuine (table 3).results of laboratory analysis the results of the identification test are not shown in the table, as all the samples were confirmed to contain pioglitazone.figure 1a, b, and c shows the frequency of the mean api in the quantity test of all samples collected between 2012 and 2015.however, in the case of the myanmar and personal import samples, there was a major problem with dissolution.among the n = 60 myanmar samples, 13.3% were non-compliant in the dissolution test.discussion our findings revealed serious issues with pioglitazone purchased via the internet and imported into japan.according to the pharmaceutical affairs law in japan, selling prescription drugs without a prescription is prohibited, but among the n = 32 online pharmacy sites visited, none required a prescription to sell pioglitazone (table 2).in addition, 45 mg pioglitazone is not approved for sale in japan, but 4 pharmacies were selling this formulation.also, 14 pharmacies were selling pioglitazone without any restriction on the amount purchased.the authenticity of the products and legitimacy of the manufacturers remained unclear due to the poor responses to our questionnaire (supplemental files s1 and s2) from both manufacturers and medicine regulatory authorities (table 3), as observed previously [17, 38, 39].however, there is clearly a need to improve legitimacy verification as well [40, 41].in the case of china, only 1 sample out of =52 failed to meet the pharmacopoeial requirement for api (table 4).this may mean that quality of pioglitazone in china is better than has been suggested [22, 42, 43].time course studies of the noncomplaint samples showed a marked differences in the dissolution behavior compared to the standard sample.
number of words= 729
[{'rouge-1': {'f': 0.36388513363407043, 'p': 0.6509128630705394,'r': 0.25252933507170794}, 'rouge-2': {'f': 0.177910776812346, 'p': 0.27,'r': 0.13266318537859006}, 'rouge-l': {'f': 0.3526739733920432, 'p': 0.6078787878787879,'r': 0.24839195979899498}}]
-----------------------------------------------------------------------------------------------------------------------------------
p323:
Extractive Summary:
background gasoline station attendants are an important group at occupational risk to btx (benzene, toluene, and xylene) compounds [1–3].among the constituents of gasoline, benzene stands out for its hazardous effects on human health [4–6].furthermore, simultaneous exposure to benzene and other aromatic hydrocarbons, such as toluene and xylene, contributes to maximizing benzene toxicity [7].however, there are health concerns for workers related to low levels of occupational exposure.these concerns are linked to the fact that benzene is a well-recognized genotoxic human carcinogen, classified as a group i chemical by the international agency for research on cancer, and without any known threshold dose [3].in addition, it is well known that the hematologic disorders, leukemia and myelodysplastic syndrome are caused by occupational benzene exposure, as well as various deleterious effects on many other biological systems after long-term exposures [9, 10].epidemiological and experimental studies have shown that benzene exposure can lead to non-cancer health effects, such as, genotoxicity, hematotoxicity, hepatotoxicity and nephrotoxicity [2, 11, 12].the mechanisms of benzene toxicity remain elusive.however, it is well-known that reactive oxygen species (superoxide anion, hydrogen peroxide, hydroxyl radical) resulting from benzene metabolism may damage biomolecules, inducing oxidative stress [10, 11].biomonitoring is a mandatory health protection measure for workers occupationally exposed to benzene [13, 14].biological monitoring has been used as a potential tool for better assessing integrated benzene exposures and may contribute to the diagnosis and treatment of occupational diseases [15].since there are no safe limits for exposures to carcinogens such as benzene, simultaneous evaluation of biomarkers of exposure and effect may be useful in the estimation and reduction of risks caused by occupational exposure to this xenobiotic.such assessments may also be adopted for preventative health initiatives for improving the safety of occupationally exposed workers [9, 16].a number of previous studies by our research group were conducted with the aim of establishing new biomarkers of early damage, which might be helpful in biomonitoring actions related to occupational exposure to benzene.many early biomarkers of genetic [11], hematological [4], renal, hepatic [2], and immunological [2, 17] alterations, caused by low levels of occupational benzene exposure, were proposed for the continuous monitoring of occupational benzene hazards.furthermore, these findings may be key elements in the elucidation of different benzene toxicity mechanisms.based on immunological biomarkers previously proposed by moro et al. [4], the work conducted by sauer et al. [17] showed possible immunotoxicity and carcinogenicity mechanisms arising from benzene exposure.furthermore, alterations in inflammation biomarkers were also reported by sauer et al. [17].in order to continue the previous studies of our research group, this paper aimed to evaluate immunological, inflammatory, and oxidative stress biomarkers in gasoline station attendants, and verify the possible influence of benzene exposure on these biomarkers of effect.methods study population sixty-six individuals were enrolled in this study.the exposed group consisted of 38 gasoline station attendants (gsa) from rio grande do sul, brazil.all subjects had been working in their current job position for at least 6 months.the non-exposed (ne) group consisted of 28 subjects who were non-smokers and who had no history of occupational exposure to benzene or other xenobiotics.each participant was interviewed about aspects of general health, lifestyle, smoking status, and history of exposure.the average ages of the gsa and control groups were 32.1 ± 1.7 years and 30.4 ± 1.8 years, respectively.no significant difference between the gsa and control groups was found related to age.the mean exposure time in the gsa group was 8.5 ± 1.6 years (range: 0.5–32 years).this study was approved by the research ethics committee of the federal university of rio grande do sul/rs (no. 21728/11) and written informed consent was obtained from all participants prior to their enrollment in the study.sample collection the sampling was conducted at the end of the work shift after 3–4 consecutive days of exposure.personal monitors were used to assess airborne btx concentration during the daily work shift, for approximately 8 h. after air sampling, the samplers were stored at − 20 °c until analysis.urine samples were collected for benzene, toluene, and xylene metabolites and creatinine determination.the samples were stored in polyethylene bottles at − 80 °c until further analysis.venous blood samples were collected by venipuncture using vacuum tubes.edta-blood tubes were collected and centrifuged at 1500 g for 10 min at 4 °c. aliquots of edta-plasma were stored at − 80 °c until analysis of the protein carbonyl content.a blood heparin tube was collected for glutathione s-transferase enzymatic activity determination and co-stimulatory molecules of cd80 e and cd86 assay analysis.a vacuum blood tube without anticoagulant was centrifuged at 1500 g for 10 min at room temperature.the serum obtained was aliquoted and stored at − 80 °c until inflammatory cytokine analysis.exposure assessment personal breathing-zone air samples were collected using passive samplers (skc 575–002®).btx (benzene, toluene, o-, m-, p-xylene) were desorbed with dichloromethane and analyzed by gas chromatography and flame ionization detection (gc-fid; perkinelmer, usa).gc column innowax (25m, 0.2mm, 0.4 μm) was used.furthermore, the data suggest a relationship among the evaluated biomarkers of effect, which, if continuously monitored, could contribute to providing early signs of damage to biomolecules in subjects that are occupationally exposed to btx compounds.
number of words= 841
[{'rouge-1': {'f': 0.40853464306017667, 'p': 0.7164646464646465,'r': 0.2857303370786517}, 'rouge-2': {'f': 0.24526674606454602, 'p': 0.39432432432432435,'r': 0.17798650168728908}, 'rouge-l': {'f': 0.3773221757322176, 'p': 0.62625,'r': 0.27}}]
-----------------------------------------------------------------------------------------------------------------------------------
p324:
Extractive Summary:
background it has been suggested that cigarette craving is the most sensitive and consistent predictor of smoking behavior and smoking relapse [1, 2].cigarette craving has been shown to contribute to and sustain nicotine addiction [3] and is seen by many smokers as a key barrier to successful cessation of smoking [4].three studies have investigated the hypothesis that craving is indeed the mechanism through which pharmacological treatment has an effect on smoking cessation and found evidence of partial mediation [5–7].therefore, management of cravings is fundamental to maximizing the chances of success in a quit attempt.nrt increases the success rate of attempts to quit smoking by approximately 50% versus placebo [9, 10], however, long-term success rates are low [11].a new gum that contains 6 mg nicotine was therefore developed, with the aim of providing an additional dosing option for smokers who are highly dependent (> 20 cigarettes per day), and/or requiring enhanced craving relief.the new gum contains nicotine resinate as the active ingredient, with addition of sodium hydrogen carbonate to facilitate absorption of nicotine through the oral mucosa.a previous study has shown that the 6 mg gum releases approximately 50% more nicotine than the 4 mg gum [15].methods study design this randomized, 2-way crossover study compared urges-to-smoke in abstinent smokers following singledose administration of either nicotine 6mg or 4 mg gum (fig. 1).female subjects of childbearing age had to be using an effective form of contraception.a full list of inclusion and exclusion criteria for this study can be found in additional file 1.subjects were financially compensated for their participation.the subjects were to pour the gums straight into their mouths without looking at the contents of the bag.subjects were then instructed to chew the gums slowly for 30 min, with breaks as they considered most convenient.after chewing, without looking at the gum, each subject was to place the used gum on a piece of foil, which was immediately wrapped around the gum by study personnel.the gums were then placed in the labelled aluminium bag a labelled plastic bag and stored at − 20 °c until transport to the analysis laboratory.periods of at least 36 h without nrt separated the two visits.they were instructed not to smoke after 8 pm on the evening prior to each visit; on arrival at the study site around 7.45 am the following morning, a carbon monoxide monitor was used to verify abstinence from smoking (subjects were rescheduled if co > 20 ppm).the aqueous phase was analyzed using a high performance liquid chromatography system and nicotine was quantified using a calibration curve.urges to smoke were scored on a 100mm visual analogue scale (vas) 10 and 3 min before, and at 3, 5, 10, 20, 30, 40, 50 min and 1, 2, 3, 4, 5 h after treatment administration.on the vas, zero represented ‘no urge to smoke’ and 100 represented ‘extreme urge to smoke’ (additional file 2).this type of scale is reliable [17, 18] and can be administered quickly and repeatedly, measures the momentary intensity of urges-to-smoke, and are used regularly in smoking research [19–24].pair-wise treatment comparisons of the average score changes from time zero until 3, 5 and 10 min post-administration were performed in the same manner.the sample size calculation was based on a requirement to have at least 90% statistical power for a true mean difference in average score change at 1 h and 3 h of 12.9mm and 4.3mm, respectively.consequently, the sample size calculation used a significance level of 2.5%.in addition to the planned analyses, two post-hoc analyses were performed to investigate speed and duration of relief of urges to smoke.the endpoints for speed of relief were the time points at which the reduction in the vas urges-to-smoke score first equalled 25, 50 and 75%, respectively, of the baseline value; as estimated using linear interpolation.these were based on follow-up to 5 h post-administration and were calculated using linear interpolation.for statistical analysis, sas v 9.2 was used.table 1 shows the subject characteristics of the 240 enrolled subjects in the study.mean amounts of 4.0 (±1.28) mg and 2.64 (±0.90) mg nicotine i.e. 67 and 66% of the nicotine content of the gums were released from 6mg and 4mg gums, respectively.the 6 mg gum reduced urges to smoke by a mean of 44mm over the first 60 min and 39mm over the first 3 h after administration.post-hoc analysis post-hoc comparisons of the estimated times to 25, 50, and 75% reductions of the baseline urges to smoke scores showed that at all levels of reduction, the times to endpoint were statistically significantly shorter with 6 mg than 4 mg gum.discussion extensive review of present nrt studies and their data establishes that it is clear that nrt increases the chances of stopping smoking by approximately 50% regardless of nrt type [10].a previous systemic review of nrt studies also found that there was a benefit to highly dependent smokers when using a 4 mg nicotine gum compared to a 2 mg nicotine gum [11].urges to smoke was assessed following 12 h of abstinence from smoking, which has been demonstrated to be a reliable method to provoke cravings in a controlled setting [26].the crossover design employed in our study permitted within-subject comparisons of the two study treatments.hence per strict definition this was an open label trial, with the potential of bias following that type of setting.we found that the 6 mg gum provided statistically significantly greater relief of urges to smoke than 4 mg gum within intervals up to 3, 5, 10 min, 1, 2, 3, 4 and 5 h after start of administration, i.e. 6 mg gums provide a greater craving relief than 4 mg gums in a population of smokers who were highly dependent.
number of words= 942
[{'rouge-1': {'f': 0.4084199933725529, 'p': 0.8817154811715482,'r': 0.26576185671039354}, 'rouge-2': {'f': 0.25021219481017515, 'p': 0.48176470588235293,'r': 0.168989898989899}, 'rouge-l': {'f': 0.36233145681724566, 'p': 0.7116666666666667,'r': 0.24303370786516854}}]
-----------------------------------------------------------------------------------------------------------------------------------
p325:
Extractive Summary:
for example, non-steroidal anti-inflammatory drugs (nsaids) are widely used for the management of inflammatory pain conditions, are unable to completely abolish pain and are associated with several serious adverse effects including gastrointestinal bleeding and renal toxicity [8].therefore, in the present study various animal models of pain were carried out to assess the anti-allodynic, anti-hyperalgesic and anti-nociceptive properties of poncirin as a potential new analgesic to treat different types of inflammatory pain.the study was also approved by bioethical committee (approval no: bec-fbs-qau 2017–2) of qau university, islamabad.the vehicle control received only normal saline with 2% dmso and no other treatment or inducer was given.the negative control group received either acetic acid, formalin, carrageenan or cfa and no other treatment was administered.the positive control either received piroxicam (in case of formalin and acetic acid-induced models), while dexamethasone was administered in case of the carrageenan and cfa-induced inflammatory models.the inclusion and exclusion criteria was followed as reported previously [16].the animals were anesthetized with the xylazine + ketamine injection (16 mg + 60 mg, i.p) to avoid distress to the mice (to make them unconscious and reduce the painful feeling associated with the euthanasia) and then co2 chamber was used to euthanize the mice.the institutional ethical committee regulated the overall process of euthanasia.mice were then placed in large glass cylinder (10 cm diameter) and writhing response was measured i.e. number of writhes occurring between 0 and 30 min after acetic acid injection.drugs were administered by i.p route 40 min prior to acetic acid induction, control group was treated with vehicle (2% dmso in 300 μl saline, i.p), positive control was treated with piroxicam (5 mg/kg, i.p) and the treatment group received poncirin (5 mg/kg, 15 mg/kg or 30 mg/kg, i.p).control group was treated with vehicle (2% dmso in 300 μl saline, i.p) and positive control was treated with piroxicam (5 mg/kg), while treatment group received poncirin (5 mg/kg, 15 mg/kg or 30 mg/kg, i.p).the animals were randomly divided into various groups as described above.the anti-hyperalgesic response of poncirin was recorded 4 h after the carrageenan injection.however, the dose of poncirin was skipped at day 5 in order to check any tolerance effects (to see whether the effect of the drug remains persistent or it should be administered daily to achieve the response) as described previously [28].muscle strength and motor activity the muscle strength of mice was determined by using weights test and kondziela’s inverted screen tests, in order to assess the effect of poncirin on motor activity of mice [33].serum aspartate aminotransferase (ast), alanine aminotransferase (alt) and creatinine concentration were determined as indicators of liver and kidney functions respectively [37, 38].paw tissue blocks sections were made of 4 μm thickness, stained with hematoxylin-eosin and observed by microscopy (40×) as per reported protocols [21].similarly, the x-ray analysis (philips 612 machine 40 kw for 0.01 s) was performed as to assess the soft tissue swelling and bone erosion as described previously [21].statistical analysis all results are expressed as mean ± sem.the differences between the control and normal groups were tested by one-way analysis of variance (anova) followed by student’s t-test using spss (version 10.0, chicago, il).a value of p < 0.05 was chosen as the criterion for statistical significance.the graphs were plotted using sigma plot version 12.0, chicago, usa.intraperitoneal administration of poncirin 40 min prior to the acetic acid administration considerably (p < 0.05) reduced the number of abdominal writhing movements as compared to vehicle-treated group (fig. 1).the first phase, named as nociceptive phase, is a result of direct stimulation of nociceptors and is mediated centrally, while the second phase is an inflammatory phase, caused by the local release of hyperalgesic and inflammatory mediators [40].formalin injection produced biphasic paw licking response with the first phase ranged from 0 to 10 min, while the second phase ranged from 10 to 30 min.administration of poncirin 40 min prior to formalin-induction significantly reduced the paw licking response dose dependently in both early phase (fig. 2a) and late phase (fig. 2b) respectively, however, the dose of 30 mg/kg showed maximum response (p < 0.05).the positive control treated with piroxicam (5 mg/kg) also showed the significant anti-nociceptive effect in both phases.poncirin inhibits carrageenan-induced mechanical and thermal hyperalgesia the compound poncirin was tested in three different doses (5 mg/kg, 15 mg/kg or 30 mg/kg, i.p) in first two set of experiments and 30 mg/kg dose of poncirin produced significant analgesic responses when compared with negative control groups, therefore, poncirin (30 mg/kg) was used in subsequent experiments.next, the anti-nociceptive effect of poncirin (30mg/kg, i.p) was assessed in carrageenan-induced acute inflammatory pain model.poncirin noticeably inhibited (p < 0.05) carrageenan- induced mechanical (fig. 3a) and thermal (fig. 3b) hyperalgesia at given dose after 4 h of carrageenan injection.in vehicle-treated group marked increase in pain sensitivity was observed in mechanical (fig. 3a) and thermal hyperalgesia (fig. 3b).whereas the dexamethasone used as positive control also inhibited mechanical and thermal hyperalgesia (fig. 3).the positive control (dexamethasone 5 mg/kg, i.p) also exhibited significant reduction in allodynic responses compared, however, the negative control group showed decreased in pain threshold and hypersensitivity to the allodynic stimulus.furthermore, the positive control group also significantly inhibited the carrageenan-induced paw edema in mice (fig. 3d).dexamethasone also significantly increased the pain threshold as compared to negative control (fig. 4).inhibition of thermal hyperalgesia and cold allodynia by poncirin in cfa-induced pain model in order to investigate the effects of poncirin in cfa-induced thermal hyperalgesia and cold allodynia, animals were treated with poncirin (30 mg/kg, i.p) 40 min prior to cfa injection.dexamethasone treated group also significantly increased the pain threshold as compared to negative control.poncirin (30 mg/kg, i.p) daily treatment significantly increased pain thresholds (fig. 5b) indicating the effectiveness of poncirin in chronic inflammatory pain model.allodynia (fig. 5c) at 4 and 6 h after cfa injection.poncirin inhibited cfa-induced paw edema poncirin inhibited paw edema induced by cfa in both acute and chronic inflammation models.similarly, the positive control (dexamethasone 5 mg/kg) also significantly attenuated the acute paw edema 2, 4 and 6 h after cfa administration, while the daily administration of dexamethasone (5 mg/kg) for 6 days also markedly attenuated the cfa-induced paw edema compared to the negative control (fig. 6b).poncirin doesn’t have any effect on the motor activity of mice poncirin chronic administration does not effected the motor function of the mice, which was evaluated by weights test and kondziela’s screen tests utilized as a screening tool in preliminary drug research for evaluation of motor function (fig. 7a and b) [33, 41].body weight assessment each group of mice were weighted before the disease induction with cfa.similarly, the weight of mice were also recorded at day 6 of the cfa administration at the end of the experiment as shown (additional file 1).the cfa administration markedly increased the production of no in mice plasma at day 6 of the administration.similarly, the dexamethasone- treated group also showed obvious decrease in no production (approximately 83% decrease was noted in no production) (fig. 8).qrt-pcr results showed increased expressions of tnf-α, il-1β, il-6 and vegf mrna in cfa-induced mice paw tissue (fig. 9a, b, c and d).whereas, poncirin treatment strikingly inhibited the mrna expression levels of pro-algesic and inflammatory cytokines such as tnf-α, il-1β, il-6 and vegf (fig. 9).nrf2 activates antioxidant response element (are) that in turns is responsible for the expression of phase ii antioxidant enzymes [42].discussion inflammatory pain is a common chief complaint associated with many disease conditions including irritable bowel syndrome, rheumatoid arthritis and osteoarthritis [2].in the current study, it was demonstrated that systemic administration of poncirin in various inflammatory pain models successfully alleviated pain associated with inflammation.the anti-hyperalgesic effects of poncirin in inflammatory pain can be attributed to suppression of pro-inflammatory cytokines including tnf-α, il-1β and il-6, enhancing the expression of antioxidant genes and enzymes (nrf2, ho-1 and sod2) respectively.acetic acid-induced abdominal writhing is a simple method for novel drugs screening in visceral pain [20].acetic acid administration causes the activation of peritoneal macrophages and mast cells which leads to local release cytokines such as tnf-α and il-1β and other mediators like eicosanoids and sympathomimetic amines [38].since poncirin administration inhibited acetic acid-induced writhing markedly, it is likely that the anti-nociceptive activity of poncirin might be contributed due to its inhibitory activity cytokines production.in present study, systemic administration of flavanone glycoside poncirin remarkably inhibited the mechanical, thermal hyperalgesia and mechanical and cold allodynia induced by carrageenan and cfa in both acute and chronic inflammation models in mice.following the local injection of cfa there is a release of various inflammatory mediators including tnf-α, il-1β and il-6.no also contributes to the hyperalgesic state by indirectly sensitizing the nociceptors through the production of prostanoids such as pge2.in addition, these cytokines increase the synaptic transmission by directly activating nociceptors [4].nrf2 is transcription factor responsible for the induction antioxidant enzymes including glutathione peroxidase (gpx), glutathione s transferase (gst) and ho-1 [42].nrf2 have multiple protective actions including antioxidant activity by induction of antioxidant enzymes, anti-inflammatory role in many diseases as well as protective action in wound healing [57, 58].ho-1 is one of the major anti-inflammatory and cytoprotective enzymes, expression of which is controlled by nrf2 [59].the poncirin treatment was not associated with any toxic effect on the muscle strength and coordination.in addition, poncirin administration also significantly produced anti-allodynic and anti-hyperalgesic effects in carrageenan- and cfa-induced models.importantly, chronic treatment with poncirin in cfa model did not produce any side effects.the mrna expression of vegf was also inhibited by poncirin, which correlates to reduction in paw edema and increased pain thresholds in cfa induced inflammatory pain models.
number of words= 1577
[{'rouge-1': {'f': 0.3187369280953612, 'p': 0.8053951890034365,'r': 0.19868310282621768}, 'rouge-2': {'f': 0.19706619843421355, 'p': 0.4113793103448276,'r': 0.12956678700361013}, 'rouge-l': {'f': 0.33998399385795164, 'p': 0.6405882352941177,'r': 0.23139767054908486}}]
-----------------------------------------------------------------------------------------------------------------------------------
p326:
Extractive Summary:
background optimal dosing and timing, as well as the route of administration and justified use essentially influence the outcome of antibiotic therapy.the excessive use of antibiotics, including overuse and inappropriate application (including using suboptimal doses) of antimicrobial agents fosters bacterial resistance, recognized as one of the most urgent global public health threats, and being the main reason for nosocomial infections [1].therefore, the world health organization (who) makes continuous efforts to raise the awareness of healthcare professionals and patients about proper antibiotic use.some aspects of antibiotic interactions have been extensively studied.it is well known that the coadministration of several antibiotics with milk products should be avoided as bivalent ions, i.e. calcium and magnesium in milk form complexes with these pharmacons, thereby decreasing their absorption [2].although some food and herb interactions with antibiotics have also been described, the number of studies focusing on this issue is limited.for the majority of medicinal herbs it is still undiscovered whether they alter the pharmacokinetics of antibiotics.however, there are reasonable theoretical considerations that suggest the risk of potential interactions.some components of medicinal plants may interfere with the bioavailability of antibiotics by decreasing their absorption.the two most important family of compounds that may potentially affect the pharmacokinetics of antibiotics include fibres and polyphenols, which most frequently occur in commonly used plants.these compounds may affect the absorption of different drugs via chemical or physical interactions.the effect of fibres (chemically polysaccharides) is non-specific, hence these compounds affect the bioavailability of various medicines, such as digoxin, lovastatin, metformin and penicillins [3].polyphenols are known to react chemically with amine-containing molecules resulting in precipitation which may lead to decreased bioavailability [4].although several drug molecules contain amine functionalities, this potential interaction has not been studied in detail.this possible interaction may be avoided by consuming polyphenol-rich food (e.g. fruits) separately from medicines, however, the risk of taking medicines with drinks containing polyphenols is neglected.the most popular drink with a remarkably high polyphenol content is tea, especially green tea.tea is the most widely consumed beverage aside from water, and its consumption is growing worldwide [5].although, globally, black tea is although, globally, the most frequently consumed the most frequently, even in countries where it is the top type of tea (e.g. morocco, turkey, ireland and the united kingdom), green tea consumption shows an increasing popularity due to a growing consumer interest toward healthier beverages [6].green tea is one of the most popular beverages in japan, china, as well as in some countries in north africa and the middle east [7].generally, 40–70% of the japanese population consumesconsume green tea regularly (at least 1 cup/day) [8, 9].in europe is consumed as the top in europe, green tea is the most type of tea in belgium, denmark and france [10].the health benefits of green teathe health benefit of green tea has been extensively studied: it has proved antioxidant, anti-inflammatory, anticarcinogenic, and antimicrobial effects [11, 12].these health promoting effects are mainly associated with its polyphenol content, that may constitute up to 30% of the dry leaf weigh [7].however, the amoxicillin molecule contains amine functionalities, thus it might be subject to interaction with polyphenols.during the whole procedure, the regulations of the hungarian act no. xxviii of 1998 on the protection and care of animals were strictly followed.every procedure (handling, treatment, anaesthesia) executed in this experiment was approved by the committee on ethics of animal experiments of the university of szeged and the directorate of food safety and animal health care, government agency of csongrád county (permit number: xxi./151/2013.).blood sampling the number of occasions for blood sampling were determined according to the pharmacokinetics of the test substance [32, 33].50 μl of each centrifuged plasma sample was pipetted into 250 μl meoh on a 0.5 ml 96-well costar plate and cooled to 4 °c for 1 h. samples were centrifuged (2000 rcf) for 30 min at 8 °c. 50 μl supernatant was taken and evaporated.the concentration of amoxicillin and clavulanic acid was measured using an agilent 1290 hplc coupled to an agilent 6470a triple quadrupole mass spectrometer equipped with an rp column (luna 3 μ c8(2) 100 å 50 × 2 mm; phenomenex 00b-4251-b0).changes of amoxicillin level are shown in fig. 1.however, the highest plasma level of amoxicillin was significantly lower in the amg group, which suggests an interaction between green tea polyphenols and the antibiotic.discussion the aim of our study was to assess the presumable interaction between a frequently used antibiotic (the combination of amoxicillin and clavulanic acid) and green tea infusion, in a setting mimicking the intake of the medicine with green tea.based on literature, in-vitro experiments demonstrate that catechins, the main polyphenolic compounds of green tea, do influence the antimicrobial activity of amoxicillin, and the in-vitro antibacterial effect of amoxicillin against helicobacter pylori is significantly enhanced by the presence of epigallocatechin gallate [36].similarly, the synergistic effect of catechins and amoxicillin against escherichia coli is also reported [17].an opposite effect was also observed in mice infected with methicillinresistant staphylococcus aureus: in an in-vivo experiment green tea extract was found to weaken the effect of amoxicillin after enteral administration (gastric perfusion) [37].our results confirm this finding, and offer a possible explanation for green tea ameliorating the antibacterial efficacy of amoxicillin treatment.as these supplements are likely to be taken concomitantly with medications, the occurrence of an interaction between polyphenols/catechins and amoxicillin has relevance in real-life settings.further studies are needed to explore this interaction regarding per os human therapeutic use.
number of words= 896
[{'rouge-1': {'f': 0.34242668344114546, 'p': 0.8119354838709678,'r': 0.21696485623003195}, 'rouge-2': {'f': 0.1848348897450968, 'p': 0.3510810810810811,'r': 0.12543710021321963}, 'rouge-l': {'f': 0.3015865061419055, 'p': 0.6427272727272728,'r': 0.19701612903225807}}]
-----------------------------------------------------------------------------------------------------------------------------------
p327:
Extractive Summary:
although the etiology of ethanol induced optic neuropathy is multifactorial; malnutrition and direct toxic effects of ethanol are the main factors [2, 3].methods the study was approved by the local ethics committee of ataturk university, erzurum, turkey (ethics committee number: 137, date: 10.26.2017).all animal experiments were performed in accordance with the arvo statement for the use of animals in ophthalmic and vision research.study animals totally 24 albino wistar male rats (265–278 g) were housed at room temperature (22 °c), fed twice a day and had access to water ad libitum for 1 week before the study.the rats were obtained from the ataturk university medical experiments application and research center.experimental procedure in this experiment, to the rats in tet-20 and tpg groups, 20 mg/kg thiamine pyrophosphate was administered via intraperitoneal route.different doses of ethanol were used to produce oxidative stress in organs and tissues of animals.at the end of this period, the animals were sacrificed with high dose (50mg/kg) i.p.biochemical evaluations blood samples were obtained from the tail veins of the rats and collected into separation gel vacutainer serum tubes.from these samples serum malondialdehyde (mda), reduced glutathione (gsh), interleukin 1 beta (il-1β) and tumor necrosis factor alpha (tnf-α) levels were investigated.similarly, tissue il-1β, tnf-α and mda levels, studied on optic nerve tissue, were also the highest in etoh group which were significantly lower in thiamine pyrophosphate administered group.on the other hand, serum and tissue reduced gsh levels were the lowest in etoh group which were significantly higher in thiamine pyrophosphate administered group.on the other hand, the histopathological findings in tet-20 group were near normal except a mild edema.chronic alcohol consumption is clearly known to enhance the production of reactive oxygen species and augment the oxidative stress and oxidative stress is one of the main mechanisms of ethanol induced tissue damage [18, 19].reactive oxygen species produced as a result of ethanol consumption reacts with biological macromolecules such as dna and results in the lipid peroxidation.glutathione is an important non-enzymatic antioxidant decreasing lipid peroxidation in tissues [23].we have determined significantly higher reduced glutathione levels in serum and tissues in thiamine pyrophosphate group compared with the ethanol group showing the anti-oxidant effects of thiamine pyrophosphate.johnsen-soriano et al. [25] reported that chronic alcohol consumption was causing an alteration on the retinal redox status which was returned with a biologically active selenium-organic compound treatment.thiamine is a water-soluble vitamin having a critical role in glucose metabolism.cinici et al. [29] reported that thiamine pyrophosphate significantly reduced the degree of hyperglycemia-induced retinopathy by preventing the oxidative damage.however, this hypothesis should be checked by measuring serum thiamine levels at different time intervals.these recent studies were also supporting our findings that thiamine pyrophosphate was having protective effects on oxidative stress induced ocular damage.first is that; this is an experimental study and direct dosage and pharmacokinetic comparisons between rats and humans may be difficult due to the inter-species differences in metabolism and administration paradigms.second is the low number of rats that is especially kept low regarding the animal rights.moreover, gene based analysis may also show the pathophysiological aspect of these effects of thiamine pyrophosphate.
number of words= 512
[{'rouge-1': {'f': 0.558682280259725, 'p': 0.7122018348623853,'r': 0.45961038961038964}, 'rouge-2': {'f': 0.3094072113531209, 'p': 0.3828834355828221,'r': 0.2595910780669145}, 'rouge-l': {'f': 0.45387722589099877, 'p': 0.581764705882353,'r': 0.3720833333333333}}]
-----------------------------------------------------------------------------------------------------------------------------------
p328:
Extractive Summary:
background in toxicological, entomological and environmental studies, doses of toxicants that kill a defined proportion of organisms, e.g., the median lethal dose (ld50) which kills 50% of the population, are typically used as indicators of acute toxicity.comparing the activities of different toxicants in a specific population or determining the relative susceptibilities of different populations to a single toxicant are common research goals.the relative potency, which assumes that the regression lines of the two toxicants being compared are parallel, provides a convenient comparison of the toxicities of two toxicants [1].however, in practice, many regression lines are not parallel, particularly those derived from bioassays of toxicants with different modes of action, or from same-action toxicants administered to populations with different resistance levels.the 95% confidence limits (cls) of a lethal dose ratio can be calculated by estimating the slopes and intercepts of two probit regression lines and constructing their variance and covariance matrices.the 95% cls of this ratio indicate whether the lethal doses of the two toxicants are statistically different from one another [2].polo-plus software, developed by robertson et al. [3], separately analyzes the data for each substance using probit or logit models based on the joint probability of all observations and calculates lethal dose ratios and their cls at different significance levels.ibm spss provides solution to calculate the lethal doses with 95% cls based on probit or logit models, and also the relative median potency (rmp) assuming that the two regression lines are parallel [4].in this study, we calculated lethal doses and 95% cls of toxicants at different significance levels, as well as the lethal dose ratio and its 95% cls for two toxicants, from probit-log(dose) regression models constructed using the maximum likelihood method in microsoft excel.the effectiveness of this method was compared with that of polo-plus and ibm spss.methods construction of probit-log(dose) regression models for a single toxicant or population for a population treated with serial doses (i) of a toxicant, in which n subjects were treated and r subjects exhibited a characteristic response to each dose, the empirical proportion (p*) of responders was given by where i = 1 to k and k indicated the number of toxicant doses.if the characteristic response occurred in the control group (natural response) with proportion c, the proportions of responders were corrected using the abbott equation for each treatment dose [5]: the corrected proportion (pi) was then converted to a probit value (yi) [1]: which was calculated as yi = norm.s.inv(pi) in excel.a provisional regression line between yi and the logarithm of the dose (xi) was established: yi ¼ α0 þ β0xi: ð4þ in the regression equation, i = 1 to m, where m is the number of toxicant doses at which the corrected proportion was not equal to 1 or 0.the intercept (α0) and slope (β0) could be calculated by the least-squares procedure and were retrieved using the intercept(yi, xi) and slope(yi, xi) functions, respectively, in excel.we then calculated the expected probits (y) for all dose sets, included those where the corrected proportion was 1 or 0:yi ¼ α0 þ β0xi: ð5þ in eq. (5), i = 1 to k. we next calculated the expected response proportion (pi) for each dose set [1].pi ¼ φðyiþ  ð1−cþ þ c; ð6þ where φ(yi) returned the cumulative probability of the standard normal distribution corresponding to (yi), obtainable using the norm.s.dist (yi) function in excel, and c was the natural response proportion, if one existed, in eq. (2).an optimized set of expected probits was then derived from the linear regression equation of working probits weighted on xi, with each yi being assigned a weight, niwi, where wi was the weighting coefficient.  where c was the natural response proportion in eq. (2).the slope β of the working probit-log10(dose) regression equation was the χ2 statistic of the probit regression equation was [1] the significance level p of the χ2 statistic was calculated as the right-tailed probability of the chi-squared distribution (chisq.dist.rt) with k – 2 degrees of freedom (d.f.).a significant χ2 statistic (p < 0.05) might indicate either that the population did not respond independently or that the fitted probit-log(dose) regression line did not adequately describe the dose-response relationship in the test samples.to get an optimal fit of the probit-log10(dose) regression, we substituted α and β for α0 and β0 and repeated the calculations of eq. (5) to eq. (15) until a stable χ2 appeared, indicating convergence.this procedure was a maximum likelihood (ml) method [1].biologically, the slope of a probit or logit regression line represents the change in the proportion of responders per unit change in dose.toxicological evidence suggested that the slope of a dose–response regression line reflected host enzyme activity [19].thus, non-parallel lines might indicate different modes of action of the two toxicants.parallelism between regression pairs was essential for determining the level at which to compare the effects of two toxicants.generally, there were three main categories of parallelism: (i) the two regression lines were statistically parallel (e.g., fairfax vs pixley; fig. 2a); (ii) the two regression lines were not statistically parallel but did not cross within the dominant region (20–80%) of the response proportions (e.g., rotenone vs deguelin; fig. 2b); and (iii) the two regression lines crossed around the median lethal dose (e.g., bugres vs buglab; fig. 2c).in the first case, reporting the ld50s of the two toxicants and their ratios was sufficient.in the second case, one should report both ld50s and ld90s (and/or ld10s) and their ratios.in the third case, reporting the ratios of ld10s, ld50s, ld90s is meaningless, but the significance of difference between the two slopes should be valid.conclusions we successfully developed a method to calculate the lethal doses of a toxicant at different significance levels, and compare lethal dose ratios using probit-log(dose) regression by the ml procedure implemented in microsoft excel.lethal doses calculated using this method at different significance levels, as well as lethal dose ratios with their 95% cls, were identical or close to those calculated using polo-plus and spss.when judged by whether the 95% cls of the lethal ratios included 1.0, all methods reached the same conclusions regarding toxicity differences between two sampl
number of words= 1012
[{'rouge-1': {'f': 0.39809398592066464, 'p': 0.7988732394366198,'r': 0.2650989632422243}, 'rouge-2': {'f': 0.23217136999167579, 'p': 0.41275618374558304,'r': 0.16150943396226414}, 'rouge-l': {'f': 0.3638324057291647, 'p': 0.6037837837837838,'r': 0.26036144578313253}}]
-----------------------------------------------------------------------------------------------------------------------------------
p329:
Extractive Summary:
drug resistance of p. falciparum is the greatest challenge in the fight against malaria, as the parasite has developed resistance to most of the drugs presently used for the treatment of malaria.these factors have led to the earnest need to discover new molecular targets and identify new drug leads against those targets [2].post-translational modifications are necessary for the progression of p. falciparum life cycle [3, 4].among these modifications, protein ubiquitylation plays an important role.ubiquitylation of proteins can be proteasomedependent or independent.proteasome-dependent ubiquitylation pathway involves various biological processes comprising specific components, which make this pathway a potential therapeutic target to develop new anti-malarial drugs.ubiquitin system participates in various cellular functions as well as host-parasite interactions [3].ubiquitin is a highly conserved 76 amino acids’ polypeptide that covalently attaches to target proteins through the combined action of three classes of enzymes namely, the ubiquitinactivating enzyme (e1), the ubiquitin-conjugating enzyme (e2) and the ubiquitin-protein ligase (e3) [5].at the onset, the ubiquitin-activating enzyme (e1) activates ubiquitin through an atp-dependent reaction.in this reaction, the terminal glycine residue of ubiquitin is adenylated and transferred to an internal cysteine residue to generate a high-energy thiol ester intermediate.the activated ubiquitin is then transferred to the ubiquitin-conjugating enzyme (e2), forming another thiol-ether intermediate, which then specifically binds to the ubiquitin-protein ligase (e3).lastly, e3 ligase transfers the activated ubiquitin from e2 to the side chain of a lysine residue of the target protein [3, 5].protein ubiquitylation can be monoubiquitylation (attachment of one ubiquitin), multiubiquitylation (several ubiquitin molecules attached to different lysine residues) or polyubiquitylation (polyubiquitin chain generation) [4].ubiquitin e1 and e2, the two enzymatic components of ubiquitylation system, are highly conserved within eukaryotes [4].however, p. falciparum e3 ubiquitin ligases are considerably diverse and thus may serve as promising targets for new antimalarial discovery [5].the hypothesis behind this study is that targeting the e3 ligases functions of ubiquitin system would selectively kill the malaria parasite and would be less toxic to the mammalian cells.the selective antimalarial inhibitors were further investigated to analyze the effect of e3 ligase inhibitors on intraerythrocytic p. falciparum asexual life cycle development.these studies suggest ubiquitin e3 ligases as potential antimalarial drug targets and support previous reports that the ubiquitin proteosomal degradation pathway may be essential for the survival of the malarial parasite [4, 5].methods cell cultures cq-sensitive (d6, sierra leone) and -resistant (w2, indochina) strains of p. falciparum cultures were obtained from malaria research and reference reagent resource center (mr4), usa and maintained at 5% hematocrit in complete culture medium (rpmi 1640 supplemented with 60 μg/ml amikacin, 27 mm nahco3 and 10% heat-inactivated normal human type a+ serum).the medium of the culture was replaced every 48 h, flushed with a gas mixture of 90% n2, 5% co2 and 5% o2 and incubated at 37 °c. vero cells and thp1 cells were maintained in rpmi 1640 supplemented with 10% fetal bovine serum (fbs) at 37 °c in an incubator with 5% co2.both, vero cells and thp1 cells were obtained from american type culture collection (atcc), usa having catalogue number atcc® ccl-81™ and atcc® tib-202™ respectively and were subcultured twice a week.chemicals eight e3 ligase inhibitors namely, thalidomide, protame, nsc 66811, nutlin 3, hli 373, jnj 26854165, smer 3 and nsc 146109 were used in the study.thalidomide targets cereblon (crbn), which makes an e3 ubiquitin ligase complex with damaged dna-binding protein 1 (ddb1) and cullin-4a (cul4a).thalidomide binds with crbn and inhibits the associated ubiquitin ligase activity [6, 7].protame, a cell-permeable prodrug, converts to an active molecule, tosyl-l-arginine methyl ester (tame) and inhibits the ubiquitin ligase activity of the anaphasepromoting complex/cyclosome (apc/c) [8, 9].nsc 66811 and nutlin 3 have been reported as mdm2 (murine double minute 2) e3 ligase inhibitors [10, 11].jnj 26854165 and hli 373 are hdm2 (human double minute 2) e3 ligase antagonists.these concentrations were almost equivalent to the ic90 of the inhibitors against pfd6 at 48 h. the effect of the compounds was monitored after every 8 h (0, 8, 16, 24, 32, 40 and 48 h) by preparing thin smears of the infected erythrocytes.among the eight e3 ligase inhibitors tested (fig. 1), thalidomide and protame did not show noticeable antimalarial activity against pfd6 and pfw2 up to 50 μm concentration and 120 h of exposure.antimalarial activity of above three inhibitors against pfd6 was not significantly different from the activity against pfw2 except smer 3.smer 3 showed significantly better antimalarial activity at 72 h (p < 0.05) against pfw2 as compared to pfd6.nsc 146109, jnj 26854165 and hli 373 showed lower ic50 values (below 6 μm) against both pfd6 and pfw2 and exhibited early growth inhibition.the ic50 values of hli 373 against pfd6 were significantly different and were almost 2 fold higher against pfw2 as compared to pfd6 at 48 (p < 0.05), 72 (p < 0.01), 96 (p < 0.01) and 120 h (p < 0.01).similarly, nsc 146109 showed significantly better antimalarial activity against pfw2 as compared to pfd6 at 48 (p < 0.0001) and 72 h (p < 0.001).smer 3 and nsc 146109 were toxic against vero cells, and their ic50 values were not considerably different from corresponding ic50 values against pfd6 and pfw2.nsc 146109 and smer 3 were toxic against thp1 cells, and ic50 values of nsc 146109 was not considerably different from corresponding ic50 values against pfd6 and pfw2.nutlin 3, hli 373 and jnj-26854165 were considerably less toxic against thp1 cells as compared to pfd6 (p-value <0.001, <0.001 and <0.001 respectively) and pfw2 (p-value <0.01, <0.001 and <0.001 respectively).the overall level of parasitemia did not change considerably in vehicle treated pfd6 culture up to 48 h, as compared to 0 h (5.28% vs. 5.98%) (fig. 2).cq treatment significantly decreased the overall parasitemia at 40 (p < 0.001) and 48 (p < 0.01) hours, as compared to the vehicle control (fig. 2a).the change in differential parasite counts in cq-treated cells, at 48 h post-treatment, were statistically not significant compared to control vehicle treated pfd6 culture, due to large individual variations.however, the rings detected in cq-treated pfd6 cultures were apparently healthy, but the trophozoites were shrunk (fig. 4).the parasites treated with jnj 2648544858 apparently showed a different life-cycle stage development pattern as compared to control.the parasitemia was significantly reduced in nutlin 3 treated p. falciparum culture as compared to control at 40 h (p < 0.001) (fig. 2d).like nutlin 3, nsc 66811 also is an mdm 2 inhibitor with ki value 120 nm but showed less prominent activity against pfd6 and pfw2 as compare to nutlin 3 [11].conclusions e3 ligase inhibitors namely, jnj 26854165, hli 373 and nutlin 3 showed prominent antimalarial activity against both cq- sensitive pfd6 and -resistant pfw2 strains.jnj 26854165 showed considerably different activity against pfd6 as compared to pfw2 at 72-h post treatment.
number of words= 1111
[{'rouge-1': {'f': 0.3986798276250599, 'p': 0.8039743589743589,'r': 0.26505962521294724}, 'rouge-2': {'f': 0.23207515452413557, 'p': 0.41405144694533763,'r': 0.16121909633418585}, 'rouge-l': {'f': 0.39041242230419526, 'p': 0.6664912280701754,'r': 0.27606060606060606}}]
-----------------------------------------------------------------------------------------------------------------------------------
p330:
Extractive Summary:
background triage, aiming at identifying patients with critical and time-sensitive conditions, is an integrated process in managing patients seeking care at emergency departments (eds) worldwide.it is crucial in order to achieve medically safe prioritization [1–3].however, triage has been shown to be the most error-prone activity at the ed.[2, 4, 5] a vast number of triage systems exist, but the absolute majority lack components for triaging obstetric patients.failure to adequately assess women during pregnancy and the puerperium has repeatedly led to avoidable maternal morbidity and mortality [6, 7].obstetric triage is more complicated and specialised than general triage, requiring assessment of both woman and fetus, as well as of labour status [8, 9].moreover, obstetric patients’ physiology differs from that in the non-obstetric population, and obstetric patients may present with either pregnancy-related complaints or with symptoms of disorders unrelated to pregnancy.a small number of obstetric triage systems (ots) have been introduced internationally since the early 2010s.these otss have adequate interrater reliability (irr), assessed in obstetric settings [10–14].however, to the best of our knowledge, their implementation in a general ed setting has not been evaluated.to ensure that the system is adequate for assessing obstetric patients, the irr must be established for both staff that are trained and untrained in obstetrics.until 2017, there was no ots for assessment of obstetric patients seeking emergency care in sweden.patients have traditionally been assessed according to “who came first” or with triage systems that were not adapted to the physiological changes and spectrum of disease associated with pregnancy.the gothenburg obstetric triage system (gots), developed in 2016 and in clinical use since 2017, has similarities with other ots such as the obstetrical triage acuity scale (otas) [12].the gots, described by lindroos et al. in 2021 (submitted for publication), is a five-level triage scale with cut-off levels for vital signs adapted to the physiological changes of pregnancy [15].the gots includes 14 chief complaint algorithms comprising contractions, suspected rupture of membranes, vaginal bleeding, reduced fetal movements, suspected hypertensive disorder, neurological symptoms, abdominal/back pain, trauma, postpartum haemorrhage, signs of intra- or postpartum infection, chest pain and/or breathing problems, suspected thromboembolic disease, hyperemesis and suspected mastitis.the triage assessment is a construct based on the chief complaint and vital sign parameters.if two different acuity levels emerge from the chief complaint and the vital signs, the patient is allocated to the higher level.initial validity research on the gots has shown a substantial ability to identify and adequately triage patients requiring admittance to hospital [15].the gots differs from other ots in that it has embedded recommendations for initial management, such as laboratory analyses, as well as brief information on both obstetric and non-obstetric possible causes of symptoms.moreover, it has an attached documentation form.the aim of this study was to determine the irr of the gots in obstetric and non-obstetric emergency care staff.a comparison with a reference group was also performed to assess the clinical accuracy and relevance of the irr.methods setting the study was carried out in 2019 at a tertiary care hospital in western sweden with approximately 10,000 deliveries/ year.the hospital has an obstetric ed, annually facilitating about 14,000 obstetric emergency care visits between gestational week 18 + 0 and 12 weeks postpartum.triage is based on the gots and performed by midwives with experience in antenatal care and delivery.obstetric patients with severe circulatory and/or respiratory failure, or suspicion of stroke with severe neurological symptoms, are directed to the general ed, according to hospital routines.at the time of the study, the general ed facilitated about 55,000 emergency visits annually, and the rapid emergency triage and treatment system (retts) was used for triage of patients presenting with medical and surgical complaints [16].study design the study is an irr study based on 30 real-life cases, chosen from a two-month period during 2018.the cases were consecutively selected to represent all 14 gots chief complaint algorithms and cover all five acuity levels.thus, the cases were not representative of the actual patient spectrum, as only 0,5–1% of patients seeking care at the obstetric ed are triaged as red, i.e., the highest acuity level.the real-life cases were converted to paper cases using the information of the cases available to the midwife triaging the patient in real-life, extracted from the associated gots documentation form.the information included a description of symptoms, findings at triage such as fetal heart rate and bleeding as well as vital sign parameters.the conversion was performed by ll and proofread by vs. a reference group was established, consisting of two midwives and two obstetricians with experience in obstetrics ranging from seven to 30 years.a reference triage level, considered to be the true triage level for each case, was established by discussion until complete agreement within the consensus group was reached.all participants as well as the members of the reference group, were unaware of the triage level of the previous real-life assessments of the patients.the study was performed and reported according to the graaschecklist for reporting of studies of reliability and agreement.participants all midwives and registered nurses (rns), respectively performing triage on a daily basis at the hospital’s obstetric and general ed were invited to participate in the study (convenience sampling).the midwives used gots daily in clinical routine while the rns used triage as a working method but with another similarly structured triage system, not entailing pregnancy complications, on a daily basis.information on the study and an invitation to participate were distributed at workplace meetings.however, the cases were selected consecutively during a period of 2 months in order to represent all of the chief complaint algorithm and the five acuity levels in the gots, which was deemed essential.conclusions our findings suggest that gots, the first ots to be implemented and studied in a swedish emergency setting, is a reliable tool for triaging obstetric patients and enables a safe and standardised triage process unrelated to staff’s level of experience in assessing and managing obstetric patients.however, further studies on validity, as well as on patient and staff satisfaction with the triage process, are needed to establish gots as an o
number of words= 996
[{'rouge-1': {'f': 0.3973064877416181, 'p': 0.7832867132867134,'r': 0.2661538461538462}, 'rouge-2': {'f': 0.2336878318806763, 'p': 0.41035087719298247,'r': 0.1633589990375361}, 'rouge-l': {'f': 0.3929650329484343, 'p': 0.6314035087719299,'r': 0.28524663677130047}}]
-----------------------------------------------------------------------------------------------------------------------------------
p331:
Extractive Summary:
an increasing body of evidence suggests that foc can affect a woman’s relationship with the baby, her partner and her family [2, 3], and often results in more frequent requests for epidural analgesia and cesarean section [4–7].it is normal for foc to differ across countries considering that birth is an omnifarious experience.as foc is a negative emotional experience, we speculate that resilience may have an impact on it.this is the first study to examine the impact of resilience on foc and the relationships among foc, childbirth self-efficacy and resilience in pregnant women.responses are given on a four-point likert scale, and scores range from 16 to 64, with higher scores indicating higher foc.the scale included four domains: fear of fetal health; fear of losing control during childbirth; fear of childbirth pain; fear of medical intervention and the hospital environment.caq total scores were categorized as none (16–27), mild (28–39), moderate (40–51) and severe (52–64).wei wand her colleagues translated the scale into chinese, and this scale has good reliability (cronbach’s α = 0.91) and validity (content validity index (cvi) = 0.924) [36].cronbach’s α was 0.92, and the cvi was 0.930 in this study.cronbach’s α coefficient represents internal consistency reliability, and an α coefficient ≥ 0.70 indicates acceptable reliability [37].childbirth self‑efficacy the short form of 32-item chinese childbirth self-efficacy inventory (cbsei-c32) was used to measure childbirth self-efficacy.the outcome expectancy subscale (oe-16) and efficacy expectancy subscale (ee-16) make up the cbsei-c32 [38].each item is answered on a tenpoint likert scale ranging from 1 to 10.total scores range from 32 to 320, and the higher scores are, the higher the self-efficacy.the cronbach’s alpha for each subscale was 0.96 and 0.97, and the cvi was 0.962 in this study.resilience to measure resilience in pregnant chinese women, we used the 10-item connor-davidson resilience scale (cd-risc-10) in this study.campbell-sills and stein created the original english version of the cd-risc-10 [40].then, the scale was translated into chinese and used to measure resilience in chinese earthquake victims by wang and his colleagues [41].responses are given on a four-point likert scale, ranging from 0(“never”) to 4 (“nearly always”), with higher total scores representing better levels of resilience.in this study, the cronbach’s alpha was 0.91, and the cvi was 0.925.statistical analysis the mean and standard deviation (sd) were used to describe continuous variables and frequencies with percentages were used to summarize categorical variables.if the above variables had a p value < 0.05 in a t-test/anova or pearson correlation analysis, they were retained in the hierarchical regression analysis model.cook’s distances (< 1.0) were computed to identify influential cases and outliers.the dw value was 1.900 in our study.the variance inflation factor (vif) was applied to diagnose the possibility of multicollinearity among all the explanatory variables.a vif less than 5 indicates that there is no serious multicollinearity.all the vif values were < 5 in this study.a p value < 0.05 was considered statistically significant.data were recorded using epidata version 3.1 after checking for completeness, and analyses were conducted using ibm spss statistics version 25.we performed structural equation modeling to analyze the mediation model.maximum likelihood estimation was employed as a global test of models.it is believed that an indirect effect is significant at the 0.05 level if the bias corrected 95% confidence interval (ci) from 5000 bootstrap samples does not include 0.amos 23.0 was used for the modeling.results description of participants’ basic characteristics and their correlations with foc table 1 shows the sample’s sociodemographic and obstetric characteristics and their associations with the caq scores.the age range was 18 to 42 years, with a mean age of 28.7 (sd = 3.8).foc levels and the correlations among foc, self‑efficacy, and resilience the caq, self-efficacy, and resilience scale scores are shown in table 2. among a total of 646 participants, the prevalence rates of mild, moderate, and severe foc were 45.4% (n = 293), 19.5% (n = 126), and 2.2% (n = 14), respectively.overall, the final model explained 64.5% of the variance in foc and revealed six variables that contributed significantly to foc.childbirth self-efficacy was the strongest predictor of foc, followed by resilience.regarding sociodemographic variables, advanced age, late pregnancy, being nulliparous, and poor spousal support were predictors of a higher degree of foc.according to the model, foc was significantly predicted by resilience and childbirth self-efficacy.the standardized direct effect value of childbirth self-efficacy on foc was -0.58(p < 0.001), and the standardized direct effect value of resilience on foc was -0.33 (p < 0.001).it is difficult to compare the incidence of foc across countries due to differences in the measures and definitions used.in addition, poor partner support was correlated with foc.among the obstetric features, gestational age and parity are significant predictors of foc.to the best of our knowledge, this is the first study to explore the association between resilience and foc, providing a new perspective for developing related interventions.it is clear that childbirth self-efficacy plays an important role in predicting foc.the mediation effect rate was 53.5%, confirming that resilience indirectly acted on foc through self-efficacy.this finding showed that preventive interventions aimed at enhancing resilience and self-efficacy may be conducive to effectively alleviating pregnant women’s fear.conclusions in sum, we found a high prevalence of foc among pregnant women in china.second, because this study was cross-sectional, conclusions about the causal relation between foc and related factors could not be derived.
number of words= 877
[{'rouge-1': {'f': 0.41527664846547996, 'p': 0.7734482758620689,'r': 0.2838364779874214}, 'rouge-2': {'f': 0.2148985695497526, 'p': 0.35027681660899657,'r': 0.15499475341028332}, 'rouge-l': {'f': 0.37576899343154774, 'p': 0.6154545454545455,'r': 0.270445434298441}}]
-----------------------------------------------------------------------------------------------------------------------------------
p332:
Extractive Summary:
background pregnancy is characterized by various changes, including hormonal changes, different personal expectations and a new coordination of the professional and social environment, often accompanied by financial or health concerns [1–3].depending on the circumstances, the degree of support from the environment and many other (individual) factors, these changes can lead to intense emotional states and stress [4].a negative maternal emotional state or maternal stress over a longer period of time not only affects mental health of the becoming mother but can interfere with the development of the offspring: several studies have shown adverse effects of intense negative maternal emotional states, ranging from prenatal distress to peripartum mental disorders, on fetal and infant development [5–8].more specifically, adverse effects on the physiological, metabolic and neuronal development of the fetus have been reported [9–11].the fetal autonomic nervous system reacts and adapts rapidly to environmental changes: higher fetal heart rate variability and lower fetal heart rate are indicators for fetal well-being.accordingly, several studies have shown that increased maternal depressive or stress symptoms lead to changes in fetal cardiovascular activity [12–14].furthermore, recent studies reported a relation between maternal stress and the offspring’s physiological stress reactivity and cortisol levels [15–18].altered maternal cortisol levels due to high and chronic stress have been hypothesized to be related to the development of emotional and behavioral psychopathology across the child’s lifespan, potentially also affecting cognitive performance and brain volume [19–21].the difference in the pre-rating values of vas between the groups was not statistically significant, f (1, 34)=1.318, p = .259.numerically, women with higher ga (group 2) had lower values in the pre-rating, meaning that they might have been somewhat less stressed (mean (sd) group 1: 2.33 (1.59); group 2: 1.79 (1.72)) beforehand.as an explorative approach, an anova showed a significant difference between ga groups in mean vas delta, f (1, 34)=4.971, p = .032, ηp 2 = .128, i.e., women with higher ga had less change in vas levels (delta mean: 1.02) compared to women with lower ga (delta mean: 1.23).thus, women earlier in the third trimester (30th–34th gestational week) appear to be able to relax more easily compared to women with higher ga.however, when we included chronic stress and ‘participating in another measurement before at our center’ as covariates, this effect was no longer significant, f (1, 32)=2.113, p = .156.discussion this study aimed to investigate the effect of three different types of relaxation interventions on subjective and physiological stress-related parameters in the last trimester of pregnancy.we observed that maternal physiological measures (hr, scl) showed a decrease independent of the type of intervention, albeit not to a significant extent.thus, relaxation effects are still ongoing during the recovery phase, but are less intense than in the relaxation phase.furthermore, our study shows a significant improvement in women’s subjective stress levels and mood independent of intervention, with slight differences in subscales between interventions.in our study ga was associated with subjective stress ratings, indicating that women in later stages of pregnancy (35th–40th gestational week) did profit less from relaxation than women in the earlier weeks of gestation (30th–34th gestational week).however, we did not find any significant differences between active (guided imagery group) and passive (music group, resting group) mental-based interventions.the fact that no significant differences were observed between the interventions indicates that quiet, comfortable sitting can, in certain situations, be just as effective as an active mental-based relaxation technique in the short term, on both objective and subjective relaxation parameters.moreover, the significant differences between relaxation and recovery phase emphasize that duration of relaxation might play a role.we showed that, during a 20- min relaxation and recovery period, the relaxation effect increased over time.these results are consistent with previous studies which described an ongoing decrease in maternal physiological parameters following relaxation interventions in pregnant women [30, 43].in our study, we used different relaxation techniques, but explicitly considered each technique separately to determine possible differences in effectiveness between acute active and acute passive relaxation techniques.for maternal heart rate, we found an increase for the guided imagery group despite the fact that women rated the guided imagery as relaxing.this may be due to the active relaxation stimulation.however, when comparing all participants, we did not find significant differences between the groups.this is in line with a study by teixera and colleagues [25] who showed a greater general decrease in maternal heart rate following combined active and passive relaxation for 58 women between 28th and -34th weeks of gestation.a study by urech and colleagues [44] reported significant differences between mental-based active (guided imagery), body-based active (progressive muscle relaxation) and passive (quiet sitting) relaxation intervention in 39 healthy pregnant women on the basis of subjective ratings and cardiovascular activity.to our knowledge, this study is the first of its kind showing a significant decrease in scl from relaxation to recovery after a short acute relaxation intervention in pregnant women.the direction of this effect is not in line with a study by dipietro and colleagues [28], who reported a difference in scl between the relaxation and recovery phase, showing an increase in the latter.however, in their study a combined relaxation intervention of progressive muscle relaxation, audio-recorded guided imagery and self-selected music was used.the baseline measurement, the following stimulation intervention and the post-relaxation phase each lasted 18 min.in addition, 41% of the participants were given a 18-min pre-baseline (rest).it should be noted that in the study of dipietro and colleagues, relaxation and recovery was interrupted when the lights were turned on and different questions were answered.this might explain why they reported a significant increase in scl from baseline to relaxation and from relaxation to recovery.additionally, dipietro and colleagues showed positive effects on subscales such as physiological tension, physical assessment and cognitive tension which are in line with our results.in general, our results show that an acute relaxation intervention during pregnancy without disturbance for 10 min can still lead to relaxation when followed by a recovery phase (also lasting for 10 min).all types of interventions used were effective in generating a subjective feeling of relaxation as indicated by a significant decrease in subjective stress ratings post- compared to prerelaxation intervention.our results indicate overall lower subjective stress symptoms after relaxation induction and a decrease in depressive and anxiety symptoms, and are therefore in line with the literature [25, 26, 44–46].notably, in this study we included the maternal chronic stress level as covariate since participants in the three intervention groups had significantly different chronic stress levels, which can be a driving factor in stress perception.as pointed out above, stress during pregnancy can have serious consequences for the mother and the child.clinical interest in the prevention of stress and depression during and after pregnancy is therefore high.the results of this study may be a starting point in helping to find a particularly suitable intervention for pregnant women and to develop brief but effective relaxation programs for practical use.limitations there are several limitations to this study.all participating women were at our center for the first time, but some women participated in another independent measurement before and thus were therefore already familiar with the environment, and were presumably calmer, which could have possibly affected our parameters.by including a variable entitled ‘participated in another measurement’ as covariate in our group comparisons we tried to control for this difference.with regard to the differences in gas, it must be noted that when we adjusted for our covariates, the results did not remain significant.this may confirm our assumption that women who were in the center longer before the measurement were calmer and thus rated to be less stressed on the pre-rating of the vas.of the nine women who had already participated in a study before our measurement, n= 7 were in the older ga group and n= 2 in the younger ga group.this might explain why women in older ga group felt less stressed at the beginning.furthermore, almost all participating women had a high school diploma or higher education, which can be seen as a proxy for a higher socio-economic status.previous studies have shown that women with a lower socio-economic status have a higher stress level per se.this could be due to financial problems, work overload, lack of family support etc.[2, 47–49].further studies should therefore include participants with more diverse socio-economic statuses.on the basis of a study by doberenz and colleagues [50, 51], scl and heart rates are known to be lower in sleep than when awake.during the measurement, five women fell asleep (music: n = 2; guided imagery: n = 2, resting: n = 1), which can lead to a different baseline in scl and thus to a less significant decrease in scl compared to women who remained awake for the whole intervention.in addition, we asked all women before each measurement if the room temperature was comfortable, but we did not check this with a thermometer in the room.nevertheless, we can assume that the temperature was constantly warm over the course of the study (approximately between 22 and 25 degrees (celsius)).however, since scl are directly related to environmental temperature, this must be listed under limiting factors of the study.additionally, we did not follow up the women in later periods of pregnancy to realize if acute relaxation can affect their stress.conclusion the results of this study suggest that prenatal acute relaxation, even if applied only once, can be a useful nonpharmacological tool to provide a higher state of maternal well-being during pregnancy.improving maternal wellbeing and reducing stress in this way might mitigate potential negative effects of maternal stress on the fetus.despite its limitations, this study was the first of its kind to show that, independent of acute active or acute passive relaxation, a relaxation intervention (music, guided imagery, resting) can affect a reduction in maternal stress symptoms in the third trimester of pregnancy.interestingly, it seemed that the subjective psychological relaxation effect tends to decrease with ga.notably, the results of this study show that there are differences between perceived subjective relaxation and the underlying physiological correlates.despite some women rating the guided imagery as very relaxing, maternal heart rate increased over time in this group.further research is clearly required to specify methods of relaxation during pregnancy.follow-up studies will therefore be necessary, in particular to investigate the long-term effects of relaxation techniques on subjective and physiological stress levels on both the mother and the unborn chi
number of words= 1701
[{'rouge-1': {'f': 0.3089052294752589, 'p': 0.756335403726708,'r': 0.19408759124087593}, 'rouge-2': {'f': 0.15743615594086474, 'p': 0.28495327102803736,'r': 0.10876404494382023}, 'rouge-l': {'f': 0.32713788433379043, 'p': 0.6029670329670329,'r': 0.22445859872611465}}]
-----------------------------------------------------------------------------------------------------------------------------------
p333:
Extractive Summary:
thus, targeted monitoring and management for women with pregnancy complications are recommended to decrease the rates of cardiovascular diseases (cvd) after delivery [4, 5].in the us, twin births continue to rise from 1.9% in 1980 to 3.3% of all live births in 2009 [6].moreover, this condition is usually considered as a risk for the mother as well.accordingly, this study aimed to determine whether multiple gestations are associated with htn beyond the peripartum period.methods characteristics of the data this study was conducted by merging the databases of the korea national health insurance (knhi) claims and national health screening examination (nhse).a flowchart of patient enrollment is shown in fig. 1. to facilitate the evaluation of the pre-pregnancy characteristics, women were included in the analysis if they underwent an nhse within one year prior to their pregnancy.among them, women with prepregnancy htn were excluded.obesity was defined as bmi ≥ 25 kg/m2.blood pressure (bp) was measured using a standard mercury sphygmomanometer.women with systolic bp (sbp) ≥ 140 and/or diastolic bp (dbp) ≥ 90 mmhg were excluded from the study.blood samples were obtained after a fast of at least 8 h. the levels of fasting glucose, total cholesterol (tc), aspartate aminotransferase (ast), and alanine aminotransferase (alt) were measured.high tc was defined as tc ≥ 200 mg/dl and abnormal liver function test was defined as ast ≥ 31 or alt ≥ 31 mg/ dl.the cumulative incidence of htn was estimated using the kaplan–meier method and compared using the log-rank test.we also determined the incidence rate of htn (person-years).cox proportional hazards models were used to estimate the adjusted hazard ratios (hrs) and 95% confidence intervals (cis) for the development of htn.participants were censored if they developed htn or on december 31, 2015, in those without htn.all tests were two-sided, and p-values < 0.05 were considered statistically significant.statistical analyses were performed using sas for windows, version 9.4 (sas inc., cary, nc, usa).results among 362,821 women who gave birth during the study period and underwent an nhse within one year prior to their pregnancy, 4,944 (1.36%) women had multiple gestations.characteristics of participants with respect to the number of pregnancies the pre-pregnancy and pregnancy characteristics of participants are presented in table 1.women with multiple gestations had a higher level of fasting glucose, tc, ast, the prevalence of obesity, and dm and a lower prevalence of smoking but higher prevalence of regular exercise compared with women with singleton pregnancy.however, there were no differences in bmi, bp, alt levels, and the prevalence of atrial fibrillation, high tc and abnormal lft between the two groups.risk of htn with respect to the number of pregnancies figure 2 shows the kaplan–meier curves for the cumulative incidence of htn between singleton and multiple gestations groups.during follow-up, the cumulative incidence of htn was higher in multiple gestations group compared with singleton group (5.95% vs. 3.78%, p < 0.01, respectively).discussion main finding in this study, we evaluated the association between multiple gestations and the development of htn after delivery and found that women with multiple gestations had an increased risk of htn during 7 years follow-up period compared with those with singleton.however, in this study, multiple gestations remained significantly associated with the development of htn later in life when preeclampsia was adjusted in multivariable analysis or participants were divided based on the presence of preeclampsia, suggesting the existence of another mechanism.it has been reported that the rate of severe preeclampsia was significantly increased in triplet pregnancy as compared to twin pregnancy although there was no change in the overall rate of preeclampsia [14].in addition, women who had multiple gestations before 2007 and had subsequent singleton pregnancy were not considered as history of multiple gestation.the knhi databases does not contain detailed previous obstetric histories and chorionicity in each patient.
number of words= 620
[{'rouge-1': {'f': 0.4703456695861874, 'p': 0.8167248908296942,'r': 0.3302739726027397}, 'rouge-2': {'f': 0.32780128976925993, 'p': 0.5436842105263158,'r': 0.23463414634146343}, 'rouge-l': {'f': 0.48294845331505193, 'p': 0.7039869281045752,'r': 0.36754601226993866}}]
-----------------------------------------------------------------------------------------------------------------------------------
p334:
Extractive Summary:
before 2019, we were inexperienced in villus cell culture techniques.therefore, the cases received chorion villus sampling were those with severe fetal structural abnormalities.no villus cell culture was applied for those cases, and only cnvs was provided.laboratory testing cell culture and cytogenetic karyotyping amniotic fluid and umbilical blood samples were set up for cell culture following the standard protocols.chromosome preparations were g-banded using trypsin-giemsa staining for cytogenetic karyotyping after a series of standard protocols including colchicine treatment, hypotonic treatment, fixation and centrifugation.karyotypes were diagnosed according to the international system for human cytogenetic nomenclature (iscn, 2009) [9, 10].the classification and abbreviations of abnormal karyotypes in this study were as follow: ds, es, patau syndrome (ps), super female syndrome (xxx), super male syndrome (xyy), klinefelter syndrome (xxy), turner syndrome (turner), abnormal sex chromosome number mosaic (sex a mosaic), autosomal aneuploid mosaic (auto a mosaic), possibly balanced mutual translocation (translocation), chromosome polymorphism (polymorphism), triploid, chromosome fragment duplication/deletion, subtle structural variations such as inv.(21), inv.(4), dup (21), inv.(y), inv.(1), inv.(5), inv.(12), inv.(8), inv.(19), inv.(y), inv.(10), inv.(16).a total of 52 cases had only cnv-seq results, but no karyotyping results.among them, 44 cases who received chorion villus sampling, since our center cannot provide villus cell culture during that time; 8 cases encountered amniotic fluid cell culture failure.the maximum and minimum gestational weeks of amniotic fluid cell culture failure were 31 and 20 weeks, respectively.it should be noted that none of the 52 cases with missing results were used in the data analysis of this study.cnv-seq and result interpretation nextseq 550ar platform (illumina, san diego, ca) was used for dna sequencing, with an average sequencing depth of 0.08×, following the q30 sequencing quality standard.the amount of fetal dna used for cnv-seq was 10 ~ 50 ng for each prenatal sample.annoroadpd software (annoroad gene technology co., ltd., beijing, china) was applied to analyze the sequencing data referring to the human reference genome grch37/hg19.the identified fetal cnv were interpreted [11] and classified into five categories: pathogenic (p-), likely pathogenic (lp-), uncertain significance (vus-), likely benign (lb-) and benign (b-), according to the standards and guidelines that were jointly developed by the american college of medical genetics and genomics (acmg), the association for molecular pathology (amp) and the college of american pathologists (cap) in 2015.to conveniently show the cnv-seq results, we used “p- “as abbreviation for pathogenic chromosome microdeletion/ duplication, “none” for no copy number variation found, “auto a” for autosomal aneuploidy, “sex a” for abnormal sex chromosomes number, “auto am” for autosomal aneuploidy mosaic, and “sex am” for abnormal sex chromosomes number mosaic.cytogenetic karyotyping was the diagnostic method for numerical and structural chromosome abnormalities, and high-throughput sequencing for cnv.for lpand vus-, family (parents and fetuses) cnv-seq tests, fluorescence in-situ hybridization (fish) or multiplex ligation probe amplification (mlpa) were used for further verification.statistical analysis the data were analyzed statistically using ibm spss statistics (version 22.0, ibm corp., armonk, ny, usa).continuous variables (for example, age and gestational weeks at prenatal diagnosis) were expressed as “median [lower quartile, upper quartile]”, and analyzed using kruskal-wallis one-way analysis of variance.categorical variables are represented by “n (%)” and analyzed using chi-square test for two-way disordered r × c table.calculation for sensitivity and specificity: sensitivity = true positive / (true positive + false negative) *100%; specificity = true negative / (true negative + false positive) *100%.paired chi-square test was used to test the difference between cnv-seq and various prenatal screening methods and prenatal diagnosis results (p< 0.01 was considered statistically significant).missing items were not applied in data analysis.results basic characteristics of study patients basic information of 9452 cases of prenatal diagnosis was listed and statistically analyzed in table 1.among study patients, 9452 (100%) had received one to two times of fetal ultrasonography in our center, 5688 (60.2%) had maternal serum screening, 1409 (14.9%) had nipt, 551 (0.58%) had both nipt and maternal serum screening, and 3142 cases (33.2%) were in advanced maternal age.the results showed that only 1165 (12.3%) of patients received invasive prenatal diagnosis due to high risk of nipt.other indications for prenatal diagnosis included advanced maternal age, abnormal fetal ultrasound scanning, high risk of maternal serum screening, adverse reproductive history, family history of singlegene genetic diseases, or others.comparison of results between cnv-seq and cytogenetic karyotyping the results of 9452 cases of cytogenetic karyotyping were listed in table 2: a total of 704 (7.5%) cases of fetal chromosome abnormalities, 171 (1.8%) chromosomal polymorphism, 20 (0.2%) subtle structural variations, 74 (0.7%) mutual translocation (possibly balanced), 52 (0.6%) without karyotyping results, and 8431 (89.2%) normal karyotypes were detected.the results of cnv-seq in tables 2, 8,354 fetuses with cnv-seq findings were included as none, b-, and lb-, cytogenetic karyotyping showed that except for 2 cases of triploid, the rest 271 cases of abnormal karyotypes had good prognosis.a total of 530 cases of fetal aneuploidies (ds, es, ps, xxy, xyy) were diagnosed, and the results of karyotyping and cnv-seq were consistent.the details for 60 cases of pathogenic microdeletion/duplication detected by cnv-seq were shown in table 3.cnv-seq detected 1 case of chromosomal aneuploidy and 2 cases of mosaic in fetuses with normal cytogenetic karyotypes.furthermore, 2 cases (no.29 and 30) of pathogenic microdeletion/duplication were detected in 9 fetuses with marker chromosomes, and 1 case (no.38) of pathogenic microdeletion/duplication were detected in fetuses with mutual translocations (seen in table 4).therefore, we may conclude that the combination of the two methodologies significantly improved the accuracy of prenatal diagnosis for fetal pathogenic cnv and was helpful to assess fetal prognosis.
number of words= 905
[{'rouge-1': {'f': 0.36925711668869465, 'p': 0.8087387387387388,'r': 0.2392466460268318}, 'rouge-2': {'f': 0.20806424319791, 'p': 0.3867420814479638,'r': 0.14231404958677685}, 'rouge-l': {'f': 0.34316123261312953, 'p': 0.7165517241379311,'r': 0.22560165975103735}}]
-----------------------------------------------------------------------------------------------------------------------------------
p335:
Extractive Summary:
we aimed to assess the correlation between vd serum level and vat during early pregnancy.exclusion criteria were twin pregnancy, diabetes mellitus, previous gestational diabetes or preeclampsia, fetal malformations, maternal mental disease.the mean age was 26 ± 5.76 years and pregnant women were included in the study with a 12.3 ± 2.5 weeks of gestation.129 (67.8 %) of pregnant women had vitamin d insuficiency, i.e., serum value of 25 (oh) d < 30 ng/ml.pregnant women with or without vdd did not differ in age, gestational age, nutritional status and vat (table 1).the spearman value correlation between vat and 25(oh) d was – 0.057, (p value = 0.435).table 2 presents spearman correlation between vat, 25(oh) d and age, weight and bmi.discussion in the present study we did not find association between serum vd values and visceral fat in pregnant women.some studies have described an inverse association between vdd and visceral fat in non-pregnancy populations [24, 14].however, among the various physiological changes that occur during pregnancy, the redistribution of adipose tissue is one of them.although not yet properly studied, it is admitted that an increase in visceral adipose tissue occurs, but the specific function of visceral adipose in pregnancy is still unknown.recently, carreras-badosa et al. found that maternal serum vd was inversely associated with visceral fat and in their offspring at the age of 5–6 years [25].however, diet and lifestyle habits were not studied in both mothers and their children, and this, rather than maternal vd status could explain the relationship between lower maternal serum vd and offspring adiposity.an increased risk of vdd has been described worldwide in obese individual.the underlying explanations are not clear.other explanation is based on experimental findings that deficiency of vd can increases lipogenesis upregulating adipocyte calcium signaling and improving the secretion of parathyroid hormone [27].in the present study, the prevalence of vdd was high (68 %).this value was similar to other brazilian studies and higher than that described in countries such as spain and united states [28].brazil is a tropical country with continental dimensions and abundant sunshine.in the multicenter hapo study, increased maternal bmi was associated with lower maternal 25(oh) d levels [30].however, bmi is not an accurate measure of fat tissue, especially during pregnancy.at first, a crosssectional design was performed and we could not determine the causal relationship between vd and vat.however, as this was the first study to assess the association between vat and vd in pregnant women, further studies are needed to confirm these findin
number of words= 410
[{'rouge-1': {'f': 0.42893741971003857, 'p': 0.7410526315789474,'r': 0.3018181818181818}, 'rouge-2': {'f': 0.248727881256209, 'p': 0.39450331125827814,'r': 0.18161731207289294}, 'rouge-l': {'f': 0.42399553864322276, 'p': 0.6786956521739131,'r': 0.30829787234042555}}]
-----------------------------------------------------------------------------------------------------------------------------------
p336:
Extractive Summary:
background premature ovarian insufficiency (poi) is an ovarian insufficiency syndrome before the age of 40 years affecting approximately 1–2% women [26, 12].it is characterized by a continuous decline in ovarian function, and resulting in an earlier cessation of menstruation than normal [12].women with poi are faced with increased risk of low chance of natural conception [4, 38], urogenital atrophy [34], decrease in bone mineral density [3], autoimmune and thyroid disease risk [11], cognitive dysfunction [18], shortened life expectancy [30], and cardiovascular disease [44, 15].poi is a multifactorial disease [33].spontaneous poi is associated with genetic defects, autoimmunue diseases, enzyme deficiency and environmental factors, and iatrogenic poi occurs mainly due to surgical intervention, chemotherapy and radiotherapy [18, 33].there is ample evidence indicating that ovary is damaged by autoimmunity through alteration of t-cell subsets, t-cell-mediated injury, increasing of autoantibodyproducing b-cells, decreasing of effector suppressor/cytotoxic lymphocytes, and decreasing of natural killer cells [11].it supports that antoimmune etiology exists in poi based on the presence of lymphocytic oophoritis, association with autoimmune disorders, and autoantibodies [11, 21].a number of studies have demonstrated that gut microbiome play a key role in the autoimmune process [45].such as the peptides generated by gut microbiota could induce immune cells to become autoreactive and crossactivated [45].moreover, the dysbiosis of gut microbiome not only can affect the activation of b lymphocytes and the production of autoantibodies, but also induce the aberrant activation of innate immune cells, which leads to the upregulation of proinflammatory cytokines [45].in addition, increasing evidences have strongly suggested that gut microbiome play an important role in poiassociated symptoms, including autoimmune dysfunction [39, 8], bone health [20, 29], cognitive and neurological health [28, 14].gut microbiota and its metabolites also have the ability to regulate inflammation pathway activation, brain-gut peptide secretion and the destruction of islet β-cell [46, 22].all these studies indicate a relationship may exist between the gut microbiome and poi.in order to study the community profile of gut microbiome in women with poi, and how the changes of gut microbiota correlated with the sex hormones, 35 women with poi and 18 healthy women were recruited in this study.sequencing of the v3-v4 regions of 16s rrna gene in fecal samples was performed to reveal the substantial differences of gut microbiota between the poi subjects and controls.methods study cohort a total of 35 women with spontaneous poi and 18 healthy women, aged 24 to 40 years, were recruited at the shenzhen maternity&child healthcare hospital from august 2019 to september 2019.spontaneous poi was diagnosed and assessed according to the previously reported [17], primary or secondary amenorrhea for at least 4 months before 40 years, and with at least two instances of serum follicle-stimulating hormone (fsh) levels exceeded 40 iu/l with an interval of 4–6 weeks.all the control women had normal ovarian function, without history of menstrual dysfunction and infertility, with regular menstruation and normal levels of fsh (< 10 iu/l).participants were excluded if with following situations: non-46-xx karyotype, poi with family history, pregnancy, tumor, chronic diarrhea, autoimmune diseases, use of antibiotics/medications with the preceding 3 months, pelvic surgery, gastrointestinal disease, active infections, body mass index < 18.5 or > 23.9 kg/m2, smoking and chemo/ radiotherapy treatment.the study protocol was approved by the ethics committee of shenzhen maternity&child healthcare hospital.written informed consents were obtained from all participants prior to enrollment.and clinical characteristics were extracted from the health records.sampling all participants were examined in the morning after > 8 h fasting.accumulating researches indicates that estrogen regulate glucose and lipid metabolism, bone formation and inflammatory response, its reduction can impair estrogen-dependent processes, triggering cardiovascular disease, osteoporosis and so on [2, 24].these symptoms are all related to poi.yet due to limited data in this study, the mechanism under the relations between these microbes and sex hormones is not clear.poi leads to several complications, including decrease in bone mineral density, autoimmune, thyroid disease risk and cognitive dysfunction.this study not only revealed the association between gut microbiota and autoimmunity, but also the relationship between gut microbiota and these complications.these indicated that the dysbiosis of gut microbiota was related to the development of poi discussed above.however, limited sample size, participants from the same hospital and the observation of association but not causality, large sample size and multi-center are needed in the further studies.moreover, metagenome sequencing, measurements of metabolites produced by gut microbiota, animal experiments should also be considered to explore the potential causal mechanism.conclusions in summary, poi may cause by autoimmune etiology, and the autoimmune process is affected by gut microbiome, so we may consider some relationship may exist between gut microbiome and poi.this study demonstrated an altered gut microbial pattern in women with poi against healthy controls, with an increase in bacteroides, bifidobacterium, megamonas, prevotella and a decrease in blautia, clostridium, coprococcus, aecalibacterium, roseburia and ruminococcu.and we also found that these changes of microbes were closely related to serum hormones.this will help us to make a foundation for revealing the interaction between gut microbiota and poi certain
number of words= 819
[{'rouge-1': {'f': 0.327560706401766, 'p': 0.6643396226415095,'r': 0.21736842105263157}, 'rouge-2': {'f': 0.19309590659095935, 'p': 0.33540284360189576,'r': 0.1355737704918033}, 'rouge-l': {'f': 0.2913105069712787, 'p': 0.5211278195488722,'r': 0.20215859030837005}}]
-----------------------------------------------------------------------------------------------------------------------------------
p337:
Extractive Summary:
background cardiac ventricular aneurysms affect 1 in 200,000 live births [1].case presentation a 25-year-old primiparous woman was referred for tertiary specialist fetal assessment at 23 weeks’ gestation following a mid-trimester ultrasound indicating right outflow tract obstruction with pericardial effusion.she had no significant past history.weekly follow up showed a small, gradually reducing, pericardial effusion, with resolution of the ascites and normalisation of the ductus venosus doppler waveform.birth at term to minimise risks of prematurity and optimise birthweight prior to definitive surgical repair on cardiac bypass soon after birth was planned [3].at 35 + 1 weeks’, a large left ventricular pseudoaneurysm persisted, but the pericardial effusion had completely resolved (fig. 3).she remained stable and was transferred on day 1 of age to a quaternary paediatric cardiac centre for surgical management, undergoing aneurysmal resection with patch closure and ligation of a patent ductus arteriosus at 10 days of age.she made a good post-operative recovery and was discharged home at 19 days of age.on follow-up at 4 months of age she is thriving with a repeat echocardiogram showing good biventricular function.due to the small isolated defect, it is thought to have occurred secondary to a single coronary artery occlusion causing a localised myocardial infarction.utilising joint decision making with the parents, a caesarean birth was chosen as the optimal mode of delivery to minimise fetal risk.as the left ventricle transitions from pumping against the low resistance placental circulation to the high resistance neonatal systemic circulation, there is a significant increase in afterload, and in this case, an increased risk of pseudoaneurysmal rupture and catastrophic bleed [6].this prevents the rapid and large increase in left ventricular afterload associated with cord clamping [7].
number of words= 276
[{'rouge-1': {'f': 0.49478759466725736, 'p': 0.6890476190476191,'r': 0.3859722222222222}, 'rouge-2': {'f': 0.22768721645147078, 'p': 0.29602739726027394,'r': 0.18498257839721255}, 'rouge-l': {'f': 0.3884469512038441, 'p': 0.5291836734693878,'r': 0.3068421052631579}}]
-----------------------------------------------------------------------------------------------------------------------------------
p338:
Extractive Summary:
however, as cesarean delivery is currently performed more frequently than before, its adverse effects on mothers and infants have gradually been revealed.compared with those who undergo vaginal delivery, women who have a cesarean delivery are more prone to febrile diseases [1] and small bowel obstruction [2], and they have a higher risk of severe acute maternal morbidity (samm) [3], postpartum depression [4], and postpartum death [5].in 2007, the american college of obstetricians and gynecologists (acog) defined cdmr as a primary cesarean delivery on maternal request in the absence of any medical or obstetric indication [13].cdmr has limited significance in benefiting maternal and infantile physical health in contrast to cesarean delivery with medical indications.for instance, mothers are more likely to suffer from short-term ill effects, such as wound infection [14, 15].therefore, unnecessary cesarean delivery in the absence of any medical or obstetric indications should be strictly avoided.according to a world health organization (who) survey in 2010, the average cesarean delivery rate in nine countries in asia was 27.3 %, whereas it reached 46.2 % in china [22].although lower than before, this rate is still far higher than the 10-15 % recommended by the who.the population of cdmr reported internationally accounts for 4.4-17.3 % of the total cesarean delivery population [22, 29–31], whereas that number is 25.2-31.4 % in china [22, 26, 32], which is far higher than the international average level.until now, few reports have analyzed the rate of cesarean delivery in southwestern china.this study mainly used the follow-up data of the late pregnancy period closest to the delivery time as the basic situation of the pregnant women before delivery.after delivery, the delivery situation was recorded, and the delivery outcome was obtained.the psychological condition was described by standard scales.stress was measured by the pregnancy pressure scale (pps) compiled by zhanghui chen et al. [37].the pps comprises 30 items, and the average score calculated from the total score of all questions is used to measure the stress, with 0 for no stress and a score ≥ 0.01 for stress.the cronbach’s α coefficient for pps in this study was 0.949, the kmo test statistic was 0.954, and bartlett’s spherical test showed statistical significance (χ2 = 13041.917, p < 0.001).anxiety was measured using the hamilton anxiety scale (hama) [37], which contains two dimensions, mental anxiety and somatic anxiety, with a total of 14 items, and the total score of all of the questions was calculated.a score of ≤ 7 indicates no anxiety, a score of 8–14 indicates suspected anxiety, and a score of ≥ 15 indicates anxiety.the cronbach’s α coefficient of hama in this study was 0.919, the kmo test statistic was 0.942, and bartlett’s spherical test showed statistical significance (χ2 = 5071.247, p < 0.001).the χ2 test and fisher’s exact test were used to compare the differences in delivery modes among women with different characteristics and to screen for initial potential variables.by july 2019, the delivery outcomes of 1283 pregnant women were obtained.the factors influencing cdmr were analyzed by taking 736 women who had no clinical medical indication during delivery as the study subjects.delivery situation and the indications of cesarean delivery among the 1283 pregnant women, 815 had a vaginal delivery (i.e., spontaneous vaginal delivery without a medical intervention), 6 had an ovd (excluded from the analysis), and 462 had a cesarean delivery (of which 108 were cdmr), with a cesarean delivery rate of 36.01 %.“maternal request” was the primary indication for cesarean delivery, accounting for 23.38 % (108/462) of the total number of cesarean deliveries.general characteristics the age of the participants ranged from 16 years to 44 years, and 62.09 % of them were primiparous women.the overall level of education was relatively high, as 40.90 % of the women were highly educated.women with a family monthly income of more than 5, 000 rmb accounted for 35.05 % of the sample, whereas those below 3,000 rmb accounted for 24.05 %.willingness of delivery mode in late pregnancy, 55.51 % of the women preferred vaginal delivery, 4.46 % of them preferred cesarean delivery, and 40.03 % of them did not have a clear choice of delivery mode (table 2).pregnant women (n = 327) who had no definite willingness to undergo cesarean delivery or who were willing to have a cesarean delivery were asked about their motivations to have a cesarean delivery, whereas those who were willing to have a vaginal delivery were asked about their motivations to have a vaginal delivery (data not shown).combined with information from the literature and professional knowledge, maternal age and bmi were included in the model for adjustment [33, 35].as shown in table 4, compared with women under 25 years of age, women over 30 years of age were 4.3 times more likely to have cdmr (or = 4.292, 95 % ci: 1.984–9.283).compared with multiparous women, primiparous women had a higher risk of cdmr (or = 6.792, 95 % ci: 3.230-14.281).however, women in frequent contact with mothers who had undergone vaginal deliveries had a reduced risk of cdmr (or = 0.547, 95 % ci: 0.311–0.961).the cdmr rate was 8.42 %, accounting for 23.88 % of the cesarean delivery population.although the who no longer gives a recommended specific value of the cesarean delivery rate, it still warns against unnecessary cesarean delivery [41].in recent years, china has taken some measures, such as health education, painless delivery and the introduction of corresponding policies to reduce the rate of cesarean delivery [42].therefore, the high cesarean delivery rate may be due to other factors.however, women who have experienced childbirth in the past have fewer concerns about vaginal delivery.our study confirms this view that women who are giving birth for the first time are more likely to choose cesarean delivery, even without any medical indications, compared to women who have previously experienced delivery.according to a norwegian study, some primiparous women planned to have cesarean delivery since they were teenagers because of their fear of childbirth and a lack of understanding of fertility [48].in china, the “two-child policy” was implemented in 2016, allowing a couple to have two children.therefore, the choice of delivery mode for primiparous women is very important to control the overall cesarean delivery rate.in late pregnancy, pregnant women are preparing to give birth.during this period, most pregnant women have adapted to the pregnancy process and have a certain degree of understanding of delivery knowledge.in addition, when exploring the motivation of pregnant women in late pregnancy to consider cesarean delivery, we found that “afraid of vaginal labor pains, think cesarean delivery is less painful” was the most important factor.moreover, nearly one-third of women did not have confidence in vaginal delivery.moreover, due to china’s unique social and cultural influence, some people in the country connect the birth time of a child with his or her fortune and want to give birth at a particular time for an “auspicious” future.the successful experience of other parturient women’s vaginal delivery increased their confidence in delivery, whereas a failure of a trial of labor in other women possibly made them fear vaginal delivery and choose cesarean delivery.a survey of obstetricians showed that those with lower educational backgrounds and less work experience who thought that the advantages of cesarean delivery outweighed the disadvantages had a higher incidence of cdmr among their patients [36].furthermore, because of the exclusion criteria, no women had a history of cesarean delivery.
number of words= 1215
[{'rouge-1': {'f': 0.39110575279664894, 'p': 0.745531914893617,'r': 0.2650844854070661}, 'rouge-2': {'f': 0.221779354696925, 'p': 0.374,'r': 0.1576249039200615}, 'rouge-l': {'f': 0.3740831185368941, 'p': 0.5746728971962616,'r': 0.27729366602687144}}]
-----------------------------------------------------------------------------------------------------------------------------------
p339:
Extractive Summary:
several complications are associated with preterm delivery making it the leading cause of perinatal mortality and a major cause of child death in many middle and high income countries [2].preterm birth (ptb) puts a serious burden on the healthcare system as babies who are born prematurely present both short and long term complications and are at a higher risk of morbidity [3].other adverse perinatal outcomes such as low birth weight (lbw), small for gestational age (sga), and large for gestational age (lga) have also a direct or indirect impact on the health of the newborn and may result in long-term sequelae [4–7].these adverse events are complex and are associated with a number of factors, including biological, obstetrical, behavioral, psychological, and socio-economic.over the past two decades, there has been an increasing interest in the association of antenatal depression with adverse perinatal outcomes [8–10].furthermore, antenatal depression has been identified as a risk factor for low birth weight (lbw) [13].the mechanism underlying the association of depression and psychosocial stress with birth weight is also not clear [20].cortisol levels in pregnancy are shown to be inversely proportional to birth weight but these levels cannot be explained by prenatal stress [21].different mechanisms have been suggested to explain the possible association of depression and ptb.depressive symptoms in pregnancy have been associated with adverse health habits, such as smoking and alcohol consumption, which are known risk factors for ptb [2, 23].other psychosocial problems that may exist before or during pregnancy have been also described as potential risk factors of perinatal adverse outcomes.women with posttraumatic stress disorder (ptsd) were also shown to be at a greater risk of preterm birth [24].alseaidan et al. (2016) looked at birth outcomes in a prospective pregnancy-birth cohort study in kuwait and found that the prevalence of ptb and sga was similar to other developed countries, while macrosomia and lga were in fact greater than what it was expected [26].thus, the aim of this study is to investigate whether antenatal depressive symptoms predict preterm birth, small for gestational age or large for gestational age babies using data collected from a prospective cohort study in a population where antenatal depressive symptoms were found to be relatively common [26, 27].the tracer study was open to both kuwaiti and non-kuwaiti women attending public and private clinics, thus making the sample representative of the population in kuwait.women were eligible to participate if they were between 18 and 45 years old, had a singleton pregnancy, and were fluent in arabic or english.most of the women were enrolled in the second trimester of their pregnancy but they were also eligible to participate if they were in their first or third trimester.data collection data were collected from may 2012 until august 2015.the enrolled participants were also contacted via phone at a median time of 6 weeks (iqr: 3–9) after delivery to obtain information about the birth date and birth weight and, at the same contact, the majority of the women also answered a postnatal questionnaire which included questions about the health and the diet of the baby, including breastfeeding.during the same call, the mother was asked to report any diagnosis and treatment for gestational diabetes and gestational hypertension in her last pregnancy.ptb was defined as a gestational age at birth that was less than 37 weeks [28].the questionnaire was not validated in this study population, therefore the cutoff point of eds ≥ 10 was used to define depressive symptoms, similar to other multi-ethnic studies [32, 33].we further examined the role of pre-pregnancy bmi and self-reported factors describing maternal reproductive health, such as parity, preterm delivery in a previous pregnancy, conception by in-vitro fertilization (ivf), as well as gestational diabetes and gestational hypertension in the current pregnancy.we also examined the association of depressive symptoms in pregnancy with each of the three outcomes of interest (ptb, sga and lga) using the chi-square test of independence and univariate logistic regression models and reported crude odds ratios (ors) and the corresponding 95% confidence intervals (95% ci).all statistical analyses were conducted using sas 9.3 (sas institute, cary, nc, usa) and statistical significance was defined as a p-value< 0.05 using two-sided tests.results a total of 2038 women were enrolled in the tracer study and completed the baseline and stress questionnaires with usable data.we excluded women without a score for the antenatal depressive symptoms (n = 92) and women who reported taking anxiety or depression medication during the current pregnancy (n = 6) or who completed the stress questionnaire in the first trimester (n = 24).overall, the prevalence of ptb, sga, and lga in our sample was 7.3, 7.1, and 22.6%, respectively (table 1) while the prevalence of depressive symptoms was 19.5%.none of the factors examined (nationality and pre-pregnancy bmi) was found to be a modifier of the effect of antenatal depressive symptoms on the outcomes of interest.non-kuwaiti women had increased odds of delivering a lga baby (or = 1.92, 95% ci: 1.19, 3.08).the correlation between the eds score and gestational length was rs = − 0.02 (p = 0.39) and that of eds and birth weight was rs = − 0.04 (p = 0.07).however, none of these associations reached statistical significance either in univariate models or in multivariable models after controlling for socio-demographic and pregnancy related variables.in our analysis, we tried to include complications in current or previous pregnancies that may have increased the risk of adverse birth outcomes, to the extent that data were available.similar to sga, our results did not suggest that bmi modifies the association between depressive symptoms and lga.in addition, information was collected by a phone interview, therefore women had probably access to the health card of their baby.finally, the women recruited in the study included both kuwaitis and non-kuwaitis, who attended public and private clinics in all six governorates of kuwait, in a way that represented the heterogeneous composition of the population of kuwait.similarly, our findings showed that antenatal depressive symptoms are not associated with lga though further research is needed to examine this association, given that the available evidence is quite limite
number of words= 1002
[{'rouge-1': {'f': 0.3877080686527755, 'p': 0.8030827067669173,'r': 0.25553758325404374}, 'rouge-2': {'f': 0.2429000989695523, 'p': 0.45113207547169815,'r': 0.1661904761904762}, 'rouge-l': {'f': 0.37991189427312777, 'p': 0.6533333333333333,'r': 0.26782608695652177}}]
-----------------------------------------------------------------------------------------------------------------------------------
p340:
Extractive Summary:
according to worldwide community studies, the pooled prevalence of any mental disorder in adolescents is 13.4%, with prevalence rates ranging between 8.3 and 19.9% [1].attrition refers to dropping out of (outpatient) mental health therapy early, namely, terminating therapy before the therapist would agree that it is appropriate to do so, thus leading to an attenuation of therapy outcomes.a high attrition rate is a problem in the field of child and adolescent psychiatry.decision-making capacity in adolescence adolescence is a culturally defined concept without clear-cut starting and ending points [13].studies have demonstrated that a subset of adolescents aged 14 and older have the capacity to consent to medical treatments in specific contexts [16, 17].legally binding international texts such as the united nations convention on rights of the child (articles 12 and 13) and the convention on human rights and biomedicine (article 6) provide that the voices of children and adolescents should be heard and given due weight.indeed, the right of adolescents to be involved in treatment decisions has been expanded in recent years [19].adolescents’ involvement in medical decisions is important to them.decision-making involvement helps them become better decision-makers in the future [15] and learn that they are beings of ‘moral worth’ [20].there is no universal agreement on adolescents’ decision-making capacity (dmc).furthermore, other authors highlight the role of factors such as the development of skills and mature critical thinking; the development of values, emotions and moral authority; literacy; culture; previous experiences; the involvement of family; and family relationships [14, 30–35].moreover, adolescents are profoundly influenced by other persons, especially parents, health professionals and peers [37, 38].the recent economic crisis may have profound and lasting effects on the mental health of adolescents.adolescents’ behavioural health is particularly vulnerable to conditions of economic hardship [39].kontorini et al. noted that out of 319 children and adolescents (131 girls and 188 boys) who sought psychotherapeutic help in the department of children and adolescents’ therapy (dcat) of the institute of behaviour research and therapy (ibrt) during the last decade (january 2010–may 2020, namely, the decade after the economic crisis), 42.6% successfully completed it, 20.7% did not attend and 13.5% dropped out [41].to the best of the authors’ knowledge, little research has been conducted to date on factors that may be barriers to or facilitators of mental health care engagement among decision-making competent adolescents in greece.research questions the overarching question delineating the focus of this study was the following: what are the perceived facilitators of and barriers to psychotherapy engagement among adolescents who show a reliable level of decision-making capacity?the secondary research questions were the following: a) what are the major factors that can profoundly shape the attitude of adolescents with mental disorders towards their treatment?b) what are the factors that determine the consent of adolescents with mental disorders to their treatment?c) what are the factors that determine the treatment decision making of adolescents with mental disorders?moreover, screening activities to determine participant eligibility were performed following their consent to continue with the research while protecting the privacy of potential participants and the confidentiality of information collected about them.stages of the study a. a psychometric evaluation of the adolescent patients who were willing to participate in this study was conducted for the primary purpose of applying inclusion and exclusion criteria to the research sample.in addition to eligibility screening, the psychometric assessment was expected to help us identify more factors potentially affecting the patients’ attitude towards their treatment and carry out a more nuanced and reliable qualitative analysis, namely, obtain better results from our qualitative data analysis.it was assessed whether participants had adequately developed emotions and a set of values as well as the abilities to organize their values and find a balance between extreme emotions.the participants were outpatients attending their scheduled clinic appointments in the child and adolescents psychiatry department, tertiary referral hospital hippokratio of thessaloniki.a total of 52 patients were initially recruited by the main researcher (et).while 50 patients agreed to be interviewed, 2 patients chose not to participate.the inclusion criteria the inclusion criteria for participation in the study were (1) being an outpatient with mental disorders, (2) being adolescents less than 18 years, (3) being already engaged in a therapeutic relationship in our healthcare setting, and (4) being not involuntarily engaged in this therapeutic relationship.to determine whether the participants had adequate capacity to make decisions, the researchers examined their cognitive functioning, emotions, core values, and practical wisdom to the extent possible.therefore, the following exclusion criteria were further included: (3) the lack of an adequately developed set of values (explored through participant screening interviews conducted prior to the qualitative research interviews, as presented below), (4) very low intelligence or (5) severe depression symptoms that may have a negative impact on cognitive functions and decision-making processes.therefore, patients with severe disorders in cognitive functioning or severe emotional (major depressive) disorders were excluded.both parents/legal guardians and adolescents were told at the start of the study that they have the right to withdraw from the research at any time and without giving any reason and without reprisal.before each interview, each participant and his or her parent( s) were given information on the study and informed that his or her participation was voluntary, placing great weight on the importance of maintaining confidentiality.screening screening measures potential participants were administered two psychometric scales: a) an intelligence test (wechsler intelligence scale for children, wisc iii) [42] and a self-report measure of depression (beck depression inventory, bdi ii) [43].then, in a second step, adolescents whose intelligence and depression symptoms were not ranked as “very low” or “severe”, respectively, were clinically assessed to determine whether they had developed a set of (not pathogenic) values strictly and stably over time allied to their narrative identity as well as whether they had adequate (though not extreme) emotions.the assessment was not difficult provided that the adolescents had already been in therapy for many months, namely, at least four months, though most had been in therapy six months.the narrative enabled the researchers to enter into the being (inner world) of participants to appreciate these abilities.to determine whether these (additional) criteria were met, the researchers conducted participant screening interviews with potential participants, conducted prior to the qualitative interviews albeit after the psychometric assessment.in addition, the participant screening interviews were rapport-building.what do you believe are the main reasons why you have to comply with the treatment offered?participants were encouraged to expand upon issues they considered most relevant and speak as freely as possible about them.interviews were held at interviewees’ preferred time.during the interview, only the interviewer and the interviewee were present, with the exception of a few patients who were accompanied in the interview by their parents, who for the most part stayed silent apart from brief input.the interviewer answered any questions that the patients asked.the number of participants was not set from the beginning.authors engaged with one another to limit research bias.furthermore, credibility was established using prolonged engagement and maximum variance in participant selection.disagreements between the authors that arose during the data analysis were easily addressed with re-examination of the data and further discussion.barriers or facilitators that were highly recurrent or mostly emphasized during the interviews were classified as major factors.if adolescents and their parents were willing to participate, they were given adequate information about the design, purpose, nature and confidentiality of the study, including that participation was voluntary and that consent could be withdrawn at any time during the course of the study.verbal informed consent to participate was then obtained from each participant and his or her parent(s) prior to participating in this study and documented in the recording at the time of the interviews.to preserve participants’ anonymity, no names are used in this paper.a total of 50 adolescents (aged 13–18 years, mean = 14.85, sd = 1.67) with psychiatric disorders participated in the study (see table 1).from the interview data analysis emerged a variety of distinct factors that can be regarded as barriers to and facilitators of psychotherapy engagement for adolescents with mental disorders who are decision-making competent.below are the six central themes and the subthemes that emerged from the interview data analysis [for additional representative quotes, see additional file 1].they expressed their strong willingness to undergo it.the vast majority of participants not only were committed to therapy but also invested in it to varying degrees.“i don’t think of anything special, it’s a new experience as well, which will help me; it will get somewhere, but i don’t need to have a special feeling for it …are you scared of something?typical comments included, “nothing would make me stop treatment” (participant 31, girl 16yo, f50) ineffectiveness of therapy as a barrier perceived ineffective treatment may be a good reason for terminating therapy prematurely.the following is one of the comments illustrating this point: “… [i’d interrupt the treatment] if it is extremely difficult to follow it or i didn’t notice any result in some time” (participant 4, girl 17yo, f50) some participants related the perceived ineffectiveness of therapy to their therapist.for example, one participant said, “… the relationship with the therapist made me continue … if it didn’t help me [i’d stop treatment]” (participant 2, boy 13yo, f51.3) the crucial role of the adolescent-therapist relationship it was strikingly apparent in the data analysis that the adolescent-therapist relationship plays a crucial role in adolescents’ therapy engagement.this relationship may be regarded as both a strong barrier (when it is “bad”) or a strong facilitator (when it is “good”).adolescents search for specific characteristics in the therapist.they require that their therapists have a warm and friendly demeanour and strive to create a trustful relationship with their adolescent patients.trust in the therapist and in the promise of confidentiality are important elements for the continuation of the therapeutic relationship and a prerequisite for adolescents.personal liking for the therapist is often an essential condition for continuing treatment.“if i didn’t like her as much as i do, i’d be more hesitant to continue treatment”.however, adolescents require that the therapist does not play the role of an expert dictating the life of the adolescent through a paternalistic model of the doctor-patient relationship.it is indicative that two participants used the terms “collaborate [with the therapist]” (participant 33, girl 17yo, f42) and “[the therapist] is cooperative” (participant 20, girl 18yo, panic attacks).participants who perceived the relationship with their therapist as ‘good’ stated that their therapist was “very good” and that they had developed a friendly relationship of trust with him or her.a typical comment was: “our relationship is good, she is friendly, i can trust her with many things and talk about my personal life and i consider her confidential” (participant 4, girl 17yo, f50) participants reported the following factors as fundamental to establishing a good relationship with their therapist: “she understands me”, “she listens to me” (that is, she puts up with me), “i can speak freely”, “she offers solutions”, and “i feel comfortable”.typical comments included: “i feel an intimacy; she advises how to deal with my difficulties … i feel her close to me and i trust her” (participant 45, girl 13yo, f34) “my relationship with my therapist is very good …a participant stated, “i think that if the therapist succeeds in creating an atmosphere of trust and confidence, then it’d be easier for me to accept the proposed treatment” (participant 4, girl 17yo, f50) the (mostly positive) attitudes towards the therapist the attitudes towards the therapist ranged from mild reservation to strong attachment.some participants strongly associated their steady commitment to therapy with their good relationship with their therapist.“for example, the following comment was typical.it’s too early … for the time beingi haven’t noticed being affected.typical comments included: “she was nice, nothing special” (participant 25, boy 13yo, f39; participant 43, girl 17yo, f32) the adolescents unilaterally determine what constitutes a “good relationship” with the therapist it is noteworthy that many participants unilaterally set the terms of a “good relationship” with the therapist.the need to preserve their selves, instead of a radical change through treatment, seems to be the key point for adolescents.“if she starts bringing her personal stuff in the conversation, that’s a red flag for me” (participant 14, girl 14yo, f32) in addition, it should be noted that some participants used the terms “good relationship” or “bad relationship” [with the therapist], “like [the therapist]”, “i feel good” or “i feel bad” [with the therapist] without providing further clarification of what they meant by these terms.moreover, adolescents desire good communication with the therapist (participant 47, girl 16yo, f32) and clarity on what the therapist says (participant 5, girl 12.5yo, f45, f40).the therapy as a means of pursuing and achieving goals to improve the adolescents’ well-being psychotherapy emerged in the data analysis as being thought of as a requirement or an effective means of pursuing and achieving adolescents’ goals.the goal of eliminating the symptoms and the negative consequences of a mental disorder the vast majority of the participants in our study seek help to handle their difficulties and get free of symptoms related to their mental disorder, which in all likelihood limit and affect their functional activities of daily living and obstruct the development of social relationships.a total of 22.4% of the participants in our study had sleep disorders.furthermore, 44.9% of the adolescents in our study were affected by mood disorders, and increased rates of pessimistic symptoms (36.7%), energy reduction (30.6%), and concentration difficulties (28.6%) were observed.in a related question during their evaluation, they expressed their indifference towards such issues.focusing on participants’ personal well-being some participants were focused on their personal well-being (personal aspect of quality of life) and declared that they pursued the purpose of effectively dealing with their difficulties and challenges.the following were typical comments: “i expect to become better and i’m not afraid of anything” (participant 19, boy 16yo, f42); “… i expect it will relieve me” (participant 3, girl 14yo, f39); “[ …] … i want to pursue treatment because i will feel better for myself” (participant 45, girl 13yo, f34).some other participants placed great weight on their symptoms, which, in addition, wreak havoc on their social relationships.for example, participants stated, “… i am and make better company with other peers …” (participant 24, boy 13yo, f90).“…i believe i’ll gain some new friends, it’ll help me live more pleasant moments” (participant 19, girl 16yo, f42).“i have few friends” (participant 38, boy 17yo, panic attacks, stress) the remission of symptoms as a barrier to therapy engagement importantly, while the vast majority of participants clearly acknowledged the benefit they received and were willing to undergo treatment, many participants expressed their desire to terminate psychotherapy prematurely once the symptoms subsided or went away.from this perspective, it may be a barrier to therapy engagement.the goal of personal independence some participants regarded therapy as a way to become more independent.the majority of adolescents identified their schooling and future career preferences as the main priority.the goal of confessing to a trustworthy person three participants regarded the therapy as a confession.while the vast majority of participants had friends and quite a few of them, most participants chose not to announce to their friend group their problem and the fact that they were in treatment.many participants distinguished between their close trusted friends and their not-so-close friends.close friends may know about the adolescent’s therapeutic relationship, and they potentially act supportively with emotions of solidarity, understanding and motivation.“…very few were slightly cautious in making this statement.in almost all of the cases, adolescents were backed by family.yes [my parents brought me here], butother participants fear others making fun of them (their mental illness), which may facilitate their obtaining therapy.the fear of stigma related to mental health was both a facilitator and barrier.…this was described as a factor that can cause a high attrition rate.friends were reported as having a role ranging from neutral to mildly supportive.discussion the authors of this paper would not expect to find quite different results among adolescents without dmcs, with the exception of results related to the adolescenttherapist relationship.from this perspective, below, the authors discuss the results of this study.to that effect, many theorists suggest that “children may have far more potential to understand complex illness concepts than they have previously been given credit for” [56].it is important that quite a few adolescents seem to try to “have control” of the therapeutic relationship, setting their own terms for treatment delivery, which they mainly relate to the quality of their relationship with the therapist (see below).naturally, this agrees with the feeling of invincibility and risk-taking during adolescence.a possible explanation may be that the remission of symptoms may considerably reduce both public stigma (related to mental disorder) and self-stigmatization.in addition, with the remission of the symptoms, the need for follow-up is eliminated.they said that this was a significant facilitator of their therapy engagement.the findings were consistent with previous studies [5, 54].participants appreciated the fact that the therapist gave them thoughtful and effective advice.this is inconsistent with prior literature [68].indeed, reciprocal understanding between the adolescent and therapist is of great importance.motivational interviewing, used as a pre-treatment intervention, is a promising way to facilitate engagement in adolescent mental health settings [75].in relation to the above, it should be mentioned that each minor experiences the outside world in his or her own unique way, even though he or she lives in the same social-cultural context as other children [76].furthermore, therapists should balance the views of parents and children [78] while making every effort to involve the adolescent’s family in the decision-making process [36, 79, 80].in that sense, it is important to note almroth et al.’s statement that “interventions aimed at increasing aspirations and engagement in school may prevent mental health problems in adolescence” [85].the role of peers acceptance by peers and improvement of social skills as essential preconditions for acceptance in the social environment seem to constitute a strong motive for seeking treatment.adolescents pursue their need for independence by placing a considerable emphasis on attempting to shift from dependency on parents and family towards greater belonging among peers [5].the mental disorder-related stigma attitudes of peer groups towards adolescents in psychotherapy may result in adolescents feeling scorned [5].the role of family in almost all of the cases, adolescents were backed by family to some extent.adolescents in the study acknowledged the important role of family in their decisions.tsiantis et al. state that when an adolescent comes for therapy, he or she has already been exposed to familial, friend and social influences, and this can make his or her attitude towards treatment positive [89].the american academy of pediatrics claims that parents have no absolute legal right to make autonomous treatment decisions regarding their children [17].parents do have a responsibility to preserve family relationships and further the best interests of their children.moreover, a collaborative relationship between the family and the health provider may increase engagement [4].the fear of stigma related to mental health disorder the fear of mental health-related stigma emerged in the interview excerpts regarding the interviewee-therapist and interviewee-peer relationships.however, this assessment involved a degree of uncertainty and therefore may be seen as a limitation.conclusions a number of more or less strong barriers and facilitators were identified.furthermore, and most importantly, a “good” adolescent-therapist relationship was reported as a strong facilitator, whereas negative experiences of participants with their therapist were reported as a strong barrier.moreover, goals such as getting rid of symptoms, improving personal well-being, and improving social skills and relationships (especially with peers) emerged as strong facilitators of therapy engagement.among the weaker (“minor”) perceived facilitators were goals such as confessing to a trustworthy person, becoming able to achieve personal expectations and life goals, enhancing independence and self-esteem, and developing a positive self-image.the stigma related to mental health emerged as both a (“minor”) facilitator of and barrier to therapy engagement for participants.for the most part, the findings of this study are consistent with the findings of previous research on the topic of interest.they enhance the findings of prior studies.
number of words= 3291
[{'rouge-1': {'f': 0.2777671298549033, 'p': 0.9754441260744986,'r': 0.1619406459121327}, 'rouge-2': {'f': 0.21613220341408126, 'p': 0.656206896551724,'r': 0.12937136204889407}, 'rouge-l': {'f': 0.307105228383914, 'p': 0.8935294117647059,'r': 0.1854163231657049}}]
-----------------------------------------------------------------------------------------------------------------------------------
p341:
Extractive Summary:
these numbers are drastically higher than the estimated 4% prevalence of lifetime ptsd diagnosis reported in a 25-country study that included south africa as the sole african country [2].sub-saharan africa is also disproportionately affected by the hiv/aids pandemic.while hiv has become a chronic illness, due to the wide spread roll out of antiretroviral therapy (art), and mortality from aids greatly reduced [4], mental health issues remain common among persons living with hiv (plwh) [5].these issues include ptsd, depression, substance use disorders, eating disorders, and anxiety disorders, and they commonly co-occur [6, 7].among plwh, probable ptsd may negatively affect hiv treatment outcomes [11, 12], adherence [13, 14], and retention in hiv care [15].eligibility further includes being positive for heavy alcohol use via the audit-c (≥3 for women; ≥4 for men); positive for recent alcohol use based on a urine ethyl glucuronide (etg) dipstick test (300 ng/ml cutoff, by confirm biosciences, san diego, california); having alanine transaminase (alt) and aspartate transaminase (ast) levels <2x the upper limit of normal; being cleared of active tb (those reporting tb symptoms); not being pregnant, and having a positive tuberculin skin test (tst) with an induration ≥5mm 48–72 h after injection with purified protein derivative (ppd).the study questionnaire, including the ptsd scale described below, was translated into runyankole, back-translated to english, and the original and back-translated questionnaires were compared to assess how accurately each item was translated.dependent variable - post-traumatic stress disorder we measured probable ptsd at baseline using the primary care ptsd screening scale from the dsm-5 (pcptsd- 5) [24].this scale first screens for ever experiencing a serious traumatic event, including a serious accident or fire, a physical or sexual assault or abuse, an earthquake or flood, a war, seeing someone being killed or seriously injured, or having a loved one die through homicide or suicide.independent variable - alcohol use the alcohol use disorder identification test- consumption (audit-c), a brief screening for heavy drinking and or active alcohol use disorder [26], was used to assess the level of drinking; we used a version modified to assess drinking in the prior 3 months [27, 28].covariates we collected participant characteristics that included age, sex, level of education, and marital status.social desirability was considered as a potential confounder for probable ptsd and alcohol use.we measured spirituality/intrinsic religiosity using the duke university religion index (durel scale [33]) and also assessed religion.religion and spirituality were of interest as they may impact one’s response when faced with trauma.we also conducted two sensitivity analyses of the final model: 1) to examine whether sex acts as an effect modifier for the association between alcohol use and ptsd, we included and tested for an interaction between participant sex and alcohol use in the adjusted model; 2) we excluded depression to see its impact on the association of level of alcohol use with ptsd, i.e., exploring whether it was on the causal pathway from level of alcohol use to probable ptsd.results we screened 3293 persons with hiv from may 2018 through march 2020 (fig. 1). of those screened, 2611 people were eligible after the initial screening step; the main reasons for exclusion at this step were having a history of taking tb medications, a history of active tb, or taking nevirapine.four hundred twenty-one (421) participants were included in this analysis, as 14 had missing answers to the ptsd questions.of the included participants, more than two-thirds (68.2%) were male and the median age was 40 (interquartile range [iqr] 32–47).participants reported their religious affiliation as catholic (50.4%), protestant (43%) and other denominations (6.7%).one in five of the participants (18.3%) reported having depressive symptoms (ces-d ≥ 16).nearly 40% of participants reported having ever smoked tobacco in their lifetime, and 12.8% of participants reported ever using illicit drugs (table 1).khat (a chewed stimulant) was the most commonly used illicit drug, reported by 10%, marijuana was reported by 3.8%, kuba (a ugandan informal word for a form of smokeless tobacco) by 3.1%, petrol sniffing by 1%; while no participants reported having ever used other illicit drugs.ptsd symptoms experienced in the past month included avoidance (28.3%), having nightmares (27.3%), feeling on guard or easily startled (21.6%), feeling guilty or blaming one’s self or others (11.6%), and feeling numb or detached (10.2%).in the unadjusted analyses, audit-c score (or for one unit increase 1.09; 95% ci: 0.99, 1.20) was associated with ptsd, as were age (or = 0.96; 95% ci: 0.93, 0.98), the presence of depressive symptoms (or = 2.37; 95% ci: 1.37, 4.09), and intrinsic religiosity (or for one unit increase in score = 1.07; 95% ci: 1.00, 1.14) (p < 0.10) (table 2).in sensitivity analyses, we found no significant interaction between gender and audit-c level, and the association between audit-c level and ptsd did not substantially change after excluding depressive symptoms (data not shown).the probable ptsd prevalence observed in this analysis was similar to the 19.6% prevalence of probable ptsd found in a study of plwh in care in urban uganda [34], lower than that observed in plwh in a study conducted in zimbabwe (55.3%) [35], and higher than that reported by a study among plwh in post-conflict northern uganda (8.3%) [9].the prevalence we observed is quite similar to the 22% prevalence reported from a meta-analysis of data from 10 countries in sub- saharan africa that included some studies among plwh, but did not calculate probable ptsd prevalence by hiv status [1].however, we were only including people if they used alcohol at unhealthy levels, thus within this restricted sample, alcohol use severity and ptsd symptoms were not associated.alcohol use may lead to traumatic events that may cause probable ptsd, or may serve as self-medication to deal with symptoms of ptsd and therefore be a coping mechanism [20].this association could be due to common overlapping symptoms between depression and probable ptsd [41].another possible explanation for this association could be the cooccurence of mental health disorders [35].although the populations consisted of persons with confirmed heavy alcohol use, the level of drinking was self-reported and this may be subject to social desirability.strengths all participants who were included in this study were confirmed to be persons who engage in heavy drinking through ethyl glucuronide testing.
number of words= 1020
[{'rouge-1': {'f': 0.41115055065170636, 'p': 0.774968944099379,'r': 0.2797966728280961}, 'rouge-2': {'f': 0.2117669801948729, 'p': 0.34725856697819313,'r': 0.15233117483811287}, 'rouge-l': {'f': 0.355618869844761, 'p': 0.6180225988700565,'r': 0.24962962962962965}}]
-----------------------------------------------------------------------------------------------------------------------------------
p342:
Extractive Summary:
almost 80 million people are currently being forced to flee their homes, whereof 26 million are refugees and 4.2 million are asylum seekers, i.e., seeking international protection but whose claim for refugee status has not yet been determined [1].consequently, refugees and asylum seekers are at high risk for complex health conditions.previous research has demonstrated high rates of both ptsd and associated comorbidity such as depression and anxiety [4, 7, 9, 13], sleep disturbances, chronic pain, fatigue, and functional impairments [14–18].ptsd and other stress-related disorders are also associated with poor health behaviors, including low levels of physical activity (pa) [22–25], which may additionally complicate the already burdened health situation among forced migrant populations.both international [28] and national pa guidelines [29] suggest that at least 150 min of moderate-intensity pa per week is needed to obtain important health benefits.however, the potential impact of pa has received scarce research attention in the field of ptsd and forced migrants’ health, and particularly among the vulnerable group of asylum seekers.despite the complex mental health needs related to stress and trauma exposure; neither the prevalence of pa, nor its association to ptsd and exposure to grave trauma, especially torture, has to our knowledge previously been examined.participants and setting the study setting was three large housing facilities for asylum seekers located in sweden.a questionnaire including sociodemographic data, trauma history, measures of mental health, and level of pa, was used.the swedish migration board provided a list of eligible participants, including sociodemographic data, based on the ethical approval from the regional ethical review board.age of participants was categorized into two groups: 18–30 years and 31–64 years.physical activity level of pa was assessed by the exercise vital sign (evs) [42].it assesses the average time spent in moderate to strenuous activity, displayed in minutes per week, by multiplying the responses on two self-report questions: 1) “on average, how many days per week do you engage in moderate to strenuous exercise (like a brisk walk)?” (response options: 1–7 days) and 2) “on average, how many minutes per day do you engage in exercise at this level?in this study, the evs score was divided into three pa categories according to established cut-offs in pa guidelines [28]: inactive (0 min·wk.−1), insufficient pa (1–149 min·wk.−1), and sufficient pa (≥ 150 min·wk.−1).the htq symptom scale is crossculturally validated and frequently used among refugee populations for both evaluation of treatment outcome [46] and in screening for ptsd [10, 13].ptsd symptom severity is computed by averaging the responses on the list of symptoms, giving a score between 1 and 4.exposure to torture two questions from the refugee trauma history checklist (rthc) [49] was used to assess whether participants had experienced torture before and/or during their flight.in addition to the questions regarding torture exposure, the checklist includes questions targeting other potentially traumatic events, e.g., war at close quarters, violence, and forced separation from family and close friends.the two questions regarding exposure to torture prior to flight and during the flight were combined to establish exposure, i.e., participants who endorsed either or both of the questions were considered exposed.although no significant testing of the differences between strata were performed, non-overlapping ci’s were viewed as indicating significant differences in prevalence estimates.the differences between participants designated to different evs categories (levels of pa) in regard to htq score (ptsd symptom severity) and its subscales, ais and ans, were assessed by univariate and multivariate analysis of variance (anova and manova).oneway anova for independent group was used to assess the difference in regard to total htq score, while manova was used for assessing the overall differences in ais and ans due to high intercorrelation between these symptom clusters (r = 0.83).assumption about homogeneity of variance for anova for total htq score, and manova for ans and ais, were tested by levene’s test and box’s test of equality of covariance matrices.multiple regression analysis with htq score as the dependent variable was performed to assess the overall associations between sex, age, exposure to torture, and level of pa, indicated by evs categories.stepwise hierarchical regression in three steps were applied in order to assess the contribution of the evs categories to ptsd symptom severity, indicated by htq score, by controlling for sex, age, and exposure to torture.subsequently, the variables sex and age group were entered in the first step of the model, exposure to torture was added in the second step, and lastly, the variable evs categories were added in the final step of the model.characteristics of the participants are presented in table 1.in brief, the majority of participants were men, between 18 and 30 years of age, not living with a partner, and had 9 years or less of education.among the participants, 56% had been exposed to torture (57% among women and 55% among men).the addition of exposure to torture in this model accounted for an additional 11% of the total variance in ptsd symptom severity and the change in r2 was significant (f(1, 309) = 40.51, p < .001).in the final model, all the above-mentioned variables were included with the addition of level of pa.this model accounted for 25% of the total variance in ptsd symptom severity, i.e., the addition of the variable pa uniquely explained an additional 12% of the total variance in ptsd symptom severity.discussion despite the well-known impact of pa on mental health and wellbeing [26–28], little is currently known about pa among asylum seekers, a population which is known to display high prevalence of trauma exposure and mental ill health including ptsd [3, 4, 8, 9].a proportion corresponding to almost half of the cohort of this study not meeting the recommendations of sufficient pa appears as noticeably high compared to both international and national estimates of insufficient pa (also including those being completely inactive).in regard to promotion of pa, and partially related to our findings of more than one in eight asylum seekers being completely inactive, it may also be noted that recent research has demonstrated important health benefits even at much lower doses than advocated by generic pa guidelines, and especially when moving from completely inactive to some activity [64, 65].concerning pa, presuming an equivalent analogy, this would imply that ptsd symptom severity may partially influence level of pa while also simultaneously be adversely influenced by insufficient pa.for example, there are several proposed symptom overlaps between pain and ptsd, such as anxiety, avoidance behavior, and elevated somatic focus, which may also influence level of pa.while exposure to torture displayed an expected high explanatory function for ptsd symptom severity, insufficient pa provided additional high explanation for the variation in ptsd symptom severity beyond exposure to torture.in addition, there may be other symptoms or conditions that may influence both ptsd symptoms and level of pa, such as poor social support [69], low self-efficacy [70], and as previously highlighted, chronic pain and sleeping problems [15–17], which are common in the context of displacement and exposure to severe trauma, and thus, warranted further investigation in future studies.our results regarding different clusters of ptsd symptoms, i.e., arousal/intrusion and avoidance/numbing, showed similar patterns of differences and associations with insufficient pa as that of the overall ptsd symptom severity.moreover, these symptom clusters may as well be closely interrelated in regard to their influence on pa, such as avoiding activities or exercise due to lack of energy or motivation, fear of bodily arousal (e.g., muscle tension, increased heart rate, shortness of breath), or fear of intrusive memories that may be triggered by physical strain.however, the generalizability of the estimates of prevalence of pa to other settings and other forced migrant groups may be limited.specifically, it has previously been reported that many self-report pa questionnaires, including the evs, generally overestimate the minutes of pa per week compared to objective measures such as accelerometry or direct observations [44, 75].it is thus possible that the true proportion of insufficiently active asylum seekers may be even higher than suggested by our results.the selection of torture as a single worst trauma may be another limitation, as it has been suggested that a cumulative trauma score may provide more explanatory power.nevertheless, the results provide some evidence for the potential importance of pa in regard to ptsd symptomatology and mental health of asylum seekers.our results also encourage more in-depth examination of pa and mental health among forced migrants and provide an interesting starting point for future studies using prospective and longitudinal designs.the results of this study support previous evidence of pa as a potentially important factor in the context of ptsd and forced migrants’ health, however, further research is warranted to clarify causality and to examine the potential efficacy of pa promotion in this regar
number of words= 1430
[{'rouge-1': {'f': 0.5249840726222108, 'p': 0.6819402985074627,'r': 0.426760374832664}, 'rouge-2': {'f': 0.24619482710784196, 'p': 0.30448275862068963,'r': 0.20663764233087745}, 'rouge-l': {'f': 0.42764302346322397, 'p': 0.5824223602484473,'r': 0.33785714285714286}}]
-----------------------------------------------------------------------------------------------------------------------------------
p343:
Extractive Summary:
the health of the nations outcome scales for children and adolescents (honosca) is one of these outcome measures.the honosca is part of a larger body of work - the health of the nation outcome scales (honos) developed by wing et al., [11, 12] in the united kingdom as a brief mental health measure with the aim of tracking progress towards improving the health and social functioning of mentally ill people.several studies have examined the psychometric properties of the honosca [13, 14, 16] and because of its good psychometric properties and ease of use in routine clinical services, it has been translated into different languages [18].section b relates to the clinician’s assessment of the patients and caregivers’ understanding of the services and management options available to them.the effectiveness of the honosca is independent of the type of mental disorder involved, and its psychometric properties has been found to have moderate to good inter-rater reliability, with intraclass correlation coefficients above 0.7 [14, 15] and has validity demonstrated in number of studies [27–30].caregivers were not allowed to access the adolescent’s information and clinicians were blinded to the adolescents’ responses.data analysis item means, standard deviations, frequencies and percentages were calculated for the socio-demographic variables.internal consistency to investigate the reproducibility and consistency of the self and clinician rated versions of the honosca, reliability coefficients as measured by cronbach’s alpha were calculated.we used shrout’s [35] standards for the reliability results: virtually none: 0.00–0.10; slight: 0.11– 0.40; fair: 0.41–0.60; moderate: 0.61–0.80; and substantial 0.81–1.0.we also carried out inter-rater reliability between self-report and clinician rated versions using intraclass correlation (icc, absolute-agreement, 2-way mixed-effects model).validity concurrent validity was examined by comparing the pearson’s correlation coefficients between totals of the self- and clinician rated honosca and total psc scores, as well as the scores on first and second followup.sensitivity to change sensitivity to change (test-retest reliability) was assessed in the second/third session (first and second follow-up).a subsample of participants (n = 98) was available to be evaluated again with honosca by both adolescents and clinicians.the ability of the two honosca versions to reflect changes over time was assessed by observing mean differences between the scores across time points using both pearson’s correlation coefficient, and t-tests.dimensionality before performing factor analysis, the correlation matrix was inspected to check for the strength of correlation and then factorability was tested using exploratory factor analysis using kaiser-meyer-olkin (kmo) measure of sampling adequacy and bartlett’s test of sphericity.to assess dimensionality underlying the self-rated honosca items and extract the proper factor structure, we conducted exploratory factor analysis (efa) based on maximum likelihood.results the majority of the participants in our study were female at 54.2% and the mean age of our participants was 15.88 years.the most referrals to the clinic were made by academic institutions (84.2%).only 11.9% of the participants had previously received prior treatment.efa revealed four components that had eigenvalues greater than one and which explained 54.74% of the variance.as a final efa model, we retained these four factors.the four factors were related to relationship problems (1,3,10,11,12,13), severe psychiatric symptoms (3,7,9), school problems (2,5,11) and physical problems (6).table 3 reproduces the factor pattern of efa structure with promax-rotated factors.our study found both versions of the honosca easy to use and reliable to assess global severity of mental health problems experienced by children and adolescents seeking services in the youth clinic at knh.inter-rater reliability for both scales was found to be strong and similar to those found in other studies, which range from 0.72 to 0.96 [17, 18, 27–29].concurrent validity between the self-rated version and the psc were moderate.the poor correlations between clinician rated and self-reported honosca or psc scores were similar as in other studies.the different item scores poorly correlated with each other on both versions and this was found to be similar with the original study by gowers et al. [14] whose correlations range of correlations were 0.01–0.41, suggesting that the honosca is unidimensional scale.change in the scores at follow-up were moderately correlated with initial scores on both scales of the honosca.we found a four-factor solution, our factor analysis did not confirm the areas of functioning as hypothesised a priori by gowers et al. [14] or as evidenced by tiffin and rolling [38].further investigation is required to assess the factor structure in our local context as subscales may not translate easily across cultures.strengths and limitations to our knowledge, this is the first study to assess the psychometric properties of the honosca in a lmic context, specifically africa.some limitations of our study included: the test-retest took place at different times with a mean of 5 days (s.d = 4.72) between initial assessment and first follow-up and 10 days (s.d = 15.38) between first and second follow-up.this may have influenced internal consistency of the different versions.our clinicians (n = 69) varied in academic background and level of expertise with some having diploma in counselling, bachelor’s in counselling/nursing, master’s in counselling/ clinical psychology/ psychiatric social-work/ nursing and doctorate level, together with the large number of raters may have influenced the internal consistency of the clinician rated honosca.another limitation is that our study only recruited patients seeking outpatient services, and with less severe or complex mental health problems.the study was also carried out in an urban area and being a public hospital may draw patients with high levels of psychosocial disadvantage.the length of treatment was also not suitable enough to assess outcomes.conclusion the honosca has proven to be easy to use in routine care and useful to achieve outcome measurements in child and adolescent mental health services.we would benefit from further testing of honosca in our context with a larger sample, and strict parameters between sessions.internal consistency and concurrent validity of the clinician version was low warranting further evaluati
number of words= 949
[{'rouge-1': {'f': 0.36704242496012984, 'p': 0.7945762711864406,'r': 0.23863905325443788}, 'rouge-2': {'f': 0.21192518550708128, 'p': 0.39340425531914897,'r': 0.14502467917077988}, 'rouge-l': {'f': 0.3433265720081136, 'p': 0.6200000000000001,'r': 0.2373913043478261}}]
-----------------------------------------------------------------------------------------------------------------------------------
p344:
Extractive Summary:
however, care for psd in low- and middle-income countries (lmics) in southeast europe (see) is characterized by meetings between clinicians and patients dominated by pharmacotherapy while psychosocial aspects tend to be overlooked, reportedly because of the lack of qualified staff and sufficient funding of mental health services [4, 5].clinicians, patients and carers were purposively recruited from mental health services, service user organisations and mental health non-governmental organisations.thirty focus groups were conducted: eight with clinicians, six with policy makers, nine with patients and seven with carers.topic guides the topic guides explored participants’ views on barriers, facilitators and perceived benefits of implementing dialog+.intervention characteristics the intervention’s characteristics, likely the starting point of adopters’ engagement with the intervention, were largely interpreted as facilitators to the implementation across participant groups.clinicians, patients and policymakers viewed dialog+ as clear and easy to use.clinician45 from serbia expressed “[dialog+] couldn’t be simpler”.clinicians, patients and policymakers viewed dialog+ as adding structure to routine clinician-patient meetings while involving the psychosocial needs of the patient.clinicians spoke about dialog+ creating a workflow for patients and clinicians that could lead to more comprehensive therapeutic approaches.clinicians, patients and carers found the ability of the intervention to track patients and their treatment progress as particularly attractive.patient2: medical staff of this clinic, they are great, but there just aren’t enough of them.furthermore, some policymakers expressed that implementing dialog+ might lead to “loss of individualized approach” to patients.some participants expect that there will be clinicians’ resistance to innovation.… i need to talk more, to tell what’s on my mind, but the doctor never has enough time.policymaker2: i think opinions would be divided.you cannot expect all [clinicians] to accept something; some people have resistance towards technology … for most clinicians in the study it is vital that their service managers support them in the implementation of dialog+.clinician14 stated “i think that we must first have the support of management”.participants only received a short presentation describing the intervention, which is likely the motivation for such accounts.clinician3: we would have to get some instructions, this presentation was great, but we would need detailed training.policymaker8: no, [dialog+] won’t be different [in the skills required] (…) all of these [clinicians] are trained and they talk to patients – some for therapy, some for supportive psychotherapy – and each of them has his own way.patient3: i am good with computers, this is too easy.patient10: i don’t use a smart phone … so i would need [training].an important implementation facilitator interpreted from the data is the participants’ perceived need for dialog+ implementation, particularly as it supports innovation in mental health care.patient45: [clinicians] also pay little attention to the patient during the appointments, only 10–15 min or 5, irrelevant, but i think [dialog+] is already better.clinician9: i think that [dialog+] is very useful, that is exactly what we need.policymaker17: in the new national strategy that is under preparation, we have included a mental health category.so this project comes at the right time so that we can think about which services we can develop.we are very thin in this field.it should start inter-sector cooperation at the local level.this project can help us a lot … carer26: i think it will succeed, [implementing dialog+] is a step forward in medicine, having in mind how patients are treated, all is done in the old fashioned way, and nothing has improved until now.participants expressed that poor mental health of patients can be a barrier to dialog+ implementation.however, patients and clinicians also showed reluctance about the extent to carers’ involvement during sessions.policymaker3: i think caregivers should take part in this.whether they take part on the sessions or afterwards … it depends on what the patient wants...patient8: i would like [family members to be involved] in that part where doctor gives tasks to family members and me.i wouldn’t like them there all the time.other potential benefits at an organisational level were also reported: dialog+ would help shift the mental health services away from a typical medical approach to care by reinforcing a psychosocial care approach (clinicians, patients, carers); care could become more patient-centred (patients and carers); the structure and follow-up of care could be improved (all participant groups), and dialog+ could offer an opportunity for care to be standardized across services (by policymakers).these subthemes reflect how dialog+ could add to the routine service delivery and the elicited views are likely related to what participants are missing from their current care.carer36: the clinician gets closer to the patient, and the patient increases his confidence in the clinician [with dialog+].if this intervention goes on for a longer time, this can only deepen and expand the relationship.patient3: i like that we have freedom, that we can follow this app and that we have information from previous sessions.we see if we have made progress or not, we can see if the doctor’s therapy, advice, or drug were adequate.about the area ‘job situation’, policymaker4 said “it should be clearer that it also refers to education, or how the patient is dealing with being unemployed.” clinician36 spoke about the longer time needed to see changes in patients’ satisfaction with areas such as ‘job situation’ and ‘accommodation’ than that with their mental health, thus “perhaps these should be assessed only at a 6-month interval.” discussion our study’s findings offer important implications for implementation of digital psychosocial intervention for psd in low-resource settings.rather, participants’ accounts focused more on the lack of human resources.attractive intervention characteristics, such as clarity and easiness of use, and ability to provide structure to patient-clinician meetings, involve patient’s psychosocial needs and track patient’s progress were implementation facilitators considered important by the key stakeholders.‘perceptions of the innovation’ has been identified as one of the basic determinants associated with the rate of implementation of the intervention [28], thus it is paramount that key stakeholders have positive perceptions of the intervention.some clinicians and patients directly expressed a need to receive more training, largely explained by receiving only a short presentation of the intervention prior to the focus group discussions and some patient’s lack of familiarity with smart phones.such training could be delivered by the clinicians who can explain key principles and procedures to the patient during their first sessions.additionally, many positive attitudes and sufficient level of preparedness for dialog+ implementation across potential intervention adopters was reported, particularly related to the perceived existence of the required skills and high stakeholders’ motivation.dialog+ was mostly seen as consistent with existing norms and practices, an aspect that is often reported among strong enablers of innovation implementation across health care [29, 30].however, some important opposing views regarding the incompatibility of dialog+ with the mental healthcare funding policy in north macedonia and serbia and the lack of intervention flexibility should be considered.similarly, the leap highlighted that asking questions in a mechanical way could make the sessions ‘boring and overwhelming’.previous research in the uk similarly identified repetitiveness of dialog+ as a barrier [10].this result highlights the importance of considering individual patient’s needs regarding how frequently the intervention is used to avoid repetitiveness.the uk trial of dialog+ suggested that the intervention be used monthly for 3 months and flexibly afterwards [8].however, the initial perception among participants in this study was that the intervention should be delivered once a month.clinicians expressed that institutional support is a strong determinant for successful dialog+ implementation.this insight adds to the limited evidence of contextual and organisational factors regarding implementation of digital interventions for psd [19].increased patient empowerment and strengthened clinician-patient relationship were considered to be important potential individual benefits of dialog+ by all key stakeholders.this is in line with results from the uk study researching mechanisms of action of dialog+ [10].the opportunity of dialog+ to shift the typical medical approach to a more psychosocial approach – an approach that was recognized by all stakeholders as often neglected – was expressed as a potential organisational benefit.these views were also shared by the leap.the identified potential benefits from participants’ accounts present important facilitators to further the implementation process of digital psychosocial interventions in the explored context in see.digital interventions are becoming an integral part of the transformation of mental health care because of their potential to advance current services and to enable new pathways for provision of innovative psychosocial therapies [31, 32].the major implementation barrier identified in our current study was psychiatrists’ time constrains, which is consistent with other lmic settings considering implementation of digital psychosocial interventions.in order for technology to meaningfully ‘disrupt’ the current approach to mental health care, it needs to be inclusive of people with severe mental illnesses, consider how mental health providers can use such technologies most adequately and develop digital interventions with having low-resource settings in mind.participants’ responses about changes to dialog+ and its delivery frequency were not explored in detail during the focus groups, limiting the subsequent data analysis.nonetheless, these data were included since this they were related to the study’s research aim; however future research should explore the related themes further.this might have occurred because of various styles of individual facilitators and differences in their background (e.g., medical vs. psychological), as well as because of the varied motivation of participants to contribute to a theme.additionally, all transcripts were translated into english from five different languages, which introduced a risk of losing contextual-specific meaning and traits [35].however, this was a pragmatic decision as early stage translation enabled collaboration in the analysis process within the research team.the transcripts were not back-translated or checked with focus group participants; thereby, the process of translating transcripts could have introduced a bias that might have influenced the analysis of transcripts.however, the analysis team was in regular contact with focus group facilitators to clarify any ambiguous meaning of words.the analysis team consisted of researchers that were both proficient in english and some of the see languages and were able to go back to original transcripts where necessary.additional limitation of the study was not analysing data after each focus group in order to determine the saturation of data.to future cross-national, multi-disciplinary research teams conducting qualitative studies, we recommend in-depth training in qualitative methods, involving researchers from all countries in the analysis process and maintaining transcripts in their original languages.we hope that our approach and lessons learned can contribute to the literature on conducting such qualitative research.conclusion this study provides increased understanding from different stakeholders’ perspectives regarding the implementation of a digital psychosocial intervention for psd in five lmics in see.
number of words= 1716
[{'rouge-1': {'f': 0.33892334474931274, 'p': 0.8044632768361581,'r': 0.2146855870895938}, 'rouge-2': {'f': 0.17135886554604648, 'p': 0.3136260623229462,'r': 0.11788418708240536}, 'rouge-l': {'f': 0.3524901041663432, 'p': 0.6387203791469194,'r': 0.24341040462427746}}]
-----------------------------------------------------------------------------------------------------------------------------------
p345:
Extractive Summary:
ppd is defined as “a major depressive episode with peripartum onset and onset of mood symptoms occurs during pregnancy or within 4 weeks following delivery” according to the diagnostic and statistical manual of mental disorder (dsm-5) [3, 4].the prevalence of ppd is estimated to be 10–20% globally and higher in low-income regions (18.7, 95% ci 17.8–19.7, in low and middle income countries vs. 9.5, 95% ci 8.9–10.1, in high income countries) [5–8].in addition to ppd, postpartum post-traumatic stress disorder (pp-ptsd), a more severe mental disorder, has gained increased attention from researchers and clinicians worldwide [2].like post-traumatic stress disorder (ptsd), the three core symptoms for pp-ptsd are re-experiencing, avoidance and numbing, and hyperarousal [10].the association of ppd and/or pp-ptsd with poor maternal and child outcomes is well-documented [14–18], and includes impaired mother-infant bonding, low breastfeeding rates, an adverse effect on child development, and long-term somatic and psychiatric morbidity [15, 18].the prevalence of ppd has only been reported in several regions of china and there is wide variability among the studies (6.7% in hunan [20], 11.8% in guangzhou [21], 21.4% in fujian [22], and 23.2% in shanghai [23]).moreover, there is less or no available data on pp-ptsd at present.ppd and ppptsd co-morbidity has been reported [2, 24], but no studies discuss the two clinical entities together in china.due to differences in cultural values, health care policies, and welfare systems [35], risk factors for pp-ptsd in the medical literature might not apply to chinese women.the participants were recruited from one of the 3-a-class specialized hospitals, which is also one of the earliest provincial and municipal maternal and child health centers in china with > 800 beds, 30 wards, and approximately 30,000 births annually (shanghai first maternity and infant hospital, 2020).eligible women were approached by the researcher and given enough time to think about participation.to complete the online questionnaire, women needed to be fluent in mandarin and have access to a mobile phone.” and the response options were “low, medium, and high”.symptoms of ppd were measured with the edinburgh postnatal depression scale (epds), the most widely used tool for identifying possible ppd and has been recommended by the american college of obstetricians and gynecologists (acog) and the american academy of pediatrics (aap) [3].symptoms of pp-ptsd were measured with the perinatal post-traumatic stress questionnaire (ppq).researchers approached potential participants and gave a brief introduction of the study (research purpose, potential impact, and participants’ rights).women were informed that their participation was entirely voluntary, and whether they agreed to participate or not did not impact their treatment.all participants were assured that their data would be kept confidential and only accessed by the researcher.data analyses data were analyzed using the statistical package for social sciences (spss, version 22.0 for windows).the majority of participants had a bachelor’s degree or above (n = 961 [84.6%]), 2 participants (0.2%) reported smoking cigarettes during pregnancy, 479 participants (42.2%) had only 1 child in the family, and 821 participants (72.3%) were primiparas.prevalence and factors for ppd symptoms the mean epds score was 9.54 ± 4.46 (range = 3–25), and 267 (23.5%) women had ppd symptoms with an epds score ≥ 13.the presence of pp-ptsd symptoms increased the risk of ppd symptoms (or, 9.170; 95% ci, 4.773–17.617).women with pregnancy-induced hypertension had a higher risk of pp-ptsd symptoms than women who did not have pregnancy-induced hypertension (or, 5.041; 95% ci, 1.724–14.743).the presence of ppd symptoms increased the risk of pp-ptsd symptoms (or, 9.807; 95% ci, 5.071–18.962).discussion prevalence of ppd and pp-ptsd symptoms maternal mental disorders, including ppd, anxiety, and pp-ptsd, are common but often overlooked in china.due to diversity of the regional economy, culture and policy, and heterogeneity in the prevalence of maternal health disorders has been reported among regions of china [20–22].we reported a pp-ptsd symptom prevalence of 6.1% among parturients 6–8 weeks postpartum.in this study, we also confirm that pp-ptsd is the strongest risk factor for ppd and vice versa.for example, informational and instrumental support may be more in need at 3 weeks postpartum, while social integration may be more in need at 8 weeks postpartum [49].the association between sleep quality and ppd was confirmed in our study, which is consistent with previous studies [29, 50].poor sleep quality has been reported in women from late pregnancy to even 3 years postpartum [50].china, it should be noted, is one of the world’s largest multi-ethnic countries, with 55 ethnic minorities accounting for approximately 8.5% of the overall population [51].compared with the han ethnicity, the non-han ethnicity (ethnic minorities) is often under greater social and economic pressure and has more difficulties to get advanced medical resources [51].the majority of our participants were born under the influence of the one-child policy in china (1978–2015) and approximately one-half of the participants (n = 479 [42.2%]) were the only child in the family [52].compared to women with siblings, women who were the only child in the family usually had more parental attention, higher family support, more opportunities for a higher education, and lower economic pressures [52].the moderating role of the one child status in direct and indirect relationships between negative life events and ptsd symptoms have been tested in non-postpartum samples [53, 54].several factors, such as low education [48], cigarette smoking [48], primiparity [48] and a history of a cesarean section [33, 34] have been reported to be associated with pp-ptsd in some studies, but not confirmed in our study.the reason for this discrepancy might be differences among study populations, regions, study settings, and the measurement tools used.more quantitative research is needed to further evaluate these associations.a possible explanation is that women with newborn had complications may receive more meticulous care and support from both healthcare providers and family member.qualitative and quantitative research can be carried out to further explore these issues.implications although there has been an extensive improvement in prenatal care, psychological care is still largely overlooked.because the comorbidity of mental health disorders existed, for women who exhibited early signs and symptoms of mental health, a more comprehensive and multiple screening of mental disorders is suggested.obstetricians and midwives can play a crucial role in the initial assessment.limitations firstly, the nature of a cross-sectional study has its own limitation.finally, some important factors such as marital problems, pregnancy depression were not collected in this study, which needs to be further confirmed by later studies.conclusions this study addressed some gaps in the literature and provided a better understanding of ppd and pp-ptsd in china, which may contribute to early detection and early intervention.
number of words= 1071
[{'rouge-1': {'f': 0.3704156623241782, 'p': 0.8013432835820895,'r': 0.24088055797733218}, 'rouge-2': {'f': 0.22206407612940016, 'p': 0.41831460674157306,'r': 0.151151832460733}, 'rouge-l': {'f': 0.33321376808523256, 'p': 0.677142857142857,'r': 0.22097690941385437}}]
-----------------------------------------------------------------------------------------------------------------------------------
p346:
Extractive Summary:
there was a large variation in practices between the units when it came to inviting patients and relatives to a conversation with personnel, together and/or separately, to discuss family involvement (items 6, 8, 9 and 10).only two of the units used checklists to standardise the content of such conversations, and the topics usually covered varied between clinicians and between sites (items 3 and 4).the use of crisis/coping plans (item 11) and documentation of family involvement in the patients’ discharge report (item 13) also varied considerably.there were small differences in average scores on several items, between the units who offered fpe and those who did not.to investigate any correlation between the bfis scores and the units’ fpe status, we employed an independent samples mann-whitney u test with twotailed significance level α = 0.05.for the average bfis scores we calculated u = 27.5 and p = 0.955.p-values for individual items varied greatly, from p = 1.0 (items 1, 3, 5 and 14) to p = 0.054 (item 13).thus, no statistically significant correlation was found.family psychoeducation eight of fifteen sites offered fpe to patients with psychotic disorders and their relatives.the percentage of patients with psychotic disorders who had received or were receiving fpe in all units was 4.2%, ranging from 0 to 17.5% between sites.in the sites that offered fpe, the percentage was 9.4%, ranging from 1.9 to 17.5%.one unit offered both fpe and another family intervention inspired by open dialogue [39], but the remaining seven did not provide such interventions to their patients at all.item distributions for the fpe and goi scales are listed in tables 4 and 5.the mean fidelity score on the fpe scale was 2.78, ranging from 1.00 to 4.77.however, the distribution was markedly bimodal, since the seven units who did not offer fpe were scored 1 on all items.in the eight sites that did offer fpe, the mean score was 4.34, ranging from 4.00 to 4.77, showing that all of them practiced the model with adequate fidelity.only four sites had appointed personnel to coordinate fpe activities (item 1).in general, clinicians remained true to the structure and content of the model (items 2–6, 8, 9 and 11–13), but the use of multimedia sources varied (item 10).active recruitment of patients and relatives to fpe was generally low, with an average fidelity score of 2.5 in the sites that offered fpe (item 14).a similar tendency was seen in the goi scores, where only one unit had a standardised form of eligibility identification (item 2) and none of them had provided fpe to more than 20% of eligible patients (item 3).our premise when rating item 3 was that all patients with psychotic disorders were eligible to receive fpe, which is probably an overestimate.the average goi score in all 15 sites was 1.78, ranging from 1.00 to 3.00.among the eight sites who had implemented fpe, the average goi score was 2.46, ranging from 1.92 to 3.00, indicating that none of these had achieved an adequate integration of fpe in their organisation.psychometric properties from the present survey in 15 sites, we have calculated the percentage of exact agreement and the intra-cluster correlation coefficient (icc) for each item, and the mean total fidelity of the bfis scale (table 3).concerning the fpe scale, we calculated an icc of 0.93 for mean total fidelity, whereas the goi scale had an icc of 0.96.both numbers suggest a high level of agreement between raters.these calculations were only based on the results from the eight sites that offered fpe, because including the unanimous scores from the units who did not offer fpe would produce an artificially high correlation.discussion basic family involvement and support the results from this study demonstrate a general lack of structures and standard procedures in norwegian cmhcs, when it comes to family involvement and support for persons with psychotic disorders.several units had local resource persons with special competence in family involvement, who worked hard to increase the awareness and recognition of their field.during this survey, the project group took note of many exemplary practices that could inspire other units and clinicians in the subsequent phases of the ifip trial.some of the clinical sites had established local structures and routines for basic family involvement and support, and several had information meetings or other support measures for relatives.in most units however, contact with and involvement of relatives appeared both random and inadequate, depending highly on the practice of the patient’s clinician.as such, the results of this systematic survey of mental health services is consistent with the findings of previous research on relatives’ experiences [2, 3].the poor organisation of family involvement and support for adult relatives contrasted distinctly with the legally mandated structures, procedures and responsibilities for children as next of kin.nearly all the units in our survey had personnel responsible for taking care of children as next of kin and written procedures on this subject, which were widely used among the remaining personnel.the legislation concerning children as next of kin was passed in 2009, whereas the guidelines on family involvement in the health- and care services were published in 2017.the differences in implementation rates may be primarily due to legal incentives (and sanctions) being more important to administrators than following guidelines, but also related to the longer time span and family work towards children receiving more attention.in any case, it shows that improvement in cmhcs’ family work is feasible with appropriate focus, support, and incentives.several clinicians had frequent contact with relatives by phone, and the bfis scale does not include the penetration rate of such calls.the low percentage of relatives who were invited to a conversation at the cmhc, with or without the patient present, indicate that such conversations are not part of the standard approach in most units.the variable use of crisis plans and infrequent documentation of family involvement in the patients’ discharge reports may disrupt the continuity of care that is vital to this patient group and their next of kin.the fact that none of the units had annual training of their clinical personnel in family involvement is a particularly important finding, since the education of health professionals in norway have generally given limited attention to this subject.it therefore requires substantial effort within the health services to implement family involvement as a standard approach among clinicians.there may be several reasons why family involvement has received such little attention, in both training and implementation in norwegian mental healthcare.the research literature suggest that poor implementation is a problem internationally, and that barriers to family involvement exist on multiple levels.on a system level, these include a lack of financial incentives and explicit prioritization from managers and politicians, organisational cultures and paradigms, attitudes of leaders and staff towards evidence-based practices in general and/or family involvement in particular, inter-professional struggles, and poor access to training and supervision [4, 17, 18].as part of the ifip trial, we aim to investigate barriers to and facilitators for family involvement practices on a clinical, organisational and political level in the norwegian context, trough qualitative methods.the bfis scale the present model for basic family involvement and support is novel and has not yet been investigated scientifically as a whole.it consists of elements whose rationale varies from scientific evidence to legal frameworks and rights, as well as moral obligations.this reflects the composite nature of the guidelines that the model is based on.as such, the bfis fidelity scale is one of the first instruments of its kind to measure the implementation of guidelines and practices that are not exclusively evidence-based.we would argue that this new application of the fidelity methodology is justified, since many practices within mental health services are based on predominantly ethical and/or legal considerations, rather than expectations of treatment effect.the scale should also be appropriate to measure basic family involvement and support for patients with other forms of severe mental illness.perhaps, with some modifications, it may be suitable for health services towards other patient groups with chronic and severe illness.concerning psychometric properties, the scale shows promising irr, appears to have relevant content and captures variability in practice, but we cannot yet establish its benchmark value.
number of words= 1339
[{'rouge-1': {'f': 0.37774752892894203, 'p': 0.8177477477477477,'r': 0.2455994358251058}, 'rouge-2': {'f': 0.20152487927380183, 'p': 0.3651807228915663,'r': 0.13916019760056458}, 'rouge-l': {'f': 0.3484570491487001, 'p': 0.6533333333333333,'r': 0.23758747697974217}}]
-----------------------------------------------------------------------------------------------------------------------------------
p347:
Extractive Summary:
background studying public mental health through social media is a burgeoning area of research [1, 2] which is changing the way in which we understand mental health indicators, such as social isolation and suicidality, in the general population [3, 4].direct contact on social media has enabled sharing of concerns beyond the boundaries of face-to-face interactions and connecting people who would otherwise not have communicated [5].in 2018, a study of more than 4000 uk and us adults found that over 80% were using one or more social networking sites [6], making it possible to study population-level disclosure of mental health symptoms.reddit is a popular public online forum covering a diverse range of topics, featuring a user-voting system to rank posts, comments, and links within its subcommunities (known as subreddits).most reddit users (54%) come from the usa, with the uk ranking second and canada third [7].suicidewatch is a subreddit where people post about their suicidal thoughts (or about suicide-related issues regarding someone they know) to receive feedback and support from the community and it should be emphasised that posts do not necessarily reflect authorship by people experiencing suicidal ideation.nevertheless, because this is a highly sensitive topic, human moderators make sure that comments left for a post are not abusive [8].work to date includes attempts to automatically identify helpful comments [8] and changes in posting activity after high-profile suicides [9].weekly variation in suicide mortality has been investigated in many studies in both the us and uk, with the majority reporting peaks on mondays declining to weekend lows [10–12], although others have noted a peak in the middle of the week with higher suicide rates on wednesdays [13].diurnal patterns appear to vary by gender, age, and chronological time period, with one italian study showing peaks in both suicide attempts and deaths in the morning and early afternoon [14].however findings may differ by country and subpopulation, as indicated by a recent study national data from 1974 to 2014 in japan [15].this showed suicide by middle-aged males was most frequent in the early morning especially on mondays after the end of japan’s high growth period.large midnight peaks in suicide deaths were also observed among young and middle-aged males.the proportion of early morning suicide deaths by young and middle-aged males increased as the country’s unemployment rose.females and elderly males were more likely to die by suicide during the day than at night.interestingly the authors note the limitation that they studied the time of death, and of course this will be later than the attempt, and indeed the antecedent suicidal crisis when social media posting might occur.we investigated whether there are similar weekly and daily trends in the way that reddit is used by those posting to suicidewatch with their public user accounts, and with anonymous accounts: so-called ‘throwaway accounts’, (which we shall term ‘anonymous sw’).these accounts may reveal more sensitive information and socially unacceptable feelings owing to their anonymity [16].we compared how these posting behaviours are different from general reddit use by all users, and by those authors who post with identifiers on suicidewatch.methods sample we acquired a complete, publicly available, reddit data dump for the period between december 2008 and august 2015 (https://redd.it/3mg812).using a set of carefully curated keywords related to mental health [17], we had previously identified subreddits associated with mental health [18, 19].we refer to this dataset as ‘sw’ hereafter.overall, reddit users comprise younger males, with poll data showing 64% of reddit users are between 18 and 29 years of age and 69% of american reddit users are male [7].a subset of this subreddit is comprised of authors that contain the keyword ‘throw’, who can be considered to have ‘anonymous sw accounts’ (5882 authors, 13.8% of the sw authors).the use of the keyword ‘throw’ has been shown to be a high precision technique for identifying anonymous accounts [16, 20].there were only n = 108 deleted posts in the dataset and we are unable to deduce the number of unique deleted authors.furthermore, in order to aggregate and extract the temporal characteristics of the posting behaviour, we considered a time window of 1 week.starting from monday through sunday, we extracted the timestamp of posts and kept both hour of the day and day of the week according to the author’s local time.the final number of weekly observations varied for each data source, being 341 weeks for sw, 246 for anonymous sw, 391 weeks for askreddit and 354 weeks for askreddit (sw-authors).we also estimated the mean difference in proportions (md) and corresponding 95% confidence intervals (95% ci) of sw compared to askreddit and askreddit (swauthors) for six-hour intervals within a 24-h day - chosen a priori as early hours of the morning (midnight to 05:59 h), morning (06:00–11:59 h), afternoon (noon- 17:59 h) and night (18:00–23.59 h) - for the complete time period under study.for clarity only the trajectory of mean percentages of posts for sw and askreddit are summarised in the figure (anonymous sw followed a similar trajectory to sw and askreddit (sw-authors) followed the askreddit trend.users of sw had the highest probability of posting to this subreddit on mondays compared to all other days of the week (ratios ranged from 1.025 (95%ci: 1.020– 1.030) to 1.189 (95% ci: 1.180–1.198); all p ≤ 0.001 - see online table 1).they also showed higher probability of posting on tuesdays compared all other days of the week, apart from monday (ratios ranged from 1.106 (95%ci: 1.097–1.115) to 1.160 (95% ci: 1.151–1.170); all p ≤ 0.001 - see online table 1).overall, higher levels of posting in askreddit by sw authors could indicate that those posting to sw are higher intensity users of the reddit platform, posting to other subreddits too, and finding it helpful to seek support via the sw forum when in crisis.monday was associated with the highest sw posting levels compared to other days of the week.
number of words= 969
[{'rouge-1': {'f': 0.3446759143526195, 'p': 0.7270247933884297,'r': 0.22588235294117648}, 'rouge-2': {'f': 0.14055205421537376, 'p': 0.21522821576763487,'r': 0.10434739941118745}, 'rouge-l': {'f': 0.3185525273318108, 'p': 0.5951798561151078,'r': 0.2174747474747475}}]
-----------------------------------------------------------------------------------------------------------------------------------
p348:
Extractive Summary:
since the first report by swedo et all [3] of a form of ocd arising after streptococcal infection, pandas (pediatric autoimmune neuropsychiatric disorder associated with streptococcal infections), many authors have studied the role of inflammation in ocd patients, hypothesizing a role for immune dysregulation in ocd pathophysiology.this disparity is due to major methodological differences across studies, with sample sizes, matching of individuals, age of participants, presence of comorbidities and medication status as key limiting factors [10, 14, 15].additionally, it is possible that a combination of etiological factors, with specific mechanisms yet incompletely understood, is involved in the development of ocd, and thus the possible involvement of inflammation cannot be ruled out [2].on this regard, in spite of the inconsistencies across studies and following the hypothesis for the presence of a pro-inflammatory state among ocd patients, we conducted an exploratory study with a small subset of patients by evaluating the levels of plasma neutrophil gelatinase-associated lipocalin (ngal).ngal is an acute-phase pro-inflammatory protein [17] that is involved in iron homeostasis [18] and in spine morphology and neuronal excitability in the cns [19].increased levels of ngal have been detected among multiple sclerosis (ms) patients [20] as well as in heart failure, stroke and coronary artery disease, where it has prognostic significance [21].it has also been associated with depressive symptom severity [22, 23], although this later finding remains to be proven [24].in rodents, lipocalin- 2 (lcn2) – the analogous form of ngal – has been shown to be expressed in response to peripheral inflammation [25], focal brain ischemia [26] and spinal cord injury [27, 28].it is also up-regulated during the active phase of an animal model for ms [20] and seems to be involved in the pathophysiology of alzheimer’s disease [29, 30] and parkinson’s disease [31].most interestingly, a study using lcn2-null mice showed these animals exhibit anxiety- and depressivelike behavior and cognitive impairments in spatial learning tasks that correlated with increased hypothalamicpituitary- adrenal (hpa) axis activity and morphological alterations within the hippocampus, specifically, an atrophy of the dorsal hippocampus – which is involved in memory and cognition – and a hypertrophy of the ventral region – which plays a role in emotion.furthermore, neuronal morphology and dendritic spine density within the hippocampus were altered in mutant mice [32].this is in accordance with previous studies suggesting a role for lcn2 in the regulation of neuronal morphology and excitability in both the hippocampus and basolateral amygdala in response to stress [33, 34].further work with lcn2-null mice demonstrated lcn2 is involved in adult neurogenesis, differentiation and survival, where impairments at this level translate into poor performance in a hippocampal-dependent contextual fear discriminative task [35].it is noteworthy, however, that both the morphological and behavioral changes observed in lcn-2 null mice are in line with those observed in animals after chronic stress exposure or prolonged administration of exogenous glucocorticoids [36], which raises the question whether or not lcn2 is also involved in the regulation of the stress response, a feature that is known to be disrupted in a number of psychiatric diseases, for instance, ocd.ocd patients were recruited from the psychiatric outpatient clinic of hospital de braga, portugal, over a period of 3 months, and characterized with a comprehensive clinical assessment.the diagnosis of the disorder was established by experienced psychiatrists, using a semi-structured interview based on diagnostic and statistical manual of mental disorders, fifth edition (dsm-5).severity of disease was assessed using the yale-brown obsessive-compulsive scale (y-bocs) [37, 38].this clinical assessment allowed for the exclusion of presence of other psychiatric diagnoses, particularly depression and other anxiety disorders.because one female ocd patient was in clinical remission and asymptomatic at the time of the analysis, this subject was excluded from the study.blood sampling and analysis serum samples were obtained according to standardized protocols in the hospital de braga, portugal, at the time of recruitment.human lipocalin-2/ngal quantikine® elisa kit (r&d systems, inc., minneapolis, usa) was used according to the manufacturer’s instructions.briefly, a 96-plate was incubated with 100 μl/per well of assay diluent rd1–52, and 50 μl of sample was added to each well.the plate was covered and incubated for 2 h, at 8 °c. then, it was washed four times with wash buffer (400 μl), and 200 μl of human lipocalin-2 conjugate was added to each well.results socio-demographic and clinical characteristics the groups were similar in respect to age and gender [control group (cont), n = 19; 10 females/9 males; median and interquartile range (iqr) 28.00 (5) years of age; ocd group, n = 20; 9 females/11 males; 30.00 (7) years of age] (see table 1 for further details).
number of words= 755
[{'rouge-1': {'f': 0.3782432276273388, 'p': 0.7714925373134329,'r': 0.25053777208706784}, 'rouge-2': {'f': 0.20820487070047702, 'p': 0.365,'r': 0.14564102564102566}, 'rouge-l': {'f': 0.33396366774896225, 'p': 0.6211811023622047,'r': 0.2283710407239819}}]
-----------------------------------------------------------------------------------------------------------------------------------
p349:
Extractive Summary:
sexual abuse and physical abuse are strongly associated with depression [2–5].these types of abuse are highly prevalent [6, 7] and are associated with depression onset, suboptimal treatment response and poor prognosis [8–12].sexual abuse and physical abuse are not reported more often during a depressive episode [14].previous studies have reported association between abuse and suicidal behaviours in individuals with or without depression [15–18].one study of homeless women suggested that sexual and physical abuse are associated with low self-esteem [21].studies of individual depressive symptoms are lacking.studying symptom-level associations is important because depression is a phenotypically heterogenous condition.it is possible that sex may have no mechanistic role beyond higher rates of victimisation among females [24].on the other hand, it is possible that sex-specific physiological responses to sexual/physical abuse may influence risk for mental health disorders among adults [25].using the uk biobank, we explored associations of childhood sexual and physical abuse with individual depressive symptoms in adults.of these individuals, 152,447 had data on current depressive symptoms and 151,396 participants had complete data for exposure, outcome, and covariates.assessment of childhood sexual and physical abuse childhood sexual and physical abuse were assessed in 2016–2017 using an online mental health questionnaire which included information on childhood adversity.phq-9 scores the nine dsm-iv criteria for depressive symptoms (anhedonia, psychomotor change, change in appetite/weight, fatigue, sleep disturbance, low mood, concentration difficulties, low selfesteem, suicidal behaviours) as zero (not at all), one (several days), two (more than half the days) or three (nearly every day) [34].current depressive symptoms score was used as the outcome measure in our analysis.assessment of lifetime depression we defined adult lifetime depression as self-reported probable moderate and severe major depression, as previously used in uk biobank [37].binary measures of probable moderate and severe major depression were derived from a touchscreen questionnaire assessed at uk biobank assessment centres at baseline [37].age was based on self-reported date of birth at recruitment.higher scores represent greater deprivation.we therefore used current depressive symptoms as a measure using a cutoff score of 10 [34–36] .logistic regression was used to calculate odds ratios (or) with 95% confidence intervals (ci) for individual depressive symptoms and current depressive symptoms score.ors represent the odds of the depressive symptoms measure for individuals exposed to abuse compared with unexposed.holm-bonferroni p-value correction was performed to correct for multiple testing [39].to test for interaction with sex, we ran adjusted logistic regression with current depressive symptoms score as the outcome and included interaction terms with sex for all of the covariates and exposures listed in table 2.results characteristics of the sample the total sample used for analysis (n = 155,223) predominantly comprised individuals of white ethnicity (97.2%) and low deprivation (tdi mean = − 1.7; sd = 2.8) (table 2).sexual abuse was almost twice as common in women (11.1%) than men (5.8%).physical abuse was more common in men (8.3%) than women (8.0%).association between childhood sexual/physical abuse and adult current depressive symptoms score in the total sample (n = 151,396), sexual abuse was associated with increased risk for current depressive symptoms score after adjusting for potential covariates (adjusted or = 1.74; 95% ci = 1.63, 1.85) (table 3).in sex-stratified analysis, sexual abuse was associated with current depressive symptoms score in both men and women (table 3).ors were similar between the sexes.physical abuse was also associated with current depressive symptoms score in adulthood after adjusting for potential covariates (adjusted or = 2.17; 95% ci = 2.04, 2.31) (table 4).physical abuse as a child was associated with current depressive symptoms score in men and women (table 4).physical abuse was also associated with all individual depressive symptoms (table 4).physical abuse was most strongly associated with suicidal behaviours, psychomotor change, and low self-esteem in women, with adjusted ors ranging from 2.06 to 2.94.physical abuse was most strongly associated with change in suicidal behaviours, psychomotor change, and low mood in men, with adjusted ors ranging from 2.15 to 3.01 (table 4).sensitivity analysis: association between childhood sexual/physical abuse and adult lifetime depression in the total sample of participants with data on lifetime depression (n = 40,829), sexual abuse was associated with lifetime depression (or = 2.12; 95% ci = 1.97, 2.29) (table 5).ors were similar between the sexes.evidence for this association remained after adjusting for potential covariates including current depressive symptoms score (adjusted or = 1.45; 95% ci = 1.33, 1.58).moreover, genetic studies from the uk biobank indicate that heterogeneity within the depression syndrome is a function of environmental exposures, such as childhood adversity [29, 30].our results confirm that environmental factors, such as childhood sexual/physical abuse, play an important role in the onset of depressive symptoms [40].experiences of childhood sexual/physical abuse may lead to feelings of entrapment, habituation to pain, and reduced fear for death which may result in greater capacity for suicidal behaviours as a means of escape [19].the psychological consequences of exposure to sexual/ physical abuse may lead to hpa axis dysregulation, inflammation, and subsequent psychological depressive symptoms [45–47].a longitudinal approach would have been helpful to better understand trajectories from childhood sexual/physical abuse to depressive symptoms in adulthood.the nature of uk biobank data collection meant this was not possible.furthermore, the use of self-reported recall of childhood sexual/physical abuse by adults could introduce some level of recall bias.we attempted to minimise the impact of recall bias by adjusting for current mood.we found that differential recall of negative events by depressed individuals was not the sole explanation for the association between sexual/physical abuse and depressive symptoms, but other factors could still contribute to bias.longitudinal studies with large sample sizes are required to investigate biopsychosocial mechanisms affecting the relationship between childhood sexual/physical abuse and depressive symptoms in men and wom
number of words= 930
[{'rouge-1': {'f': 0.3740876577625291, 'p': 0.856046511627907,'r': 0.23933867735470943}, 'rouge-2': {'f': 0.22277174158939458, 'p': 0.4391588785046729,'r': 0.14923771313941825}, 'rouge-l': {'f': 0.3484089021757183, 'p': 0.6699999999999999,'r': 0.23541353383458646}}]
-----------------------------------------------------------------------------------------------------------------------------------
p350:
Extractive Summary:
likert-item scale responses from the faith and hope facets of the whoqol- srpb scale were modeled as latent sub-domains of “spirituality and religion." mental health and iq were modeled as observed variables using computed scores.list sorting working memory, flanker inhibitory control and attention, and dccs tests scores (cognitive flexibility) were modeled as latent "executive function." (fig. 1).additionally, for comparison purposes, we tested a model in which the hope and faith facets were modeled together with happiness, life satisfaction, meaning and purpose, and self-efficacy as additional subdomains of psychological well-being.we assessed model fit using the root mean square error of approximation (rmsea), comparative fit index (cfi), and tucker–lewis index (tli).a good model fit is indicated by rmsea < 0.08, cfi > 0.90 and tli > 0.95.we interpreted correlation coefficients > 0.70 as strong, between 0.70 and 0.40 as moderate, and < 0.40 as weak.our analysis accounted for clustering of subjects within family, and models were sex stratified.all analysis was conducted using mplus 8.0 using the weighted least square mean and variance (wlsmv) estimator for categorical and ordinal responses using pairwise deletion of missing values [49].results study participants (704 women; 564 men) had a mean age of 47 years.table 1 shows means and sd of the scores of completed tests.models in women and men showed adequate fit (rmsea = 0.04; cfi = 0.95, tli = 0.95, and rmsea = 0.04, cfi = 0.95, tli = 0.94, respectively).models combining the hope and faith facets with the psychological well-being components indicated a small decrease in goodness-of-fit indices (rmsea = 0.05, cfi = 0.94, tli = 0.93 in women, and rmsea = 0.04, cfi = 0.94 and tli = 0.93 in men).thus, we decided to keep the model that differentiates spirituality and religion from psychological well-being.first-order factor loadings for scales assessing happiness, life satisfaction, meaning and purpose, self-efficacy, emotional support, hope, and faith are presented in additional file 1: table 2.second-order cfa showed that the theorized subcomponents for psychological well-being, and spirituality and religion were highly loaded into their underlying constructs.we also found computed scores for list sorting working memory, flanker inhibitory control and attention, and dccs tests loaded onto the executive function latent construct (table 2).intercorrelation matrices between latent domains and observed variables in women and men are presented in tables 3 and 4, respectively.in women, psychological well-being was moderately associated with spirituality and religion (r = 0.68, p < 0.001), weakly correlated with emotional support (r = 0.34, p < 0.001), mental health (r = 0.32, p < 0.001) and iq (r = 0.15, p < 0.001), and showed no association with executive function.mental health was weakly correlated with emotional support (r = 0.18, p < 0.001), spirituality, and religion (r = 0.16, p < 0.001), and showed no association with iq and executive function.we also found moderate correlations between executive function and iq (r = 0.63, p < 0.001) (table 3).we observed a moderate correlation between psychological well-being and spirituality and religion (r = 0.70, p < 0.001).psychological well-being was weakly correlated with emotional support (r = 0.35, p < 0.001), mental health (r = 0.35, p < 0.001), iq (r = 0.25, p < 0.001) and executive function (r = 0.23, p < 0.001).mental health was weakly associated with emotional support (r = 0.09, p < 0.05), spirituality and religion (r = 0.12, p < 0.05), and iq ( r = 0.09, p < 0.05), and showed no association with executive function.we also found moderate correlations between executive function and iq (r = 0.70, p < 0.001) (table 4).discussion we investigated associations of psychological well-being and mental health with executive function, iq, spirituality and religion, and emotional support in a population of guatemalan adults born in contexts of poverty and malnutrition.our results derived from cfa support the intercorrelation matrices between latent domains and observed variables in women and men are presented in tables 3 and 4, respectively.in women, psychological well-being was moderately associated with spirituality and religion (r = 0.68, p < 0.001), weakly correlated with emotional support (r = 0.34, p < 0.001), mental health (r = 0.32, p < 0.001) and iq (r = 0.15, p < 0.001), and showed no association with executive function.mental health was weakly correlated with emotional support (r = 0.18, p < 0.001), spirituality, and religion (r = 0.16, p < 0.001), and showed no association with iq and executive function.we also found moderate correlations between executive function and iq (r = 0.63, p < 0.001) (table 3).the correlation matrix in men showed similar results.we observed a moderate correlation between psychological well-being and spirituality and religion (r = 0.70, p < 0.001).psychological well-being was weakly correlated with emotional support (r = 0.35, p < 0.001), mental health (r = 0.35, p < 0.001), iq (r = 0.25, p < 0.001) and executive function (r = 0.23, p < 0.001).mental health was weakly associated with emotional support (r = 0.09, p < 0.05), spirituality and religion (r = 0.12, p < 0.05), and iq ( r = 0.09, p < 0.05), and showed no association with executive function.we also found moderate correlations between executive function and iq (r = 0.70, p < 0.001) (table 4).discussion we investigated associations of psychological well-being and mental health with executive function, iq, spirituality and religion, and emotional support in a population of guatemalan adults born in contexts of poverty and malnutrition.our results derived from cfa support the findings demonstrate the construct validity of first order (i.e., happiness, life satisfaction, meaning and purpose, selfefficacy, emotional support, faith, hope and executive function) and second order (i.e., psychological well-being and spirituality and religion) factor structures.in both sexes, spirituality and religion was moderately correlated with psychological well-being and weakly correlated with mental health.much debate has revolved around whether there is a meaningful differentiation between spirituality and religion from psychological well-being components.in agreement with previous studies [50], our findings support differentiation between these two constructs.the positive association of psychological well-being with spirituality and religion is well documented [51–53].our study findings show positive correlations of spirituality and religion with subjective well-being (r = 0.68 in women and r = 0.70 in men), that are in line with previous research conducted in non-western contexts, indicating small but consistent positive associations between religiosity and psychological well-being [19, 53], even after controlling for difficult life circumstances [53].using data from 153 nations, diener and collaborators found that in religious societies experiencing difficult life circumstances (e.g., poverty, low education, malnutrition, low life expectancy, etc.), religious individuals had greater levels of subjective well-being than non-religious individuals, and this association was mediated by social support, respect and purpose in life [53].it is possible that organized religion provides supportive social structures that can, to some extent, diminish the harmful effects of difficult life circumstances.furthermore, the authors found that difficult individual circumstances were associated with religiosity at the individual level (r = 0.29) and country level (r = 0.65), suggesting that difficult life circumstances could lead to greater religiosity [53].the mechanism by which spirituality and religion could influence psychological well-being has been suggested to involve psychosocial factors such as providing a sense of identity and social support and promoting an active and socially engaged lifestyle [54].our study found weak associations of emotional support (our measure of social support) with psychological well-being, and spirituality and religion, in both sexes.however, our social support measure was limited to emotional aspects and did not include components of instrumental support or social networks, which could be underestimating the associations.the religious landscape in guatemala may provide additional insights into the observed associations.pentecostal congregations rose in popularity in guatemala during the late 1970s, turning it into one of the most protestant countries in latin america [55].this is relevant because pentecostal churches are very supportive of their adherents, providing them with various social services.the extent to which social support mediates the association of spirituality and religion with psychological well-being in this population remains to be investigated.
number of words= 1314
[{'rouge-1': {'f': 0.4035387965397896, 'p': 0.8312732095490716,'r': 0.26644079397672826}, 'rouge-2': {'f': 0.27142792171621816, 'p': 0.5141489361702127,'r': 0.18438356164383563}, 'rouge-l': {'f': 0.37919413151131004, 'p': 0.609877300613497,'r': 0.2751282051282051}}]
-----------------------------------------------------------------------------------------------------------------------------------
p351:
Extractive Summary:
based on the national survey on drug use and health, the abuser of methamphetamine, heroin, methadone, and sedative was 1.1 million, 808,000, 256,000, and 751,000 [5].they are less subject to change.this study found several robust differences in temperament and character among substance use disorder.material and methods design this was a case–control design in which data from patients with substance use disorder control subjects were collected in a convenience sample.all patients with substance use disorder were chosen randomly from patients with substance use disorder visiting ibn-e-sina hospital, mashhad, iran, in 2016–2018 and consulted by a psychiatrist.the control group was composed of ageunmatched and gender-matched subjects recruited from among individuals without substance use disorder.inclusion criteria were diagnosed with substance use disorder by dsm v criteria, and exclusion criteria were unwilling to the study and all subjects with comorbid axis i disorders were also excluded in this study.the control group was composed of age-unmatched and gender-matched subjects recruited from among individuals without substance use disorder and all of them assessed by a psychiatrist.in this study, we classified substance use disorder patients into five categories, including 1-methadone, sedative, opium 2-methadone, amphetamine 3-methadone, sedative, amphetamine 4-methadone, opium 5-amphetamine, opium.personality traits were assessed using the self-administrated brazilian version of tci-r consisting of 240 selfdescriptive true/false items, assessing four temperament dimensions: ns (range 0–40; sign: positive; minimum significant score: n/a); ha (range 0–35; sign: positive; minimum significant score: n/a); rd (range 0–24; sign: positive; minimum significant score: n/a); p (range 0–8; sign: positive; minimum significant score: n/a); sd (range 0–44; sign: positive; minimum significant score: n/a); and three character dimensions: c (range 0–42; sign: positive; minimum significant score: n/a); and st (range 0–33; sign: positive; minimum significant score: n/a) [20].the homogeneity of groups in terms of demographic variables were assessed by chi-square or t-test.multivariate analysis of covariance (mancova) was employed to assess the relationship between temperament and character traits and patterns of substance use.results in this study, 70 men with substance use disorder and 70 healthy individuals were examined.twenty-five percent of people had a history of psychological disorders, and 20% had a past medical history.in contrast, the score of novel seeking was significantly higher in the case group (p < 0.05).on the other hand, harm avoidance was not significantly different between the two studied groups (p = 0.637) (table 2).multiple analysis since sex, age, marital status, and level of education were significantly different between groups, we run the multivariate analysis of covariance (mancova) to adjust the effect of these variables considering the correlation between subscales of tci-r.the score of rd (f (1,126) = 14.24, p < 0.001, η2 = 0.102), ps (f (1,126) = 6.91, p = 0.01, η2 = 0.052), sd (f (1, 126) = 12.83, p < 0.001, η2 = 0.092), co (f (1, 126) = 46.22, p < 0.001, η2 = 0.268) and st (f (1, 126) = 11.99, p = 0.001, η2 = 0.087).methadone, sedative, amphetamine: overall, 23 individuals (32.9%) were dependent on all three substances of methadone, sedatives, and amphetamine.discussion in this case–control study comparing 70 men with substance used disorder patients with 70 unmatched control, the combined scores for tci were significantly different.substance use disorder patients presented lower rd, ps, sd, co, st, and higher ns compare to the control group.however, some study assumed that ns was even lower in patients with substance use disorder than a control, for instance, süleyman can et al. study on substance abusers in the turkish military population and concluded that ns and ha, and significantly lower scores for pe, sd, and co were detected in substance abusers than in the controls [24].based on previous studies performed in iran, including abolghasemi et al. and ketabi et al. study, both ha and ns were higher in substance disorder patients.following numerous studies, including abolghasemi et al. and ketabi et al. studies, in the present study, ns was significantly higher in the case compared to the healthy group [25, 26].pournaghash et al. compared the tci score of amphetamine use disorder to opium used disorder and concluded that all tci-r components were significantly higher in amphetamine use disorder.in contrast, the present study showed that patients with opium plus methadone and amphetamine plus methadone use disorder were the same in all tci scores.however, in methadone plus opium use disorder, sd was lower [23].in our study, patients with a polysubstance use disorder presented lower rd, ps, and st, while in koller et al., studies on ns and st were higher in polysubstance use disorder [27].controversy present in almost every research in tci and addiction’s field.these disagreements might exist because of diversity in population or even substantial differences worldwide.in conclusion, higher novelty sicking in patients with substance use disorder is common and different traits, and temperaments would choose different substance combinations.optimally the causal hypotheses of the kind made are more reliable if found in prospective follow-up studies.other limitations of the study is the small sample size, in future studies, we strongly suggested that future studies design to investigate the effect of this method with higher sample size and prospective follow-up studi
number of words= 838
[{'rouge-1': {'f': 0.3885363846443152, 'p': 0.8352582159624413,'r': 0.2531460674157303}, 'rouge-2': {'f': 0.2711450792158513, 'p': 0.5369811320754717,'r': 0.18136107986501687}, 'rouge-l': {'f': 0.40274432384812425, 'p': 0.7165517241379311,'r': 0.28008403361344536}}]
-----------------------------------------------------------------------------------------------------------------------------------
p352:
Extractive Summary:
background armed conflicts are an unfortunate constant of human civilization [1].most of the communities exposed to armed conflict experiences (ace) live in low/middle- income countries with limited resources for social investment and mental health support [2].mental health studies among people exposed to ace have focused on characterizing highly prevalent mental health disorders (e.g. depression, anxiety) and/or their symptoms associated with traumatic ace, using grouping criteria according to a legal frame (e.g. ex-combatants and victims) [3, 4].in this sense, tobón et al. [5] and sánchez- padilla et al. [6] characterized adult ex-combatants and victims.they found that 39.9% of such population described distress or anxiety and 39.3% showed sad feelings together with recurrent crying.they also reported diagnosis of depression (18.2%), acute stress disorder (9.9%), and ptsd (8.4%).moreover, other mental health dispositions such as empathy have also been evaluated.authors have found that different empathic dispositional profiles (e.g., low empathic concern and personal distress) were observed in ex-combatants when compared to controls based on the interpersonal reactivity index (iri) assessment [5].they found that ex-combatants and victims with low empathic scores showed lower neuropsychological rates in working memory and inhibitory control than those with high empathy [5, 6].other studies such as the colombian mental health survey [7] used semi-structured surveys like the self- reporting questionnaire (srq) and socio-demographic questionnaires in civilians [8, 9].they evaluated the associations between exposure to ace and mental health disorders and revealed that individuals exposed to ace had a higher probability of showing mental health disorders when compared to non-exposed people [7].additionally, other studies identified that civilians exposed to ace also experienced a higher prevalence of mental health disorders with emotional and psychological affections [8, 10–12].colombia is a well-known world referent of a longterm and low-intensity armed conflict with a wide impact on the continent.official entities for victims such as ruv for its abbreviations i in spanish (unique registry of victims) inform that 18.5% of colombian population has been a victim.reported events were mainly forced displacement (7,553,750), homicide (1,010,989), and harassment (419,229) [13].moreover, according to the colombian normalization and reincorporation agency (arn: agencia para la reincorporación y normalización), 74,277 people left illegal armed groups between 2001 and 2019 [14].in addition to these actors (victims and ex-combatants), the colombian armed conflict has also impacted the general population.the latest mental health survey in the country showed a prevalence of traumatic events related to the armed conflict of 7.7% (95% ci 6.9–8.5) in the general population between 18 and 44 years old [7, 9].government programs have prioritized individuals identified as victims or ex-combatants based on the evidence mentioned above.however, there is a limited inclusive analysis of mental health outcomes in civilians exposed to ace [15].thus, related studies include civilians (victims or not) and ex-combatants in a common quantitative category due to the lack of validated measures to control the level of ace.these situations have blinded the characterization of mental health outcomes in populations exposed to ace and have constrained the evidence for developing public-health-based screenings, assessments, and interventions focused on reducing the burden of mental health symptoms [15, 16].classifying the population according to their level of ace allows tackling difficulties related to: (a) data quality, by addressing populations at risk with ace as a measure of exposure; (b) ecological fallacy, by attributing effects that occur at a macro level to individuals [17]; and (c) the influence of ace in different dimensions (i.e. social, cultural, health) to characterize these events.quantifying the exposure to ace will support establishing mental health risks and, therefore, prioritizing key vulnerable groups [18–20].previous studies in armed conflict and mental health identified limitations in the reliability of scales and questionnaires aiming to characterize relations between mental health and levels of exposure to ace [18, 20–22].there are few instruments validated in spanish used for this purpose [23].in this context, in giraldo et al. [23] we previously validated the extreme experiences scale ( ex2) with populations exposed to ace in colombia.this instrument allowed us to enhance the reliability for classifying individuals according to their ace in terms of levels of exposure (e.g. low or high).it was sensitive to capture the chronic exposure to ace expected in scenarios such as the colombian one.this instrument showed content and face validity, and internal consistency (kr- 20: 0.80, 95% ci 0.76–0.84).a two-dimensional factorial structure (direct or indirect exposure to extreme experiences) with an adequate model adjustment was found (cfi 0.91, tli 0.90, rmsea 0.05).in the present study, we aimed to evaluate mental health outcomes in a population with different levels of ace.our hypotheses were: (a) ex2 will reliably discriminate different levels of ace in a sample comprised of ex-combatants, victims, and general population (nonvictims); and (b) populations with low and high ace have differential patterns related to mental health outcomes (i.e., post-traumatic stress disorder, other anxiety, and mood disorders, as well as in positive mental health aspects such as variations in empathic dimensions).we expect that this study will contribute with relevant knowledge about the relation between mental health outcomes and ace, and that it will enhance the attention of mental health services provided to these populations by the local government agencies.methods participants a sample of 220 adult subjects participated in this study.35 of them were excluded because of missing data (we excluded subjects if more than 5% of the items on the ex2 and iri scales were not answered).additionally, instead of considering only their mental health diagnosis, we also considered the use of socio-cognitive instruments to evidence social and affective aspects of mental health, such as it is presented in empathy dimensions.we expect that this approach will improve the effectiveness of the attention to screen, assess, intervene, and potentially prevent outcomes in populations affected by these events.although our sample size was small when compared to previous studies [6], our statistical model guaranteed: (1) the reliability of the results, because the regression model is a robust model adapted for small sample sizes; (2) we found no differences in socio-demographic variables that commonly work as confounding; and finally, (3) our findings are aligned with previous studies that used larger samples [8, 19, 20].additionally, studies on mental health of populations affected by armed conflicts have shown limitations in the reliability of the questionnaires to measure the exposure to ace [2, 20, 22].our study controlled this by using a validated instrument ( ex2) [23].the results of our study represent an important piece of evidence for mental health professionals, especially to direct their efforts on strategies oriented to screen, assess, and implement effective interventions required in populations affected by armed conflicts.moreover, we suggest the importance of considering not only the aspects reported in this study but also other elements of their particular social context (e.g., access to health and educational services).we expect that future studies could develop two lines of actions: (a) to perform a systematic characterization of the samples based on reliable inventories such as ex2 in populations affected by ace; and (b) to implement evidence-based interventions focused on enhancing social abilities, responding to particular contexts and beliefs as reported in previous studies [8, 49, 53].this would contribute to integrate different approaches such as public health strategies and, therefore, developing cost-effective models to assess mental health risks across populations exposed to ace.such intervention might enhance the sensitivity to evaluate mental health outcomes in armed conflict contexts, providing new evidence to transfer to epidemiological and clinical fields [33, 53].we envisage that the replication of our results will inform mental health public policies adapted to populations exposed to ace.we expect that future studies will promote the use and transference of these associative models, not only to communities chronically exposed to armed conflicts but also populations with extreme vulnerability experiences, such as refugees and people affected by forced displacement.additionally, we expect further advances in the study of mental health outcomes and coping strategies observed in populations exposed to ace [10].conclusion this is one of the first studies focused on classifying people exposed to ace in terms of low or high levels of exposure and establishing an association with mental health outcomes such as anxiety, risk of suicide, ptsd, and fantasy.the ex2 is one of the first instruments, which allow classifying populations based on the exposure to ace, avoiding the use of legal labels (e.g., victim, military, or ex-combatant).
number of words= 1363
[{'rouge-1': {'f': 0.32415429216034997, 'p': 0.8209881422924901,'r': 0.20194444444444445}, 'rouge-2': {'f': 0.1846121749234399, 'p': 0.3715873015873016,'r': 0.12281445448227937}, 'rouge-l': {'f': 0.3317617377096449, 'p': 0.6459493670886076,'r': 0.2231986531986532}}]
-----------------------------------------------------------------------------------------------------------------------------------
p353:
Extractive Summary:
considering that the unidimensional factorial structure of the bas-2 has been previously evaluated, this study employed cfa to confirm that one latent variable (i.e. in this case ba) appropriately applies.secondly, local independence assumes that a participant’s response to a question is only conditional to the level of the latent trait, and thus, independent of responses to other items [14].chen and thissen [12] propose that the local dependency (ld) χ2 statistic can determine such occurrences by comparing observed and expected frequencies between item responses.accordingly, ld χ2 values larger than 10 could indicate local dependence concerns [12].this assumption was met as ld χ2 bas-2 items were < 10.finally, monotonicity refers to the constant increment of a variable as a function of another variable.in irt contexts, this represents that the probability of endorsing an item should increase as trait levels increase [38].in other words, a functional form (in this case an ‘s’ shaped curve) should be observed when plotting the function specified by the model [14].bas-2 items demonstrated a functional form and thus met the assumption of monotonicity (this can be observed in fig. 5).furthermore, irt models employed included the unidimensional gr and gpc [8].the gr model deals with ordered polytomous categories and is the preferred method for assessing questionnaires with likert scales.the gpc estimates partial credit points for correctly endorsing some aspects of the item [26].maximum marginal likelihood methods of estimation were employed in line with past recommendations for ordinal polytomous irt models [7].considering the tendency of χ2 values to inflate with the use of large sample, as is the case here, the best fitting irt model was combinedly determined by (i) the loglikelihood index of fit [10], (ii) rmsea < 0.05 as criteria for sufficient fit [18], and (iii) bayesian and akaike information criterion (bic and aic) with smaller values demonstrating a better model fit [14].subsequently, item parameter characteristics were assessed with the item characteristic curve (icc) and item information function (iif), while test characteristics were assessed with the test information function (tif) and the test characteristic curve (tcc [14]).results measurement invariance first, the bas unidimensional factorial structure was assessed across gender groups.both groups demonstrated acceptable fit according to acceptance criteria for rmsea, tli and cfi suggested by [18] (males: χ2 = 80.044, df = 35, p < 0.001, cfi = 0.972, tli = 0.963, rmsea = 0.077; females: χ2 = 59.404, df = 35, p = 0.006, cfi = 0.980, tli = 0.975, rmsea = 0.064).unstandardised item loadings for men ranged from 1 to 1.45 (fig. 1) and for women ranged from 0.84 to 1.61 (fig. 2).both groups demonstrated good internal reliability coefficients (males cronbach’s α = 0.955, and mcdonald’s ω = 0.957; females α = 0.943, ω = 0.946).second, mi was conducted for men and women scoring on the bas.the unidimensional bas configural model showed acceptable fit for the sample (χ2 = 161.20, p < 0.001, cfi = 0.974, tli = 0.967, rmsea = 0.072) with a statistically significant decrease in absolute fit (satorra– bentler scaled δχ2 = 19.38, p = 0.022) and non-significant change in incremental fit (s-b scaled δcfi = 0.004; δrmsea = 0.001) at the metric level (table 2).given the significant decrease in absolute fit between configural and metric models, no meaningful observations could be inferred between metric and scalar model comparison.therefore, we proceeded to identify non-invariant parameters by evaluating modification indices and utilising the benjamini–hochberg procedure.as presented in table 3, parameters that produced a significant s-b scaled δχ2 were λ 2, λ 8, λ 9, α 1 and α 9.after calculating benjamini–hochberg adjusted p values it was determined that all 5 parameters presented a “p < bh p” condition, thus remaining significant for partial invariance purposes.indeed, free estimation of factor loading 2, 8 and 9, and intercepts 1 and 9 achieved a not significant decrease when compared to the configural model (s-b scaled δχ2 = 10.61, p = 0.056).psychometric irt properties following past recommendations [6, 7], we employed marginal likelihood information statistics with one and two-way marginal table to assess goodness of fit ( m2 [710] = 1443.09, p < 0.001, rmsea = 0.05).given that m2 is sensitive to sample size, rmsea was sufficient to determine goodness of fit to data [24].comparisons across the graded response model (gr) and generalised partial credit model (gpc) were conducted.the gr demonstrated better fit to data (χ2loglikelihood = 8111.38; rmsea = 0.06; bic = 8410.20; aic = 8211.38) when compared to the gpc (χ2loglikelihood = 8182.21; rm sea = 0.06; bic = 8481.02; aic = 8282.21), and thus item parameters discussed subsequently were obtained with the gr model.when discrimination parameters (i.e., α) where constrained to be equal across models, a significant decrease in fit indices was observed (χ2loglikelihood = 11,414.61; bic = 11,653.66; aic = 11,494.61).thus, suggesting that the pc does not appropriately model observed data.discrimination parameters for all ten items fell within the very high range (0 = non discriminative; 0.01– 0.34 = very low; 0.35–0.64 = low; 0.65–1.34 = moderate; 1.35–1.69 = high; > 1.70 = very high) between 1.87 (α item 5) and 5.19 (α item 4).the descending sequence of the items’ discrimination power (α) is 4, 6, 2, 3, 9, 10, 8, 7, 1 and 5 (see table 4).furthermore, the item difficulty parameters (β), demonstrated a considerable level of fluctuations between the different thresholds across the 10 items.indicatively, for the first threshold the ascending item sequence of difficulty was 6, 10, 9, 8, 2, 4, 3, 1 and 5.considering the fourth threshold, this alternated to 3, 4, 10, 9, 1, 6, 7, 5, 8 and 
2.nevertheless, the threshold difficulty parameters progressively increased between the first and the last threshold across all items (see table 4 and fig. 3).conclusively, irt analyses indicated that: (i) while increasing item scores correctly described increasing levels of ba behaviours across all items, the rate of increment is different across the items, and (ii) different thresholds perform differently across items considering their level of difficulty.considering the items’ reliability across the different levels of the latent trait, controlling concurrently for the different levels of items’ difficulty, meaningful variations were confirmed.indicatively, the iif of item 4 provided the highest level of information/reliability in the ranges between 2 and 1 and a half sd below and above the mean and the area around half sd below and above the mean.the iifs of items 2, 6, 9 and 10 showed better performance in the range between 2 sds above and below the mean (although with some variability of less than 1 point).items 1 and 5 showed a rather low and undifferentiated level of reliability in the area between minus 3 sds below the mean and 2 sds above the mean with significant drop for behaviours exceeding 2 sds above the mean.finally, item 7 showed average reliability for the area between 2 sds above and below the mean and significant drop for score around 3 sds above or below the mean (see fig. 4).the performance of the scale as whole is visualized by the test characteristic curve (tcc) and the test information function (tif).the tcc graph illustrates that the trait of ba inclined steeply, as the total score reported increased (from 4 to 49; see fig. 5).these results suggest that the scale (as a whole) provides a sufficient and reliable psychometric measure for assessing individuals with high and low levels of the ba behaviours in the range between 2 sds below and above the mean.nevertheless, it may not be an ideal measure for extremely low and high ba in the areas around 3 sds above and below the mean.the ba behaviour at the levels of 2 sds below and above the mean trait level correspond with raw scores of 4 and 39 respectively, and based on these, they could be suggested as conditional (before clinical assessment confirmation) diagnostic cutoff points.considering dif of bas-2 across men and women, sources of non-invariance at the item level were detected.dif statistics were observed (table 5) for all items with significant discrepancies (p = 0.05) in items 1 and 9 across men and women including all parameters (χ2), items 3 and 9 including only discrimination (χ2 a), and item 1 including only difficulty (χ2 cja).following past recommendations [25], we anchored invariant items and calculated dif statistics with only non-invariant items to avoid increasing type i error.as observed in table 5, noninvariance was observed for both items (1 and 9) including all parameters, with a significant difference including only discrimination (p = 0.01) in item 9, and a significant difference including only difficulty (p = 0.01) in item 1.that is, endorsing categories in item 1 requires lower levels of ba in women.figure 6 offers a visual representation of this relationship.for example, in item 1 men with 1 sd below the mean are more likely to endorse category 1 (seldom) than women with 1 sd below the mean.similarly, women with 1 sd above the mean are more likely to endorse category 4 (always) compared to men.in item 9 however, significantly different discrimination (α) indicates that women with 2 sd below the mean are more likely to endorse category 0 (never) compared to men, and this relationship is reversed as levels of ba increase (i.e., women with 2 sd above the mean are more likely to endorse category 4 [always] compared to men).discussion the present study is the first of this type to combine ctt and irt procedures to assess bas-2 psychometric properties at both the scale and the item level for an englishspeaking sample.considering mi, the loadings of items 2, 8 and 9, and the intercepts of items 1 and 9 were shown to be non-invariant across males and females, when strict (χ2) comparisons were applied.considering the irt evaluation, and although all items presented with high discrimination capacity, this fluctuated according to the following descending sequence of items 4, 6, 2, 3, 9, 10, 8, 7, 1 and 5.similarly, items’ difficulty parameters differed across the different item thresholds.finally, although the scale as a whole seems to perform sufficiently and reliably when examining ba levels that lie ∓ 2 sd beyond the mean, it is not ideal for extremely low and high levels of ba that lie ∓ 3 sd beyond the mean.uni‑dimensionality and measurement invariance across genders in line with past studies, bas-2 demonstrated an appropriate unidimensional factorial structure with all items loading saliently and significantly on a single latent variable [20, 28, 36, 41].
number of words= 1722
[{'rouge-1': {'f': 0.24153787841290073, 'p': 0.6911453744493392,'r': 0.14634001082837034}, 'rouge-2': {'f': 0.1324827246173269, 'p': 0.24256637168141593,'r': 0.09112676056338029}, 'rouge-l': {'f': 0.28994018554687506, 'p': 0.5700000000000001,'r': 0.19441679626749614}}]
-----------------------------------------------------------------------------------------------------------------------------------
p354:
Extractive Summary:
aging is a period of life in which older people are exposed to potential threats such as chronic conditions, loneliness, isolation, lack of social support, and a decline in independence [2, 3].additionally, both physical and psychological (e.g., dementia) chronic diseases tend to become more common with age [3].it is one of the most debilitating neurological conditions that cause chronic and severe disabilities [8, 9].informal caregivers are defined as individuals who provide some type of unpaid, ongoing assistance with activities of daily living or instrumental activities of daily living for individuals with a chronic illness or disability [14].the burden of care is used to describe the side effects of care that are extremely problematic for the patients and their families [19].caregiver mental health can be even more at risk when caregivers perceive that the patient’s care needs exceed their caregiving capabilities [10].according to lazarus and folkman et al. [30], coping is a process that addresses how people respond and act both when experiencing stress and when the level of exposure to stress rises.coping strategies are the cognitive and behavioral efforts of individuals to interpret and overcome problems and challenges [31, 32].coping strategies have been conceptualized in a variety of ways in the literature, however more broadly, they have been considered to fall into two main categories: problem-focused and emotion-focused [37].problemfocused coping strategies aim to change the situation and take control of the source of stress.they involve evaluating the source of stress and actively considering and implementing potential solutions to reduce the aversive effects of the stressor [38, 39].on the other hand, emotion- based coping involves emotional response to stressors.emotion-focused coping strategies can also entail enlisting emotional support from others [40].eight ways of coping are identified based on the ways of coping (woc) questionnaire by the folkman and lazarus.these include: confrontive coping and planful problem-solving classified under problem-focused coping, and distancing, self-controlling, accepting responsibility, positive reappraisals, and escape-avoidance considered to be emotion-focused coping.the coping strategy of seeking social support functions as both problem and emotion-focused coping.confrontive coping refers to the aggressive effort to modify stressful situations while planful problem solving involves analytic approach to providing solutions to problems [41].distancing relates to denial, distracting, or detaching [42].for caregivers this can be considered an adaptive and useful coping strategy.it is often associated with personal growth and some religious dimensions [32].for caregivers, positive appraisal may involve considering stroke as unavoidable or inevitable.escape-avoidance is described as the thoughts and behavioral efforts to escape stressful situations or problems [47].this may involve actively avoiding the problem or withdrawing from others or the situation causing stress.seeking social support is a commonly used adaptive coping strategy which refers to sharing feelings and thoughts, or seeking care, resources, or emotional support from others [55, 56].methods this cross-sectional and correlational study was conducted on 110 caregivers of older patients with a history of stroke.to identify the caregivers, medical files of stroke patients hospitalized in the valiasr hospital in the six-month period before the start of the study were extracted and reviewed.caregivers who were caring for multiple patients were not considered for inclusion in this study.older patients who were included in the study are 60 years old and above, have been diagnosed with stroke, and have a family caregiver.data collection was carried out from december 2017 to may 2018.it measures 8 subscales consisting of confrontive coping (6 questions), distancing (6 questions), self-controlling (7 questions), seeking social support (6 questions), accepting responsibility (4 questions), escape-avoidance (8 questions), problem-solving (6 questions), and positive reappraisal (7 questions).higher scores in each subscale indicate greater use of that particular coping strategy [65].the lazarus coping strategies questionnaire was validated in iranian students [66].internal consistency of this questionnaire is high (cronbach’s alpha = 0.85) as confirmed by ramzi et al. [67].ethical considerations all stages of the study were based on the provisions of the declaration of helsinki of 1975.the study was approved by the ethics committee of qazvin university of medical sciences, qazvin, iran (ir.qums.rec.descriptive statistics were used to report the characteristics of the caregivers and the severity of burden of care.the statistical significance level was set at p < 0.05. results a total of 110 caregivers with mean age of 32.09 ± 8.80 years participated in this descriptive and cross-sectional study.caregivers were predominately women (n = 77, 70.0%), married (n = 75, 68.2%), unemployed (n = 76, 79.1%), and had children (n = 67, 60.9%).nearly 90% of the caregivers reported not having any chronic diseases.results of the independent t-test showed that male caregivers used the positive reappraisal strategy (t(108) = 2.76; p = 0.007) and accepting responsibility (t(108) = 2.26; p = 0.026) significantly more than female caregivers.the reason for these different results in the care burden of caregivers of stroke patients may be related to the degree of dependence of the patients for the daily living activities.a possible explanation for this might be that women believe that caregiving is their role and responsibility, as caregiving of children and other family members is often undertaken by women.the results showed that caregivers used more positive reappraisal and social support.the results of this study showed that male caregivers were more likely to use the strategies of positive reappraisal and accepting responsibility compared to female caregivers.in addition, suriyamoorthi et al. [36] showed that male caregivers of patients with bipolar disorder used self-distraction and substance use as coping strategies while females used religion and denial.these different results may be related to the type of care needed based on the differences in patients’ illnesses.the results of many previous studies are in line with this result.abbasi et al. [83] also showed that there was a direct and significant relationship between the use of emotion-focused coping skills and increasing care burden of the caregivers of cancer patients.third, convenience sampling was used, which reduces generalizations to all caregivers of older patients with a history of stroke.
number of words= 969
[{'rouge-1': {'f': 0.4120738792771936, 'p': 0.8028767123287672,'r': 0.27716360116166505}, 'rouge-2': {'f': 0.2440380307520199, 'p': 0.42738831615120276,'r': 0.17077519379844963}, 'rouge-l': {'f': 0.4070172535963357, 'p': 0.7046153846153846,'r': 0.2861572052401747}}]
-----------------------------------------------------------------------------------------------------------------------------------
p355:
Extractive Summary:
people with an intellectual disability experience higher rates of mental health problems compared to the general population, including higher rates of risk factors for suicide [2, 3].australian researchers have found that while disability workers note suicidal behaviour amongst their clients with intellectual disability, few disability services utilise suicide risk assessment tools [5].despite a higher prevalence of mental health problems, people with an intellectual disability seek professional help at lower rates than the general public, and even fewer people receive mental health support from services that are specifically for people with an intellectual disability [6, 7].for these reasons, it is important that people who work or live with people with an intellectual disability know how to recognise symptoms of mental health problems (to avoid diagnostic overshadowing and misattributing signs of mental health problems), have the skills to provide appropriate support, and encourage mental health help-seeking.these are the keys skills when providing mental health first aid.the aim of this study was to develop guidelines for how a family member, friend, concerned community member or disability worker without specialist mental health qualifications should give initial assistance (mental health first aid) to a person with an intellectual disability who may be experiencing mental health problems or in a mental health crisis.this particular delphi methodology has been used to develop a number of similar mental health first aid guidelines [16].it involved five steps: (1) literature search (2) survey development (3) recruitment of expert panels (4) data collection and analysis and (5) guidelines development.the methodology is briefly described below and a more detailed description of the delphi methodology can be found in the article by bond and colleagues [17].literature search a systematic search of grey and academic literature was conducted (including websites, books and academic articles) to collect information about how to offer assistance to someone with an intellectual disability who is experiencing a mental health problem or in a mental health crisis.the top 50 websites, books and journal articles for each search was reviewed (see fig. 1 for results).survey monkey was used to administer all surveys (see additional file 1).a panel of at least 23 has been found to yield stable results [18] and the study aimed to recruit at least 30 to each of the three expert panels (carer, mental health professional or advocate) to allow for attrition.data collection and analysis participants rated the aforementioned statements over three rounds using a 5-point likert scale: ‘essential’ to be included in the guidelines; ‘important’ to be included in the guidelines; ‘don’t know/depends’; ‘unimportant’; or, ‘should not be included’ in the guidelines.guidelines development the first author drafted the guidelines document using the endorsed items and the working group reviewed and finalised the guidelines document.seven (13.2%) participants were mhfa instructors and the sample had a varied amount of experience with intellectual disability (see table 2).item ratings a total of 202 items were rated over the three rounds to yield a total of 170 endorsed items and 74 rejected items (see additional file 2 for a list of the endorsed and rejected items).figure 2 presents the information about the total number of items rated, endorsed and rejected over the three rounds.participant qualitative responses in round 1 participants were given the opportunity to provide qualitative feedback that was used to modify or create items to be included in the round 2 survey.•difficulties the first aider may encounter including aggression, sexually inappropriate behaviour and managing personal boundaries.• what to do in a crisis situation.there is also specific information for disability workers and what to do if the person needs to be taken to hospital.the course may reduce the risk of symptoms being dismissed by workers and carers and diagnostic overshadowing [8] as specific changes and symptoms that indicate a mental health problem can be better recognised and communicated to health professionals.research indicates that self-determination in people with an intellectual disability leads to better quality of life, well-being and improved mental health [20, 21].these guidelines are to be used in conjunction with other mental health first aid guidelines, including the guidelines for depression [22], suicidal thoughts [23], psychosis [24] and drug and alcohol use problems [25, 26].however, in contrast to this limitation, by eliminating ‘consensus by discussion’, all voices equally influence the endorsement process.this is a major strength of this study.it is anticipated that the guidelines and resultant course will be a valuable resource to disability carers, disability workers and members of the public.
number of words= 732
[{'rouge-1': {'f': 0.48439382177819, 'p': 0.8435849056603775,'r': 0.33973684210526317}, 'rouge-2': {'f': 0.3001512143160399, 'p': 0.4904545454545455,'r': 0.21624505928853754}, 'rouge-l': {'f': 0.4133531899816197, 'p': 0.6715037593984963,'r': 0.2985714285714286}}]
-----------------------------------------------------------------------------------------------------------------------------------
p356:
Extractive Summary:
the development of the latter aspect occurs mostly in middle childhood, a relatively uninvestigated period when it comes to the relationship between attachment and ic.although there has been substantial research on predictors of ef in young children [for a meta-analysis, see 22], external factors relating to ef in older children have been explored far less.therefore, the core function of ec is the use of attentional processes to regulate one’s emotional arousal, motivation, and behaviour.attachment and cognitive development child-parent bonds constitute the most intense and enduring relationships across childhood, and thus they are likely to be one of the prime candidates to account for environmentally-driven individual differences in children’s ef [5].from attachment theory, several hypotheses concerning the effects of child-parent attachment on the child’s cognitive development can be derived [28].this is the social-network hypothesis [28], which predicts that secure children are more motivated to engage in frequent harmonious social interactions and to openly communicate with other people compared with insecure children.in a direct or indirect way (e.g. by contributing to the development of the child’s verbal ability), this stimulates cognitive development [28].moreover, parents of securely attached children tend to develop responsive and open communication with their offspring [29], and this was shown to engage the child in active construction of meaning, stimulate language development, and thus support the development of cognitive control [e.g. 30].given that in middle childhood parents still remain the child’s primary attachment figures [31] and considering the fact that it is a period of further significant growth in language skills (and a large part of children’s interactions with their parents is verbal), it seems reasonable to expect that language in this period is an important tool of ef development.interestingly, the role of child–father attachment in ef development is still mostly unexplored territory compared to child–mother attachment.on the other hand, a growing body of research suggests that attachment to the father is a significant factor in determining individual differences in specific domains of the child’s development [e.g. 32].according to some authors, fathers contribute to their children’s sense of security mostly by providing them with sensitive support during explorative and gently challenging play, whereas mothers contribute mainly by providing comfort when the child is in distress [35], and thus fathers are thought to play a more crucial role than mothers in the development of children’s exploration, autonomy and relationships with peers [33].some researchers claim that in the domain of cognitive activities, this specific paternal function results, for example, in higher levels of paternal than maternal demands in problem-solving tasks [36, 37, p. 992] and children’s academic performance [38].attachment as a predictor of executive function previous research on the links between children’s attachment and ef relates almost exclusively to early childhood.bernier and colleagues [5] found that more securely attached children at 15 and 24 months of age showed a higher level of cool ef at 3 years of age, and this relationship remained significant even when the child’s language development, family ses, and maternal sensitivity were controlled.the link between child–mother security and cool ic in pre-schoolers was also found by heikamp and colleagues [40].meanwhile, some authors consider that fathers might play an even more significant role than mothers in children’s linguistic development, as they act as “a kind of linguistic bridge between the child’s familial environment and the outside world” [37, p. 999].moreover, self-reported attachment security with fathers was found to be the unique predictor of school children’s language mastery [37].furthermore, secure children have relatively more positive relationships outside the family, which in turn provokes a richer language environment.on the other hand, di folco et al. [33] found that unlike attachment with the mother, narrative measures of attachment with the father were not related to the 6-yearold children’s verbal iq.as those authors note, a possible explanation may be that compared to mothers, fathers use less supportive but more directive and informative language when interacting with their children.according to zelazo et al. [51], the development of cognitive control results from age-related increases in the ability to reflect on the rules children formulate and represent during problem-solving (see also [52]).also, labelling per se facilitates redirecting attention to crucial aspects of the task and thus directs selfreflection, which in turn promotes increased flexibility in thinking and acting [53].in their longitudinal investigation, matte-gagné and bernier [56] found that children’s expressive language mediated the relation between maternal autonomy support assessed in infancy and hot ef at the age of 3, above and beyond the child’s previous ef and ses.however, no such mediating role of language was found for cool ef.of note, the mediating role of language in the relation between the children’s family environment and ef is also suggested by research focusing on aspects of that environment other than attachment bonds, such as parental behaviour and family ses.for example, noble et al. [57] found in a sample of first-grade children that controlling for language ability eliminated the significant association between family ses and children’s cognitive control, raising the possibility that language mediates the link between those variables.in a study by catale et al. [58], the relationship between parental educational status and children’s ef (aged 6–7 and 10–11) was partially mediated by children’s language skills.meanwhile, as some authors indicate [e.g. 35], the relationship between the child and the father has slightly different characteristics than that with the mother.more specifically, mothers are preferred as a safe haven for soothing the child’s distress, while fathers are preferred as the secure base for exploration.the consequence of these differences might be distinct patterns of relationships between attachment to mothers and fathers and the two aspects of children’s ef.the current study despite the growing interest in the role of child-parent attachment in ef development, little is known about how the attachment is related to individual differences in ef in middle childhood.we also aimed to check the potential mediating role of the child’s verbal ability in those links (objective 2).the development of the latter aspect occurs mostly in middle childhood—a relatively unknown period as far as the link between attachment and ef is concerned.we also predicted that those links would be mediated by the child’s verbal ability (hypothesis 2).families lived in one of the large metropolitan areas in poland, and their parents had higher (65% of mothers, 62% of fathers), secondary (30% of mothers, 32% of fathers), or basic education (5% of mothers, 6% of fathers).the one-factor structure of the polish version of the attachment security scale was confirmed [60], and cronbach’s alphas in the present study were 0.75 and 0.76 for security with mothers and fathers, respectively.in this task, participants made choices between less valuable rewards (0.20, 0.40, 0.80, 1, 2, 3, 4, and 5 zlotys), which they could receive immediately, and a more valuable reward (6 zlotys), which they could get later (after 2, 5, or 7 days).for each delay, the large constant reward was paired with each of the small rewards and presented as a choice, e.g. “would you prefer 1 zloty now, or 6 zlotys in 7 days?” each small immediate reward was paired twice with every delay for the large reward, resulting in a total of 48 choice trials.to encourage participants to take their choices seriously, they were informed that after the completion of the task, one of their answers would be randomly drawn, and the amount of money indicated in the answer would be converted into a real prize (stationery item, mini-game, or key ring) and delivered to them after the time that was indicated in their response had elapsed.depending on the participant’s response, on the next trial, the amount of money offered immediately was decreased to the subsequent amount in the set (if the participant previously chose the immediate reward) or increased (if the participant already chose the delayed reward).thus, the higher the value of this index, the better the child’s hot ic (range: − 16.58‒0).the task presents adequate test– retest reliability across a 1-week interval [70].the test measures verbal understanding, the ability to reproduce acquired knowledge, and the scope and adequacy of defining concepts.estimates of split-half reliability for the polish version of the vocabulary subtest are similar in magnitude to its original version and range from 0.85 to 0.91 [72].the indicator of verbal ability was the sum of points for answers (range 0‒40).the interrater reliability for the overall test score in the present study was high (kendall’s w = 0.97; χ2 (24) = 89.96; p = 0.002).procedure after approval by the local ethics committee and permission from the local schools’ principals, invitation letters were sent to parents.given also that ef is sensitive to neurological perturbations, such as, e.g. adhd and learning disabilities [e.g. 68], children with those difficulties were excluded.children were recruited with parental written informed consent; 1,000 families were asked, and 165 (17%) consented to their children’s participation.children were assessed individually by a female experimenter in a quiet room in the school they attended for two sessions about one week apart.in the first session, the following tasks were administered in a fixed1 order: go/no-go, and the attachment security scale (child– mother version).in the second session the tasks were following: delay discounting task, vocabulary test, and attachment security scale (child–father version).the short demographic questionnaire was completed by the mothers of the children.trials with response times shorter than 120 ms on the nth trial, combined with no reaction on the previous (n − 1th) trial ( rn−1 = no), were treated as missing and not taken into account in the analysis (n = 24).children who did not respond to more than 25% of the go trials (errors of omission) were also excluded from the analysis on account of having too few data points (n = 1).next, we analysed distributions of all our variables and screened them for extreme scores.all statistical analyses were performed using spss version 25.we applied model 4 of the process macro [79] with 20,000 resamples drawn with replacements from the original sample (n = 160) to derive the confidence interval (ci) for the unstandardised regression coefficient of the indirect effect.the indirect effect through verbal ability was considered significant when the ci did not include 0.results preliminary analyses first, descriptive statistics and pearson zero-order and partial correlations (controlling for age) for key study variables were calculated (see table 1).we then examined the associations between both aspects of the child’s ic and demographic variables (child’s age, sex, and family ses), as well as the child’s performance on the verbal ability task.as displayed in table 1 (below the diagonal), zeroorder correlations revealed that cool and hot ic were positively and weakly related to each other.furthermore, children’s verbal ability was positively linked to both hot and cool ic.sex (dummy coded) was related to cool ic only, with girls superior to boys.perceptions of attachment security to both parents were positively and moderately related.objective 1: links between attachment security with parents and ic cool ic table 1 presents pearson’s zero-order and partial correlations (controlling for age) between the child-parent attachment and both aspects of the ic.regarding relations between attachment security with mothers and cool ic, we found no significant correlation between this variable and the child’s cool ic.regarding attachment to fathers, we found a weak association between attachment security and cool ic, which nevertheless did not approach significance, r(158) = 0.14, p = 0.07.however, since there was a significant correlation between cool ic, age, and sex that could suppress the correlation between ic and father–child attachment, we checked whether the latter variable would significantly predict cool ic when taking into account both age and sex.a similar analysis for mother–child attachment has shown that this variable remained a non-significant predictor of cool ic when both age and sex were taken into account, β = 0.04, p = 0.59; f(3, 156) = 7.15, p < 0.001.objective 2: mediating role of verbal ability in the links between attachment security with parents and ic in the next set of analyses, we tested whether children’s verbal ability served as a mediator in the links between attachment security and both aspects of ic (the hot and the cool).first, we examined associations between our potential predictors (i.e. attachment variables) and the potential mediator (i.e. verbal ability) and between the mediator and a given aspect of ic as an outcome [77].recall that correlation analysis indicated that our attachment variables were associated with neither hot nor cool ic.in both cases, full mediation occurred through children’s verbal ability.however, those effects were also not significant (for cool ic 95% ci = [− 0.001; 003]; for hot ic 95% ci = [− 0.107; 0.009]), indicating that the intervening effects were independent for attachment with the mother and the father as predictors.discussion the purpose of this study was to examine the concurrent links between children’s attachment security with parents and two aspects of ic, along with the intervening role of children’s verbal ability in those links.it was expected that higher attachment security with parents would be related to better hot and cool ic (hypothesis 1).first, after accounting for the child’s age and sex, we found that there was a direct relationship between perceived attachment security with the father and cool ic.however, the design of our study was correlational and cross-sectional; thus, longitudinal studies are needed to provide more reliable evidence suggesting a directional, and potentially causal, link between the attachment security and the child’s verbal ability and hot ic.only such an approach would support the notion that secure attachment is associated with children’s verbal ability, thereby fostering their ability to inhibit impulsive responses [28, 29].thus, the links between attachment and verbal ability may be bidirectional in their nature.our findings also seem to be in line with the well-established theoretical and empirical links between children’s verbal ability and cognitive control [e.g. 53] and especially with other research evidence that children’s receptive vocabulary and verbal self-instruction were related to the strategies used for waiting in delayed gratification measures [e.g. 80].regarding the fact that the maturation of hot ic lags behind the development of cool ic and that it develops rapidly in the transition to adolescence [1], one should consider the possibility that communication with parents might be more significant for the development of hot than cool ic in middle childhood.fathers are thought to play a more crucial role than mothers in the development of children’s exploration, autonomy, and relationships with peers, and some researchers claim that this specific paternal function results in higher levels of paternal than maternal demands in problem-solving tasks.this kind of activity, in turn, might create for the child an opportunity to exercise their skills in planning and self-monitoring their actions and flexibly modifying them [28, 82].hence, another possibility is that child–father interactions in middle childhood, especially those in the context of explorative and gently challenging play [35], might be particularly significant for shaping the child’s psychobiological mechanisms necessary to effectively control their behaviour and successfully solve the “cool” problems [83].lubiewska [84] pointed out that due to the fast cultural changes in poland in the last decades, there exist micro-cultural discrepancies between relatedness-oriented mothers and their autonomy-oriented children.therefore, as polish fathers might be less overprotective and collectivistic in their socialisation goals than mothers, they might become primary attachment figures in the transition to adolescence.high maternal sensitivity has been proven to foster secure attachment in children, while lower maternal sensitivity has been linked to attachment insecurity [e.g. 63].maternal sensitivity is also associated with higher levels of language development and academic achievement [89, 90].this pattern of correlation precludes ses from explaining the relationship between attachment security and verbal abilities, as well as between verbal abilities and both cool and hot ic measures.another possibility is that ses might moderate the links between attachment, verbal ability, and ic, with stronger links between those variables among children from low ses families.in those studies the magnitude of the significant links between attachment and ef was 0.08–0.31.as far as the links between verbal ability and both age and hot ic are concerned, they are similar to those found, for example, by carlson and wang [7], with a moderate magnitude of relation between age and verbal ability (r = 0.54 vs r = 0.53 in our study), and quite comparable links between verbal ability and hot ic (r = 0.33 vs r = 0.27 in our study).lab-based protocols, such as semi-projective story stems, observational codes, or attachment interviews may be more successful in assessing unconscious attachment processes.therefore, systematic longitudinal research is needed to confirm the mediational links observed in our study and to clarify whether (and if so, how) those patterns of association change across different ages and which factors account for the potential change.in replication studies, more participants should be involved in each age group.hence, those results cannot be generalised to lower ses families.hence, in future studies not only ses but also parental sensitivity should be controlled.this approach would help us to answer whether the links observed in our study are ef-component-specific or more general.it adds to the existing body of research in several regards.verbal ability mediates the links between attachment security with parents and hot, but not cool, ic.besides, attachment with both parents seems to play a significant role in the child’s verbal functioning.
number of words= 2837
[{'rouge-1': {'f': 0.26259563705930455, 'p': 0.8923684210526315,'r': 0.153948959032908}, 'rouge-2': {'f': 0.18985386671221333, 'p': 0.5221452145214521,'r': 0.11601948270070542}, 'rouge-l': {'f': 0.28877803249277384, 'p': 0.7366666666666666,'r': 0.1795890410958904}}]
-----------------------------------------------------------------------------------------------------------------------------------
p357:
Extractive Summary:
all continuous variables were grand centered, and dichotomous variables were dummy coded (0 versus 1).in the first step, we entered loneliness, thus yielding its crude or unadjusted relationship with anxiety or depressive symptoms.in the second and third step, the resilience variables and the soc measure was added, respectively.in the final block, their interaction terms were additionally included (loneliness × rsa or loneliness × soc) along with a final adjustment by including the covariates (e.g., age, gender, and education).the performance of these models was gauged with the adjusted r-square index (range 0–1) indicating the degree of variance explained by the model.results descriptive statistics the score range, means (or proportions), standard deviations of all variables, as well as the reliability coefficients of the measurement scales, are presented in table 1.the interrelationships between these variables are given as pearson correlation coefficients.the psychometric properties of the rsa, the hscl-25, and soc-13 were adequate.the reliability coefficients for the subscales of the rsa were acceptable as the cronbach’s alphas varied in the range between .81–.87 for the subscales planned future, family cohesion, social resources, personal competence, and social competence (in falling order).the subscale “structured style” was however in the sub-optimal range (α = .66).loneliness was in general strongly associated with higher levels of anxiety and depressive symptoms.moreover, loneliness correlated significantly and negatively with most facets of the rsa, except for the subscales of family cohesion and structured style.the strongest correlation was the negative one between loneliness and soc, thus indicating that people feeling lonely also experience their life as less meaningful, comprehensible, and manageable.moreover, the rsa and the soc were strongly positively correlated, as has been previously reported [19].confirmatory factor analysis the fit of the six-factor rsa measurement model was examined in a confirmatory factor analysis, which confirmed adequate fit in terms of a low degree of model misspecification (rmsea = .052, ci.95 .048–.056; srmr = .066), whereas the relative fit was mediocre (cfi = .898, tli = .888).switching this item to the family cohesion factor, which is reasonable given the overlap in semantic meaning, improved absolute fit (rmsea = .049, ci.95 .045–.054, srmr = .063) and relative fit (cfi = .909, tli = .900).the standardized factor loadings are given in table 2.the relationship between loneliness and mental health, and the contributing role of rsa and soc as protective factors (or moderators) loneliness was regressed upon depression (table 3) and anxiety (table 4) and stratified to retain genderspecific effects.as a single variable (crude effect), loneliness had the highest association with depressive symptoms in men (r2 = 41%), thereafter depressive symptoms in women (r2 = 38%), and then anxiety in both men and women (r2 = 20 and 21%, respectively).adding resilience to the equation in the second block explained substantially more of the variance in mental health, thus validating the danish version of the rsa as a significant contributor in explaining mental health.adding soc in the third block, explained a substantial extra amount of the variance in the hscl, as expected.in the final block, rsa and soc were added as moderators of the relationship between loneliness and hscl in order to examine if these two respective was associated with an extra layer of protection in addition to their compensatory main effects.the rsa contributed significantly as moderators of depressive symptoms in men (notably, the subscales of perception of self and family coherence) and anxiety symptoms in men (rsa perception of self).similar protective effects against depressive symptoms were not observed in women, whereas soc was associated with a protective role against depressive symptoms in women.these findings indicated that both rsa and soc showed compensatory (main) and protective (moderator) effects.the latter effects were more pronounced for the rsa measure.discussion the current study showed that loneliness was related to both anxiety and depression and that all facets of resiliency were negatively related to loneliness, where higher loneliness was associated with lower resiliency, indicating that young adults who show a high degree of resilience also tended to feel less lonely.the relation between levels of loneliness and resilience is particularly interesting as it has not previously been reported, but it also supports the construct validity of the rsa.a possible explanation for the relation between levels of resilience and levels of loneliness is that resilience represents the presence of both intra- and interpersonal resources that improve the adaptation to a more lonely existence.similarly, situational characteristics, such as having few social resources, shallow or non-existing interpersonal relationships, are hypothesized as a predisposing factor for developing loneliness [59].thus, higher scores on loneliness, indicating the absence of social resources, would thus be expected to be associated with lower scores related to social resources such as resilience.social resources are thought to be essential in mental health as researchers highlight that healthy adaptation is a process [8, 15, 60].more specifically, it can be defined as a transactional process where resilience is developed through the individuals’ dynamic interaction with their environment.it may also be described as the individual’s ability to navigate between available resources [61].based on the narrow definition and measurement of loneliness in the present study, we cannot ascertain to what extent lonely people have or use social resources despite observing a strong negative correlation between loneliness and social resources.according to the model of loneliness on cognition [7], feelings of loneliness are maintained through the individual’s interaction with his or her social environment.feelings of loneliness change cognitive expectations that may reinforce maladaptive behavior, e.g., hesitance, submissiveness, or withdrawal related to perceptions of the social sphere as threatening that also shapes memories of social interactions as more negative as compared to non-lonely people [7].loneliness as a subjective experience is typically distinguished from e.g. social isolation, which describes social circumstances more objectively.future studies into the association between loneliness and resiliency could therefore benefit from a multi-dimensional approach to the study of loneliness encompassing both subjective and objective dimensions [62].furthermore, this study illustrated, that loneliness was strongly associated with worse mental health, and in particular, depression.this association between loneliness and other mental health problems adds to an understanding of loneliness as a complex phenomenon [6] associated with a range of challenges.this finding has also been replicated elsewhere in the literature, where loneliness seems to correlate with other mental health problems in reciprocal relationships [3, 63, 64].however, this study showed that resilience, specifically the facets of perception of self and family coherence, could explain a substantial amount of the variance associated with symptoms of depression in relation to loneliness, and the facet of perception of self was associated with anxiety in relation to loneliness.the measure of rsa and the soc are positively correlated which indicates that they both measure individual resources.the design of the study makes it impossible to detect any causal relation between the two or a causal relation between loneliness and soc.the study can only report on a negative correlation between soc and ucla, which we can discuss from a theoretical perspective.the negative correlation is indicating that people feeling lonely also experience their life as less meaningful, comprehensible, and manageable.the social nature of humans makes relationships and the sense of belongingness a core component of how creates meaning in our lives.a key channel for humans to make lives understandable and comprehensible is to discuss, engage, and interact with other humans.in his original work from 1979, antonovsky [65] argued that life experiences shape the sense of coherence and that soc is a stable entity around the age of 30.
number of words= 1227
[{'rouge-1': {'f': 0.31811928472085854, 'p': 0.8592156862745097,'r': 0.19519440124416798}, 'rouge-2': {'f': 0.24950405503161574, 'p': 0.6167980295566502,'r': 0.15638132295719845}, 'rouge-l': {'f': 0.34791584484765126, 'p': 0.7682758620689656,'r': 0.22487571701720843}}]
-----------------------------------------------------------------------------------------------------------------------------------
p358:
Extractive Summary:
background researchers and clinicians agree that the course of alcohol dependence is better characterized as a widely varying process with varying consequences than as a uniform pattern of steadily declining health and functioning.differences in the course of alcohol problem use and dependence have been identified and characterized via work on subtypes, etiology, treatment, and trajectories of alcohol use disorders [1–6].twin research had indicated that differences in remission phenotypes are largely due to environmental effects [12].when considering more psychological risk factors, relapse following treatment has been found to be predicted by experiencing negative emotional and interpersonal problems [18] and reduced participation in outpatient treatment or alcoholics anonymous (aa) meetings [16].a key limitation of association studies is that the predictors of any given milestone will necessarily include the predictors of all preceding milestones.that is, the predictors of ad will also include the predictors of initiation of drinking since initiation always precedes ad.survival analysis, which can examine the time it takes to experience a given event and what predicts a quicker or slower progression, is one such model.for example, in a study of young adults, sartor et al. [29] used conditional survival analysis to examine risk factors associated with years to first drink and years from first drink to ad.this study was the first to differentiate between risk factors in the timing of their impact on progression.results indicated that both risk and protective factors influenced initiation of alcohol use but that they were differentiated in their influence on the latter two developmental stages with protective factors slowing transition to at risk drinking and largely risk factors accelerating transition to ad.unfortunately, few association studies have included later stages of remission and relapse in midlife in the course of alcohol use disorders.the current study addressed this need by examining onset of, remission from, and relapse to alcohol problem use through the 40’s and 50’s.specifically, the current study examined predictors associated with the duration of different stages in the progression of alcohol use in a sample of male veteran twins with ad history.after examining risk factors for age of first drink, the three successive transitions of onset, remission, and relapse were examined for both heavy drinking and ad.thus, seven survival analysis models examined significant predictors of the time function, which permitted identification of risk and protective factors that accelerated or delayed the rates of transition through each distinct developmental period.the current study examined psychiatric variables, family history of ad, personality traits, alcohol expectancies, and drinking motives as possible predictors of transitions.we had the following general hypotheses, which are exploratory in nature but are based on patterns seen in past research on predictors of different aspects of alcohol use: 1 the same risk factors associated with initiation of drinking will also be associated with progression to heavy drinking and progression to ad (the three “onset” transitions).3 factors associated with relapse will be similar to those associated with onset and progression of heavy drinking and ad.methods participants the participants in this study were twin veterans who were members of the vietnam era twin (vet) registry, although the twins were treated as individuals in the study.the vet registry has been described in detail elsewhere [31–34], so only salient aspects will be described here.this national military twin registry comprises male twin pairs born between 1939 and 1957 in which both members served in the united states military between may 1965 and august 1975.in 1987, registry members completed a mailed questionnaire about their military experience, general health, marital and family history, etc.[31, 33] in 1992, a psychiatric interview was administered by telephone to twins who completed the questionnaire, assessing a range of psychiatric disorders, including alcohol and drug use disorders [34].information from the 1987 and 1992 assessments was used to design two offspring-of-twins studies that provided the current sample: one that began in 2000 focused on alcohol dependence history (children of alcoholics study, coa [35]) and another that began in 2003 focused on drug dependence history (twins as parents study, tap [36, 37]).equivalent procedures and identical assessments were used in both studies that permitted the planned combining of data across the two samples.this high-risk sampling plan identified 1062 vet registry pairs, where cases with comorbid drug and alcohol histories represented the higher end of the substance dependence severity continuum.both samples also included non-dependent cases with and without substance abuse histories to represent the lower end of the severity continuum.although selection was based on pairs, each twin was individually contacted for participation, and twins were not excluded if a co-twin declined to participate.concerted attempts were made to locate and enroll every individual selected for the studies.in the coa, 83% (n = 1295) of the individual twins were interviewed; while the comparable rate for tap was 81% (n = 725).this resulted in 1774 individual veteran twin cases and 1769 cases met minimum data requirements for the current study.information provided from previous interviews with the vet registry twins was also combined with the coa/tap collected data and used in data analyses.thus, the predictor variables used in the analysis were collected at various timepoints and did not necessarily occur prior to the age of event; although we refer to predictors for increased or decreased time-toevent, it is important to note that the risk and protective factors are associated with, rather than causally related to, the speed of transition.assessments procedures for obtaining verbal informed consent were approved by institutional review boards.interviewers, who were blind to the twin’s substance use history, used a computer-assisted telephone interview system with standardized interview questions and probes.this assessment yields retrospective information about different phases of drinking across the life course, defined by the interviewee as a time when drinking frequency or quantity changed.to get information about their drinking life, the participants report starting and ending ages for each time period of different drinking pattern as well as other information about drinking during each phase including quantity/frequency of drinking and alcohol abuse and dependence symptomatology.these data provided the information we needed for the transitions of interest: age of onset for alcohol use and dependence diagnosis and ages for the changes in the patterns of drinking that indicate remission and relapse.age of first ad was defined as the age at which dsm-iv criteria for ad were first met (i.e., yes to at least three of the dsm-iv criteria occurring within the same 12 months).the same rules were followed for heavy drinking age patterns, where heavy drinking was defined as a quantityfrequency index (qfi) score greater than or equal to 60 (quantity of drinks per occasion times the number of times drinking per month), which for men would be 14 drinks per week which is the cut-off for niaaa’s [42] definition of “at risk/heavy” drinking.nicotine dependence was used as a risk factor.an externalizing risk variable, to index outward conflict, was created by the average of the z-scores for the symptom counts for antisocial personality disorder (aspd) and other drug dependence (dd).an internalizing risk variable, to index internal distress, was created by the average of the z-scores for the symptoms counts for panic disorder, generalized anxiety disorder (gad), posttraumatic stress disorder (ptsd), and major depression disorder (mdd).although other mood and anxiety disorders also fall under the internalizing spectrum [45], other diagnoses were either not assessed or were uncommon (e.g. dysthymia).sample sizes for these individual variables are smaller than for the psychiatric variables as the mailed questionnaire was only returned by 77% (n = 1001 of 1295) of the coa sample and was not completed by the tap sample.the first transition was the number of years to the first full drink.then onset, remission, and relapse transitions were examined for heavy drinking.then, we repeated examination of the same onset, remission, and relapse periods but with respect to ad diagnosis; that is, fifth was the number of years from the first drink to the start of the first phase the veteran was given an ad diagnosis; sixth, years from ad to the start of remission; and seventh, years in remission until relapse to next phase ad diagnosis was met.concerning the progression of alcohol use, 98.2% of the entire sample (1738 of 1769 cases) endorsed having a first full drink.of those who initiated drinking, 74.5% (1295) had a period of heavy drinking, 72.0% (932) of heavy drinkers remitted, and 16.8% (157) of remitters relapsed back into heavy drinking.hypothesis 1 examined the consistency of factors predicting the three transitions of onset of alcohol use problems: years to first drink and subsequent years to either heavy drinking or ad.four risk factors were identified as being associated with accelerating all three onset variables (hazard ratios greater than 1.0): nicotine dependence (ors = 1.19–1.40), externalizing disorders (ors = 1.14–1.41), cotwin drinking history (ors = 1.28– 2.22), and the alcohol expectancy of risk and aggression (ors = 1.08–1.14).for years from first drink to first ad, the specific risks of internalizing disorders (or = 1.38), father drinking history (or = 1.30), and coping drinking motives (or = 1.05) were related to quicker transition.surprisingly, the major onset risk factors of externalizing and cotwin drinking history predicted relapse of ad and predicted relapse of heaving drinking only at a p < .05 level.hypothesis 1 was supported in that there was consistency across predictors of the three onset transitions (years to first drink, years from initiation to first heavy drinking, and years from initiation to first ad) with four risk factors associated with acceleration to all three onset transitions and one consistent across the two more problematic drinking transitions.the first two predictors were psychiatric variables that are comorbid with alcohol use: nicotine dependence and externalizing disorders.risky behavior is not always viewed as a negative side effect of drinking, and it appears that those who favor risk and aggression will more quickly progress to alcohol use onset.enhancement motives have been shown to predict increased drinking quantity and drinking misuse from adolescence to middle-adulthood [63].given that the average age of first transitions to heavy drinking and ad is in adolescence and early adulthood, it is not surprising that positive feelings of enhancement and enjoyment predict accelerated transition to greater alcohol use.patrick and schulenburg [62] used growth models to show that adolescent binge drinking was related to avoiding boredom and that early adult binge drinking was related to getting away from one’s problems, findings that suggest a tension reduction expectation specific to heavy drinking.further research could investigate the effect of personality on various aspects of drinking onset to examine potential differential prediction.thus, it was other aspects of our measured domains that had important connections to a quicker decrease in alcohol use.first, in terms of clinical variables, internalizing disorders speeded progress toward remission of heavy drinking.some researchers have found links between depressive disorder and more excessive drinking trajectories as opposed to recovering drinkers [3], while others have found no systematic link between episodes of emotional dysregulation and episodes of alcohol problem use, as to whether the emotional regulation issues preceded, co-occurred, or followed the alcohol use [66] and no link between having (or not having) depression and anxiety disorders and remission from alcohol dependence [15].the other consistent onset risk factors of cotwin nicotine dependence, risk and aggression expectancies, and enhancement motives were also not significant for relapse.this finding underscores the greater risk of “coping by drinking” for those who have met criteria for ad and who are probably later in their alcohol use career.other research has also found that drinking to avoid problems was related to heavier drinking in later early adulthood (age 22–30) [62].the current data suggests a close relationship between alcohol dependence and alcohol coping.
number of words= 1915
[{'rouge-1': {'f': 0.2936494477891816, 'p': 0.9133734939759037,'r': 0.17494752623688156}, 'rouge-2': {'f': 0.189516814843448, 'p': 0.46516129032258063,'r': 0.11900000000000001}, 'rouge-l': {'f': 0.29744642964326445, 'p': 0.7366666666666666,'r': 0.18634349030470915}}]
-----------------------------------------------------------------------------------------------------------------------------------
p359:
Extractive Summary:
it serves to combine the functions of portable media players, low-end compact digital cameras, pocket video cameras, and gps navigation units [3].furthermore, smartphones are being used for more than just a phone but rather a device that provides multiple functions including surfing the internet, email, navigation, social networking, and games [4].smartphones are gaining increasing importance in health care and researchers and developers are enticed with their applications related to health [5].malaysian communications and multimedia commission (mcmc) has reported that in malaysia, 24.5 million users have access to the internet in 2016 [10].over the past several years, there has been an increasing amount of studies that explored smartphone addiction [11–14].previous studies have shown links between smartphone addiction and depression [23–25], smartphone addiction and anxiety [26–29], smartphone addiction and stress [11, 30], smartphone addiction and neuroticism [27, 31].however, there are not many studies done on smartphone addiction and its effects on medical undergraduates.neuroticism is present among medical students [46].medical students with neurotic tendencies behave negatively to academic stress, and this becomes a contributing factor to low academic performance [46].2.there is a significant correlation between smartphone addiction and depression among usm medical students.3.4.an informed consent to participate and publication was obtained from all participants.bias was not explored in this study.this inventory was created explicitly to identify personal traits of malaysian candidates who seek to apply to the medical course in usm [48].we selected the 3 items that represent neuroticism to assess the neuroticism personality trait.the coefficient for cronbach alpha correlation obtained is 0.91 for smartphone addiction short version [3].the strength of sas-sv is that it can be used to discern a potentially high-risk group for smartphone addiction, both in the educational field and community [3].data collection data collection was performed via a self-guided questionnaire.individuals were screened for one inclusion criteria and one exclusion criteria.participants who submitted incomplete responses were excluded from this study.ethical consideration ethical clearance was obtained from the human research ethics committee of usm (jepem) with study protocol code (usm/jepem/18070352).signed consents were taken from medical students.each medical student was given an id for tracing and profiling purposes.they were informed that the results of this study will not affect their academic results in any way.the questionnaires were distributed to all medical students after lecture sessions.spearman correlation and simple linear regression tests were performed to examine the relationships of smartphone addiction with psychological distress and neuroticism.in the regression analysis, depression, anxiety, stress and neuroticism are independent variables and smartphone addiction is the dependent variable.the proportion of students in each year of study was more or less similar or equal in numbers.in this study, the prevalence of smartphone addiction found was 40.6%.correlation of smartphone addiction, psychological distress and neuroticism the correlation analysis for smartphone addiction with psychological health and neuroticism is shown in table 2.however, there was a poor positive correlation between smartphone addiction and neuroticism (r = 0.173, p-value < 0.001).linear regression of smartphone addiction, psychological distress and neuroticism the regression analysis for smartphone addiction with psychological health and neuroticism is shown in table 3.the simple linear regression study showed a significant increase in depression, anxiety, stress and neuroticism levels upon one unit increase in smartphone addiction (bdepression = 0.101, p-value < 0.001; banxiety = 0.120, p-value < 0.001; bstress = 0.132, p-value < 0.001; bneuroticism = 0.404, p-value < 0.05).other studies found a higher prevalence of smartphone addiction in females compared to males [3, 12, 22, 54, 55].interestingly, previous studies [24, 25] did not report that gender is associated with smartphone addiction.the percentage of smartphone consumers gradually rose from 68.7% in 2016 to 75.9% in 2017 [10].another observation is that medical students are using smartphones for social media messaging services such as whatsapp and wechat for communication purposes as well as for their studies, hence smartphones are becoming a vital tool in personal and professional life [56].the higher prevalence of smartphone addiction in male medical students may be due to male medical students using their smartphone more for their entertainment such as online games while females use their smartphones for social interactions [58–60].likewise, previous studies among adults reported a strong positive correlation between smartphone addiction and depression [28].other findings further support the fact that high levels of smartphone addiction were correlated with depression [29].in the malaysian context, a study among university students showed students who had high scores of smartphone addiction reported high scores of depression [61] that suggests a relationship between smartphone addiction and depression.however, other studies have found no relationship between smartphone addiction and depression [62].these facts consistently suggested a positive correlation between smartphone addiction and depression.the regression analysis showed the increase of smartphone addiction scores leads to the increase of depression scores, indicating it is a relationship factor.another study also supports this finding, in which it reported the severity of smartphone use predicted depression [12].these facts suggested that smartphone uses among university students should be considered as high-risk behaviour that negatively affects their psychological health.individuals with mood disorders are more prone to become a smartphone addict [64].one study stated that individuals with lower levels of selfperceived health conditions and emotions tended to display an excessive use of smartphones [65].previous studies reported that stress leads to smartphone use [56, 69], while another study proposes that smartphone use may cause stress [70].in our regression analysis, increased smartphone addiction scores are a significant relationship factor in increased stress scores.conversely, in a sample of taiwanese university students reported a positive predictive effect of family and emotional stresses on smartphone addiction [11].smartphone addiction is influenced by self-control [72].relationship between smartphone addiction and neuroticism in this study, the results indicate that there is a poor positive correlation between (r = 0.173) smartphone addiction and neuroticism, as in research hypothesis 5.the results show that there is a high prevalence of smartphone addiction among medical students.this might affect the performance of the medical school as a whole in terms of academic results.the high prevalence of smartphone addiction in this study shows that they are at risk to have problems.we also have to take into account the high prevalence of male medical students.thus, there is a need to create and implement programs to promote healthy smartphone usage to minimize the impact of smartphone addiction on psychological health.
number of words= 1029
[{'rouge-1': {'f': 0.4657426446924914, 'p': 0.8753097345132743,'r': 0.31728260869565217}, 'rouge-2': {'f': 0.3137792087535902, 'p': 0.555207100591716,'r': 0.2186854034451496}, 'rouge-l': {'f': 0.42277110292693065, 'p': 0.7411409395973154,'r': 0.29573363431151245}}]
-----------------------------------------------------------------------------------------------------------------------------------
p360:
Extractive Summary:
background studies on firearm laws and firearm-related violence have focused on the association between the rates of firearm-related violence within the state and the aggregate number [1] or categories of state-level firearm laws [2–6].a recent systematic review found that stronger state-level firearm laws are associated with reductions in firearm-related homicide rates; however, it also found inconclusive and conflicting results for many of the different categories of laws [7].prior studies have relied on measuring the proportions of firearms originating from source states, or the relative differences in statelevel firearm laws between two states and the movement of firearms between them.we hypothesize that states with fewer firearm laws serve as source states of crime-related firearms recovered in other states, and that states with more firearm laws serve as destination states of crime-related firearms from other states.methods study design and data source this is an analysis of state-level data from 2010 to 2017.the study was determined to be exempt from institutional review board regulation at the university of california, los angeles because it uses de-identified, publicly available, state-level data.using these data, we defined a network tie between two states when there was movement of 100 or more firearms in a given year from the state where the firearm is purchased (“source state”) to the state where the firearm is recovered (“destination state”).this variable was scaled to have a mean of 0 and a standard deviation of 1.covariates we used the following state-level data to adjust for characteristics previously associated with firearm-related violence: poverty rate [21–23], a validated proxy measure for state-level firearm ownership [24], and countyweighted state density as a proxy for the average urbanicity of the state (the sum across all counties in the state of [county population / county land area] *these variables were scaled to have a mean of 0 and a standard deviation of 1.a poisson model assumes that the conditional mean is equal to the conditional variance.in the context of this analysis, firearm movement, say, from alaska to florida is likely to be an excess zero because of the distance involved, while movement from georgia to florida is likely to be driven by policy.first, we fitted similar zero-inflated negative binomial regression models using the cut-offs of 50, 60, 70, 80, and 90 or more firearms.the networks were constructed and analyzed using igraph version 1.2.4.1 and sna version 2.4 packages, and linear regression with prais-winsten estimator was conducted using prais version 1.1.1 package for r. zeroinflated negative-binomial regression with robust confidence intervals and negative binomial regression analyses were conducted using stata version 16.this ranged from california in 2017, which was the destination for 100 or more crime-related firearms from 22 states, to 181 observations (45%) over eight years in which a state was the destination of 100 or more crime-related firearms from no states that year.the network of interstate firearm movement is depicted in fig. 2, which shows the average movement of firearms across states over 8 years, when the average is 100 or more firearms.states that do not have an average of 100 or more firearms move across its borders are excluded from this figure, as are new hampshire and massachusetts, which are connected to each other but not to other states.the exceptions are the movement from texas to california (523 firearms), virginia to new york (451 firearms), georgia to new york (391 firearms), and florida to new york (360 firearms).in addition, there is movement across long distances, going north from georgia to new jersey (181 firearms) and texas to new york (106 firearms), and west from louisiana to california (123 firearms) and ohio to california (120 firearms).table 2 presents the results of the multivariable zeroinflated negative binomial analysis estimating the relationship between firearm laws in a state and the number of states for which it serves as the source of 100 or more crime-related firearms.each additional standard deviation in the number of firearm laws was associated with 42% fewer states to which a given state is the source of 100 or more crime-related firearms (incidence rate ratio [irr] = 0.58; b = -0.55, 95% confidence interval [ci] -0.71, -0.38, p < 0.001), adjusting for covariates.each additional standard deviation in the firearm ownership was associated with 37% fewer states to which it is the source of 100 or more crime-related firearms (irr = 0.63; b = -0.46, 95% ci -0.58, -0.35, p < 0.001).multivariable zero-inflated negative-binomial analysis estimating the relationship between firearm laws and the number of states to which a state is the destination of 100 or more crime-related firearms is shown in table 3.each additional standard deviation in the number of firearm laws in a state is associated with 73% more states for which it is the destination of 100 or more crimerelated firearms (irr = 1.73; b = 0.55, 95% ci 0.44, 0.65, p < 0.001), adjusting for covariates.for analyses estimating the relationship between firearm laws and the number of states for which a state is the destination of crime-related firearms, the association remained robust across all cut offs.further, we found that more firearm laws was associated with a state being the destination of 100 or more crimerelated firearms from more states.our findings may lead to the question of whether the protective effect of strict firearm laws on fiream violence prevention is negated by interstate firearm movement.from 2010 to 2017, the states identified in our analysis as sources of 100 or more crime-related firearms had an average, age-adjusted, firearm-related homicide rate of 4.21 (sd 2.20) per 100,000 population, while states that were destinations of 100 or more crime-related firearms had an average, ageadjusted, firearm-related homicide rate of 4.50 (sd 2.15) per 100,000 population [32].recent studies have begun to examine the independent effects of state-level firearm laws and firearm ownership on firearm-related outcomes.our gun ownership variable was a proxy measure and we did not have detailed data on actual gun ownership at the state level.the firearm trace data used in our analysis only included firearms used in crimes that were recovered by law enforcement, not all firearms or all firearms used in crimes.these results suggest that firearms travel in complicated webs among states, and therefore, state-level firearm policies may not sufficiently address firearm availability within states.
number of words= 1030
[{'rouge-1': {'f': 0.4393935710892102, 'p': 0.8947422680412371,'r': 0.29119815668202764}, 'rouge-2': {'f': 0.2824340911055407, 'p': 0.5286206896551724,'r': 0.19269372693726938}, 'rouge-l': {'f': 0.4045346558603395, 'p': 0.6999212598425197,'r': 0.28447721179624663}}]
-----------------------------------------------------------------------------------------------------------------------------------
p361:
Extractive Summary:
introduction healthy practices established at early ages, such as following a well-balanced diet with adequate amounts of fruit and vegetables, have the potential to last a lifetime [1, 2].these eating habits reduce the prevalence of dietrelated diseases such as type 2 diabetes and cardiovascular disease later in life [3, 4].therefore, the compulsory primary school years may provide a useful starting point for the promotion of healthy eating among children [5, 6].to date, both federal and states governments have made an effort to promote healthy eating through primary schools.in addition, some aspects of food and nutrition education (fne) are addressed in the australian curriculum [17] as well as in the curriculum of the state of victoria [18].a significant barrier for public policymakers implementing interventions and programs in schools is the likely acceptance of these by stakeholders [19].this emphasises the importance of obtaining stakeholders’ views on school nutrition interventions to better develop and execute future interventions [19, 20].however, this descriptive study provides an integrated exploration of all aspects of school food issues, including fne, food environments and food policy in primary schools.’ responses to a single question: what strengths and weaknesses do you think primary schools possess in promoting healthy eating?the methodology for both studies is outlined below.social constructivism formed the foundation of this study [30].a qualitative descriptive approach [31] was adopted.thirtyseven parents and 26 teachers responded to the advertisement via emails.the selected participants gave their written consent to participate and permission to audio-record the interviews.they were all offered a $20 shopping voucher as compensation for their time.there were no prior relationships between the researchers and the participants.the recruitment ceased when the ‘conceptual depth’ (data saturation) point was reached, which is in the words of nelson: ‘ a sufficient depth of understanding that can allow the researcher to theorise’ [36].interview procedure semi-structured qualitative interviews were conducted to obtain a detailed understanding of australian parents’ and teachers’ views on the victorian primary schools’ strength and weaknesses in promoting healthy eating.face-to-face interviews were conducted in public locations such as libraries or participants’ residences.to enhance the trustworthiness of the findings, themes generated by the manual coding using nvivo were compared with those generated by leximancer [40, 41].results nineteen parents of children attending a primary school in victoria from the suburbs of melbourne and regional areas of victoria were interviewed.all interviews were held from january 2020 to june 2020.‘i think probably the stephanie alexander kitchen [an independent kitchen and garden program offered in some australian primary schools] is a huge thing.so, they grow the fruit or food, vegetables, and then they pick it and then they cook it and then they eat.’ parent 18.’ teacher 3.primary school children’s young age and the significant amount of time spent at school were also cited as advantages that contribute to the effectiveness of fne delivered at primary schools.‘they are at their optimal learning.so, we have time, and we have the minds that are ready to learn those things.if you tell this to a 50-year-old, i’m sure they’re going to struggle to try and change their diet and habits.whereas, a kid, they’re very willing to take on that advice from someone they trust.’ teacher 13.‘i think getting in while they’re youngteach them early so they don’t forget, or they don’t think of as an additional tool.it becomes part of their routine.’ parent 19.they have a captive audience.‘when my little boy starts prep [first year of primary/elementary school] this year, the parents are provided with a brochure about healthy eating and how to prepare lunchbox, a healthy lunch box for the kids, and they give us a lot of ideas for the lunchboxes, which is good., i feel like that is one of their strengths.just that therei guess, communication with parents too, in newsletters etc.so, what is being taught in the classroom is then being replicated at home, or being used at home, i guess.‘if you don’t have any fruit in your lunchbox today, it would be wonderful if you have some tomorrow.the next day there’ll be a piece of fruit in the lunch box.’ teacher 2.‘i guess they’re in a position of authority, just as much as parents are, and i think maybe more so, to be able to impart that type of information on students.they recognised it as an effective strategy to increase the fruit or vegetable consumption of children.some reported that fruit baskets were usually available for children at fruit breaks (morning break before the lunchtime).these fruit baskets were often donated by charities or nearby supermarkets.i’ve started bringing in like a bag of apples each week, then for mostly one particular student in my class that doesn’t have a very healthy, balanced diet in his lunch box, and so i’ll just give him an apple at recess.‘the idea of this crunchy fruit time‘ with strengths, it’s nude food.it makes you more conscious of your choices, you have to be a bit more creative.generally having a canteen at school, i think is a really good thing.so, i was surprised when they introduced that once a week.and from time to time, they do have another day just to provide sushi or something.‘i know that their school canteen is amazing.so, they have a really lovely canteen there; tuck shop.you can buy lunch there or snacks.perceived weaknesses of primary schools in the promotion of healthy eating limited time a lack of time and other competing priorities of teachers were regarded as a barrier to promoting healthy eating among children, particularly by teachers (n = 10).‘i think the limitation is just the lack of knowledge.‘i don’t think the teachers though are trained as a nutritionist, or a dietitian.i think even the basics, or whether they get somebody in live meetings.’ teacher 13.‘you need your leadership to back you.you might just be on your own.they felt that these environments undermine health messages given in the classes.‘they ask us to bring just water to drink.that is important, very good, but they sell some of these milk, with flavour, that also has sugar.and you can buy the other things that are in plastic.’ parent 3.‘we do all this focus in class and in the kitchen and garden, and then it’s sort of like, “here’s your canteen list, take your pick of all the foods we’re saying “sometimes food”.‘to me the lunch order is more of a treat, or a friday type thing.so my kids have hotdogs or chicken nuggets.my kids are not going to have a salad for lunch.‘letting kids bring lollies for their birthdays.thereand even if they have their birthday on the school holidays, then they have to bring cupcakes on the friday before the school holiday.thereso, i think in that regard, that does have a negative impact.it would be good to be able to replace some of those fundraising events with healthier options.for example, because in a fundraising event, there were rum balls [a sweet treat].so, instead of rum balls like maybe it could be like protein bars or protein slices.’ teacher 3.‘i think some of the weaknesses are when they have a school activity, like if they do a sports day or carnivalthey claimed that they could not run cooking and gardening activities or take children on farm or supermarket visits because of financial restraints.one teacher also mentioned that they could not invite an expert speaker because their school could not afford the cost.’ teacher 4.‘i think it comes down to cost.’ teacher 3.(supplementary file).previous studies have demonstrated similar levels of support among parents [44] and teachers [29, 45, 46] for increasing the inclusion of fne in primary schools.these reports are similar to concerns across the world on the effects of food production on climate change [57, 58] and growing interest in improving the sustainability of food consumed at school [59].leadership has been consistently reported internationally as an important enabler of the success of the promotion of health eating in schools [63, 64].as it was argued that public schools were more disadvantaged than private schools in terms of funding they receive [71], priority should be given to public ones.these findings match those observed in previous studies that reported inconsistent implementation of canteen policies in victoria and low adherence to canteen policy [72] with many discretionary food items and lack of healthy food items in the canteen menus [73].australian primary schools’ inadequate adherence to nutrition policy is not unique.this is in line with hardy et al.’s study [80], which reported that australian primary school students who purchase lunch orders ≥2 times a week were more likely to have unhealthy weight status compared to ones purchasing ≤1 time/week.however, it is important to note that qualitative studies do not aim to represent populations but identify themes or meanings that are likely to be present in the general population without measuring their extent.conclusion parents and teachers identified the primary school as a well-positioned setting to support the development and practice of healthy dietary behaviours and food skills.
number of words= 1480
[{'rouge-1': {'f': 0.3061321359355269, 'p': 0.8313168724279836,'r': 0.1876096630642085}, 'rouge-2': {'f': 0.15892988613210615, 'p': 0.30966942148760335,'r': 0.10689567430025446}, 'rouge-l': {'f': 0.3033507181256519, 'p': 0.6826760563380281,'r': 0.195}}]
-----------------------------------------------------------------------------------------------------------------------------------
p362:
Extractive Summary:
background in the early months of the covid-19 pandemic, state and local governments throughout the united states recommended minimizing contact between people, leading to the closure of schools and nonessential businesses, and shelter-in-place orders to reduce the spread of the disease.in some states, including california, these restrictions continued, particularly school closures.researchers have posited that social distancing and stay-at-home guidelines have resulted in less partnered sex for most young people as they face increased parental monitoring and reduced privacy [10].comparatively few studies in this area have included adolescent or young adult populations.covid-19 and social distancing guidelines may lead to an increase in young people’s online romantic or sexual interactions.previous research has found that some adolescents initiate new relationships online [21–23], and young people often communicate with their partners through phone or text, including sexting [24, 25].we also compared the impact of the pandemic on romantic relationships and sexual activity between adolescents and young adults.on march 19, 2020, california issued the first statewide shelter-in-place order to mitigate the spread of covid-19, directing all residents to stay home except to go to an essential job or seek basic necessities, such as food, prescriptions, and health care [30].however, rising covid-19 cases and hospitalizations led to a statewide mandate for all californians to wear masks in public on june 18, 2020, and to close most businesses again in mid-july, 2020 [32].throughout the summer of 2020, fresno county had significantly higher rates of covid-19 compared to california, and the test positivity rate as of february 2021 remained higher than the state average [33].participants were initially recruited from 49 different sites, including alternative schools, foster homes, and afterschool programs.the original study procedures included completing online surveys at baseline and at 3 and 9 months after baseline.participants received a $10–$20 gift certificate after completing each survey.in june 2020, 797 participants (out of 1260 who enrolled in the larger study) were invited to complete a supplemental online survey.measures all of the measures used in the analyses were collected in the supplemental survey, except where noted below.physical distancing from intimate partners participants were asked if they had “dated or been in a romantic or sexual relationship with anyone” in the last 3 months.” response options ranged from 1 (never) to 5 (daily).check all that apply.” the response options are shown in table 3.participants also were asked to rate their agreement with the statement: “if you’re only dating one person, it’s okay to see each other during shelter-inplace.two items assessed how often participants had sex before the covid-19 restrictions and in the last 3 months.” age group we created a binary variable for developmental age group at the time of the supplemental survey.in response to the open-ended question, several participants mentioned they had not dated anyone since the start of shelter-in-place restrictions.one youth stated, “i was finally starting to get around to dating, but covid- 19 really put a stop on that” (female, lesbian, gay, bisexual, or queer [lgbq], age 19).among those who were currently in a relationship, the majority said they had reduced or stopped spending time together in person.one responded, “i haven’t seen my boyfriend and when i have i had to wear a mask” (female, straight, age 16).one explained, “it’s made things more difficult; we’ve become more distant and fought way more than usual” (female, straight, age 18).another participant stated, “at first it ruined it due to me being on a stay at home order and couldn’t see them, but later on it helped the both of us a lot in ways that we maybe needed that time apart and even got to learn to communicate better” (female, straight, age 19).most youth agreed (47%) or strongly agreed (12%) that “it is okay for intimate partners to spend time together in person during shelter-in-place if they are only dating one person,” while 29% disagreed and 13% strongly disagreed with this statement (not shown).attitudes towards physically distancing from partners were similar across age groups.reasons for physical distancing in the open-ended responses also included parents’ restrictions and concern about becoming infected or infecting others.one youth stated, “at first, we didn’t see each other for over a month just for the fact to be safe because we both work around a lot of people” (female, straight, age 20).another said, “i was afraid of seeing my partner because i was still visiting my sibling” (female, straight, age 19).as one explained, “school isn’t happening and i don’t get to see my boyfriend every day” (female, straight, age 17).compared to adolescents, young adults were significantly more likely to have ever had sex (44% vs. 26%, p = 0.001) and sex in the past 3 months (31% vs. 17%, p = 0.003).in the open-ended responses, few focused on sexual activity, though one did mention that it affected her “relationship by not allowing contact and intimacy” (female, lgbq, age 15).another explained, “i haven’t had a extremely sexual relationship because i have strict parents.one stated, “i feel like nobody has anything better to do at home [so] guys have been more bold recently and i’ve been getting way more messages on social media than i usually would” (female, straight, age 16).these results are consistent with the increase in autonomy that typically occurs in the transition to adulthood, along with other changes such as moving away from family and entering the labor market [40].researchers have posited that sexually transmitted infections may have declined due to increased isolation and less casual sex during the pandemic, although others are concerned that the lack of testing and treatment may lead to an increase in transmission [46, 47].with fewer resources and the increased demands on schools due to the pandemic and transition to remote instruction, sexual health education may be of lower priority.however, adolescents continue to need accurate and age-appropriate sexual health information from a trusted source.schools and community-based organizations may need support to proceed with sexual health education in an online or digital format.this study has limitations.second, the study used a sample of youth participating in an ongoing study in fresno county, california, and the sample is not representative of youth in the county or california.respondents to the supplemental survey were significantly younger, more likely to be female, hispanic, have internet access at home and on their phone, and less likely to be sexually experienced than non-respondents.third, because of the limited sample size, the results of comparisons by gender and sexual orientation must be interpreted cautiously, and we were unable to examine other important characteristics, such as housing status.a higher response rate may have been achieved with additional time and efforts at follow up and/or a gift certificate of greater value.future research should assess if the covid-19 pandemic resulted in a lasting shift in relationship development among adolescents and young adults, including changes in their approach to casual sex, communication styles, and use of online dating.conclusions this is one of the first studies to examine the immediate impacts of social distancing requirements on young people’s romantic and sexual relationships, asking about their experiences during the first 3 months of the shelter-in-place restrictions.we found that adolescents and young adults have continued to engage in sexual and romantic relationships during the pandemic, although many reported physical distancing from their partners.
number of words= 1196
[{'rouge-1': {'f': 0.39174109904148186, 'p': 0.8807142857142858,'r': 0.2518910256410256}, 'rouge-2': {'f': 0.2169577798221367, 'p': 0.4140860215053764,'r': 0.14698476343223738}, 'rouge-l': {'f': 0.3621336102792001, 'p': 0.7707299270072994,'r': 0.23666666666666666}}]
-----------------------------------------------------------------------------------------------------------------------------------
p363:
Extractive Summary:
since then, the world health organization has reported over 200 million cases and over 4 million deaths worldwide, with the centers for disease control and prevention reporting over 36.7 million cases and around 620,000 deaths in the united states (us) as of august 15, 2021 [1, 2].these restrictions are ongoing and have had a significant impact on daily life and well-being [3–5].preliminary research in the first few months of the covid-19 pandemic identified child mental well-being (mwb) as being particularly vulnerable to the pandemic and associated public health restrictions [6–11].additionally, children of elementary school age may have a hard time understanding the health risks of covid-19 and changes to daily life from lockdown measures, further increasing fear and anxiety [6].the full impacts of covid-19 on mwb, pa, and sb are still unknown; however, evidence suggests poor mwb, limited pa, and increased sb in childhood may have potentially long-term and serious health consequences [21–26].the covid-19 pandemic and associated restrictions are ongoing with no clear timeframe for when restrictions will be lifted, making the potential impacts on child mwb, pa, and sb with the subsequent long-term effects on health and well-being particularly concerning.given this, the aims of this study were 1) to explore how covid-19 restrictions are impacting child mwb, pa, and sb using qualitative methods, and 2) to assess how perceived changes in pa and sb due to covid-19 restrictions are associated with perceived changes in mwb using quantitative methods.convergent parallel designs consist of conducting qualitative and quantitative methods independently and concurrently, with results from each of these methods being integrated during the interpretation of findings [30].online surveys were conducted with parents and caregivers of children in kindergarten through 5th grade to assess the association among perceived changes in pa and sb, and perceived mwb changes.louis-based neighborhood groups for parents with elementary school children.sampling methods focused on parents and caregivers in the st.interviews were conducted independently during this time period within the same population, using a combination of convenience and snowball sampling.louis approved this study with an exempt status.topics in the survey included parent, child, and household demographics, child mwb, child pa and sb, as well as parental influence and rules regarding these behaviors.qualitative interviews (n = 16) were administered via telephone or zoom and lasted between 20 and 30 min.if participants had more than one child, participants were asked to choose one child to answer questions for.questions asked during the interviews were informed by early covid-19 research and developed with expertise of the research team.parents filled out this questionnaire based on perceptions of how their child felt prior to and during covid-19 restrictions.qualitative interviews assessed child mwb by asking parents about their child’s general mood and how their child’s mood had been impacted by covid-19 restrictions.if parents noted worsening mwb, follow up questions were asked to elicit more information about why these changes to mwb occurred.qualitative in interviews with parents, covid-19 concerns were explored by asking parents about their child’s level of concern regarding covid-19, their understanding, and what questions and concerns their child voiced about covid-19.perceived changes in pa and sb quantitative to measure parent perceptions of changes in child pa and sb, questions from an existing scale, the homestead’s physical activity and screen media practices and beliefs survey were used [32].parents were asked in the online survey whether their child’s pa increased, stayed the same, or decreased due to covid- 19 restrictions.qualitative during interviews parents were asked to describe their child’s current level of pa, and if their child’s pa level had changed due to covid-19 restrictions.if parents perceived changes to child pa, follow up questions were asked about how covid-19 restrictions impacted pa levels.parents were also asked to describe the types of activities their child engaged in during covid- 19 restrictions and how these activities differed from before covid-19 restrictions.similar questions were asked about non-screen time sedentary activities (e.g. reading, coloring, building blocks).independent variables included in the analysis were pa behaviors (perceived change in pa, perceived change in organized sports, perceived change in outside play, perceived change in inside play, and perceived change in family pa), sb (perceived change in time sitting, perceived change in screen time for entertainment, and perceived change in screen time for education), and covid-19 concerns (discontent with stayat- home orders, concern about covid-19, and fear of family/self/friends getting sick).pa and sb were dichotomized into binary variables of ‘same or increased’ and ‘decreased’.covid-19 concerns were also dichotomized into ‘sometimes true or true’ and ‘not true’.the mcnemar test was run to determine if there were statistically significant differences between each mwb item of the mood and feelings questionnaire reported for prior to covid-19 restrictions compared to during covid-19 restrictions.using the final codebook, each transcript was double-coded by the three members of the research team and reviewed to eliminate discrepancies between reviewers.coded text was separated out by code to be reviewed for common themes by research team members.common themes were discussed and agreed upon through consensus.these themes were then compared to the quantitative findings and integrated by identifying convergence and places where qualitative rich text could be used to provide context for quantitative results.results the total number of parents surveyed was 245.the ages of children in the quantitative sample ranged from five to twelve years old, with an average age of 8 (sd = 1.75).a majority of children (96%) were white, 44% were female, and most (74%) lived in a household with two to three children.the majority were white (96%), married or living with a partner (93%), and employed (83%).parents reported on a total of 23 elementary school aged children, 13 of which were female.overall, 87% of households had more than one child.parents interviewed were mostly female (94%) and married (81%).parents reported a wide range of jobs from social worker and teacher, to it consultant, attorney, and business owner.most reported working from home and a few noted being laid off or furloughed due to the covid-19 pandemic.the majority of parents (75%) reported their child was discontent with stay-at-home orders.most parents (76%) reported their child was concerned about covid- 19 and 63% were afraid for themselves or a family member getting sick.each mwb item in the adapted mood and feelings questionnaire was reported to be worse during covid-19 restrictions.among aspects of child mwb, perceived loneliness increased the most, from less than 1% prior to covid-19 restrictions to 33% during covid-19 restrictions.results from chi-squared and logistic regression analyses are presented in table 3.discontentment (x2 (2)= 8.06, p = 0.005) and concern (x2 (2)=7.20, p = 0.007) about covid-19 were significantly associated with perceived changes in mwb.similarly, if a child was concerned about covid-19, they were 67% less likely to have the same or better mwb during covid-19 restrictions compared to a child who was not concerned about covid-19 (or = 0.33, 95ci 0.15– 0.75).a child whose parents perceived a decrease in playing outside during covid-19 restrictions were 72% less likely to have the same of better mwb during covid-19 restrictions than a child whose perceived time playing outside stayed the same or increased (or = 0.28, 95ci 0.10– 0.80).another prominent theme developed around child understanding and concern about covid- 19.children seemed to take precautions seriously.transitions to remote learning not only posed educational challenges, but according to our findings increased feelings of loneliness and unhappiness for children who missed school activities, such as school sports and socializing with friends.participants also reported increased worry for children, noting they felt the restrictions of not seeing grandparents, attending in-person school, or being able to use the local playground, and either drew scary conclusions about covid-19 or were unsure what to think about the covid-19 pandemic.reduced pa is problematic not only because of the importance of pa in preventing obesity and chronic disease later in life, but also because of the associations we found between pa with perceived changes in mwb [22].this finding supports previous research suggesting outside play and connection with nature have benefits for mental health and concentration [41–44].examples of technology use for being active and social could include a dance class over video call or remote yoga with friends.as such, we can only assess associations between child mwb, pa, and sb during covid-19 restrictions and cannot make causal inferences.we also did not collect data on weekly minutes of moderate and vigorous pa, limiting our ability to use u.s.although limited by sample size and parent-report, this study used a mixed methods approach, which provided context and corroboration for quantitative findings from the online survey.
number of words= 1404
[{'rouge-1': {'f': 0.3892087691992466, 'p': 0.8480979827089337,'r': 0.25255578093306286}, 'rouge-2': {'f': 0.19901075898075202, 'p': 0.35901734104046246,'r': 0.13765899864682002}, 'rouge-l': {'f': 0.3864059721190496, 'p': 0.6903208556149734,'r': 0.2682905982905983}}]
-----------------------------------------------------------------------------------------------------------------------------------
p364:
Extractive Summary:
looking at protective factors, the analyses show that people with a high level of education have a low risk (or 0.82, 95% ci [0.74,0.90]).low-risk people are also those with a non-western immigration background (or 0.63, 95% ci [0.57,0.69]) and 1st generation immigrants (or 0.72, 95% ci [0.66,0.78]).also being married or having children is a protective factor for a couple living together (or 0.64, ci 95% [0.54.0.75] for a married couple without kids, or 0.63, 95% ci [0.52,0.77] for a non-married couple with kids).these effects are weaker when the other effect is already present (or 0.58, 95% ci [0.48,0.69]).having a higher income is also a protective factor.we observe urbanicity and regional differences being mostly non-significant.figure 1 shows the approximate roc (based on percentiles to preserve privacy).each point on the curve corresponds to a threshold and shows the proportion of people who died by suicide that are above the threshold (the sensitivity) on the y-axis.on the x-axis, it shows the proportion of people in the control group who are above the threshold.the curve shows a trade-off between true and false positives and allows for an informed choice of thresholds for risk groups.the auc, which is based on the full plot, is 0.77.this means that the probability that an individual in the sample of those dying by suicide will get a higher predicted risk than an individual in the control set is 77%.a fully random model would have an auc of 0.50, while a perfect model would have an auc of 1.discussion to our knowledge, this is the first study done into suicide on socio-demographic factors with such a large and unbiased sample, where, due to the level of detail of the data, analyses could be done to control for many characteristics, giving us very robust risk factors.we found that previously discovered risk factors for suicide (middle- age, male gender, and unemployment (as measured through benefits)) remain elevated even when corrected for a wide array of socio-demographic characteristics.the same holds for commonly found protective factors for suicide, like having a higher income or immigration background.most increased risk came from being a recipient of mental health care (which includes being an inpatient as well as being an outpatient), which can be expected knowing that approximately 87% of people who die by suicide have mental health problems [10].additionally, physical healthcare being a risk factor could be explained due to hospitalisations for previous suicide attempts.however, due to the fact that the risk keeps increasing as physical health care costs increase, it is unlikely this would account for all of the increased risk.this study did not observe significant differences between rural and urban municipalities.however, it is important to note that due to the high population density in the netherlands, most rural areas in the netherlands might still be considered urban compared to rural areas in other countries.looking at raw frequencies, we see regional differences in the netherlands [11].these differences became much less when the effects of possible correlating risk factors were considered.this seems to indicate that the regional differences are primarily caused by the differences in the demographic makeup of the regions as opposed to specific local causes.when we look at level of education, we see that being highly educated remains a protective factor.however, this only holds for the highest level of education and is not particularly protective.especially when compared to the results of phillips and hempstead [12] who found large differences between the suicide rates among people with a high school degree and those among people with a college degree in the united states.combined with the protective factor of income and the high correlation between level of education and income, this seems to suggest a proxy effect.the level of education might only be a protective factor due to the associated increase in income.our model has a reasonable fit with an auc of 0.77, which is high for a model predicting suicide death [1] and comparable to the recent results of zheng et al. [13] who used deep neural networks on electronic health records to predict suicide attempts (auc of 0.769).it could be used to identify low, regular, or high-risk groups.however, the model is not usable to predict suicide risk in individuals.suicide is a rare event that on average occurs in about 1 in 10,000 people a year.this means that even if you have a tenfold increase in predicted risk, you will still have 1000 false positives for each true positive.although then not useful for prediction on an individual level, the results from this study allow for targeted prevention measures at certain risk groups.for example, it would be possible to train social workers that are in regular contact with recipients of social benefits to be gatekeepers.
number of words= 779
[{'rouge-1': {'f': 0.41975110226261525, 'p': 0.7526568265682656,'r': 0.2910274790919952}, 'rouge-2': {'f': 0.21146256860273796, 'p': 0.33296296296296296,'r': 0.15492822966507178}, 'rouge-l': {'f': 0.36258735287413296, 'p': 0.5564864864864865,'r': 0.2688950276243094}}]
-----------------------------------------------------------------------------------------------------------------------------------
p365:
Extractive Summary:
lca is a model-based clustering method for multivariate categorical data and has previously been applied in the analysis of multimorbidity [22, 23].in the case of multimorbidity, clustering using lca is more appropriate than standard distance-based methods, such as k-means or hierarchical clustering, since the appropriate probability distribution for the data is readily available.furthermore, lca allows extra flexibility for diseases to have partial membership across multiple clusters unlike other more limiting distance-based clustering methods.two sets of parameters underlie the model: the group probability τ and item probability θ.the group probability parameter represents the a priori probability that an observation belongs to a particular group, so that p(group g) = τg.the item response probability represents the probability of a success for a given item, conditional on group membership, so that p(item m= 1 | group g) = θgm.more formally, let x = x1, …, xn denote mdimensional vector-valued binary random variables, composed of g groups.the observed-data likelihood distribution for the data x can then be written: pðxj θ; τ þ ¼ qni ¼1pgg ¼1τgqmm ¼1θxim gmð1−θgmþ1−xim .the naïve bayes assumption that observations are conditionally independent based on group membership has been made for this model.direct inference using the observed-data likelihood is typically difficult and is facilitated by the introduction of latent variables z = z1, …, zn.each zi=zi1, …zig is a g-dimensional vector, representing the true cluster membership of xi as a multinomial random variable.that is, suppose that the true group membership is known for each xi and is denoted by zig= 1 if observation i belongs to group g, otherwise zig= 0.the complete-data density for an observation (xi, zi) is then pð x; zjθ; τþ ¼ qni ¼1qgg ¼1fτgqmm ¼1θxim gmð1−θgmþ1−ximg zig : lca thus allows the data to be summarised at a global and local level.the parameters θ and τ summarise the overall behaviour of the clusters in the data, while each variable zi informs us of the cluster membership, and thus behaviour, of an individual observation i. inference for our lca models was performed using an expectation-maximisation (em) algorithm.this works in two steps: the e-step, where z is estimated, based on the current values of θ and τ, and the m-step, where the complete data likelihood is maximised with respect to θ and τ based on the current value of z. the algorithm proceeds iteratively until it has deemed to converge; that is, once parameter estimates remain more or less unchanged after successive iterations.as the true number of groups g is not known in advance, each lca model was run over a range of 1–10 groups.the number of clusters was then chosen using the bayesian information criterion (bic), where bic ¼ − 2 logpðx jθ; τþ þ ðgm þ g−1þ logðp n i¼1 wiþ; wi is the survey weight attached to observation i and logp(x |θ, τ) is the survey weighted pseudo-loglikelihood.here a lower value of bic indicates a more suitable choice of model.in many practical examples as was performed in this current work a balance has to be found between model parsimony and model fit and so an “elbow” is usually identified whereby the addition of clusters has diminishing returns to model fit improvement.we applied lca using the software package lcca in r [24].code to implement this analysis and bic values for all models assessed are provided in additional file 4. results individual disease prevalence figure 1 shows the crude population weighted prevalence of the 10 medical conditions by sex and age categories.table 1 shows the odds ratio of each condition compared to the u.s.after adjusting for confounding variables.here, it can be seen, that the u.s.had significantly higher prevalence than england for all 10 medical conditions and for all, except diabetes, when compared to canada, even in adjusted models.the u.s.had significantly higher prevalence for all, except osteoporosis, when compared to ireland.the u.s.and canada had very similar prevalence of diabetes, regardless of age and sex (fig. 1).the odds of having diabetes in the u.s.was approximately double that of ireland or england (table 1) even in adjusted models.the u.s.had a pronounced higher prevalence of hypertension, arthritis, and psychological conditions across all age and sex categories.in particular, 56.8% of the u.s.population had arthritis; significantly, higher than the other countries whose prevalence was 32.3–38.2%.ireland had the highest prevalence of osteoporosis (13.85%) followed closely by the u.s.(13.0%) then canada (9.8%) and england (7.5%).additional files 5, 6 and 7 show the fully adjusted disease prevalence across countries with respect to income, education and bmi.here it can be seen that income and education gradients are more pronounced for the u.s., especially for lung disease, stroke (with respect to income), myocardial infarction, psychological illnesses (with respect to income), and high blood pressure.the prevalence of psychological illnesses in all cases was inversely related to income, however the opposite was true of education level.hence those with lower income but who are higher educated are more likely to be diagnosed with a psychological illnesses.in the u.s., 60.7% of adults, aged 52–85, had two or more medical conditions.this is considerably higher than the other countries: canada 45.3%, england 42.1% and ireland 38.6%.disease cluster compositions five latent classes (disease clusters) were identified for all four cohorts.the item response probabilities for the clusters of each country can be seen in fig. 2.figures 3, 4, 5 and 6 show the odds ratios and 95% confidence intervals for risk factors associated with consistent across countries (fig. 3, 4, 5 and 6).however, the composition of clusters across countries varies.for example, the “high probability of disease” group across all countries was associated with higher odds of being older, lower educated, lower income having a smoking history and being obese (fig. 3, 4, 5 and 6).regarding the composition of this “high probability of disease” group however, canada and england had higher probability of myocardial infarction (47.9% england, 37.2% canada, 6.32% ireland 19.2% u.s.) and angina (67.6% england, 37.5% canada, 15.3% ireland, 29.9% u.s.) (see group 1 fig. 2a, b, c, d).for ireland and the u.s., cardiovascular diseases were separated into a distinct cluster along with high blood pressure, diabetes and arthritis (see fig. 2 group 2 a, d).bmi having an elevated bmi (> 25) was strongly associated with the “high probability of disease” groups across all four countries (see group 1 figs. 3, 4, 5 and 6).it was also associated with the two groups which were predominantly cardiometabolic and metabolic in nature across countries: group 2 and group 4 (see fig. 2 (cluster patterns), figs. 3, 4, 5 and 6 (odds ratios and 95% ci), see also additional files 8, 9, 10 and 11).although, older age increased the odds of all disease groups, the “high probability of disease” group for the u.s.had a high proportion of 52–64 year olds of 37.4%.the disease cluster with the highest disease burden for the other countries (group 1) had a lower proportion of younger participants (31.6% canada, 14.2% england, 11.4% ireland).education the gradient and significance of socioeconomic indicators varied across countries for their respective disease patterns.in general, groups which consisted of high blood pressure, diabetes and arthritis and/or cardiovascular diseases were negatively associated with education.
number of words= 1174
[{'rouge-1': {'f': 0.31286566310241776, 'p': 0.6823188405797103,'r': 0.20296616837136114}, 'rouge-2': {'f': 0.15820782165123748, 'p': 0.26636363636363636,'r': 0.11251968503937008}, 'rouge-l': {'f': 0.28590430295093505, 'p': 0.5203311258278146,'r': 0.19710280373831776}}]
-----------------------------------------------------------------------------------------------------------------------------------
p366:
Extractive Summary:
background type 2 diabetes is a chronic and often progressive disease that can lead to devastating complications and long-term disability [1].the economic cost of diabetes is rising steeply, increasing from $245 billion in 2012 to $327 billion in 2017 [2].the burden of diabetes on affected individuals and on society as a whole underscores the importance of prevention.the centers for disease control and prevention (cdc) estimate that up to 88 million adults aged 18 years or older have prediabetes, [1] and many of these individuals will progress to incident type 2 diabetes over 3 years without intervention [3, 4].however, intensive lifestyle interventions (ilis), including the diabetes prevention program (dpp), can significantly lower risk of incident type 2 diabetes among overweight/obese adults with prediabetes [4].increased work-site adoption of obesity and diabetes prevention interventions, such as dpp-based ilis, can help promote healthy weight among employees, [5, 6] reduce health risks, and improve health outcomes [6].in a 2017 review of work-site translations of dpp, weight loss ranged between 1.4 and 4.9 kg at 7 to 12 months (n = 6 studies), with worksite programs offering > 16 core sessions having the most favorable outcomes [7].ongoing efforts to disseminate dpp nationally have increased the rate of program adoption by us employers, but reach and engagement among at-risk individuals remains relatively low [8].university systems represent a promising, largely untapped option for dpp dissemination to overweight/obese adults with prediabetes.in the us, there are over 4300 higher education institutions (e.g., universities and colleges that grant degrees), [9] and in many communities, universities are the largest employer.the university of california (uc) system, for example, is one of the largest employers in california with over 229,000 employees [10].university employee turnover may be lower than at for-profit organizations and many employees may also be enrolled in universitymanaged insurance programs, providing increased impetus to prevent diabetes and obesity.large university systems also have many resources to readily implement dpp-based ilis, making them an ideal setting to engage personnel in evidence-based obesity and diabetes prevention interventions.however, very few studies have examined the effectiveness of university-based dpp models.among 1863 cdc registered dpp organizations as of march 1, 2021, 50 appear to be university or college-affiliated programs (fig. 1).to our knowledge, only six published studies have focused on university based dpp adaptations.however, these studies included small sample sizes, pre-post analyses that lacked comparator groups, and short-term follow-up windows (table 1) [11–16].beginning in 2018, the uc system implemented the uc dpp initiative, which is a multi-component program to target diabetes and obesity prevention among at risk affiliates.this initiative was informed by a pilot dpp implemented at the university of california los angeles (ucla) in 2016.ucla dpp was one of the first university-based programs in the us to achieve full cdc recognition.cdc recognition is granted to programs that meet all recommended milestones for dpp delivery, including 5% mean weight loss among participants.based on the success of the ucla campus-wide dpp, campus leaders and researchers partnered with the uc office of the president (ucop) and uc health to launch a uc-wide dpp initiative in 2018.by 2019, all 10 uc campuses had implemented uc dpp, which now operates as part of routine campus activities (more details below).to our knowledge, this is one of the largest university-based efforts to adress diabetes and obesity prevention with a focus on system-wide dpp-based ili delivery.our goal is to conduct a rigorous, mixed-methods evaluation of the uc dpp initiative, focusing on the five uc campuses with large medical centers where robust ehr data for uc beneficiaries is also available; namely uc los angeles, irvine, san diego, san francisco, and davis.our goals are to identify why the uc dpp initiative succeeds (or not), and to document lessons learned across the uc system and between campuses to inform future efforts across uc and other large university systems.the university of california diabetes prevention program (uc dpp) initiative the uc dpp initiative has four key components; 1) identification of diabetes prevention as a system-wide goal, 2) a prediabetes awareness campaign targeting atrisk beneficiaries, 3) coordinated dpp implementation and delivery on every uc campus 4) coverage of dpp for all campus affiliates, including faculty, staff and students, at no cost.each campus identified local champions, engaged key stakeholders, and signed a memorandum of understanding (mou) with ucop to receive funding for program implementation.the mou outlined a commitment to the goals of the initiative, delineated guidelines for dpp delivery, and confirmed available resources.funding to campuses was based on fixed costs of delivering a dpp cohort (i.e., not based on number of participants or program outcomes).although the uc system could have easily continued to outsource dpp delivery (as many universities and workplaces do through their insurers), this initiative aimed to prioritize diabetes prevention as a highly visible system-wide goal that leverages university-based infrastructure and resources to implement all four key components.for example, campuses aimed to implement > 4 dpp cohorts in their first 2 years and then receive additional ucop funding to add cohorts based on local demand.the uc dpp coordinating center was established by ucop/uc health to support system-wide dpp activities.the uc dpp coordinating center assists uc campuses with all aspects of program delivery, including dpp coach training by ucla-based certified master trainers and data management for dpp cohorts.centralizing some activities, such as coach training, provides efficiency of scale (e.g., each individual campus is not required to spend time and money to train master trainers).the coordinating center leads monthly group calls with all uc campuses and individual calls with each campus as needed.each uc campus is also registered with the national dpp (each campus has their own unique cdc organization number in order to be eligible for cdc recognition).findings from this study will help inform future diabetes and obesity prevention efforts across uc, as well as the implementation of dpp-based interventions at other universities and large, stable employe
number of words= 966
[{'rouge-1': {'f': 0.38215947563892644, 'p': 0.7625925925925925,'r': 0.2549653808110781}, 'rouge-2': {'f': 0.21870511697735517, 'p': 0.38226765799256507,'r': 0.1531683168316832}, 'rouge-l': {'f': 0.3598875505700076, 'p': 0.616583850931677,'r': 0.25410041841004183}}]
-----------------------------------------------------------------------------------------------------------------------------------
p367:
Extractive Summary:
background childhood obesity has increased three-fold across all races and ethnicities during the past 30 years [1, 2].substantial evidence suggests that obesity in early childhood, from birth to age 5 years, is a strong predictor of adolescent and adult obesity [2, 5–8].in two cohort studies conducted within alaska’s y-k region during 2004–2010, approximately 40% of alaska native (an) adults were obese [9].specifically, nutrition interventions in indigenous communities need to address the way local perceptions and understandings of how food affects “the physical, psychological, social, and spiritual dimensions of all age and gender groups in community life” [15].an yup’ik communities have great respect for their elders as knowledge bearers [16].in addition, many of these same factors have caused a shift in the traditional food environment and contributed to a nutrition transition [20].with the elevated risk of obesity in an children, prevention efforts are underway to help curtail the trend toward obesity, including two studies that are led by the authors of this paper and funded by the national institutes of health (r01nr015417) and the u.s.department of agriculture (2018–69,001-27,544).these objectives fit into the goals of the broader intervention plan to 1) increase the consumption of nutrient dense traditional foods, and fruits and vegetables from the tundra and the store, while decreasing high-calorie low-nutrient prepared foods; 2) decrease the consumption of sugar-sweetened beverages, while increasing water intake; and 3) decrease screen time while increasing physical activity.these communities, with populations that range between 300 and 1200 people, are located off the road system, which means that they are accessible only by small aircraft year-round and by boat in the summer and snow machine in the winter.this region has one of the lowest population densities in the us and has limited infrastructure [25].in general, most children and young adults speak fluent english, attending schools where instruction is in english.thus, two an yup’ik/ cup’ik-speaking interpreters were available to travel with the research team, assisting with translation and interpretation during the focus group discussions.facilitators relied on cues from interpreters signaling an appropriate time to move to the next discussion point.data collection focus group questions sought perspectives on the importance of traditional foods and physical activities, as well as barriers to traditional ways.”, “what is the best way to teach about traditional foods?”“tell me about mealtimes as a child”, “what kinds of activities did children do before television?” and “what concerns do you have about how children are raised today in terms of the foods they eat and the activities they do?”all focus group discussions were audio-recorded, translated to english when necessary by the yup’ik/cup’ik-speaking interpreters, and transcribed.data analysis two researchers listened to each recording and read each transcript.they independently created a summary and a subsequent reflective memo that captured the most salient themes that emerged from the data, using open coding [26].the memos were then compared and contrasted until consensus was reached by the researchers on the most salient themes.after reaching consensus, the researchers created summative documents by question and themes for each community.finally, the researchers applied selective coding to the transcripts using the major themes that emerged during the first round of open coding [26].they also cited the importance of consuming fermented foods.these foods were consumed when they were young and the elders believe they should continue to be consumed today because of their health benefits since, “we know how to take care of ourselves already nutritiously.to further illustrate the superiority of traditional foods, elders compared the value of traditional foods to those bought at the store.they’re more healthier than the ones we eat like when we go into bethel [a larger community with restaurants and stores], like junk food,” (anv10).this is important because they believe, “native food is healthy, it’s good for a child” (anv5) and that, “when you start, when you have our foods right from the beginning, it affects your brain, your growth, your whole body, the way you act” (anv4).like i said, in the beginning, what we eat from when you are little, like even infants, if you want to train them to eat blackfish or native foods and stuff, they take the food and put it into their mouth.as they are growing, that is what they are going to eat.that is what they want (anv4).so they know what it is “before they even start growing” (anv2).like my oldest daughter, when she was a newborn, i let her taste some, it was very bloody, my fingers were very bloody from seal liver.as she was growing, i was surprised she loved seal liver.she liked to eat it with seal oil, raw liver.we eat it because i put it on her lips.i learned from watching and touching” (anv9).that way, they will use it when they get older and pass it onto their children.my father passed on his tradition to me and i’m slowly passing that tradition onto my children.traditional knowledge was also tied to pragmatic and essential information needed for sustainability.you go to another one [den] and then take some more.so there is a tradition too, they have to know that traditional knowledge at the same time to gather those foods (anv4).in particular, they shared angst over their lack of knowledge and consumption of traditional foods, their approaches to parenting, the perceived lack of physical activity, and their inability to pass on the traditional ways of knowing.“nowadays, the younger generation eats nothing but junk foods” (anv5) and as a result, “these young ones that don’t know, hardly eat any native food.“the results of eating so much sweet/sugary food are not good.it is common to see people with pop in their hand and drinking it.they found it problematic that the schools and usda have different standards of acceptability when it comes to donated foods than families use for themselves, which is compounded by the fact that there are limited commercial facilities for traditional food processing.as one elaborated, but one of the sad things that occurred was especially before the ‘50s, the 1950s, the old people were still, the teachers were young people at home and in the community.but at some time, we were told, the young people were told to quit speaking yup’ik.so as a result, they more or less started quieting down and not teaching what they know.elders also believed that western schools, like head start, which uses a family-style approach that allows children to serve themselves, teaches poor habits that contribute to food waste.they waste a lot of food.kids are too …they eat a little bit of this and throw the rest away.many children are introduced to sugar sweetened beverages including powdered drinks (e.g. tang) and soda at an early age through bottles and sippy cups.elders indicated they [parents] used the sugar sweetened beverages as a reward, as a babysitter, or to keep children quiet.“like they start crying and they stick a lollipop in their mouth and sit them in front of the tv and that’s it.another outside, western influence they identified was the use of screens, like televisions, smart phones, ipads, and video games.we were working out.kids are very sickly.this stood in stark contrast to how they were raised, as the elders didn’t worry about not being active because when they were children, they spent much of their time doing chores and playing games.they consistently espoused the importance of eating traditional foods, such as seal oil, fish, berries, and tundra greens, connecting these foods with both good nutrition and the practices and values of subsistence cultures.historical events experienced by an people include loss of land and land-based resources; decimation of the population through epidemics of influenza and tuberculosis; and disruption of families when children were sent to boarding schools for formal education.nutrition science enforces these elders’ observations, reporting that young children are especially vulnerable to parental food choices, becoming accustomed to foods they are introduced to early in life [31].ideally, the elders would have reviewed all findings to ensure their contributions were interpreted correctly; however, the regional review board, comprised of alaska native peoples from the region, did review and approve this manuscript per tribal policy.conclusions the risk of obesity in alaska native children is high and prevention efforts should be grounded in local knowledge and values.although they do not represent all alaska native peoples, their views provide a better understanding on local views of the importance of maintaining a traditional lifesty
number of words= 1380
[{'rouge-1': {'f': 0.30293591693952965, 'p': 0.7737037037037038,'r': 0.18833910034602075}, 'rouge-2': {'f': 0.16909235950432616, 'p': 0.3303305785123967,'r': 0.11362880886426593}, 'rouge-l': {'f': 0.27518828203439605, 'p': 0.5985714285714285,'r': 0.17866372980910428}}]
-----------------------------------------------------------------------------------------------------------------------------------
p368:
Extractive Summary:
introduction smoking is a major risk factor for noncommunicable diseases (ncds) such as cancer and heart diseases, causing more than 7 million deaths worldwide each year [1, 2].waterpipe known as ghalian in iran, is one of the oldest methods of tobacco smoking, which has now become one of the major public health challenges in the world, especially in the eastern mediterranean countries, including the arab countries, turkey, and iran [3, 4].since the late 1990s, waterpipe has been introduced as an inexpensive and social method of smoking, especially among young people and students [5].it is estimated that the prevalence of waterpipe smoking (wps) in youths eastern mediterranean region (emr), about 15% of use waterpipe [6].a longitudinal study among young people in emr showed a 40% in prevalence of wps within 2 years of follow-up [7].in iran, various studies have reported high rates of wps among young people, especially college students.for example, in studies by latifi et al. in [8], karimi-afshar et al. [9], and ghafouri et al. [10], more than a third of youths had a history of wps or were current waterpipe users.in iran, similar to arab countries, waterpipe has less social stigma than cigarettes.although, in the past, waterpipe was more common among the elderly, today it has become very popular among young people as a means of leisure, social gathering and communicating [11, 12].studies have shown that waterpipe users believe that it is healthier than cigarettes, is not addictive, and its smoke is more enjoyable than cigarettes due to the use of moassel or fruit-flavored tobaccos [13, 14].however, studies have shown that wps is associated with a number of harmful health consequences such as lung and esophageal cancers, respiratory diseases, low birth weight, and oral diseases [15].in addition to chronic diseases, waterpipe exposes people to infectious diseases such as tuberculosis, and viral infections including hepatitis and herpes, as a result of sharing between waterpipe users [5, 12, 16].despite the health risks of waterpipe, its use is increasing and has reached alarming levels in some countries [15, 17].this study was conducted in the kermanshah city in western iran, where the majority of residents are kurds.bashirian et al., in a study showed that 20.4% of female adolescents in kermanshah were current wp users [18].another study in kermanshah found that 36.1% of high school boys reported ever hookah use and 17.1% mentioned wps in the past month [19].the development of effective intervention strategies to restrict the increasing use of waterpipe requires a clear understanding of the factors influencing this behavior [20].it should also be noted that wps depends on culture, ethnicity, and other social environments [21, 22].qualitative studies are the most important tools for understanding culture-based topics that can assess the wps in a particular geographical or cultural area and provide rich information about the related factors [23].therefore, we conducted this study to investigate the wps in the young people of kermanshah in 2020 using a qualitative method.materials and methods design this was a qualitative study conducted with the approach of content analysis to explore the participants’ perspectives on the nature, and aspects associated to the wps.participants participants of this study were young people aged 17 to 25 years in kermanshah city, located in the west of iran.they were muslim kurds or persian and were also current waterpipe users.we used purposeful sampling method to select the participants by which the eligible waterpipe users were invited to interview in coffee shops or traditional restaurants.criteria for entering the research being 17 to 25 years old; both men and women, being current wp smoker i.e. wps at least once in the last 30 days [24], having the ability to speak to record the interview, giving informed consent to participate in the study.the study did not consider any limitations on factors such as financial status, family status of wps, marital status, level of education and some other underlying factors, although they may have been confounder factors.data collection the data collection method was semi-structured interview based on an interview guideline during june to august 2020.it seems that the main mechanism of peer influence on wps is learning how to use waterpipe in friendly or two-person groups [14, 41, 42].individual aspects of waterpipe use in this study, we found that individual aspects, especially psychological, are related to hookah use in young people.the sub-category of “motivational aspects” included two semantic codes: “appeal” and “relaxing”.many participants emphasized that waterpipe as a means of smoking has special charms and fundamental differences with cigarettes, which has increased its popularity.in a qualitative study, waterpipe users believed that the appeal of wps was multifaceted.taste, smell, and sight were significant sensory cues that contributed to its attractiveness.the use of a wide range of flavoring tobaccos, and innovations associated with large volumes of smoke also contribute to the attractiveness of waterpipe [43].a qualitative study in saudi women who used waterpipe showed that the smell and taste of tobacco in different flavors such as cappuccino, chewing gum, and various fruits played an important role in the attractiveness of waterpipe [31].another motivating factor for wps expressed by the participants in this study was the relaxing effect of waterpipe.similarly, saffarzadegan et al., in their study concluded that the relaxation seeking is one of the main factors in the tendency of young people to waterpipe [32].in a qualitative study, iranian turkmen men did not mention the relaxing effect of wps [44].other studies have shown that positive perceptions related to waterpipe such as relaxation, encourage and maintain wps [45, 46].given the findings that emphasize relaxation as a key feature of wps, interventions should focus on providing alternative methods that can meet this need in young people instead of waterpipe.in the present study, lack of two psycho-protective aspects against waterpipe was identified, which included risk perception and self-efficacy.consistent with many other quantitative and qualitative studies in the world, the results of the present study showed that the perceived risk related to the health effects of waterpipe and its addictive nature was low in consumers.participants in the study believed that waterpipe was less dangerous and addictive than smoking.in a similar study, those who smoked cigarette and waterpipe reported that waterpipe is less dangerous, less addictive than cigarettes, and “lighter”, “cooler” and “milder” than cigarette smoke [43].in other studies in lebanon and the united states, young people have also emphasized that waterpipe is less dangerous than cigarettes [47, 48].participants in this study, in line with the study of jawad et al. [43], reported that harmful chemicals are filtered due to the passage of smoke through the water in waterpipe, and this process reduces its risks and harms compared to cigarettes.therefore, it is necessary to implement educational interventions to increase people’s knowledge about the amount of nicotine in waterpipe tobacco, the possibility of addiction, and the inefficiency of passing smoke through water in reducing nicotine.self-efficacy is one of the most important personal factors that is used both to resist the temptation to smoke in public and to stop waterpipe among smokers [49].in wps behavior, there is an interaction between behavioral, personal and environmental factors, and the process of interactions may affect a teenager’s belief in the temptation to smoke a waterpipe.the results of a cross-sectional study in iranian male adolescents showed that lack of self-efficacy was the strongest determinant of wps [50] which was consistent with similar studies [51, 52].another study also found that most women who use waterpipe cited poor self-efficacy as an important factor in wps.they also stated that they could not resist the temptation to waterpipe use in difficult conditions [34].one of the most important limitations of this study was the uncertainty or unwelcome attitude of some young people, which led to a longer data collection time.also, in some cases, the owners of the cafeterias prevented the interview in their cafeteria.moreover, the effect of ethnicity can be considered as a limitation for the generalizability of the results of this study.it seems that the implementation of the policy of reducing access to waterpipe in public environments can reduce wps.individual aspects such as motivation, beliefs and psycho-protective also were related to the wps in participants.it is suggested that educational and interventions, based on models and theories such as health belief model (hbm), theory of planned behavior (tpb), and extended parallel process model (eppm) be implemented in order to increase young people’s belief and perception on dangers of wps, and to improve their self-efficacy to smoking cessati
number of words= 1392
[{'rouge-1': {'f': 0.3333427118975102, 'p': 0.8970042194092827,'r': 0.20470790378006873}, 'rouge-2': {'f': 0.25490617382998254, 'p': 0.6250847457627118,'r': 0.16009628610729024}, 'rouge-l': {'f': 0.3809046695699165, 'p': 0.7891780821917809,'r': 0.2510344827586207}}]
-----------------------------------------------------------------------------------------------------------------------------------
p369:
Extractive Summary:
in sub-saharan africa, it is predicted that prevalence of overweight and obesity will increase over the next few decades at a faster pace than elsewhere in the world [2].in kenya, current prevention and control strategies for overweight and obesity focus on both the individual and environmental factors that predispose one to high body mass [7].in addition, various global strategies have been proposed to guide action on the prevention and control of overweight and obesity [9].against this background, we designed a study that applies modelling techniques to evaluate a selected number of strategies to generate research-based evidence on the most impactful and cost-effective strategies for the prevention and control of overweight and obesity in kenya.context this study was planned in the context of an overall research agenda to identify the most impactful and costeffective strategies for the prevention and control of overweight and obesity in kenya.a snowball method ensued with assistance from stakeholders from two leading civil society organizations involved in decisions for health in kenya.this target was largely guided by the description of stakeholders (see table 1).the university and ministry of health stakeholders were purposively selected as they were considered resourceful in the workshop moderation and planning process.the objectives of this meeting were: 1) to provide an opportunity to receive immediate feedback on the workshop that had taken place, 2) to discuss the feedback and document recommendations for areas that required improvement.the second was a three-hour post-workshop meeting which was held between the field researcher and one stakeholder from the university sector.this was held several days after the workshop.the objective of the 2nd meeting held after the workshop was to develop a plan for the workshop report writing and subsequent publication of the workshop deliberations.the stakeholder from the humanitarian aid sector passed an apology on the day.the stakeholder present from the university sector was considered adequate to achieve the meeting’s objective.we present the outcomes of these meetings under our immediate outcomes section.the workshop was conducted in english.on arrival, the participants filled out a registration form, read through the informed consent form and each signed a copy.initially a total of 3 tables were occupied.for quality purposes, this larger discussion session was facilitated by the field researcher.a stakeholder had been assigned this role and been taken through an in-depth briefing before the workshop day.however, on the day, he sent an apology due to an urgent work commitment.in the presentations and deliberations, the participants were given the right to withdraw or add any identified strategies.once the two subgroups had presented their discussions, we considered this our level of saturation for that workshop activity.we then carried out the final workshop activity that involved the ranking of the selected strategies.all the identified strategies were listed and displayed at the front of the room.we present the displayed lists as supplementary file 1 to this publication.each stakeholder present was asked to identify the top three strategies that they propose for inclusion in our ace modelling study and rank them from number 1 to 3, with 1 being the strategy with the highest priority.each participant was given 3 colored stickers; gold, green, and blue.each was asked to stick the golden sticker against the strategy that one ranked as number 1 (highest ranking).the green stickers against the strategy that one considered as number 2 and the blue sticker for the strategy that one ranked as number 3 strategy.the entire session lasted for about 3 h. to complement the flip chart recording, the workshop assistant and the field researcher took down notes.additionally, with consent from the participants, the presentations to the larger group were audiorecorded.data management the workshop discussions were transcribed verbatim.two authors (mnw, lkb) verified the validity of the transcription by listening to the audio recordings and comparing them with the transcripts.one author (mnw) did the necessary updates and corrections to the transcripts.this was checked by another author (lkb).the trustworthiness of our findings was enhanced by reading the transcriptions, flip chart recordings, and workshop notes multiple times.as a research team, we applied reverse coding to put a weighting on the ranking done.we assigned a weighted score of 3 to any strategy that was ranked one (gold sticker) and strategies ranked second (green sticker) was assigned a weighted score of 2 and those ranked third (blue sticker) were assigned a weighted score of 1.a tally was done, and the total score put against each strategy.we then ranked the strategies from highest to lowest.this stakeholder engagement process was approved by the griffith university human research ethics committee (gu ref no: 2019/707).all methods were performed in accordance with the relevant guidelines and regulations in the griffith university research ethics manual.results processes stakeholder recruitment and composition out of the twenty-three stakeholders who confirmed attendance of the one-day workshop, fifteen (65%) participants were present.we indicate those who were present at the workshop and those who were absent with an apology.immediate outcomes outcomes from the planning meetings held before the workshop in the meetings that took place before the workshop, several recommendations were made and incorporated in planning for the workshop.we present a list of these recommendations and actions taken in table 3.remarks captured included their thoughts on the effectiveness of current strategies, comments regarding their appropriateness, relevance, and feasibility.the total weighted scores guided the ranking process where strategies with higher weighting ranking top and those with lesser weighting appearing lower in the ranking.we reflected on the feedback comments given at the workshop where the participants had expressed their willingness to remain engaged in the rest of the research.the participants had remarked that it was great to have been involved at the very early stage of research.in the future, the engagement sessions would be held in the middle of the week, with a very early morning start time.during this meeting, the field researcher and one stakeholder dispensed transport tokens to the workshop participants through the mobile money platform (m-pesa).each stakeholder received a token of kshs.2000.the award of token was decided upon during the field research and was not part of the original research plan.in the 2nd meeting held after the workshop, a plan for the report writing was prepared.tentative publications of the workshop deliberations were discussed in detail.a plan for the data transcription process was prepared.intermediate and long-term outcomes we are still in the early phase of our research and engagement process.we are therefore not able to assess and report intermediate and long-term outcomes.a second email was sent a few weeks later to share all the slide presentations used in the workshop and photos taken during the workshop.these are strategies that focus on change at the environmental level by targeting various systemic and environmental drivers of the obesity burden such as policy and economic systems that enable and promote high growth and consumption, and food supply and marketing environments that promote high energy intake.a final axample is the aidem [33] qualitative study that described the views of 27 stakeholders on criteria and processes for (health) priority setting in norway.
number of words= 1154
[{'rouge-1': {'f': 0.3551293048690007, 'p': 0.805632183908046,'r': 0.22776499589153656}, 'rouge-2': {'f': 0.22668493127660377, 'p': 0.45076923076923076,'r': 0.15141447368421052}, 'rouge-l': {'f': 0.2946504029623176, 'p': 0.5660000000000001,'r': 0.1991666666666667}}]
-----------------------------------------------------------------------------------------------------------------------------------
p370:
Extractive Summary:
recent practice guidelines for ntm-pd recommend treatment initiation rather than “watchful waiting” for patients meeting the clinical, radiographic, and microbiological criteria for diagnosis, especially in the context of positive sputum acid-fast bacilli smears and/or the presence of cavitary lesions [3].methods study design and patient selection this retrospective cohort study analyzed the medical records of patients with ntm-pd who met the diagnostic criteria of the american thoracic society/european respiratory society/european society of clinical microbiology and infectious diseases/infectious diseases society of america guidelines [3] and underwent surgical resection as adjunctive therapy between january 1, 2006, and december 31, 2020, at seoul national university hospital in south korea.recurrence was defined as the re-emergence of at least two positive mycobacterial cultures from respiratory specimens following negative culture conversion after surgery.statistical analysis categorical variables were reported as frequencies (percentages), while continuous variables were summarized as medians with interquartile ranges (iqrs).patient groups were compared using pearson’s chi-square or fisher’s exact tests for categorical variables and mann– whitney u tests for continuous variables.univariate logistic regression analysis was performed to identify the potential risk factors associated with refractoriness or recurrence.p < .05 was considered statistically significant.the most prevalent ct pattern in the study group was cavitary nodular bronchiectasis (34; 50.7%), followed by non-cavitary nodular bronchiectasis (19; 28.4%) and fibrocavitary bronchiectasis (14; 20.9%).forty-nine (73.1%) patients showed bilateral involvement of the disease, while seven (10.4%) patients showed single-lobe involvement.before surgery, most patients (61; 91%) received antibiotic treatment for ntm-pd and among them, 58 patients (95.1%) were treated with guideline-adhering regimens: a combination of clarithromycin or azithromycin, rifampin, and ethambutol with consideration of amikacin or streptomycin for mac; a combination of isoniazid, rifampin, and ethambutol for m. kansasii; and multidrug regimens including macrolides, amikacin, imipenem, or cefoxitin for mabc [1].the surgical procedures included pneumonectomy (4; 6.0%), bilobectomy with wedge resection (1; 1.5%), bilobectomy without segmentectomy or wedge resection (1; 1.5%), lobectomy with segmentectomy or wedge resection (15; 22.4%), lobectomy without segmentectomy or wedge resection (18; 26.9%), segmentectomy with wedge resection (11; 16.4%), segmentectomy without wedge resection (5; 7.5%), and wedge resection (12; 17.9%) (table 2).the number of ntmpd- related hospital admissions before and after surgery was 17.1 and 22.8 admissions per 100 person-years, respectively (p = .119).the number of emergency room (er) visits before and after surgery was 20.7 and 18.3 visits per 100 person-years, respectively (p = .544).three patients had cavitary lesions; the other two showed extensive bronchiectasis and parenchymal destruction.this patient was consequently treated with itraconazole for 1 year, while others were closely followed without the administration of any antifungal agent.fungal organisms were not isolated from the sputum cultures studied in two other patients.following surgical resection, 70% of the patients who were refractory to prior antibiotic treatment achieved initial negative culture conversion.although approximately 13% of patients experienced postoperative complications, all of them recovered with appropriate measures and none died due to surgical resection.unfavorable outcomes were associated with female sex, preoperative positive mycobacterial culture, and residual lesions after surgical resection.while these are acceptable outcomes, the culture conversion rate in our study was lower than those reported previously [9–13, 15–20, 25, 26].this may be explained by different patient compositions and outcome definitions across studies.moreover, unlike other studies, we took a more conservative approach of assessing ‘true’ postoperative negative culture conversion among those who showed persistent positive mycobacterial cultures before surgery.although postoperative complications occurred in 13.4% of patients, all were managed without enduring morbidity.only one patient required an additional surgical procedure, while the others recovered with conservative management.the postoperative complication rate was the lowest when most patients were treated with vats (additional file 1: table 3).chronic pulmonary aspergillosis (cpa) following ntm-pd, which is caused by aspergillus species, has been increasingly reported.the risk factors associated with concomitant cpa in ntm-pd include the presence of fibrocavitary lesions or emphysema and the use of corticosteroids [29].this result emphasizes the importance of an awareness of combined fungal infections when treating ntm-pd.in this study, female sex, preoperative positive mycobacterial culture, and postoperative residual lesions were associated with ntm-pd refractoriness or recurrence after surgery.the unfavorable outcomes in female patients could be explained in terms of female predilection for ntm-pd [31, 32], which might lead to higher recurrence rates in female patients.in this study, all of the recurrent cases were women.interestingly, previously reported risk factors such as old age, longer period from initial medical treatment to surgery, and infection by non-m.avium species were not predictors of outcome in the present study.similar to previous studies, we confirmed the importance of residual lesions after surgery [17, 33, 34].the most common radiographic feature of residual lesions was non-cavitary nodular bronchiectatic (69.6%), followed by cavitary nodular bronchiectatic (21.7%) and fibrocavitary pattern (2.2%).this retrospective cohort study was conducted in a single institution.a relatively small number of patients were identified over 15 years, which might imply a selection bias, where only tolerable patients with few comorbidities and good functional status were selected for surgery.treatment outcomes were evaluated according to a widely used operational definition, which is mainly based on expert consensus.thus, cautious interpretation of the results is advised.
number of words= 833
[{'rouge-1': {'f': 0.40632286995515693, 'p': 0.7366666666666666,'r': 0.2805263157894737}, 'rouge-2': {'f': 0.23247046497987348, 'p': 0.3796085409252669,'r': 0.16753363228699553}, 'rouge-l': {'f': 0.3700120651036415, 'p': 0.589774011299435,'r': 0.2695661605206074}}]
-----------------------------------------------------------------------------------------------------------------------------------
p371:
Extractive Summary:
however, 10–42% of chronic cough patients cannot clarify the etiology or receive efective treatment measures [5], causing refractory or persistent cough.recently, it was suggested that type 2 diabetes may be a risk factor for chronic cough and that self-reported chronic cough was more common in diabetic patients than in the general population [8].because increased cough sensitivity is a common feature of all chronic cough patients, it is also called cough hypersensitivity syndrome [10, 11].tis study tested the hypothesis by comparing the capsaicin cough sensitivity, induced sputum cell counts, and infammatory mediators in induced sputum supernatants among mets patients with osahs, mets patients without osahs, and healthy control populations and explored the possible mechanisms underlying the increased capsaicin cough sensitivity in mets patients.mets with osahs group: a total of 29 mets patients diagnosed in the department of endocrinology and metabolism at tongji hospital from december 2017 to may 2020 were included in this group.moreover, these patients were diagnosed with osahs by polysomnography (psg).2. mets without osahs group: a total of 22 mets patients who were diagnosed in the department of endocrinology and metabolism at tongji hospital from december 2017 to may 2020 were included in this group.te above tests were also completed in the healthy controls.statistical analysis was performed with spss version 20.0 (a relatively newer version).p<0.05 was considered statistically signifcant.results general information, pulmonary function indicators and psg monitoring results for the 3 groups te general information, pulmonary function indicators, c5 and psg monitoring results for each group are provided in table 1.waist circumference, systolic pressure, diastolic pressure, triglycerides, high-density lipoprotein-cholesterol, and fasting blood glucose were signifcantly diferent among the 3 groups.te ahi, minimum oxygen saturation, and average oxygen saturation were signifcantly different among the 3 groups.te bmi for the 3 groups was signifcantly diferent (f=13.892, p=0.000).te bmi for the mets with osahs group was signifcantly higher than that for the mets without osahs and the control groups (p=0.001, p=0.000).te gerdq scores were signifcantly diferent among the 3 groups (f=11.604, p=0.000) and were signifcantly higher in the mets with osahs and the mets without osahs groups than in the control group (p=0.000, p=0.007).ess scores were signifcantly diferent among the 3 groups (h=19.985, p=0.000).te ess score for the mets with osahs group was higher than  those for the mets without osahs and control groups (p=0.000, p=0.000).comparison of capsaicin cough sensitivity among the groups tere was a signifcant diference in c5 among the 3 groups (h=14.393, p=0.001).te c5 concentrations for the mets with osahs and mets without osahs groups were signifcantly lower than that for the control group (p=0.001, p=0.001); however, there was no diference between the 2 mets groups (p=0.750).comparison of induced sputum cell counts and cell classifcation among the groups tere was a signifcant diference in the proportion of neutrophils among the 3 groups (h=14.056, p=0.001).te proportion of neutrophils in the mets with osahs and mets without osahs groups was signifcantly higher than that in the control group (p=0.004, p=0.001); however, there was no diference between the 2 mets groups (p=0.168, fig.  2a).te proportion of macrophages were signifcantly diferent among the 3 groups (h=21.468, p=0.000) and signifcantly lower in the mets with osahs and mets without osahs groups than in the control group (healthy subjects) (p=0.000, p=0.000); however, there was no diference between the 2 mets groups (p=0.372, fig. 2b).comparison of infammatory mediators in the supernatant of induced sputum among the groups tere was a signifcant diference in cgrp concentration among the 3 groups (f=17.697, p=0.000).te cgrp concentration in the mets with osahs and mets without osahs groups was signifcantly higher than that in the control group (p=0.000, p=0.000).te sp concentration was signifcantly diferent among the 3 groups (f=5.892, p=0.008) and signifcantly higher in the mets with osahs and mets without osahs group than that in the control group (p=0.002, p=0.038).tere was a signifcant diference in il-8 concentration among the 3 groups (f=14.340, p=0.000).il-8 concentration was signifcantly higher in the mets with osahs and mets without osahs groups than that in the control group (p=0.000, p=0.000).te pepsin concentration in the mets with osahs group was signifcantly higher than other two groups (table 2).in terms of exogenous factors, there was no correlation between lgc5 and waist circumference, blood pressure, gerdq score, ess score, neck circumference, history of past rhinitis, history of past gastritis, feno, lung function indicators, regardless of whether mets patients had osahs or not.the multiple linear regression of capsaicin cough sensitivity triglycerides, age, gender, fasting glucose, gerdq score, feno, ahi, systolic blood pressure, pepsin concentration and bmi were included as independent variables into the multiple linear regression.te results showed that triglycerides, ahi, pepsin concentration and bmi were negatively correlated with cough threshold which as shown in table 3.discussion tis study found that the capsaicin cough sensitivity was increased in mets patients independent of the presence osahs, manifesting as a decreased cough threshold accompanied by an increased percentage of neutrophils in induced sputum and an increased concentration of il-8, sp, cgrp and pepsin in the supernatant; additionally, the gerdq scores for mets patients was signifcantly higher than those for the control group.triglyceride, ahi, pepsin concentration and bmi are risk factors for increased capsaicin cough sensitivity and the pepsin concentration in the mets with osahs group was higher than that in the mets without osahs group.tis study indicated that the capsaicin cough sensitivity of the mets with osahs group was higher than that of the control group.te capsaicin cough sensitivity of the mets with osahs group was negatively correlated with the ahi, suggesting that increased capsaicin cough sensitivity in mets patients is closely related to osahs.studies have found that nighttime refux events in osahs patients are increased and that the incidence of nighttime refux events is positively correlated with the ahi [27].in this study, the pepsin concentration in the mets with osahs group was higher than that in the mets without osahs group, indicating that in mets patients, osahs may aggravate the severity of gastroesophageal refux.te results of this study also indicated that history of rhinitis, concentrations of ecp, and histamine in induced sputum were not signifcantly diferent among the 3 groups, suggesting that the increase in capsaicin cough sensitivity in mets patients was not signifcantly related to rhinitis.our study also found that the capsaicin cough sensitivity of patients with mets without osahs signifcantly increased, indicating that increased cough sensitivity in mets patients was not simply caused by the presence of osahs.in patients with diabetes, esophageal motility is reduced, peristaltic waves are reduced or absent, reverse peristalsis can occur, and gastroesophageal refux-related symptoms are present [32, 33].we further explored the factors afecting the cough sensitivity of mets patients through a multiple linear regression model, and fnally confrmed triglyceride, ahi, pepsin concentration and bmi as the risk factors, which verifed our conjecture to a certain extent.in this study, the increase in il-8 concentration in the supernatant of induced sputum from mets patients also provided evidence of neutrophilic airway infammation.further studies need to be conducted to confrm the upregulation of rage expression in the lower airway of mets patients, which results in an increase in the proportion of neutrophils.te classical mechanism of cough hypersensitivity involves the infammatory-mediator activation of trpv1, which stimulates the vagal aferent terminals, resulting in neurogenic airway infammation, leading to the release of neuropeptides such as sp and cgrp which stimulates local nerves as well as transmitting signals to the central nervous system, thereby increasing cough sensitivity [42].first, induced sputum from mets patients was not retested after treatment to further confrm the causal relationship between neutrophilic airway infammation and cough.te specifc refux property and severity were not clear, but we explored the pepsin concentration in the induced sputum supernatant and the gerdq score and evaluated gastroesophageal refux from the aspects of objective detection and subjective evaluation.in summary, mets patients are susceptible to cough.increased capsaicin cough sensitivity in these patients is closely related to sleep apnea and gastroesophageal refux.in mets patients without osahs, gastroesophageal refux is an important factor that increases capsaicin cough sensitivity.airway infammation, especially airway neurogenic infammation, may plays a role in increased capsaicin cough sensitivity in mets patien
number of words= 1339
[{'rouge-1': {'f': 0.33580686066815285, 'p': 0.8181481481481481,'r': 0.21125874125874128}, 'rouge-2': {'f': 0.2384088509559804, 'p': 0.5198141263940521,'r': 0.15467459762071378}, 'rouge-l': {'f': 0.4043337479664303, 'p': 0.6645945945945946,'r': 0.2905513784461153}}]
-----------------------------------------------------------------------------------------------------------------------------------
p372:
Extractive Summary:
exposure to supra-physiological fio2 (0.5–0.6) has been demonstrated to augment lipopolysaccharide and ventilator-induced lung injury in preclinical models [3, 4].the potentially harmful effects of alveolar hyperoxia were also demonstrated in a clinical trial of mechanically ventilated patients with septic shock.although this trial was discontinued early, there were fewer ventilator-free days and more serious adverse events in the group allocated to receive maximal oxygen therapy ( fio2 1.0) [5].in an open-label randomized controlled trial of adult patients admitted to the intensive care unit (icu), conservative oxygen therapy (defined as the lowest possible fio2 to maintain pao2 between 9.3 and 13.3 kpa (70–100 mmhg) or arterial oxyhemoglobin saturation measured by pulse oximetry ( spo2) 94–98%) was reported to reduce icu mortality when compared with standard care [11].these data demonstrate that there is still uncertainty about the optimal oxygen titration targets for patients with ards.all patients meeting the berlin definition of ards [17], regardless of severity, with at least 12 h of available data were included in the study.this study was conducted within the belfast health and social care trust, who determined this study as an audit because patient management was not altered, only routinely collected data were used and the data were fully anonymized.the primary exposure of interest was the average time-weighted pao2 calculated over a maximum of 7 days from meeting ards criteria [20]: t-pao2 = mean pao2 × time (between consecutive timepoints where pao2 was measured).hyperoxaemia was defined prospectively as a pao2 greater than 14 kpa (105 mmhg) [11].excess oxygen exposure was defined as any fio2 > 0.5 in patients with a pao2 > 10.7 kpa (80 mmhg), and was calculated at 24-h intervals with a cumulative value obtained [16].outcomes the primary outcome was icu mortality.secondary outcomes included duration of ventilation and hospital mortality (both censored at 60 days).data are presented as means (standard deviation, sd), median (interquartile range, [iqr]) and number (percentage, %) as appropriate.otherwise, the wilcoxon‐mann‐whitney was used when normality was violated.a p < 0.05 was considered statistically significant.results of regression models for icu and hospital mortality are shown as odds ratio (or) [95% confidence interval (ci)].the relationship between mortality and either average timeweighted pao2 or highest pao2 was modelled through a average time-weighted pao2 t-pao21 + t-pao22+· · ·t - pao2x t-total quadratic trend.where variables were subject to collinearity, a preference was made to select averagetime weighted pao2 in the models because it was the primary exposure of interest.early hyperoxaemia was selected as an exposure because it was an potential early event in the disease course that has been previously demonstrated to be associated with increased mortality in mechanically ventilated patients without ards [19].the causal relationship was assessed in three steps: (1) indirect or mediated effect: regression models with the two mediators (high lactate and baseline tidal volume) as dependent variables and the exposure (hyperoxaemia in the first 24 h) as the independent variable together with baseline fio2 and peep as confounders; (2) effect of mediators on outcomes: regression model with icu or hospital mortality as dependent variable and each mediator and confounders as independent variables; (3) estimation of mediators effect as a combination of both steps 1 and 2.following clinical review for suitability, 202 patients were confirmed as having ards with at least 12 h of available data, and therefore included in the final analysis.a full list of exclusions is provided in the additional file 1.overall icu mortality was 31%, and overall hospital mortality was 38%.in contrast, during the first 7 days after meeting ards criteria, the highest recorded p/f ratio was lower in nonsurvivors than in survivors (38.2 kpa [iqr 26.4, 44.6] (286.5 mmhg [198.0, 334.5]) vs. 41.7 kpa [35.1, 49.5] (312.8 mmhg [263.3, 371.3]); p = 0.004), and non-survivors also had fewer hyperoxaemia episodes (defined as a pao2 > 14 kpa (105 mmhg)) (2 [0.5, 4] vs 3 [1, 6]; p = 0.04).icu mortality in univariable and multivariable analysis, average timeweighted pao2 demonstrated a u-shaped relationship with icu mortality.for values of average time-weighted pao2 < 13.5 kpa (101.3 mmhg), increasing average time-weighted pao2 is associated with a reduced mortality.of the other co-variables, age was associated with increased icu mortality whilst peep was associated with reduced icu mortality (table 3).in contrast, average time-weighted pao2 beyond 13.2 kpa (99.0 mmhg) was associated with increased mortality.duration of mechanical ventilation there was no association between average time-weighted pao2 over 7 days and duration of mechanical ventilation (− 0.05 [− 0.12, 0.02]; p = 0.19) (additional file 1: table s3).in an adjusted analysis, highest recorded p/f ratio was associated with an increased duration of mechanical ventilation (0.01 [0.00, 0.01]; p = 0.04).in an analysis restricted to patients who survived icu, there was no association between average time-weighted pao2 over 7 days and duration of mechanical ventilation (− 0.06 [− 0.15, 0.03]; p = 0.18) (additional file 1: table s5).in multivariable analysis, the use of neuromuscular blockade was associated with an increase (0.52 [0.22–0.82]; p = 0.001), and tidal volume a reduction (− 0.09 [− 0.16, − 0.01]; p = 0.03), in duration of mechanical ventilation in survivors (additional file 1: table s6).therefore, the three conditions for mediation analysis, defined in the methods section, are not satisfied.in a retrospective analysis of two large patient datasets of patients admitted to icus, it was demonstrated that hospital mortality was lowest in patients when their median spo2 was 94–98% [24].exposure to an fio2 set at 1.0 has been shown to be associated with increased mortality in critically ill patients with septic shock [5], and therefore it is possible that the harm demonstrated in this analysis (of an average time-weighted pao2 within the first 7 days of ards out-with 12.5–14 kpa (93.8–105.0 mmhg)) is at least in part mediated by exposure to high fio2.in an effort to better understand whether there was a causal relationship between hyperoxaemia exposure ( pao2 > 14 kpa (105 mmhg)) and clinical outcome, mediation analysis was performed.hyperoxaemia within the first 24 h of ards was selected as a potentially modifiable exposure that occurred early in the disease course, and as early hyperoxaemia had previously been demonstrated to be associated with increased mortality in patients without ards [19].unfortunately, the three conditions for mediation analysis were not met, which may reflect the choice of mediator.this points to the need for improved understanding of the mechanisms of potential harm from hyperoxia and hyperoxaemia.this includes patient co-morbidities, the duration of mechanical ventilation prior to ards diagnosis, the use of oxygen during tracheal intubation, and the frequency of respiratory physiotherapy or recruitment maneuvers, each of which were not included in this evaluation.furthermore, there remains a need to define hyperoxaemia, with a pragmatic definition that can be evaluated by the bedside clinician likely to be of most practical benefit.
number of words= 1114
[{'rouge-1': {'f': 0.35789780426765516, 'p': 0.8361290322580646,'r': 0.22767634854771784}, 'rouge-2': {'f': 0.23435743954533605, 'p': 0.48295546558704455,'r': 0.15471760797342193}, 'rouge-l': {'f': 0.3833994243684042, 'p': 0.6942038216560509,'r': 0.264831013916501}}]
-----------------------------------------------------------------------------------------------------------------------------------
p373:
Extractive Summary:
background te use of chest computed tomography (ct) has shown a greater diagnostic sensitivity and specifcity compared to chest-x ray in patients with sars-cov-2 pneumonia [1].tree main phenotypes on chest ct have been described with potential implications for clinical management: multiple bilateral ground glass opacities (phenotype one),  unhomogeneously distributed atelectasis with peribronchial infltrates (phenotype two) and development of an ards-like pattern (phenotype three)  [2].in the phenotype two and three, a progressive predominance of atelectasis occurs, which might beneft to moderate to higher levels of peep as well as prone position to recruit non-ventilated lung regions although other studies reported conficting results [4, 5].all patients were aged>18 years and informed consent was obtained before performing the chest ct.from this group of patients, we selected those patients who were admitted to the hospital with sars-cov-2 pneumonia diagnosed by clinical, radiology and molecular tests in the nasopharyngeal swab who underwent a chest computed tomography (fig.  1).all patients were instructed to hold breath, and ctpa images were acquired during a single breath-hold.in this sense, one radiologist specialized in sars-cov-2 pneumonia performed a visual assessment based on the single slice in the ct with the largest atelectasis size and compared it to the estimated lung volume, classifying the patients into these three groups.clinical and radiological severity in order to assess the clinical severity of pneumonia, the patient’s score on the curb 65 scale was collected upon hospital admission [7] [see additional fle  1].each of the fve lung lobes was visually scored on a scale of 0 to 5: 0: no involvement; 1: less than 5% involvement; 2: 5–25% involvement; 3: 26–49% involvement; 4: 50–75% involvement; 5: more than 75% involvement.oxygenation and oxygen therapy need sat o2/fio2 ratio from each patient was collected as an indirect estimate of lung oxygenation capacity [11] at two moments: at the time of the chest ct and the worse  ratio during the hospitalization.lung oxygenation capacity, length of in-hospital stay and days with higher oxygen therapy are shown as median (p25, p75).for the analysis of quantitative variables, a normality test was carried out and then an anova test was performed to test if there were diferences between the three groups (or a kruskal wallis test if the data set did not ft the normal distribution).for the analysis of qualitative variables, after checking if the collected variable met the minimum characteristics required, a χ2  test was done.if this analysis was signifcant, two-by-two post hoc comparisons (χ2  or fisher test) were carried out between the groups.statistical analysis was carried out using prism graphpad version 8.0 software.statistical signifcance was assumed for two-tailed p<0.05.indeed, up to 38 patients (24%) showed some degree of atelectasis at chest ct: 30 (19%) patients showed small atelectasis in the chest ct and 8 patients (5%) showed large atelectasis.te worst sato2/fio2 ratio was diferent among groups (p=0.03), being 300 (range 99 to 431) in the patients without atelectasis, 379 (range 247 to 453) in patients with small atelectasis (p=0.05), and 182 (range 113 to 359) in patients with large atelectasis (p=0.37).in this sense, in the group of patients without atelectasis 18 patients (15%) needed to be admitted in the icu, compared to 2 patients (7%) in the group of small atelectasis (p=0.37) and to 6 patients (75%) in the group of larger atelectasis (p < 0.01).te length of in-hospital stay was diferent among groups (p=0.01).furthermore, diferences were found between these last two groups (p=0.02).in-hospital mortality was not diferent among groups (p=0.55).adjusting for pe, (see additional fle  1 for more information), in the no atelectasis group, 19% of the patients with pe deceased compared to 6.9% of mortality in those who did not have pe.discussion in the present study we found that in patients with sars-cov-2 pneumonia the prevalence of atelectasis was 24%.furthermore, the greatest prevalence of atelectasis was mostly composed by mild, segmental or subsegmental atelectasis.no previous study separated smaller and larger atelectasis associated with the clinical outcome.our data suggest that the presence of large atelectasis may be a factor afecting progressive evolution to the ards [16].tis can be explained by the fact that mortality rate of sars-cov-2 pneumonia is not only related to pulmonary injury, but also to development of multiple organ failures [18].however, our hospital has all the computerized medical history data, making data collection very reliable.we did not evaluate complications occurring during the hospital stay, but we do know that there are several diferences between the groups regarding oxygenation, oxygen-therapy needs, icu admission and days of hospitalization.
number of words= 740
[{'rouge-1': {'f': 0.43200399443500914, 'p': 0.758212927756654,'r': 0.30205128205128207}, 'rouge-2': {'f': 0.24047630014538024, 'p': 0.38297709923664125,'r': 0.17526315789473684}, 'rouge-l': {'f': 0.37176663287729667, 'p': 0.6042465753424657,'r': 0.2684732824427481}}]
-----------------------------------------------------------------------------------------------------------------------------------
p374:
Extractive Summary:
in 2007, a genome-wide association study (gwas) of asthma identified single nucleotide polymorphisms (snps) flanking ormdl3 gene on chromosome 17 to be significantly associated with the disease [1].chromosome 17 ormdl3 locus is now recognised as the major predisposing factor for childhood-onset asthma.evidence now indicates that the association between the 17q21 ormdl3 locus and childhood-onset asthma is also limited to those children with hrv-induced wheezing in early life [5], however the mechanism for this interaction remains unclear.importantly, ormdl3 regulates the major hrv receptor, intercellular adhesion molecule 1 (icam1) during the il-1β- stimulated inflammatory response [6].ormdl3 was also found to promotes hrv replication in human epithelial cells [7].is recognized by toll-like receptor 3 (tlr3, also known as cd283) in a variety of cell types, including b-cells, macrophages, dendritic and epithelial cells [11, 12], triggering an innate immune response and recapitulating the major cellular, physiological and molecular changes characteristic of the host inflammatory response to viral infection.recent evidence indicates that il-17a and poly i:c act synergistically to induce production of chemo-attractants cxcl1 and il-8 [14] and release of inflammatory cytokines [15] in bronchial epithelial cells.we established ormdl3 knockdown and over-expression models in a549 epithelial and nhbe cells and stimulated the cells with either poly i:c or il-17a alone or in combination.the inflammatory response measured over a 24-h time course for mono-stimulation experiments, and at 10 h for co-stimulation experiments through mrna levels of key genes and protein levels of il-6 and il-8 in the supernatant.we finally tested the inflammatory cytokine release in ormdl3 knockdown a549 and beas-2b epithelial cells infected by live human rhinovirus (hrv16).the cells were cultured in dulbecco’s modified eagle’s medium (dmem) containing 10% vol/vol (v/v) fbs and 2 mm l-glutamine.the sv-40 immortalised human bronchial epithelial cell line beas- 2b was purchased from american type culture collection (atcc, manassas, va, usa), maintained in lhc-8 medium (invitrogen, paisley, uk) at 37 °c, 5% co2, and sub-cultured twice a week.normal human bronchial epithelial (nhbe) cells were obtained from lonza and cultured in human airway epithelial cell (haec) culture medium (epithelix, switzerland).hrv16 infection in ormdl3 knockdown a549 and beas‑2b cells human rhinovirus (hrv16) (atcc ®vr-283™) was purchased from american tissue culture collection (manassas, va, usa), and propagated in hela cells (atcc®crl-1958™).hrv16 (5 multiplicity of infection, moi) was then applied to cells and incubated for 48 h at 33 °c, 5% co2.tlr3, as indexed by affymetrix human gene 1.1 st transcript cluster 8,098,611, showed no significant difference in baseline gene expression dependent on ormdl3 knockdown status (control group expression 7.21, knockdown group 7.48, p = 5.60 × 10– 02).we examined tlr3 transcript levels in the cells.the transcript abundance of tlr3 did not differ significantly between cells over-expressing or deficient for ormdl3.ormdl3 regulates the poly i:c‑induced inflammatory response in a549 cells in order to determine whether poly i:c can induce an inflammatory response in human airway epithelial cells we stimulated a549 cells with poly i:c at concentrations of 0, 1, 1, 5, 10 and 50 μg /ml for 10 h. poly i:c induced an inflammatory response in a dose dependent manner with supernatant il-6 and il-8 levels increasing with each poly i:c dosage increment (shown in fig. 2a, b).we monitored the cell viability at each poly i:c dosage concentration.in order to assess participation of ormdl3 in the poly i:cinduced inflammatory response we stimulated ormdl3- deficient and ormdl3 over-expressing a549 cells with poly i:c for 10 h. ormdl3 knockdown resulted in a blunted inflammatory response to poly i:c, with relatively reduced il-6 and il-8 release at 10 h (p < 0.05 respectively; fig. 2c, d).these results confirm that poly i:c induces an inflammatory response in airway epithelial cells and identify ormdl3 as a factor regulating the magnitude of this response.after a 48-h transfection cells were stimulated with poly i:c (10 μg/ ml) and supernatant samples collected at 0, 6, 10, 16 and 24 h. il-6 and il-8 production in each condition was measured and compared to their respective controls.in a549 cells under conditions of ormdl3 knockdown il-6 levels were relatively reduced at all time-points except 0 h (baseline), with statistical significance achieve at 10 h only (p < 0.05).a consistent decrease in release from ormdl3 knockdown cells was observed from 10 to 24 h .for ormdl3 over expressing nhbe cells, whilst both il-6 and il-8 levels were elevated relative to control at all time points, these differences did not achieve statistical significance (shown in fig. 3g, h).our results confirm a bidirectional influence of ormdl3 on the poly i:c inflammatory response in epithelial cells.similarly, both poly i:c stimulation and il- 17a stimulation yielded a relatively larger il-8 response in ormdl3 over-expressing cells relative to control when applied independently (p < 0.05), however no difference was observed between control and ormdl3 over expressing cells under conditions of co-stimulation (p = 0.46, shown in fig. 4d).trif encodes an adaptor protein containing a toll/interleukin-1 receptor homology domain, which is an intracellular signalling domain that mediates protein–protein interactions between the tlrs and signal-transduction components.in ormdl3 over-expressing cells at baseline the transcript levels of trif and il1 were significantly raised relative to the control group (p < 0.01, p < 0.05 respectively, shown in fig. 5b).for ormdl3 knockdown cells, trif transcript levels were significantly lower than the control group (p < 0.001) (shown in fig. 5c).for ormdl3 over expressing cells, although transcript levels for all genes were higher than the control group, only the il8 transcript achieved significance (p < 0.05) (shown in fig. 5d).the results show that whilst ormdl3 did not significantly influence transcript levels of tlr3 and ifnb after poly i:c stimulation, it did impact transcript levels of trif and several cytokine genes.the inflammatory response in ormdl3 knockdown epithelial cells after hrv‑16 infection in order to determine whether ormdl3 knockdown in epithelial cells also influences the inflammatory response to live viral infection, we silenced ormdl3 in a549 and beas-2b epithelial cells and infected these cells with hrv16, one of the most common rhinoviruses.as shown in fig. 6, hrv16-dependent induction of il-6 compared with base line was significantly lower in ormdl3 knocked-down a549 cells (p < 0.05) and also bronchial epithelial cell line beas-2b (p < 0.05).discussion viral respiratory infections show robust association with asthma exacerbations in the community.human rhinovirus, in particular, is the predominant virus associated with asthma attacks [16], although other viral pathogens such as rsv and influenza may also contribute to the disease [17, 18].however, in both a549 and nhbe cells, knockdown of ormdl3 led to a blunted poly i:cinduced inflammatory response.conversely ormdl3 over-expression led to an enhanced poly i:c-induced inflammatory response in these cells.the reduction of cytokine release by ormdl3 knockdown was also confirmed in hrv16-infected cells.the il-23/th17 pathway is a central component of cellular immunity and il-17a is a signature cytokine of this pathway [19].il-17a is widely reported to regulate chronic inflammatory diseases, including respiratory diseases such as asthma [20].we show here that il-17a induces an inflammatory response in epithelial cells, and that ormdl3 regulates this response.multi-transcript profiling showed that overexpression of ormdl3 up-regulates trif, suggesting regulation of the inflammatory response through the nf-κb pathway.following poly i:c stimulation control cells showed significant decreases in the transcript levels of ifnb and trif, and increases in the cytokines il1, il8 and il6, indicating that poly i:c is a potent inflammatory inducer.our results indicate that ormdl3 is an important regulator of the inflammatory response to poly i:c in epithelial cells, and that this impact may not be restricted to the tlr3 signalling pathway—since we did not observe the significant differences in the expression of ifnb between ormdl3 knockdown and control cells, or between ormdl3 over-expressing cells and control cells.in this study, we did not observe that ormdl3 directly influenced tlr3 and ifnb transcript level after poly i:c stimulation, but it regulated transcript levels of trif and several cytokine genes.ormdl3 may work in multiple signaling pathways to regulating viral infection through er stress, sphingolipid metabolism, glycolysis and other mechanisms [10].in this report, we found ormdl3 knockdown epithelial cells release less il-6 after hrv16 infection, consistently confirm the results from poly i:c stimulations in ormdl3 knockdown cells.this is the first report on the role of ormdl3 in the poly i:c stimulation response in epithelial a549 and nhbe cells.nevertheless, the observations presented here have significant clinical implications, and warrant further investigation focusing on the precise mechanisms through which ormdl3 moderates the inflammatory response to viral infection.conclusions ormdl3 significantly influences release of cytokines following poly i:c stimulation in airway epithelial cells.
number of words= 1397
[{'rouge-1': {'f': 0.35312943273390085, 'p': 0.8848148148148147,'r': 0.22058179329226557}, 'rouge-2': {'f': 0.23626745033736757, 'p': 0.5198141263940521,'r': 0.15287671232876715}, 'rouge-l': {'f': 0.31987425145630405, 'p': 0.6559375000000001,'r': 0.21150943396226415}}]
-----------------------------------------------------------------------------------------------------------------------------------
p375:
Extractive Summary:
the rinnopari project is a general project designed to study clinical, laboratory, functional, morphological, histological and microbiological characteristics in patients with chronic inflammatory lung diseases, including cf.patients filled in self-administered questionnaires on cough (cough and sputum assessment questionnaire: casa-q), dyspnea (modified medical research council dyspnea scale: mmrc), pain (numeric rating scale: nrs), insomnia (insomnia severity index scale), sleep quality (pittsburgh sleep quality index), daytime sleepiness (epworth sleepiness scale: es), snoring (berlin questionnaire), circadian rhythm (morningness-eveningness questionnaire: meq), anxiety and depression (hospital anxiety and depression scale: had).a score between 0 and 7 is considered not to be clinically significant, a score of 8–14 indicates mild insomnia, a score of 15–21 indicates moderate insomnia and a score of 22–28 indicates severe insomnia [22].it consists of 19 mixed-format questions regarding the time individuals get up and go to bed, preferred times for physical and mental activity, and subjective alertness.the meq score ranges from 16 to 86, with scores above 58 classifying individuals as morning-type and scores below 41 as evening-type [31].international restless legs syndrome scale restless legs syndrome (rls) was diagnosed when the patient satisfied the international rls consensus criteria [24].the severity of rls was evaluated using the irls, which is composed of 10 questions.each question can be classified according to severity, as follows: none (0 point), mild (1 point), moderate (2 points), severe (3 points), and very severe (4 points).the total score ranges from 0 to 40.participants with irls scores < 10 were categorized as mild, 11–20 as moderate, 21–30 as severe, and ≥ 31 as very severe rls [32].transcutaneous carbon dioxide ( ptcco2) was monitored continuously overnight (sentec inc., therwil, switzerland) and was synchronized to the psg.psg recordings were analyzed according to the 2015 update of the american academy of sleep medicine rules for scoring respiratory events in sleep [37].apnea was defined as the absence of airflow ≥ 10 s, hypopnea was defined as reduction of airflow ≥ 30% associated with a decrease in oxygen saturation ≥ 3% or micro-awakening.the apnea–hypopnea index (ahi) was calculated as the number of apneas and hypopneas per hour of total sleep time (tst).the arousal index was defined as the number of electroencephalographic arousals per hour of tst.periodic leg movement (plm) was defined as four or more consecutive, involuntary leg movements during sleep, lasting 0.5–5 s with an interval of 5–90 s. statistical analyses data are expressed as median and range for quantitative variables and as number and percentage for qualitative variables.as the kolmogorov test showed that distributions are not normal, quantititative data were analyzed using a non-parametric wilcoxon–mann–whitney test to assess significance between different conditions.a spearman test was used to study the correlation between isi and had scores.for all analyses, a two-sided p value < 0.05 was considered significant.xlstat software (version 2019.1.3, addinsoft company, paris, france) was used to analyze and reformat data.results patient demographic and clinical characteristics fifty-nine adult patients with cf were referred to our medical center.median forced expiratory volume in one second was 2190 (1530–3667) ml, 72 (39–93) % predicted.median consumption of alcohol, coffee and tea were 0 (0–0) g/day, 1 (0–2) cup/day and 0 (0–1) cup/day, respectively and no patient used energy drinks.median screen time was 6 (4–8) h/day.patient demographic and clinical characteristics are presented in table 1.we identified a strong association between insomnia and had anxiety (r = 0.702, p < 0.0001) and depression score (r = 0.701, p < 0.0001) (table 2 and fig. 1).only one patient (7%) had sleep efficiency < 80%, total sleep time < 360 min, ahi ≥ 15/h and severe nocturnal hypoxemia (mean spo2 during sleep: 87%, with spo2 < 90% during 77% of sleep time).no patient had periodic limb movements.several screening tools can be used to diagnose insomnia.a recent meta-analysis compared the three scales most commonly used to screen for insomnia: psqi, athens insomnia scale (ais) and isi [23] and identified comparable diagnostic properties, with ais and isi providing better results in terms of specificity than psqi.of note, psqi does not directly assess insomnia symptoms, but rather evaluates a broad range of sleep domains affecting sleep quality.ais and isi were developed according to standard insomnia diagnostic criteria.the ais was designed to quantify sleep difficulties based on the international classification of diseases and related health problems-10 (icd-10).the isi captures the diagnostic criteria for insomnia defined in diagnostic and statistical manual of mental disorders-iv (dsm-iv) and the international classification of sleep disorders (icsd).for comparison, the prevalence of insomnia in healthy young adults (25–34 years old) is about 18% in france, according to a large epidemiological study conducted in 2001 [38].a recent study has shown that about one-third of adult cf patients experience rls [16].although anxiety and depression are known to be a cause of insomnia [40], a causal relationship cannot be established between insomnia and anxiety/depression, but the presence of insomnia symptoms should prompt physicians to investigate the presence of and treat anxiety/depression symptoms and vice versa.these results are consistent with those of a study reported by bouka et al. [11], in which lower sleep quality was related to vitality, emotional functioning, social, role, eating disturbances and digestive symptoms.it is noteworthy that these symptoms may also be an expression of anxiety and/or depression.this study has several limitations.it would also have been interesting to assess insomnia by actimetry over several days to obtain objectives measurements on activity and rest periods.the strong association between insomnia, impaired quality of life and higher had score should prompt physicians to be particularly attentive to the management of anxiety and depression in adult cf patients with insomn
number of words= 915
[{'rouge-1': {'f': 0.3593980909476697, 'p': 0.7156692913385827,'r': 0.23994818652849742}, 'rouge-2': {'f': 0.20551639794995244, 'p': 0.3545849802371542,'r': 0.14468879668049794}, 'rouge-l': {'f': 0.3299797191397462, 'p': 0.5611242603550296,'r': 0.23370808678500987}}]
-----------------------------------------------------------------------------------------------------------------------------------
p376:
Extractive Summary:
egfr-mutated lung cancer is reported to comprise approximately 45% of the nsclc cases in asia and approximately 15% in europe and the united states [3–6].sputum cytology positive [sc (+)] was defined as patients whose sputum cytology was class iii or higher.sub‑analysis of risk factors of sc (+) sub-analysis of risk factors for sc (+) in patients with primary lung adenocarcinoma was performed because egfr mutation cannot be detected if the sputum sample does not contain any malignant cells.ct tumor size was defined as the maximum tumor diameter measured using high resolution ct (level 600 hounsfield units [hu]; width 1600 hu) of 1 to 2 mm thickness.the preoperative pet-ct scan calculated the suvmax of the tumor lesion where fluorodeoxyglucose f 18 (18ffdg) accumulated.significance was defined as p < 0.05.statistical analyzes were performed using ezr on r commander version 1.30 (saitama medical center, jichi medical university, saitama, japan).results of the 118 patients who enrolled in this study, the number of patients with sc (+), sc (−), and nc was 13 (11.0%), 76 (64.4%), and 29 (24.6%), respectively.table 1 summarizes the radiological and cytopathological findings, and the egfr mutation status (sputum sample and surgical resected specimen) of 13 sc (+) cases.in addition, allel frequency in samples in which egfr mutation were detected in sputum samples was also shown.stas was detected in surgically resected specimens in 12 of 13 sc (+) cases.ex21 and ex19, and wild-type egfr were observed in 2, 2, and 9 cases, respectively, based on ddpcr of the sputum.among sc (+) samples, the egfr mutation status of the main tumors in ffpe sections and in sputum were identical in 12 cases, and the sensitivity, specificity, and positive predictive value to detect egfr mutations (ex19 and ex21) were 80.0%, 100%, and 100%, respectively (table 2).in one discordant case (case 4), the cytological examination of sputum was class iiia with suspected squamous cell carcinoma.figure 1 shows the results of ddpcr data analysis (1-d plot).ex21 (case 1) and ex19 (case 7) were detected in sputum by ddpcr.the lowest allele frequency in the egfr mutant sc (+) cases was 0.24% (table 1).in all three patients, stas was detected in ffpe sections of the surgically resected tumors.among 76 patients of sc (−) group, there were 9 cases in which ddpcr could not be performed because of an insufficient amount of dna collected.table 3 presents the comparison of the clinicopathological features between 13 cases of sc (+) and 105 cases of sc (−) + snc.compared to the sc (−) + snc group, the ct tumor size was larger and pet suvmax was higher in sc (+) group.stas was detected more frequently in ffpe sections of resected tumor in the sc (+) group compared to the sc (−) group (92.3% vs 34.3, p < 0.001).there was no significant difference in the egfr mutation status between the two groups (p = 0.902); there were 61.5% and 43.8% of wild-type egfr in sc (+) and sc (−) + snc group, respectively.figure 3 depicts the results of roc curve analysis for discrimination of the sc (+) and sc (−) + snc groups.the area under the roc curve (auc) regarding ct tumor size was 0.823 (95% confidence interval [ci]: 0.717–0.929).the auc regarding pet suvmax was 0.809 (95% ci 0.718–0.901).there was no significant difference between the auc of both groups (p = 0.809).the cut-off value of ct tumor size was 29 mm in roc curve analysis, and the sensitivity and specificity were 69.5% and 84.6%, respectively.the cut-off value of pet suvmax was 3.02, and the sensitivity and specificity were 58.1% and 100%, respectively.multivariate analysis based on a logistic regression model revealed that ct tumor size (odds ratio = 10.6, 95% ci 1.85–61.0, p = 0.008) and stas (odds ratio = 17.7, 95% ci 1.97–158, p = 0.010) were independent potential predictive factors for sc (+) (table 4).discussion this is the first report of the detection of egfr mutations of primary lung adenocarcinoma using ddpcr from prospectively collected sputum samples and compared it with an egfr mutation in surgically resected lung cancer.egfr mutations can be detected with high sensitivity by ddpcr from sputum sample if the sputum cytology is positive.since a ct tumor size ≥ 29 mm is a potential predictive factor for sputum cytology positive, sputum should be collected in such cases for the egfr mutation analysis.bronchoscopic, ct-guided, and surgical biopsies are currently performed in clinical practice to obtain tumor tissue for molecular analysis.however, these methodologies are invasive.an overall complication rate of 1.55% was reported for bronchoscopic biopsy, and included bleeding (0.63%) and pneumothorax (0.44%), with a mortality rate of 0.003% [8].the reported rates of mortality and serious complications for ct-guided percutaneous needle biopsy were 0.07% and 0.75%, respectively, with complication rates of 35% for pneumothorax [9, 10].plasma can be collected with minimal invasion from lung cancer patients to detect egfr mutations in cfdna.however, there is a possibility that egfr mutations cannot be detected in patients with egfr mutant lung cancer if the amount of cdna is below the threshold of detection sensitivity [12].for this reason, liquid biopsy is performed if a tissue biopsy cannot be performed in patients with poor conditional status.the use of sputum may be another option to detect egfr mutations in patients who are in poor condition.bronchial cell hyperplasia, reactive/atypical bronchial cell, or squamous metaplasia that might be misinterpreted as malignancy, can often be observed in respiratory cytology specimens obtained from patients with asthma, chronic obstructive airway disease, inflammatory disease of the lung, and past history of lung disease chemotherapy/ radiation/surgical treatment [26, 27].when performing egfr mutation analysis in combination with sputum cytology, it is necessary to understand the cytomorphologic features and past history of lung disease in these patients.hubers et al. reported that the sensitivity of egfr mutation detection was 30 to 50% in 10 sputum samples using four different egfr mutation analyses (cycleave pcr, cold-pcr, pangaea biotech sl technology, and high resolution melting) [29].presently, ddpcr analysis of 80 sputum samples revealed an 80.0% detection sensitivity for egfr mutations if the sputum cytology was positive.recently, wang et al. reported that detection sensitivity and specificity for egfr mutations of 46.2% and 100%, respectively, as detected using superarms using sputum cell-free dna from 102 sputum samples [32].it has been reported that the detection sensitivity of lung cancer in sputum is increased by a longer duration of sputum collection and with the induction of sputum by nebulization with hypertonic saline [33, 35].a future study should examine whether the sensitivity of egfr mutation detection can be improved by different sputum collection methodologies.the detection sensitivity of lung cancer in sputum cytology was reported to be 40 to 66% [13, 14].sputum cytology is highly effective for central type squamous cell carcinoma in patients with hemoptysis [16].sing et al. reported that the detection rate of sputum in adenocarcinoma in 64 patients was 25.0% [14].risse et al. similarly reported that the detection sensitivity of primary lung cancer was high when the tumor size exceeded 24 mm [16].performing sputum cytology for tumors that display these ct findings may increase the detection sensitivity of sputum cytology in patients with primary lung cancer.further studies are needed to investigate the clinicopathological factors of lung cancer patients who are clinically relevant for sputum-based egfr mutation testing.detection of t790m for the indication of osimertinib is especially important in clinical practice.the detection of t790m from sputum may play an important role similar to that of cell-free dna in plasma.further studies are needed to demonstrate the clinical usefulness of t790m detection in sputum using ddpcr in patients with primary lung cancer undergoing egfr-tki treatment.further research also is necessary to compare the detection rates of egfr mutations between sputum and cell-free dna in plas
number of words= 1269
[{'rouge-1': {'f': 0.3365507900425042, 'p': 0.8380000000000001,'r': 0.21055636896046853}, 'rouge-2': {'f': 0.2171648686623324, 'p': 0.46357429718875504,'r': 0.1417948717948718}, 'rouge-l': {'f': 0.30604932570040994, 'p': 0.6389655172413793,'r': 0.2012127236580517}}]
-----------------------------------------------------------------------------------------------------------------------------------
p377:
Extractive Summary:
since the first reported successful endoscopic removal of foreign bodies by mckechnie in 1972 [6], endoscopic treatment has become the first-choice treatment for foreign bodies in the upper digestive tract [7, 8].flexible endoscope and rigid endoscope are both widely used.mouse forceps and snares are the most commonly used auxiliary devices to improve the success rate of treatment and reduce the incidence of operation-related complications.few researches reported on the migration of esophageal foreign bodies into the bronchus.the joint management of foreign bodies by esophagoscope and bronchoscope faces a series of challenges.here we reported a successful case.an elderly female patient who accidentally ingested a fish bone incarcerated in the esophagus and delayed medical treatment, leading to the fish bone puncturing through the esophageal wall and crossing the left main bronchus.under general anesthesia, holmium laser assisted fiberoptic bronchoscope was used to successfully remove the fish bone when the expected effect was not achieved by rigid esophagoscopy.it is hoped that this study can attribute to the management of some special cases of esophageal foreign bodies in the future.case presentation a female patient, 72 years old, was admitted for “retrosternal pain for 4 days during eating” with a healthy history before.four days ago, the patient had a foreign body sensation in the pharynx after eating freshwater fish.then she ate as usual but felt retrosternal pain when eating.one day ago, the pain worsened after eating rice cake and was accompanied by severe irritant dry cough without fever, hemoptysis, or hematemesis.the patient was admitted to regional medical center for national institute of respiratory disease, sir run run shaw hospital, school of medicine, zhejiang university.chest plain computed tomography (ct) scan suggested “flat esophagus and bronchial bifurcation with a high-density shadow, a foreign body was considered with the length of 2.8 cm that crossed the left main bronchial wall, and the local bronchial wall mucosa thickened” (figs. 1, 2).treatment was supportive with fasting, fluid infusion, monitoring and so on.during the operation, a rigid esophagoscope (s121; hangzhou tonglu medical optical instrument co., ltd, zhejiang, china) was used under general anesthesia with endotracheal tube.it was observed that the mucosa ulcer was at the 2 o’clock direction in the esophagus 26 cm away from the incisors, with hyperemia and swelling of the surrounding mucosa.no obvious foreign body was found in the cavity.under a fiberoptic bronchoscope (bf-260; olympus optical co., ltd, tokyo, japan) via the endotracheal tube, a needleshaped foreign body (considering fish bone in combination with medical history) in the proximal part and near the entrance of left main bronchus could be seen intraoperatively, and penetrated the bronchial wall.local bronchial wall showed granulation tissue hyperplasia, mucosal swelling and easy bleeding (fig. 3).after the granulation tissue was cleaned intraoperatively and the fish bone was exposed, the holmium laser (versapulse 80/100 w powersuite; lumenis ltd, israel) with the settings of 1 j and 8 hz was used to broke the fish bone in the middle which was located in the left main bronchus, and foreign body forceps (jhy-fg-18-120-a4; jiuhong medical instrument co., ltd.changzhou, china) were used to remove the two segments, respectively.there was no significant active hemorrhage in the bronchus after operation (fig. 4).the entire operation under the fiberoptic bronchoscope lasted for 20 min.
number of words= 530
[{'rouge-1': {'f': 0.44004936418025886, 'p': 0.7366666666666666,'r': 0.3137275985663083}, 'rouge-2': {'f': 0.20960445404686723, 'p': 0.3113793103448276,'r': 0.1579712746858169}, 'rouge-l': {'f': 0.3460704740526618, 'p': 0.5223809523809524,'r': 0.25874172185430466}}]
-----------------------------------------------------------------------------------------------------------------------------------
p378:
Extractive Summary:
introduction in december 2019, an outbreak of novel coronavirus pneumonia (covid-19) caused by sars-cov-2 was reported in wuhan city, china.since then, covid-19 had rapidly spread to more than 17.6 million cases, with over 680,000 deaths worldwide as of august 2, 2020 [1].as the hardest-hit city by the covid-19 pandemic, wuhan initiated a metropolitan-wide quarantine on january 23, 2020, which terminated all public transportation in the city and intercity links.the risk factors for covid-19 progression include comorbidities with chronic diseases (hypertension, diabetes, cardiovascular disease and liver disease), old age, low lymphocyte and albumin counts and elevated levels of lactate dehydrogenase, c-reactive protein, red blood cell distribution width, blood urea nitrogen and direct bilirubin [14].further, older age, d-dimer level greater than 1 μg/ml and a high sequential organ failure assessment score on admission would contribute to a higher in-hospital fatality of covid-19 patients [17].liang et al. [18], based on chest radiography abnormality and nine clinical indicators were able to predict the risk of developing critical illness with an area-underthe- curve of 0.88.however, most of these studies were limited by their relatively small sample sizes, and many patients had not progressed to the study endpoints by the time the study was conducted, leading to bias and unreliable prediction for disease progression and fatality.besides, some analyses of risk factors were not adjusted for potential confounding effects, leading to false associations.in this study, we retrospectively collected the complete hospitalization information from 2,433 patients who were admitted to huoshenshan hospital during its 73 days of operation.the exclusion criteria were: (1) patients who were not confirmed by a positive result of severe acute respiratory syndrome coronavirus 2 detection in respiratory specimens by the reverse transcriptase polymerase chain reaction assay, or in serum by the specific igm and igg antibody detection; (2) patients who referred to other medical institution during hospitalization; (3) patients who were admitted to the hospital multiple times; (4) patients were younger than 18 years old; (5) patients without laboratory data included in this study within the 24 h after admission.treatment data and clinical outcomes (including the event of disease progression, time of each disease stages, fatality, duration of hospitalization and endpoint status) were also collected during the course from admission to the study endpoints.we defined the event of disease progression as a mild or moderate patient at admission would progress to severe or critical stage at the first time during hospitalization.detailed definitions for clinical symptoms were provided in the supplemental materials.we considered a patient progressing to a severe or critical disease stage when the individual had none of the severe or critical stages at admission but developed these stages for the first time during hospitalization.we conducted survival analyses on disease progression and fatality based on a competing risk framework.fifty patients died during hospitalization, and 2,383 were discharged, corresponding to a case-fatality ratio of 2.1%.patient’s median age was 60.0 years (iqr 50.0–68.0), and 50.2% were male (table 1).of 1,733 moderate patients at admission, 1,259 patients retained moderate and discharged after 11.0 (7.0–16.0) days.in contrast, 474 patients progressed to the severe state in 3.0 (1.0–7.0) days, but all recovered and were discharged after another 12.0 (6.5–18.0) days, and 9 patients deceased after 9.0 (2.5–19.0) days.of 40 critical patients at admission, 20 patients regressed to moderate severity after 10.5 (8.3–15.8) days and were discharged after another 11.5 (6.3–19.5) days, and the remaining 20 patients died after 6.5 (4.0–16.3) days (figs. 1 and 2).across all patients, it required a median of 3.0 (1.8–5.5) days to progress from mild to moderate, 3.0 (1.0–7.0) days from moderate to severe, 3.0 (1.0–8.0) days from severe to critical and 6.5 (4.0–16.3) from critical to fatality.contributing factors to disease progression and covid‑19 fatality of 1,758 mild and moderate patients at admission, 474 (27.0%) progressed to severe or critical severity during hospitalization.the 21-day cumulative incidence of fatality was 13.1% in > 74 years age group, 4.6%, 1.4% in age groups of 60–74, < 60 years respectively.the incidence of fatality at day 21 was four times higher in patients with blood glucose > 6.1 g/l (11.8%) than blood glucose in range of 3.9–6.1 mmol/l (2.7%, fig. 3c, d).discussion our study provides unique progression and outcome data on a cohort of 2,433 covid-19 patients admitted to huoshenshan hospital, a hospital designed and built solely to provide care to patients with covid-19.patients admitted with a greater disease severity requires longer to recover.our report on the time for disease progression at each disease stages allows early preparation and intervention to delay disease progression (fig. 2).but among those who did progress, they progressed to a severe or critical stage within the first 3.0 (1.0–7.0) days after admission.this highlights the importance of close monitoring of key risk indicators for disease progression in the early stages of infection.we speculate that diabetic patients have elevated expression of angiotensin-converting enzyme-2 (ace-2) receptors, making them vulnerable to sars-cov-2 infection.besides, patients living with diabetes or uncontrolled glucose level are likely to have impaired innate immunity due to dysfunction of macrophage and lymphocytes, which may lead to an increased risk of septic shock and multiple organ failures.besides, increased fibrinogen and platelets count concentrations are associated with increased coagulation activity in patients with infection or sepsis [37, 38].second, most patients on admission had moderate severity which may lead to a selection bias when identifying factors that affect progression or fatality.third, considering both the small number of events and the rule of thumb on event per variable > 10, only limited risk factors were included in the multivariable analysis of severe and critically illness at admission to fatality during hospitalization.
number of words= 922
[{'rouge-1': {'f': 0.3507128938997901, 'p': 0.7836363636363637,'r': 0.22590863952333665}, 'rouge-2': {'f': 0.205534465078637, 'p': 0.389634703196347,'r': 0.13958250497017893}, 'rouge-l': {'f': 0.3453888865567593, 'p': 0.6520895522388059,'r': 0.2349048625792812}}]
-----------------------------------------------------------------------------------------------------------------------------------
p379:
Extractive Summary:
candidate genes for asthma are wide-spread throughout the genome.acevedo et al. reported that regional dna methylation and mrna levels at the gasdermin b/ormdl sphingolipid biosynthesis regulator 3 locus were associated with the risk of childhood asthma [10].however, the results of that analysis were only at the dna level [13].we downloaded dna methylation and gene expression data from the geo database, and screened critical diferentially expressed genes (degs) with signifcant methylation changes from samples obtained from atopic asthmatic patients and compared them with samples from healthy controls.te aim of this study is to potentially provide novel diagnostic biomarkers in the nasal epithelia of children with atopic asthma.each set contains 194 peripheral blood mononuclear cell (pbmc) samples of 97 children with atopic asthma and 97 control children.moreover, the pheatmap package (version 1.0.8, https://cran.r-project.org/package=pheatmap) [16] in r software was used to perform the bidirectional hierarchical clustering analysis for the gene expression and methylation values based on euclidean distance [17, 18].gene ontology function and kegg pathway analysis for degs and dmgs initially, we compared the collection of degs and dmgs, kept the intersection of the two data sets, and analyzed the overall correlation between the degree of diference in methylation and expression levels.analysis of protein–protein interaction network string, version 10.5 [21] (https://string-db.org/), was used to search for the interaction between gene product proteins for genes with opposite expression and methylation levels, and an interactive network was built.selection and mechanism analysis of candidate agents in the comparative toxicogenomics database, 2019 update [25] (http://ctd.mdibl.org/), using “asthmatic” as a keyword, we searched for kegg pathways and genes directly related to asthma, and compared them with pathways in which the genes in the constructed interaction network were signifcantly involved in the relevant pathways.a total of 933 (239 downregulated and 694 upregulated) degs and 751 (412 hypomethylated and 339 hypermethylated) dmgs were identifed between the asthmatic and healthy control groups.volcano plots for the degs and diferentially methylated sites were shown in fig. 1a and b. after screening degs and dmgs from the gene expression and methylation profles, the corresponding gene expression and signal values were visualized in bidirectional hierarchical clustering heatmaps (fig.  1c, d).as can be seen in the fgure, the diference between the selected degs and dmgs of the asthma and control groups is signifcant.gene ontology and kegg pathway analysis we screened a total of 284 intersection genes that were diferentially expressed in the dna methylation and gene expression data set, and analyzed the relationship between gene expression and dna methylation changes by calculating the correlation coefcient (fig. 2a).critical gene expression levels and the dna methylome are shown in fig.  2b.among these, there were 35 genes with hypermethylation and decreased expression and 95 genes with hypomethylation and increased expression.te go identifed genes were involved in cellular functions including cellular defense response and oxidation reduction.protein–protein interaction network analysis in the protein–protein interaction network, a total of 119 nodes were identifed.tis included 33 hypermethylated, downregulated genes and 86 hypomethylated, upregulated genes with 426 pairs of co-expression interactions (fig. 3).go functional analysis found that these nodes were primarily involved in functions such as cellular defense response and oxidation reduction.kegg pathways were primarily involved in natural killer cell mediated  cytotoxicity, steroid hormone biosynthesis, and neuroactive ligand-receptor interaction.analysis of mirna–target gene network a total of 73 mirnas that were directly associated with asthma were screened.we screened the target genes of these 73 mirnas, and then compared the target genes with 130 genes whose expression and methylation level difered signifcantly.a total of 635 pairs were screened, and the constructed mirna-mrna regulatory network contains 133 nodes and 635 connected edges (fig. 4).construction of a pathway network directly related to asthma we screened 119 kegg pathways and 116 genes that were directly associated with asthma by searching the ctd database.discussion asthma is a complex multifactorial disease caused by the interaction of genetic and environmental factors.chemokines are a superfamily of secreted proteins involved in immunoregulatory and infammatory processes.ccl2 is a member of the cc subfamily which is characterized by two adjacent cysteine residues.in addition, the samples in these two data sets are all pbmcs, which would be more convincing if they were airway epithelial cells.but our research provides new biological insights into the development of asthma.conclusions in conclusion, our study identifed a total of 130 degs with signifcant dna methylation changes.
number of words= 714
[{'rouge-1': {'f': 0.46129965844173576, 'p': 0.7848148148148149,'r': 0.3266489361702128}, 'rouge-2': {'f': 0.27434073965714684, 'p': 0.4343122676579926,'r': 0.20049267643142477}, 'rouge-l': {'f': 0.40808888817250866, 'p': 0.6069127516778523,'r': 0.30738872403560835}}]
-----------------------------------------------------------------------------------------------------------------------------------
p380:
Extractive Summary:
background rheumatoid arthritis (ra) is a chronic inflammatory disease characterized by inflammation of the synovial membrane.the release of pro-inflammatory cytokines as well as other pro-inflammatory molecules results in joint destruction and disability [1, 2].to date, the exact cause of ra has not been identified but several studies pointed out that pro-inflammatory cytokines, including tumor necrosis factor (tnf)-α, interleukin (il)-1, il-6, il-17 and the mediators produced through downstream pathways in the arthritic joints, constitute the milieu driving cartilage and bone destruction [3].on this basis, therapeutic possibilities for ra patients include monoclonal antibodies, fusion proteins or antagonists against these molecules.however, partial and non-responses to these compounds, together with the increasing clinical drive to remission induction, requires that further therapeutic targets are identified [4].in recent years, a growing number of new cytokines as well as their function in health and disease have been identified [5].cytokines serve as the mediators of cellular differentiation, inflammation, immune pathology, and regulation of the immune response.in particular, novel inflammatory mediators with their associated cell signaling events have now been proven to have a role in experimental arthritis and in ra, including members of the il-1 (il-33, il-36, il-37, il-38) and il-12 (il-27, il-35) superfamilies, and other cytokines such as il-32, il-34.the aim of this review article is to provide an overview on these recently identified cytokines, emphasizing their pathogenic role and therapeutic potential in ra.table 1 summarizes all the available data in animal models and ra patients for each cytokine.new members of il-1 family il-33 il-1 cytokine includes 11 pro-inflammatory and antiinflammatory members, chronologically named according to their discovery, il-1 family member 1 (il-1f1) to il-1f11.however, depending on the cell type and stimulus, il-32 may be released after necrotic cell death or in vesicles such as exosomes [111, 112].one problem that remains associated with il-32 is the identification of cell surface receptor of il-32.il-32 is a pleiotropic cytokine and an important player in innate and adaptive immune responses, involved in a number of biological functions, including cell differentiation, stimulation of pro- or anti-inflammatory cytokines and cell death, especially apoptosis [113].in detail, this cytokine induces other pro-inflammatory cytokines and chemokines such as tnf-α, il-1β, il-6, and il-8 by means of the activation of nf-kb and p38-mapk.il-32, via caspase-3 activity, induces differentiation of monocytes into macrophage-like cells with characteristics of generating proinflammatory cytokines such as il-6, tnfα and chemokines [114].il-32γ, via a phospholipase c (plc)/ c-jun nterminal kinases (jnk)/nf-kb-dependent pathway, induces maturation and activation of dcs, leading to increased production of il-12 and il-6, th1- and th17-polarizing cytokines [115].moreover, il-32 synergizes with nucleotide oligomerization domain (nod) 1 and nod2 ligands for il- 1β and il-6 production, through a caspase 1-dependent mechanism [116].finally, il-32β, increasing adhesion of inflammatory cells to activated endothelial cells with consequent induction of pro-inflammatory cytokines, is involved in the propagation of vascular inflammation [117].its production is predominantly induced by il-1β, tnf-α, il-2 or ifn-γ in blood monocytes and epithelial cells [39].in addition to cytokines, microbial products, including viruses, have emerged as potent inducers of il-32 in human monocytes, macrophages, and monocyte-derived dcs [109].all the above mentioned data clearly point out that il-32 and tnf-α are strongly linked to each other and being tnf-α a key cytokine in ra pathogenesis, il-32 may play profound effects in this process [118].studies from animal models demonstrated that human il-32, when injected in joints of naïve mice, leads to increased expression of inflammatory molecules (il-1β, tnf-α, il-18, ifn-γ, il-17, il-21 and il- 23), recruitment of inflammatory cells, cartilage derangements and joint swelling [119].conversely, joint swelling and presence of inflammatory cells drastically decreased in a tnf-α deficient mouse model [120].this observation further supports the key interplay between tnf-α and il-32 in ra pathogenesis.furthermore, the unmasking of the molecular mechanism of the il-32/ tnf-α in ra open new avenues for their potential therapeutic targeting.several studies confirmed an overexpression of il-32 and il32γ in ra patients, when compared to osteoarthritis or healthy volunteers [121, 122].in particular, the il-32γ level was found significantly upregulated in cd14+ monocytes and synovial membrane of ra patients [123, 124].high levels of il-32 in synovial biopsies of ra, as compared to its absence in oa patients, suggested that il-32 is potent mediator of active osteoclastogenic activity.in particular, the synergism between il-32 and soluble receptor activator of nuclear factor κ-b ligand (srank-l) enhances the activity of osteoclasts and consequently tissue resorption [124].both il-32 and il-17 can reciprocally influence each other’s production and amplify the function of osteoclastogenesis in ra synovium [124].ra fls seem to have a key role in osteoclastic activity, as well as in pannus formation in the joint [125].il-32β, δ, and γ mrna overexpression in ra fls is primarily induced by tnf-α, ifn-γ and toll-like receptor (tlr)-2, −3, and −4 ligands, and the overexpression of il-32 seems to stabilize the mrna transcripts of other cytokines, in particular tnf-α, il-1β and il-8 [110, 126].in fls, tnf-α-activates syk/pkc-d/jnk/c-jun pathway to induce il-32 (isoforms α, β, δ, and γ) [127], suggesting a splicing of il-32γ into il-32β [110]; interestingly, il- 32β is associated with lower inflammation and less severity of ra when compared with il-32γ.il-32 stimulates the synthesis of prostaglandin e2, an important mediator of cartilage and bone destruction in ra [128].very few clinical data regarding il-32 response in patients treated with anti- tnf-α therapy are available; in particular, synovial knee biopsies showed a significant decrease in il-32 expression in ra patients treated with a tnf-α blocker [129].this observation fits with the evidence of a direct correlation between il-32, tnf-α and disease activity in ra [130].additional studies, especially in human systems, are necessary to resolve the inconsistency of il-32 in ra as well as to explore the therapeutic potential of this cytokine in ra.il-34 il-34 has been discovered in 2008 [131] and the receptor to which il-34 binds with the highest affinity, colony stimulating factor (csf)-1r, is shared with csf-1.however, il-34 and csf-1 do not share sequence homology and have different expression patterns being il-34 restricted to few tissues (brain, epidermis, spleen, bone marrow, lymph nodes) and csf-1 widespread [132].upon binding to csf-1r, il-34 stimulates monocytes and macrophages through extracellular signal-regulated kinase (erk) 1/2 or akt phosphorylation.recent data, however, demonstrated that il-34 could also bind chondroitin sulphate chains, such as ptp-ζ and syndecan-1, but with lower affinity [133, 134].this is of particular importance in tumor biology as these receptors are up-regulated in several cancer types.il-34 can be induced by a variety of pro-inflammatory cytokines, including il-1β and tnf-α, and its main function of that to promote monocyte survival, proliferation and differentiation to macrophages.recent studies revealed that il-34 drives the differentiation of monocytes into immunosuppressive m2 and that human macrophages cultured in the presence of il-34 are able to expand treg cells.interestingly, il-34-expanded treg cells display a stronger suppressive activity compared to non–il-34–expanded treg cells [135, 136].this widens the spectrum of action of il-34 towards immune tolerance.moreover, il-34 is involved in rank-l mediated osteoclastogenesis by inducing the proliferation and adhesion of osteoclast progenitors in-vitro and by inducing the formation of osteoclasts from murine splenocytes in-vivo, thereby reducing trabecular bone mass [137–139].il-34 deficient mice selectively lack langerhans cells and microglia and display weak immune responses to skin antigens and central nervous system-selective viruses, but they display neither osteopetrosis nor any autoimmune manifestation [140, 141].mice lacking csf-1r receptor are toothless and severely osteopetrotic and display circulating monocyte depletion, total depletion of microglia, significant impairment of olfactory function, defects in reproductive function and reduced bone marrow hematopoietic progenitor cells [142–144].conversely, the neutralization of csf-1r in adult mice leads to a reduction of mature monocytes in blood and bone marrow, without affecting precursors [145].with regard to experimental arthritis, it is interesting to note that the lack/blockade of csf-1 as well as the blockade of csf-1r is associated with less severe methylated bovine serum albumin (mbsa)-induced arthritis and cia [146–148].in ra patients, all available studies pointed to increased serum and sf levels of il-34 with respect to normal and disease controls (oa, psa, ankylosing spondylitis (as)) [149–153].of interest, ser0075m il-34 levels correlated with immunological markers of more severe disease including rheumatoid factor (rf), anticyclic citrullinated peptide antibody (anti-ccp) titers, erythrocyte sedimentation rate (esr), c-reactive protein (crp), and with disease activity and smoking [149–152].in this regard, serum il- 34 levels have been also associated with radiographic progression and appear to be good predictors of radiographic damage in ra patients [150, 152].interestingly, treatment with dmards or tnf-α inhibitors is able to reduce serum il-34 levels [151, 154].il-34 levels are also higher in ra sf compared to oa sf and increased in ra patients with higher disease activity [149, 153].of interest, sf il-34 levels are directly correlated with those of sf rank-l, further supporting the link between il-34 and rank-l mediated osteoclastogenesis [149].finally, il-34 is also consistently expressed in ra st, mainly in the sublining and the intimal lining layer, with its expression being associated to synovitis severity [148, 154, 155].all these observations about il-34 raise the question whether the blockade of its pro-inflammatory and bone remodeling effects are worth the loss also of the strongly suppressive il-34 driven treg cells.therefore additional data are needed to clarify its therapeutic potential in ra.conclusion the progression and severity of inflammation in ra is associated with a consistent production of pro-inflammatory cytokines and a deregulation of anti-inflammatory cytokines.although several biologic agents with different mechanisms of action are available for the treatment of ra, even now a consistent number of patients either do not respond or respond only partially to these compounds.therefore, the advance of our understanding of mediators involved in the pathogenesis of ra and in consequence, the development of novel targeted therapies, are compelling.forty years after the discovery of il-1, the never-ending quest to identify ‘the’ culprit of ra development is still a fascinating field under intense investigation.in recent years, the landscape of pro- and anti-inflammatory cytokines has rapidly expanded with the identification of new members proven to be involved at different extent in the pathogenesis of ra.in some cases, evidence from animal models and ra patients is already consistent to move forward into drug development.in others, conflicting observation and the paucity of data require further investigatio
number of words= 1673
[{'rouge-1': {'f': 0.2660221102083433, 'p': 0.9354970760233918,'r': 0.1550574712643678}, 'rouge-2': {'f': 0.2280605526453763, 'p': 0.7347058823529411,'r': 0.1349798734905118}, 'rouge-l': {'f': 0.3106635567981523, 'p': 0.8898198198198197,'r': 0.18818181818181817}}]
-----------------------------------------------------------------------------------------------------------------------------------
p381:
Extractive Summary:
this hyperuricemic threshold, or similar ones based on sua distributions in populations of men and women, are commonly used in epidemiological and clinical research [10, 11].many epidemiological studies which have explored the question of whether sua levels are associated with cardiovascular outcomes have used a single-point threshold (such as 6.8 mg/dl or 7.0 mg/dl) to classify individuals as hyperuricemic [10, 11].our study results will have implications for epidemiological and clinical studies in hyperuricemia and gout, which sometimes rely on a single sua measurement to classify participants.participants were asked to fast during the day prior to the screening visit.the rates of conversion to hyperuricemia were then compared across subgroups defined by the sua level at initial screening, as well as between subgroups with only one initial normouricemic reading (and a second check which was hyperuricemic) versus two initial normouricemic readings.comparisons between count data in groups were performed using fisher’s exact tests, due to small cell size.mean ± standard deviation study participant (n = 85) age was 27.8 ± 7.0 years and mean body mass index was 31.1 ± 7.9.(table 1).there was no significant difference in the coefficient of variation between men (8.5%) and women (8.6%) (p = 0.88), or between subjects who were initially normouricemic (sua ≤ 6.8 mg/dl) (8.6%) and those who were initially hyperuricemic (sua > 6.8 mg/dl) (8.0%) (p = 0.68) (table 2).about 21% of participants initially normouricemic were found to be hyperuricemic on subsequent checkups.our findings could influence the way studies using serum urate as an enrollment criteria or outcome are conducted or planned.our study differed from previous examinations in its calculation of a threshold for screening sua level more helpful in ruling out future hyperuricemia.we used 6.8 mg/dl as our threshold for hyperuricemia, as this is related to the physiologic saturation at which urate begins to precipitate.we found a sua threshold < 6.0 mg/dl, reliable in ruling out subsequent hyperuricemia, as the conversion rate was just over 7%.our somewhat small number of subjects did not allow us to calculate a comparable threshold value above which persistent hyperuricemia appears to be more likely.a second sua measurement did not add reliability in excluding future hyperuricemia, as there was no significant difference in the conversion rates between those with one initial value below threshold and those with two values below threshold.there has been a paucity of published data on the rate of conversion from normouricemia to hyperuricemia and from hyperuricemia to normouricemia after an initial laboratory check in the absence of intervention.a 2004 study examined the serum urate levels and 24- h urinary uric acid levels monthly for a 12-month period in 12 healthy men on self-selected diets, without medications known to effect urate levels, as well as abstinence from alcohol 7 days prior to measurement of levels.seven of the twelve subjects (58.3%) had transient hyperuricemia at some point during the study period [19].a similarly designed study found that 10/12 subjects experienced transient hyperuricemia at some point during the course of 1 year [15].the framingham heart study concluded that a higher percentage of the male population had hyperuricemia when considering four biennial determinations rather than a single determination [6].the mean coefficient of variation for sua was also examined in these two similar studies [15, 19].our mean coefficient of 8.5% was comparable to the findings of the 2004 study (9%, with a range of 5–12%) [19], and is less than that found in the second study mentioned (17.5%, with a range of 15–22%) [15].in addition to variation over months, it has also been demonstrated that uric acid levels fluctuate over the course of a single day [20].serum urate levels were significantly higher when measured in the morning than when measured in the afternoon, with a decrease of up to 30% seen in a subgroup of diabetic patients [12].in contrast to this, other studies have found that the serum urate levels are lowest when measured in the morning [21, 22].the factors driving diurnal variations in serum urate are not fully understood, but changes in the variation may be impacted by gender, age and diet [22].differences in uric acid levels have also been shown to be associated with hypertension with “non-dipping profiles” (absence of significant decrease in blood pressure during sleep), suggesting that there may be interplay between sua and blood pressure determiners [23–25].the cause of the variation in sua found by our study and previous studies is likely multifactorial.the young age of our participants could limit generalizability to older populations.a total of 13 study participants presented with an initial sua in the hyperuricemic range, which could make the conclusions obtained from that group imprecise.participants did receive monetary compensation for their participation in the trial, which could have influenced the characteristics of the study population [18].this finding diminishes somewhat the value of a single sua check, and even a second measurement was not found to add significant reliability in excluding future hyperuricemia in those who are initially normouricemic.
number of words= 816
[{'rouge-1': {'f': 0.46528626028400744, 'p': 0.7483625730994152,'r': 0.3375893886966551}, 'rouge-2': {'f': 0.19658068524015773, 'p': 0.27821114369501465,'r': 0.151986143187067}, 'rouge-l': {'f': 0.4203613676823447, 'p': 0.5891256830601093,'r': 0.32675675675675675}}]
-----------------------------------------------------------------------------------------------------------------------------------
p382:
Extractive Summary:
introduction gout is the most prevalent inflammatory arthropathy with a prevalence of 6.8% in south australia, and it is the most common form of inflammatory arthritis in men [1].a recent meta-analysis has shown an association between gout, serum uric acid and osa [5].in addition, individuals with osa had a higher risk of developing gout, although this was not statistically significant (hr 1.25 95% 0.9– 1.7).we hypothesized that gout is therefore also likely to be associated with sleep disruption, which is known to lead to significant sequelae such as reduced work performance and road safety [3].materials and methods the aims of these study were addressed with a secondary analysis of the existing database of the sleep health foundation 2019 web-based sleep health survey.the primary aim of this cross-sectional survey was to investigate prevalence of sleep disorders and sleep problems in the australian population.possible osa was categorised as self-report of witnessed breathing pauses (i.e apnoeas).patient reported outcome measures for sleep patient reported outcome measures for sleep were assessed with the following questions and answer options included in table 1.”.data analysis data were analysed using ibm spss version 26.0 (ibm corporation).multivariable logistic regression analyses were used to examine associations between gout (predictor) and each of the patient reported sleep outcomes (summarised in fig. 1), with each model adjusted for age, sex and bmi, selected based on established relationships with sleep problems in community samples [3, 9].respondents who indicated a diagnosis of gout (n = 126, 6.5%) were more likely to be male than female (11.2% v 2.0%, p < 0.001), and reported a higher prevalence of comorbidities including heart disease, arthritis, diabetes, high blood pressure and obesity than those without gout (see table 2).respondents with gout were 2.5 times more likely to report doctor diagnosed rls/plms, were 2 times more likely to worry about their sleep, and were 1.6 times more likely to have discussed their sleep with a health professional than those without gout.in contrast, 15.0% of respondents with gout indicated they have doctor diagnosed rls/plms compared to a prevalence of 6.7% in those without gout.a further 190 participants (9.8%) had symptoms suggestive of sleep apnoea without a formal diagnosis.even after accounting for age, bmi, sex, alcohol intake and the presence of arthritis, diagnosis of gout was 2.8 times more likely in respondents with possible osa.our study was novel in that we found the association also emerged in those with suspected, but undiagnosed, sleep apnea.our study design does not allow for consideration of the directionality of these relationships.prospective studies with gold-standard measurement of osa, and inclusion of serum urate measures over time, will be required to further examine these relationships.restless legs syndrome has been has reported associations with gout as well as many other comorbidities including obesity, diabetes, hypertension, thyroid disease and iron deficiency [14].given the prevalence of gout and the substantive financial burden of sleep-related conditions and flow on effects including productivity, employment, accidents and well-being, our findings highlight the importance of identifying and managing sleep problems in patients with gout [16].this result was surprising and not in keeping with our proposed hypothesis.instead, it will be important to identify patient-specific sleep complaints and concerns and manage these on a case-by-case basis.specifically, patients with gout differ from the general population, with reports of feeling they receive adequate opportunity to sleep.one of the strengths of our study was the large sample that closely matched the general australian population.it should be noted however, that while the sample was representative in regards to age, gender and geographical location across metropolitan and rural locations, there was a higher proportion of post-school qualifications, particularly bachelor degrees or higher, than population estimates.a key limitation is the cross-sectional nature of our study, so we were unable to comment on causation.sleep apnoea and gout are both associated with significant cardiovascular morbidity and mortality, but are also treatable.
number of words= 637
[{'rouge-1': {'f': 0.42931918470266855, 'p': 0.7230612244897958,'r': 0.3052941176470588}, 'rouge-2': {'f': 0.20207547169811318, 'p': 0.29950819672131146,'r': 0.1524742268041237}, 'rouge-l': {'f': 0.3901490513234623, 'p': 0.6122535211267606,'r': 0.28629213483146065}}]
-----------------------------------------------------------------------------------------------------------------------------------
p383:
Extractive Summary:
background behçet disease (bd) is an inflammatory vasculopathy with multisystemic involvement.while the etiology is unknown, a strong correlation with human leukocyte antigens, specifically hla-b51, has been observed.behçet disease has been reported all over the world, but the prevalence is particularly high in the middle east, far east, and the mediterranean.bd is also referred to as the ‘silk route disease’, acknowledging the fact that the highest incidence of bd has been reported along this ancient route.turkey has the highest prevalence of bd, followed by iran, saudi arabia, iraq, israel, northern china, and korea [2, 3].there are no pathognomonic laboratory tests to diagnose bd, and as such, the diagnosis is based on clinical criteria.patients at behçet referral centers might have a different and occasionally have atypical presentation, which renders the diagnostic approach more complicated.there is a knowledge gap in how the current diagnostic criteria perform in the setting of a dedicated ambulatory bd center.methods in a retrospective cohort study, data from patients referred to the behçet clinic of the rheumatology research center (rrc) at tehran university of medical sciences were reviewed from november 2018 to august 2019.rrc is the only national referral center for bd and is the national authority on guidelines for prevention, diagnosis, and treatment of behçet disease.inclusion criteria included first-time referral from doctors of medicine, and complete follow-up records, at least until the diagnosis was proved or ruled out.the specialty of the referring physician, as well as their practice setting (academic, non-academic), were collected.results of laboratory tests, including hla-b5, hla-b51, and hla-b27, were recorded.the icbd criteria were used to diagnose/rule-out bd, dividing patients into two groups (bd, non-bd), which were subsequently compared.the definitive diagnosis of patients who did not meet the icbd criteria was also reviewed.descriptive statistics were used to determine the frequencies and central tendencies of the cohort.groups were compared utilizing the student t-test and mann– whitney u test for parametric and non-parametric data.chi-square test was used to compare categorical data.all statistical analyses were performed with ibm spss statistics for windows, version 22.0 (ibm corp., armonk, ny).forty patients (16.8%) were diagnosed with bd, 105 patients (44.1%) had a probable diagnosis, and bd was ruled out in 93 (39.1%) patients.it should be noted that in 26 patients in the non-bd group, we did not establish a specific diagnosis.as expected, oral lesions were the most common reason for referral, as well as the most prevalent manifestation in patients finally diagnosed with bd.lichen planus, isolated ocular lesions, pemphigus, geographic tongue, and herpes simplex were the other diagnoses in this cohort, which could be an accurate estimate of the most common differential diagnoses of bd.however, they are more specific than oral lesions.uveitis is the most common ocular manifestation in bd [2, 15, 16], which was also the case in our study.that being said, a positive pathergy test was the most specific diagnostic tool in this study.while only 45% of the bd patients had a positive result, none of the non-bd group showed a positive reaction.a pathergy test is a part of our routine evaluation of patients for behçet disease.we suggest unifying the protocol for the administration of the pathergy test, which has been shown to be a highly predictive test.hla-b51 is more prevalent in bd than the general population [17].due to low specificity and a variable prevalence among ethnicities, it is not considered a diagnostic criterion in the icbd [6].this was not the case for hla-b5 and hla-b27.sixty percent of bd patients in this study were positive for hla-b51, while previous studies have determined a 27% prevalence of an hla-b51 genotype in the general population in iran [3].this study has some limitations.this study also has some strengths.this is the first study of a large national behçet center to look for the characteristic of patients referred for evaluation and not only those with a definite bd diagnosis.also, all patients were examined by a multidisciplinary team, and the risk of misdiagnosis was very low.also, patients were significantly more likely to have multi-organ (≥2 organ systems) involvement.also, the alternative diagnoses established in this study could be used as the list of the most common differential diagnoses for behçet’s diseas
number of words= 684
[{'rouge-1': {'f': 0.4660768758588055, 'p': 0.8454237288135593,'r': 0.3217193947730399}, 'rouge-2': {'f': 0.2567989279139862, 'p': 0.42319148936170214,'r': 0.1843250688705234}, 'rouge-l': {'f': 0.45037990774706715, 'p': 0.7366666666666666,'r': 0.3243352601156069}}]
-----------------------------------------------------------------------------------------------------------------------------------
p384:
Extractive Summary:
dmards remain poorly studied [1].methods this study was approved by the clinical research ethics board at the university of british columbia.patients were identified by radiographic findings consistent with retroperitoneal fibrosis and a biopsy consistent with idiopathic rpf.patients with clinical, serologic or pathologic evidence of igg4- related disease were excluded.all patients were actively flaring at the time of treatment.tissue samples were obtained through laparoscopic resection of pathological tissue surrounding the ureter in all 9 biopsies.the biopsies were reviewed by one pathologist who specializes in reviewing rpf cases (ms).these cases lacked evidence of another etiology, i.e. no granulomas, no cytologic atypia or other evidence of malignancy, no overt necrosis, no significant neutrophilic or eosinophilic inflammation, no vasculitis, no significant plasma cell component, no significant storiform fibrosis, no endarteritis or obliterative phlebitis, and no increase in igg4 positive plasma cells (as defined by igg4 related disease consensus criteria) [11].in all patients, the thickest portion of the peri-aortic disease was measured in the axial and coronal planes.details of clinical visits including patient demographics, symptoms, past treatments, disease duration, biopsies, concurrent treatments were collected pre- and post-therapy (table 1).pre-treatment laboratory values for igg4 levels, albumin, creatinine, crp, wbc, and hematocrit were also collected.statistical analysis was performed using the wilcoxon signed rank test.a probability of p < 0.05 was considered statistically significant.the relationship between disease duration and response to treatment was assessed using spearman’s rank correlation.results the average age of disease onset was 58.5 ± 11.6 years (table 1).as fully described in the methods, there were no features of malignancy or infection, none of the cases met consensus criteria for igg4- related disease, and no other apparent etiology (e.g. sarcoidosis, vasculitis, etc.) was identified.a comparison of pre and post-rituximab imaging studies were available in 10 patients and revealed statistically significant improvements in irpf diameter following treatment with rituximab on imaging in the axial and coronal planes (table 2).the rpf diameter around the aorta before and after therapy decreased from a mean of 16.1 ± 4.6mm to 10.4 ± 6.2mm, respectively (p < 0.01), as shown in fig. 1.the craniocaudal irpf mean length decreased from 108.6mm± 40.4mm to 90.6mm± 45.9mm (p = 0.02).figure 2 demonstrates marked improvement on comparison of pre and post-imaging of one patient.all 10 patients had ureter involvement before and after rituximab, with unilateral involvement increasing from 3 patients to 5 patients, and bilateral ureter involvement decreasing from 6 patients to 5 patients.infrarenal ivc involvement was present in all 10 patients pre-treatment, and 9 patients post-treatment.symptomatically, 5 of the 8 symptomatic patients reported some improvement in symptoms.one patient developed hives despite administration of diphenhydramine and was unable to complete the first infusion.he was later able to complete both infusions utilizing a lower infusion rate and different pre-medicines.one other patient developed throat irritation and rhinorrhea at their second round of rituximab infusions but was able to finish the infusion following administration of diphenhydramine.when comparing irpf diameter on imaging pre and post-rituximab, there was a statistically significant reduction in both axial and coronal planes.it is unknown how the efficacy of rituximab compares to prednisone alone, dmards or placebo in this observational case series.the remaining 19 patients had igg4-rd.on post-treatment imaging, 21 of the 25 patients (84%) had treatment response, as defined by improvement in rpf size in at least 2 imaging planes.the pre-treatment imaging was often done months prior to start of treatment, and therefore may not have accurately reflected the true ureter and vessel involvement prior to treatment.the third patient on concurrent steroids, who had been successfully treated with steroids in the past, had a less impressive improvement in rpf thickness, of approximately 18%.while there were statistically significant changes in disease on imaging post-treatment, the clinical improvement of patients was more difficult to assess.although patients seemed to have some improvement based on clinical records, it is our suspicion that rpf may cause lumbosacral plexus nerve damage given the proximity of those nerves that may result in chronic pain.more studies are needed to assess patients’ long-term outcomes.during the infusion, the patient received diphenhydramine 50 mg iv over 2–3 minutes followed by hydrocortisone 100 mg iv to mitigate the allergic reaction.
number of words= 684
[{'rouge-1': {'f': 0.4629207940098276, 'p': 0.7913740458015268,'r': 0.3271428571428571}, 'rouge-2': {'f': 0.30177798233174175, 'p': 0.48762452107279697,'r': 0.21850136239782017}, 'rouge-l': {'f': 0.4584371330785181, 'p': 0.6950000000000001,'r': 0.34202072538860107}}]
-----------------------------------------------------------------------------------------------------------------------------------
p385:
Extractive Summary:
background the concept of mixed connective tissue disease (mctd) as a unique connective tissue disease has persisted for almost 50 years.shortly thereafter, the initial reports of juvenile onset mctd (jmctd) emerged [2, 3].the female: male ratio in jmctd is 6:1 [7].the etiology of mctd and jmctd, like other systemic autoimmune rheumatic diseases, is unclear.current models support a hypothesis involving chronic immune activation after exposure to an environmental or exogenous trigger in individuals with a predisposing genetic background.certain major histocompatibility complex (hla) genes have an important role in the presentation of antigens to the immune system.specifically, hla-dr4, hla-drw53, hla-drb1*04:01 and hla-b*08 have all been found to play a role in the creation of anti-u1snrnp antibodies and ultimately the clinical manifestations of the disease [8–10].some researchers theorize that mctd represents instead an overlap syndrome or an early and unspecific phase of another defined connective tissue disease.however, the concept of mctd as a defined entity is supported by the existence of a specific and repeated clinical pattern, a characteristic antibody, and associated specific immunological and genetic findings [13].this pattern and the risk for similar end-organ disease manifestations appears similar in jmctd and mctd [14, 15].multiple sets of classification criteria for mctd have been published, including: sharp [16], kasukawa et al. [17], alercón- segovia and villareal [18] and kahn and appeboom [19].regardless of their variability, the criterion common to all is the detection of anti-u1 rnp antibodies.in jmctd, kasukawa criteria are used most frequently in published series (table 1) [20].the most common disease characteristics in jmctd are summarized in table 2 [7].there are no treatments available specific to mctd.according to the first descriptions of the disease, mctd patients were characterized by an excellent response to glucocorticoid treatment and a favorable prognosis [4].however, therapy needs to be individualized and adapted according to the severity of the manifestations at the time of presentation and organ involvement.he was diagnosed with mctd, with presumed juvenile onset given the duration of his undiagnosed symptoms [23].his therapy was escalated to include abatacept (125 mg subcutaneously given weekly) and extended release nifedipine (30 mg daily).these agents were chosen to target his ongoing active synovitis and raynaud’s phenomenon.while tumor necrosis factor a inhibition (tnfi) was considered, given the possibility of systemic lupus erythematosus features in mctd, we did not want to risk drug-induced lupus symptoms from a tnfi therapy.the purpose of classification criteria, of course, are to identify patients with a similar clinical entity for research, and classification criteria are not synonymous with diagnostic criteria.recent study indicates that the classification criteria of kasukawa et al. are the most sensitive (75%) compared to those of alarcon-segovia and villarreal’s (73%) and sharp’s (42%) in classification of patients with mctd, throughout disease progression [28].notably, capillaroscopy has been included in the updated 2013 american college of rheumatology/ european league against rheumatism (acr/eular) classification criteria for ssc (a disease with phenotypic overlap to mctd and a similarly understood vascular pathology and microangiopathy) and is considered a key investigative tool in the early phase of the disease [30, 31].mctd similarly seems to have a pattern of microangiopathy.in a recent study, a plurality (44%) of mctd patients have demonstrated an ‘early’ scleroderma pattern on capillaroscopy [36].additionally, recent study has shown that giant capillaries, as seen in our patient, might be a promising marker for interstitial lung disease in mctd patients, especially among those with a short disease duration [40].similar to mctd, jmctd is usually described as beginning with polyarthritis, raynaud’s phenomenon, hand edema (“puffy hands”), and sclerodactyly.typical findings of systemic lupus erythematosus (sle) and pm or dm-like features are more evident at the time of diagnosis in jmctd compared to mctd, and several studies have confirmed that sle-like features are more common among jmctd patients compared to adult onset disease [6, 7, 41, 42].overall, more than half of patients with mctd and jmctd demonstrate a ‘scleroderma pattern’ on nailfold capillaroscopy, and this has been reported to be associated with the development of internal organ complications.specifically, scleroderma-like abnormalities in mctd often (76% of patients) accompany interstitial lung disease (ild) [44].hence, we argue that nailfold videocapillaroscopy, which is the gold standard for detection of microvascular abnormalities and already a critical component of the systemic sclerosis classification criteria, should be considered as an early screening tool for the detection of microangiopathy in patients with the diagnosis of mctd and jmctd.additionally, given its prevalence in this population at disease diagnosis, we recommend consideration be given to nailfold video-capillarscopy as a potential classification criterion for jmctd and mctd in the future.in summary, based upon the existing research in this area, and as outlined in our above case, we propose that nailfold video capillaroscopy be included in the early assessment of all patients with mctd and jmctd.
number of words= 782
[{'rouge-1': {'f': 0.4201613225112149, 'p': 0.7735573122529644,'r': 0.28840490797546015}, 'rouge-2': {'f': 0.2365701985470337, 'p': 0.39142857142857146,'r': 0.16950859950859953}, 'rouge-l': {'f': 0.3806826690671242, 'p': 0.6160526315789474,'r': 0.27544554455445547}}]
-----------------------------------------------------------------------------------------------------------------------------------
p386:
Extractive Summary:
the training finished with picking the parameter set ƥ’ with the highest average value of the dice ratio.in our case, a dice ratio difference of 0.1% meant that on average about 500 voxels were classified differently.parameter value ranges the optimal number of trees was determined by cv-7 using a range of 1 to 150 trees.similar, the samples per decision node was varied from 1 to 0.00001% of the total number of variables, which in our case resulted in 125, 371 to 1 feature-label pairs.input parameters for gabor filters (ƥgf) were size of the 2d gabor kernel (given by a pixel window of sx x sy), wavelength of the sinusoidal wave and standard deviation of the gaussian.the following parameter ranges were used: kernel size: 3 × 3 to 26 × 26 pixel window dimension; wavelength: 0.01mm to 4 mm; standard deviation 0.01mm to 4 mm.parameters for elbp (ƥelbp) were circle radius, number of samples on the circle and radius of the second, smaller circle for the spatial relationship.the radius of the second circle was empirically set to half the radius of the first one.cv-4 was used with the following ranges: radius of the first circle: 0.5 to 5.5; number of samples: 4 to 28.segmentation workflow the trained rf was used to segment all 76 datasets.the segmentation workflow (fig. 2b) is described in more detail in the following subsections.pre processing of t1 scans mr images are often distorted by bias fields, caused by inhomogeneous magnetic fields of the coils.these image distortions were corrected in a pre processing step using the n4itk algorithm [21], which is an improvement of the well-known and established n3 (nonparametric nonuniform intensity normalization) approach [22].segmentation of hand cross sectional area the segmentation of the hand csa was performed by a succession of basic image processing methods: first a threshold was used to roughly divide the t1 weighted image into background and hand.after n4itk normalization described above, background intensities were around 10 and hand intensities around 500 units.for the particular scanner and mr acquisition protocol used in the study, a threshold of 70 was used.this resulted in an image containing one or more volumes of interests (voi): one large volume of the hand and, due to image acquisition artifacts, multiple small volumes outside the hand voi, where intensities were also higher than 70.these vois were smoothed by a 3d morphological opening.finally, the hand voi, which was always the largest voi, was extracted.hand muscle segmentation for segmentation, the features described above were calculated for each voxel using the optimal parameters ƥgf and ƥelbp determined by the rf training.the trained rf (using ƥrf as determined by the rf training) classifies each voxel into muscle and background, leading to a raw (i.e. without post processing) muscle segmentation.post processing of muscle segmentation the raw segmentation was post processed by a morphological dilation with radius 1, followed by island extraction, where islands (directly connected muscle voxels) with a size smaller than 10 voxels were discarded.finally, a gaussian function f(μ,σ) was fitted to the grey value histogram of the muscle segmentation mask and voxels with grey values outside the range [μ ± 3σ] were excluded from the segmentation.the underlying idea of this procedure was to include a wider area of voxels into the segmentation mask and then remove wrongly included voxels by the gaussian fit.the result of the automated procedure is shown in fig. 4a.the resulting muscle segmentation was reviewed by a clinical expert and manually edited if necessary.volume of interests the volume of interest (vois) are hand volume vh and muscle volume vm, obtained from the hand csa and the muscle segmentation mask, respectively.in order to increase their longitudinal and cross sectional comparability, these two vois were manually limited to the metacarpal region defined by the metacarpal bone (mcp) iii.for this purpose, the clinical expert had to set the proximal and distal mcp iii boundaries.this could be achieved by navigating to the corresponding two slices in a transversal view.fat quantification fat was quantified using the dixon fat fraction image.since muscle was difficult to detect in the ff image, the muscle segmentation mask obtained using the t1 scans was transferred to the ff image via multimodal rigid image registration (fig. 4b).the used similarity metric was mutual information as described by mattes et al. [23], optimized by the gradient descent method.in the segmentation voi the average and the absolute fat content was calculated.validation of accuracy accuracy of the rf based segmentation was determined using the 30 gold standard datasets.segmentation masks were compared between the manual and the rf approach using three different image metrics: the dice ratio (eq. 3), the average surface distance (eq. 4) and the hausdorff distance (eq. 5).the average surface distance is the average of the distances from all points of one to the corresponding closest point of the other surface: davg ¼ 1 a j j x a∈a min b∈b fdða; bþg ð4þ for d the euclidean metric was used.the hausdorff distance h is the maximum of the individual distances, i.e. the maximum local distance between the two segmentation masks: h ¼ max a∈a fmin b∈b fdða; bþgg ð5þ reanalysis precision for the determination of reanalysis precision errors, three operators analyzed 14 random data sets once (interoperator) and one operator analyzed the same 14 data sets three times (intraoperator).reanalysis precision errors were calculated as root mean square average of standard deviation (rms_sd) and coefficient of variation (cv) of individual data sets [24].precision was calculated for the hand segmentation in the t1 weighted scans, which depended on the manual determination of the mcp iii length and potential manual segmentation corrections.additionally the precision of the registration based fat quantification was calculated.implementation details the method was embedded in the medical image analysis framework (miaf, institute of medical physics, erlangen, germany).implementation was done in c++ with the help of the insight segmentation and registration toolkit (itk [25]) and the open source computer vision library (opencv [26]).for the analysis, a computer with a 3.4 ghz quadcore processor and 16 gb ram was used.results random forest training the manual hand muscle segmentation used as gold standard took 2.5 h ± 0.5 h per dataset.random forest training for each parameter set ƥ’ took between 15 min (for a rf with 7 trees) and 4 h (for a rf with 150 trees).the processing time mainly depended on the rf tree count.the optimal parameter set ƥ leading to the highest dice ratio of 96% are summarized in table 2.hand muscle segmentation the n4itk pre-processing and the hand muscle segmentation was automated and did not need any user input.pre-processing took 30 s ± 10 s. the segmentation took 2 min ± 0.2 min, of which the rf based part described required about 90% of the time.user interaction the user interaction for determining the mcp area could be completed in less than 30s.manual segmentation editing was necessary in 69 of the 76 datasets.this was mostly limited to deleting segmented forearm muscles or thicker layers of skin and was not considered tedious by the operator (see fig. 5 bottom row).the manual editing per dataset took 7 min ± 5 min.multimodal registration to dixon sequence took 1 min on average.the registration quality was visually checked; no dataset needed further adjustments.all 76 datasets could be analyzed conveniently and the results were considered appropriate, as judged by the medical expert.accuracy accuracy results based on the comparison to the 30 gold standard datasets are listed in table 3.high hausdorff distance values were observed in scanning direction at the proximal and distal ends of the analyzed vois.a closer inspection showed, that high hausdorff distances were caused by variations of the manual placements of the mcp iii borders to define the analysis vois.reanalysis precision inter- and intraoperator reanalysis precision errors for muscle segmentation and fat quantification are summarized in table 4.discussion we presented a novel, highly accurate and precise segmentation method for hand muscles in t1 weighted fat suppressed mr scans, which was based on random forest classifiers.muscle fat quantification was measured after multi-modal image registration from t1 weighted to 2-pt dixon sequences.the two main outcomes of the described method were the large reduction of required user interaction time per dataset and a high accuracy and reanalysis precision.the clinical relevance may be high, since this approach provides a realistic perspective to integrate quantitative muscle assessments in research and clinical routine.
number of words= 1380
[{'rouge-1': {'f': 0.34031105821199265, 'p': 0.7795709570957097,'r': 0.21766483516483517}, 'rouge-2': {'f': 0.16482418804661275, 'p': 0.2885430463576159,'r': 0.11536082474226805}, 'rouge-l': {'f': 0.3505872401922707, 'p': 0.6076344086021506,'r': 0.2463668430335097}}]
-----------------------------------------------------------------------------------------------------------------------------------
p387:
Extractive Summary:
the antigens of the autoantibodies are proteinase 3 and myeloperoxidase, that primarily are found in primary granules in neutrophils and peroxidase positive lysosomes in monocytes.both monocytes and neutrophils are frequently found around the inflamed vessel walls and are thought to be the main effector cells.primed neutrophils in aav patients can be stimulated by anca through binding to membrane bound pr3 or mpo and in response to this they produce reactive oxygen species (ros), de-granulates and form neutrophil extracellular traps (nets).however, it is not known why anca is formed or what primes pmns in-vivo.since eosinophils express pr3 [2] and eosinophil peroxidase (high structural homology to mpo) on their surface, anca might bind and activate also this cell type.eosinophils could also selectively suppress th1 cells via a constitutive expression of indoleamine 2,3-dioxygenase, an enzyme important for tryptophan catabolism [7] and they have proven to be essential for the survival of plasma cells by supplying necessary cytokines into the plasma cell niches [8].extracellular dna traps formation was first described in neutrophils but is now considered as a common mechanism for the innate immune system.98 gpa and mpa patients were included in the study: 74 patients with gpa, and 24 patients with mpa.the cells were divided into two tubes and incubated for 20 min with antibody mix 1 and 2 respectively (mix 1: cd10-pecy7, cd14-v500, cd16-apc-h7, cd88-pe, cd49d-apc, cd62l-fitc, cd11b-v450, cd11c-percpcy5.5 and mix 2: cd10-pecy7, cd14-percpcy5.5, cd16-apc-h7, cd35-fitc, cd49d-apc, cd64-v450, cd193-v500, siglec-8-pe.all antibodies were from bd biosciences except cd11c and siglec-8 that were purchased from biolegend.the cells were then washed by adding 3ml pbs and centrifuged for 3 min at 250 g and resuspended in 25 μl pbs and analyzed using a facscanto ii and the diva software (becton dickinson biosciences, new york, usa).doublet cells were excluded by plotting fsc height against fsc area and the single cells were divided into monocytes, lymphocytes and granulocytes based on fsc and ssc plots.cd14 positive cells were excluded from the granulocytes and eosinophils were selected as cd16−/ cd10− and cd49d+, siglec-8+ and/or cd193+ cells.the eosinophils were thereafter separated from the granulocytes using macs eosinophil isolation kit (miltenyi biotech) according to manufacturer’s instruction.100 μl of purified eosinophils or neutrophils at a concentration of 1 × 106 /ml in 0.5%hsa/rpmi were seeded on poly-l-lysin treated coverslips (sigma).the coverslips were carefully washed three times with washing buffer (3% bsa in pbs) before the cells were permeabilized for 5 min (pbs containing 3% goat serum, 3% cold water fish gelatin, 1% bsa, 0,05% tween20, 0,5% tritonx100) at room temperature.the eosinophils remain in the supernatant and are carefully collected into another tube.the eosinophils were primed for 15 min with pbs or c5a (150 ng/ml) at 37 °c and 5% co2.after priming, the eosinophils were stimulated by addition of purified igg from anca positive patients (250μg/ml) (one mpo-anca and one pr3-anca), purified igg from a healthy control (250μg/ml), pbs (negative control) and pma (positive control – pma was not used in combination with c5a) and incubated for 180 min at 37 °c and 5% co2.anca specificity from one patient was missing and one was double positive.the majority of the patients were in remission (n = 76).corticosteroid treatment has been reported to affect the number of eosinophils in peripheral blood and we saw a weak but significant correlation between corticosteroid treatment, prednisone in all of our cases, and the absolute number of eosinophils (r2 = 0.088, p = 0.008) (fig. 2a).when dividing all patients into 3 groups based on corticosteroid dose (0, > 0 to 5, and > 5 mg/day) the only difference found was between the group without corticosteroids and the group with a daily dose above 5 mg (fig. 2c).patients with active disease had significantly lower frequencies of both eosinophils and basophils compared to patients with inactive disease and hbd, but no difference were found between gpa and mpa patients (additional file 3).the patients have increased surface expression of the low affinity fcγriii (cd16, p < 0.0001), the high affinity fcγri (cd64, p = 0.0035) and the eosinophil eotaxin receptor ccr3 (cd193, p = 0.0002), and decreased expression of the complement receptors cd35 (p = 0.0022), cd88 (p < 0.0001) as well as cd11b (p = 0.0061), cd11c (p < 0.0001) and siglec-8 (p = 0.0015) (fig. 3).moreover, ros play an important regulatory role of both the innate and adaptive immune system [15, 16].peripheral whole blood from patients (n = 98, table 1) and controls (n = 121) were stimulated with pma (protein kinase c activator) or opsonized e.coli and intracellular ros production was measured by flow cytometry using the phagoburst kit.eosinophils from gpa and mpa patients showed a significantly decreased ros production both when stimulated with pma (p < 0.0001) and e.coli (p < 0.0001, fig. 5) compared with healthy controls.eosinophils form extracellular traps more easily than neutrophils extracellular traps released from neutrophils and eosinophils are thought to play an important role in the defense against pathogens and in inflammatory processes.eosinophils express pr3 [2] and eosinophil peroxidase (high structural homology to myeloperoxidase) on their surface, indicating that anca could bind and stimulate also this cell type.to investigate if anca could affect the release of eets, eosinophils were purified using the macsexpress eosinophil kit from five aav patients (3 gpa and 2 mpa) and healthy controls (n = 5).the eosinophils were primed with either pbs or c5a followed by incubation with pbs (negative control), pma (positive control), igg from healthy controls or igg from anca patients (one pr3-anca and one mpo-anca).stimulation of eosinophils from the patients with c5a followed by anca gave a higher level of eetosis (p = 0,0088, fig. 6c).this was not seen among healthy controls.the results are in line with our previous findings for neutrophils in aav patients [14].glucocorticosteroid treatment has been reported to affect the number of eosinophils in peripheral blood, by inducing apoptosis and inhibiting pro-survival signals by cytokines e.g. il-5 and gm-csf [20, 21].two patients were newly diagnosed and both of them had low levels of eosinophils already before treatment was started.this could be one explanation to why we could not detect any correlation in patients with active disease.why gpa and mpa patients have decreased frequencies of eosinophils in peripheral blood is not known.a possible explanation could be that activated eosinophils are recruited to sites of inflammation and play an active role in the pathogenesis of aav.mononuclear cells have been reported to be the most frequent cell type in interstitial infiltration of anca-positive renal biopsies from patients with suspected systemic vasculitis, however eosinophils and neutrophils were found in 20 and 27% of the cases respectively [24].in this study we found less cd62l on eosinophils from patients with active disease but lower levels of cd11b.moreover, sera from aav patients seem to degrade nets more slowly than healthy controls [29].interestingly, kraaij et al. recently showed that ancas did not influence net formation by neutrophils from a healthy donor, but autologous serum induced increased formation of nets in neutrophils from three out five studied patients [30], suggesting that it is not anca per se that induces enhanced nets formation rather the combination of anca and neutrophils from aav patients.how anca induce formation of extra cellular traps in neutrophils and eosinophils is not known.ros are major effector molecules in inflammatory processes and tightly linked to eet formation [9, 10].in this study we show that aav eosinophils have decreased capacity to produce ros compared with healthy controls.moreover, eosinophils produce eets when stimulated with tnfα or c5a and addition of anca further increase the number of eetotic cells, suggesting that eosinophil can contribute to the autoantibody driven inflammatory process in a
number of words= 1259
[{'rouge-1': {'f': 0.35355209829041867, 'p': 0.7709966777408639,'r': 0.22936555891238672}, 'rouge-2': {'f': 0.22292382659983337, 'p': 0.4266666666666667,'r': 0.15087679516250946}, 'rouge-l': {'f': 0.3451994415205758, 'p': 0.6354761904761905,'r': 0.23695957820738137}}]
-----------------------------------------------------------------------------------------------------------------------------------
p388:
Extractive Summary:
background giant cell arteritis (gca) is the most common idiopathic systemic vasculitis in persons aged 50 years or older [1] and demonstrates a predilection for involvement of the aorta and its primary branches.despite the availability of effective biologic agents for other autoimmune conditions, the treatment options for gca, todate, have been limited.although glucocorticoids (gc) have been the standard-of-care for over six decades, treatment is not curative and is associated with significant morbidity.therefore, the ongoing identification of key immune mediators in disease pathogenesis is encouraging the pursuit of clinical trials with targeted therapeutic agents.the aim of this article is to summarize the fundamental pathogenic mechanisms and new potential therapeutic options for the treatment of gca.in order to accomplish this, we searched pubmed for the following search terms: “giant cell arteritis”, “large vessel vasculitis”, “temporal arteritis”, “arteritis” and “vasculitis”.publications from the past 10 years were analyzed for pathogenic and therapeutic studies.case reports were not included, unless providing unique insights into pathogenic pathways.the date of the last search was 1 july 2017.main text genetic associations several independent studies have implicated associations between gca susceptibility and certain human leukocyte antigen (hla) class i and class ii alleles.in particular, carriage of hla-drb1*0401 and drb1*0404 haplotypes have been consistently identified in multiple gca cohorts [2–7].a large-scale, international, genomewide association study has further reinforced the importance of the hla class ii region [8].over-representation of these hla class ii genes strongly suggests that gca is mediated by an antigen-driven immune response.while hla class ii genetic factors have demonstrated the strongest association with risk of gca development, several single nucleotide polymorphism risk signals in loci among non-hla regions have also been recognized (table 1).however, it should be noted that most studies evaluating genetic associations in gca have included small groups of patients and will require validation in larger patient cohorts.nevertheless, the wide array of potentially involved cellular pathways associated with gca susceptibility underscores the polygenic nature and complex immunopathogenesis employing both the innate and adaptive immune response in this condition.infection isolation of identical t cell clones from different vasculitic sites suggests a response to a specific antigenic stimulus [9] and studies have proposed that arterial wall dendritic cells may be activated by environmental infectious agents or autoantigens [10].several microorganisms have been suggested as a possible infectious trigger including chlamydia pneumoniae [11, 12], mycoplasma pneumoniae [13], burkholderia pseudomallei [14], parvovirus b19 [15, 16], herpes simplex virus [17] and ebstein-barr virus [18].although infection-induced autoimmunity leading to loss of self-tolerance through mechanisms of molecular mimicry, bystander t-cell activation and epitope spreading is plausible, direct evidence of such remains elusive.indeed, attempts to identify pathologic organisms in temporal artery biopsy specimens have produced inconsistent results for any specific causal infectious agent [15, 19–21].varicella zoster virus (vzv) has received recent focus as a potential associated infectious aetiology.these investigators have proposed that the vzv is transported along the afferent nerves to the temporal artery inciting an inflammatory process resulting in arteritis.the presence of vzv as a causative agent for gca, however, has not been substantiated by other groups.muratore and colleagues evaluated 79 formalin-fixed and fresh-frozen temporal artery biopsies (34 tab-positive gca, 15 tab-negative gca, and 30 controls) by immunohistochemistry and pcr analysis [25].furthermore, vzv dna was not found in any of the formalin-fixed or fresh-frozen tab samples.in addition to histopathology evaluations, population level studies have failed to show a causal role of vzv in gca.at current, conclusive evidence does not support direct infection with vzv as a causal process for the development of gca and the use of acyclovir as an adjunct to, or in lieu of, immunosuppression is unsubstantiated and not recommended.innate immune system vascular dendritic cells although the specific immunostimulatory trigger(s) is unknown, the immunopathology of gca appears to originate from a dysregulated interaction between the vessel wall and both the innate and adaptive immune systems [29, 30].unlike small vessels which rely primarily on oxygen through luminal diffusion, large vessels require a microvascular network (vasa vasorum) to distribute oxygen to the media-adventia vascular cell layers.arteries with vasa vasorum contain vascular dendritic cells (vasdcs) at the media-advential border where they are thought to participate in immune surveillance.in normal arteries, vasdcs are immature and lack the capacity to stimulate t cells [31] allowing arteries to maintain immune privilege and self-tolerance.in vasculitic lesions immune privilege is lost and vasdcs become activated via toll-like receptors (tlrs), redistributing throughout the vessel wall [32].activated vasdcs are able to attract and activate t lymphocytes and macrophages through production of specific chemokine and cytokine signatures, providing a microenvironment necessary for initiating and sustaining arterial inflammation and granuloma formation [29].this central role of vasdcs in arteritis has been demonstrated using a model in which human temporal arteries were engrafted into immunodeficient mice, following which targeted depletion of cd83+ vasdcs resulted in marked reduction of observed vasculitis.due to their role early in the disease process, prevention of initial vasdcs activation is an unlikely therapeutic target.however therapies addressing persistently activated vasdcs remain a viable option.macrophages gca is a granulomatous vasculitis and multinucleated giant cells, a key feature of macrophage involvement, are a considered a pathognomonic hallmark of arterial lesions.giant cell formation is present in approximately 50% of positive temporal artery biopsies.macrophages recruited by activated vasdcs and t lymphocytes infiltrate the arterial wall through the vasa vasorum and further differentiate into m1 and m2 phenotypes according to the arterial microenvironment.on the contrary, m2 macrophages colocalize to the intima-media border and produce proangiogenic growth factors (vascular endothelial growth factor, fibroblast growth factor and platelet-derived growth factor) which results in myofibroblast proliferation, relocation, and the marked thickening of the arterial intima.the emerging paradigm of personalized medicine is encouraging development of nanoparticle-based theranostic agents, which integrate therapeutic and imaging functionalities.these molecules are being explored in cancer and inflammatory diseases [35].preclinical models for imaging and treatment of pathogenic macrophages have shown initial promise [36]; however, further advances in this evolving field will be needed prior to consideration of selective macrophage ablation, inhibition, or phenotype modulation in gca.adaptive immune system t lymphocytes normal arteries are devoid of cd4+ t cell infiltration.in gca, t cells are recruited via chemokines secreted by activated vasdcs, enter the arterial layer initially through the vasa vasorum, polarize into effector cell types and infiltrate the arterial layers.two distinct cd4+ t effector cell subtypes have been identified as key regulators in vasculitic lesions of gca; type 17 helper t cells (th17) and type 1 helper t cells (th1) [37].these t cell lineages are likely to be stimulated by independent signals from distinct antigen presenting cells (apc) with th17 cells dependent on il-1β, il-6, il-21 and il-23, while the th1 pathway requires apcs secreting il-12 and il-18 [30, 37].the th17 pathway appears to be very responsive to treatment and glucocorticoids (gcs) rapidly reduce the th17 effector cytokine production of il-1, il-6, il-17 and il-23 [30, 38] with simultaneous depletion of both circulating and tissue infiltrative th17 cells.the rapid decline in these cytokines upon gc initiation contributes to the prompt decrease of systemic inflammatory features.the th1 cytokine signature identified in chronic vasculitis in gca is associated with production of il-2 and interferon-gamma (ifn- γ) and is poorly susceptible to gcs.compared to healthy controls, tregs in patients with gca appear to be defective and potentially pathogenic, exhibiting decreased proliferation and increased production of il-17 [40].compared to patients without atlos and healthy controls, arteries with atlos had a high level of b cell activating factor (baff) and a proliferation inducing ligand (april), proteins which are strongly correlated with the differentiation and proliferation of b cells.tocilizumab (tcz), a humanized anti il-6 receptor antibody has shown efficacy in both newly diagnosed and relapsing patients in several small observational series [78–85] and two double-blind, placebocontrolled trials [86, 87].while the benefit of il-6 blockade in treatment of gca is apparent, 44–47% of patients receiving tcz in giacta were still unable to reach sustained remission at 52 weeks [86].following a remission induction phase, 41 patients in remission at week 12 were randomized to receive either placebo infusions or monthly abatacept.no relapses were observed during treatment with ustekinumab but two of three patients experienced a flare following cessation of therapy.the benefit of jak inhibition in patients with gca is currently unknown but a phase 2 trial evaluating baricitinib (oral selective jak1, jak2 inhibitor) in patients with relapsing gca is currently ongoing (clinicaltrials.gov identifier nct03026504).a randomized, double-blind, placebo-controlled study evaluating gevokizumab (monoclonal antibody targeting il-1 beta) for the treatment of patients with relapsing gca is ongoing and has recruited 13 patients to date (eudract number 2013–002778-38).this agent is quickly being incorporated into the treatment algorithm of both newly diagnosed and relapsing patients with gca.
number of words= 1425
[{'rouge-1': {'f': 0.28089682667935345, 'p': 0.846595744680851,'r': 0.16838274932614555}, 'rouge-2': {'f': 0.16277657805469578, 'p': 0.35342245989304816,'r': 0.10573836817262307}, 'rouge-l': {'f': 0.2581597858528472, 'p': 0.6273770491803279,'r': 0.16251700680272108}}]
-----------------------------------------------------------------------------------------------------------------------------------
p389:
Extractive Summary:
uncontrolled gout can cause significant joint damage, tophaceous deposits, organ damage and comorbidity, as well as impairment in quality of life and substantial economic costs [1–5].the american college of rheumatology (acr) and european league against rheumatism (eular) recommend an sua target (‘control’) level of < 6 mg/dl in the majority of clinical cases, while the british society for rheumatology recommends even stricter guidelines for those with tophaceous gout of sua target levels < 5 mg/dl [6, 7, 10].additionally, in a recent study using the cross-sectional us national health and nutrition examination surveys (nhanes) more than two-thirds of individuals with gout had sua levels above target and less than half of those treated with ult reached control levels [17].both chronic kidney disease and gout are adversely affected by hyperuricemia and require adequate control to minimize adverse events, comorbidities and mortality [24].however, despite the clear benefits of proactive care and sua control, choice and dosing of appropriate drug therapies for these patients are persistent challenges to the healthcare professional [7, 14, 25, 26].studies have shown that patients with gout have higher hru than the general population and that the burden to the healthcare system is increasing [4, 27–31].we will describe healthcare utilization trajectory during the first 5 years of care for patients with gout with and without ckd, respectively, to provide crucial insights into the health outcomes and disease management of the comorbid patient.death records, including date of death from the israel central bureau of statistics, were linked to the clalit population using the unique identification number for all israeli residents.the need for consent was waived by the helsinki ethics committee of the chs (no. 037/2015).for example, patients identified on 1/1/ 2006 were followed for 5 years through 31/12/2010 and patients identified on 31/12/2009 were followed through 30/12/2014.patients had to be at least 25 years old as of index date.adults 18–24 years were excluded because the majority was serving in the israeli military where they receive full healthcare coverage.subject with at least one of these were considered to have gout.gout patients who had documentation of at least one of the above criteria prior to the start of study eligibility were excluded.measures demographic variables such as age, sex were collected at index date.socio-economic status (ses) (low, medium, high, or missing) is an area-level score calculated based on current or last place of residence thus it likely reflects the patient’s ses at the end of follow-up.in addition, the use of the latter ses indicator, as an adjustment for the confounding effect on resource utilization, is perhaps a better indicator of the cumulative influence of ses [39].statistical analysis age-adjusted incidence was calculated using the 2009 clalit population distribution and direct standardization according to the israeli population in 2009 (central bureau of satistics, 2010) was used to calculate age-standardized incidence of gout [45].descriptive analyses were performed to characterize the patient population’s demographic, medical history, and clinical characteristics of patients at index date.during the 5-year follow-up 1673 (20.2%) healthy gout patients developed ckd.change in patient kidney disease status from index date was not adjusted for in models since the goal of the analysis was to examine utilization based on characteristics at index date.patients were right-censored according to the month they left the health plan.cox proportional hazard models were used to assess the risk of death in patients with and without ckd at index date and stratified by age group < 55 and 55+ years accounting for age, sex, ses, cci, smoking status, bmi, sua control, and gout medication use.of this cohort, 3421 patients died (26.4%) and 233 (1.8%) left clalit with a total of 55,206 person-years of follow-up.the average age of gout patients at index date was 63.8 ± 15.6 years.thirty-six percent (n = 4654) of all incident cases of gout had ckd at their index date.gout patients without ckd at index date tended to be younger (without ckd = 58.1 ± 14.7 years vs with ckd = 74.0 ± 11.3 years), male (without ckd = 80.1% vs with ckd = 68.4%), of low ses (without ckd = 19.3% vs with ckd = 16.5%), and current smokers (without ckd = 15.8% vs with ckd = 6.8%) compared to those with these conditions.the health status of gout patients at index date indicated that 43.0% of patients are obese, with a larger percentage among those without (45.3%) than with (38.8%) ckd at index date.prior to index date, 29.4% of patients had no recorded sua level.patients without compared to those with ckd at index date had substantially higher missing test values (37.8% versus 14.4%).of those with available sua test data, 19.0% of patient without ckd had controlled sua (< 6 mg/dl) at index date compared to 9.1% of patients with ckd at index date.between group differences for patients with versus without ckd were observed for unadjusted and adjusted models (p < 0.001).among those 55+ years, the rate of change differed significantly between those with and without ckd for (p < 0.05) for average number per year of general practitioner visits, specialist visits, and months of allopurinol purchases.mean survival times differed significantly between groups (without ckd < 55 years: 59.1 ± 6.0 months, with ckd < 55 years: 54.5 ± 14.3months, without ckd 55+ years: 53.5 ± 15.3months, with ckd 55+ years: 45.4 ± 20.3 months; p < 0.001).cox regression models assessed hazard of survival from diagnosis and adjusted for index date characteristics: age, sex, ses, cci, smoking status, bmi, sua control (< or ≥ 6mg/dl), and gout medication use.for both age groups, the risk of dying was higher among those with compared to those without ckd (hr<55 years = 1.65; 95% ci 1.01–2.71; hr55+ years = 1.50; 95% ci 1.37–1.65).an increased mortality risk of 65 and 71% for those with ckd compared to without ckd was observed for patients < 55 years and 55+ years, respectively.reaching this target requires close monitoring due to potential drug-drug interactions and adverse events related to ult use especially for ckd patients [48, 49].among ckd patients, control of hyperuricemia is debated among experts [26, 48, 50] and clinical guidelines for these patients note that there is insufficient evidence to recommend lowering serum uric acid in order to prevent ckd progression [51, 52].the lack of this information detracts from assessing the full extent of gout, regardless of subgroup on the economic burden to the healthcare system.in addition, while the relatively large sample size allowed for increased power to detect difference, caution should be applied to the relevance of difference.
number of words= 1073
[{'rouge-1': {'f': 0.4326498745945935, 'p': 0.7885792349726777,'r': 0.298100607111882}, 'rouge-2': {'f': 0.22083310560795294, 'p': 0.35493150684931507,'r': 0.1602777777777778}, 'rouge-l': {'f': 0.38513372520386363, 'p': 0.6361375661375661,'r': 0.27616570327552986}}]
-----------------------------------------------------------------------------------------------------------------------------------
p390:
Extractive Summary:
as greater height and lean mass is related to superior performance [2], it is common for athletes to weigh above the criterion and utilize weight loss techniques over the 24 h before competition to qualify for racing [1].dehydration is a common technique used to achieve weight loss because it allows for temporary reductions in weight without muscle protein loss, and hydration status can be at least partially restored over the 2-h rehydration window between weigh-ins and racing [3].however, dehydration may still reduce performance [4].rowing performance relies on multiple components including an athlete’s efficient use of the aerobic and anaerobic energy systems, and capacity to produce power [5].additionally, the neuromotor system controls stroke execution and balance maintenance by continuously adapting the execution signal in response to sensory feedback [6, 7].these systems have been demonstrated to be vulnerable to weight reduction via dehydration in settings outside of rowing [8].it is unclear how and if changes in these systems contribute to changes in rowing performance following weight-reducing dehydration practices.some research suggests that the effect of weight-reducing dehydration practices may depend on the dehydration technique used [8, 9].four common options exist for rowers to achieve weight loss through dehydration before weighing in: diuretics, exercise, thermal exposure, and fluid abstinence [10].as use of diuretics is illegal in elite sport [11] and exercise-induced dehydration appears to impair aerobic [12], anaerobic [9], and balance performance [13] to a greater extent than other dehydration techniques, these are poor candidates for rowers.instead, rowers may choose to reduce weight by using either thermal exposure or fluid abstinence.previous work [3, 4, 14–17] has demonstrated effects of large amounts of dehydration on 2000 m rowing performance: a common performance test in rowing.within a pilot study [18], we found that this performance test may be negatively affected by smaller amounts of dehydration but that this effect depended on the technique used to elicit dehydration.currently, there is a need to better understand how smaller amounts of dehydration effect rowing performance, the physiologic systems involved, and whether the dehydration technique used matters.the present study examined elite collegiate rowers within an exploratory, quasi-experimental, within-participants design to investigate whether changes in weight resulting from mild weight-reducing dehydration practices are related to changes in bodily systems involved in rowing.further, we sought to determine whether these relationships were dependent on the technique through which weight loss was achieved.we examined proxies of the aerobic energy system, anaerobic lactic energy system, anaerobic alactic energy system, and the neuromotor control system following weight loss via thermal exposure and fluid abstinence.affected systems may contribute to poorer rowing performance following weight-reducing practices.methods participants a total of 6 heavyweight and 6 lightweight competitive male rowers participated in the study (table 1).females were not included because body weight and total body water vary idiosyncratically with the menstrual cycle [19].participation was restricted to individuals ≥ 16 years who had competed at least 1 season at the university level or were members of a canadian national rowing hub.all methods, including the study experiments and dehydration protocols (developed in consultation with coaches and athletes), were approved by the trent university research ethics board (reb#25493) and complied with the declaration of helsinki.all participants provided informed written consent prior to participating in the study.design rowers completed an incremental vo2max test, peak power test, and 3 visuomotor tasks after 3 different weight loss conditions: once following weight loss through thermal exposure (sau), once following weight loss through fluid abstinence and thermal exposure (fa + sau), and once following no weight loss (con).these hydration manipulations were used to achieve varying amounts of weight loss that further varied in the amount of each dehydration technique used to cause it.we quantified the observed amount of weight loss owed to each dehydration technique by measuring the change in weight across the timespan allotted for it on each of the 3 testing days.we summed these values to get the total weight change on each day.through this process, we obtained a weight loss for each participant on each testing day associated with fluid abstinence, thermal exposure, and their sum (which approached zero on con day because they were unexposed during the timespans).these changes in weight were used as the independent variables in our analyses (further details follow).participants were tested on 3 separate days within a 2-week period and experienced the weight-loss conditions in a counterbalanced order.blood osmolality, urine osmolality, and 2 dehydration questionnaires were recorded with body mass to assess whether this weight loss could be attributed to dehydration.procedure baseline body weight, from which the weight reduction was calculated, was determined from a series of 5 baseline weigh-ins that were completed on non-test days immediately prior, during, and immediately following the study period.on the first testing day involving weight loss, participants attempted to achieve a weight reduction of 2.5% their average body mass by an official weigh-in time.this target weight loss was used as a guide to enable participants to reasonably match weight loss on future testing days.this target weight loss was selected based on findings from a pilot study [18].on a participant’s second testing day involving weight loss, they attempted to match their previous weight loss.in all conditions, participants were required to fast from midnight until after the official weigh-in to minimize the impact of differing food consumption on body weight.on the sau day, the participants achieved weight loss by sitting in a sauna.on the fa + sau day, participants were instructed to abstain from fluid intake for 15 h over the evening and night prior testing to achieve weight loss.similar fluid restriction protocols have been previously employed by other authors [16, 17, 20, 21].this fluid restriction protocol has been previously used over 12 h within a pilot study to induce 1–2% body mass loss [18].this was followed by time in the sauna, if necessary.in both conditions, exposure to the sauna (70 °c) was provided in 15-min increments up to a total of 60 min.temperatures within this range have been previously used to elicit dehydration [22, 23] and were used within our pilot study [18].if at the end of any of these 15-min increments the target weight loss was achieved, the participant was exempt from further time in the sauna and maintained their weight loss until the official weigh-in time.sauna exposure on fa + sau day was included to enable matched weight loss across testing days and to ensure that total weight loss was not correlated with dehydration technique.on the con day, the participants’ weight was not manipulated.participants waited in the laboratory until their official weigh-in time for a matched amount of time (1 h and 15 min); their hydration was not restricted during this time.mimicking international competition, participants were provided 2 h to rehydrate between their official weigh-in time and exercise testing.it was recommended they rehydrate aggressively.water, sport electrolyte drinks and breakfast foods were provided, and participants recorded the fluid and food they consumed.they completed a 24-h diet recall for the day prior (50 min following their official weigh-in) and a battery of visuomotor tests (80 min following their official weigh-in).this 24-h diet recall was also completed the day prior to testing to assess fluid and food intake 2 days prior testing.participants then completed a 10-min warm up period at a self-prescribed intensity, a peak power test, and, finally, an incremental vo2max test.all tests were completed within 90 min of the end of the rehydration window.all protocols were completed between 7 am and 1 pm under thermoneutral conditions.to track weight loss/dehydration over the testing day(s), participants were weighed and completed thirst and dehydration symptomology questionnaires upon arrival, at the time of the official weigh-in and following the rehydration window.blood and urine samples were taken at the time of the official weigh-in and following the rehydration window.instruments and tasks body mass change all body mass measurements were completed postvoided on a scale in pre-weighed clothes that were factored in (health-o-meter 349klx, health-o-meter professional, mccook, usa).baseline body mass measurements were completed in the morning (6am-9am) after fasting from midnight onwards (without restricting water consumption).these baseline body mass measurements were completed on non-test days (con, sau, fa + sau).weight changes were expressed as percent changes in body mass (%bmc).the total change in weight (total%bmc), the change in weight due to thermal exposure (thermal%bmc), and the change in weight due to fluid abstinence (abstinence%bmc) was calculated for each testing day (con, sau and fa + sau; see equations in additional file 1).each measure should approach 0 on con day.weight changes due to fluid abstinence and thermal exposure were considered the observed weight changes over the time allotted for each (i.e. abstinence%bmc = the weight change at arrival; thermal%bmc = the weight change between arrival and the official weigh-in).the change in weight over the rehydration window was also calculated.the equations for determining total%bmc and partitioning abstinence%bmc and thermal%bmc are provided in the supplementary material.additional measures of hydration we checked our hydration manipulation by assessing the athlete’s blood plasma osmolality (mmol/l) and urine osmolality (mmol/l).blood was collected in bd pst gel and lithium heparin vacutainers according via standard phlebotomy procedures [24] and immediately centrifuged (relative centrifugal force: 1000; 10 min).urine samples were self-collected mid-stream.blood and urine samples were aliquoted and subjected, in triplicate, to freezing point depression osmometry (3320 single- sample micro osmometer, advanced instruments, norwood, usa).the average osmolality was recorded for each specimen.additionally, questionnaires were used to measure rowers’ perception of thirst (visual analogue scale of thirst; vast) [25] and dehydration symptomology (symptom evaluation subscale [ses] of the sport concussion assessment test 3; scat3) [26] and assessed relative to scores recorded during the baseline weigh-in sessions.the scat3 has previously been used to assess the effects of dehydration due to its overlap in symptomology with concussion [27].visuomotor tasks three tests were used to assess aspects of visuomotor control: the double-step, stop-signal, and interception tasks [28].the tests were coded in matlab (the mathworks, inc., natick, usa), using the psychophysics toolbox extensions [29–31] and were presented on the detachable touchscreen of a personal computer (surface book 2, microsoft, redmond, wa).the code is available: https:// github.
number of words= 1661
[{'rouge-1': {'f': 0.25495265743088474, 'p': 0.7291928251121076,'r': 0.15448275862068966}, 'rouge-2': {'f': 0.1793797794358627, 'p': 0.41684684684684686,'r': 0.11427832087406556}, 'rouge-l': {'f': 0.2854628500039827, 'p': 0.6513953488372093,'r': 0.18278195488721805}}]
-----------------------------------------------------------------------------------------------------------------------------------
p391:
Extractive Summary:
statistical analysis the ess observed under the bws and nw conditions were compared with a paired two-tailed t-test.the cot values as well as observed es and pws with bws and nw were compared with a two-way repeated measures analysis of variance (anova) within-participants using online software (anova 4).if a significant f values was obtained, ryan’s post hoc test was further applied to the appropriate data sets to detect significant mean differences; its statistical power has been reported to be equivalent to tukey’s post hoc test [20], and it can be used regardless of the data distribution [20].a bland– altman plot was applied to evaluate whether each data set involved systematic bias [21], which consists of ’fixed’ bias and ’proportional’ bias.if 95% confidence interval ( ci95%) of the individual pws–es differences included zero, there was no systematic bias between es and pws in each condition.the ci95% was determined by a following equation where dm is the mean of the individual pws–es differences, t39 is the t-distribution value with a degree of freedom of 39, sdd is the standard deviation of the individual pws–es differences, and n is the total number of the participants.to evaluate whether the data set involved proportional bias, a correlation analysis was conducted between individual pws–es differences and individual average of the es and pws.if there was a significant correlation, there was a proportional bias between es and pws.the upper and lower limits of agreement were determined using a following equation: statistical significance was set at p < 0.05.all data were presented as mean ± sd.ci95% = dm ± t39 · sdd · n−1 (4) (5) upper and lower limits of agreement = dm ± 1.96·sdd results the cot values were significantly decreased by 9.2% (1.56 m s− 1), 15.6% (1.78 m s− 1), and 20.8% (2.00 m s− 1) with bws than nw, but not at speeds slower than 1.33 m s− 1 (f = 7.901, p = 0.006; fig. 2).a significantly faster es was observed with bws (1.61 ± 0.11 m s− 1) than nw (1.39 ± 0.06 m s− 1) (t = 11.420, p < 0.001; fig. 2).the pws was not significantly different from the es under both conditions (f = 0.215, p = 0.646; fig. 3a).discussion the cot values were significantly lower with bws than nw at speeds faster than 1.56 m s− 1, however, no significantly lower cot values were observed at speeds slower than 1.33 m s− 1 (fig. 2), resulting in a significantly faster es with bws than nw (fig. 2).these results supported our first hypothesis.however, the metabolic rate during walking at any walking speed has been reported to decrease linearly with increasing body weight support [13, 22–24], and the reduced metabolic rate seemed to depend on walking speed [13, 22–24].these partial discrepancies between our present study and some previous studies would be derived from different characteristics of used apparatus, such as torso suspension with a long elastic harness [22–24] and lower-body positive pressure [13].as technical considerations, the faster es with bws was potentially associated with a characteristics of a fluctuation of the vertical force caused by spring-like characteristics of our apparatus (fig. 1b).the stroke length of the spring of our bws apparatus is much shorter than others [22–24].when using such an apparatus during walking, the vertical force tends to fluctuate especially at faster speeds (fig. 1b), because vertical displacement of the center of body mass increases as walking speed increases.at speeds faster than 1.56 m s− 1, a percentage of bws increased by ~ 34% at the propulsive phase of a gait cycle (fig. 1b), which allows more mechanical work to redirect the center of body mass upward and forward [25].reducing the mechanical work at the propulsive phase potentially reduce the metabolic rate or energy cost of walking [26–28], indicating that the reduced cot values with bws at speeds faster than 1.56 m s− 1 could be attributed to the accelerated increase in the vertical force at the propulsive phase by the springlike behavior of our bws apparatus.in support of our second hypothesis, the pws was equivalent to the es not only under the nw but also with bws (fig. 3a).no fixed bias was found between es and pws in both conditions, because the dm was almost zero (blue lines in fig. 3b, c).instead, there was a proportional bias between es and pws under nw, but not with bws, because there was a significant relationship between individual pws–es difference and individual average of the es and pws under nw (r = 0.670, p < 0.001; fig. 3b).however, most of the plots under nw was within the upper and lower limits of agreements (fig. 3b).the ci95% values ranged between negative and positive values in both conditions (fig. 3b, c), indicating that there was no systematic bias between es and pws in both conditions.these statistical results mean that the pws was almost equivalent to the es, and the pws can be a substitutable index of the individual es, at least, in our participants.a recent longitudinal survey revealed that normal walking speed in daily lives, being essentially linked to the pws, was already associated with physical and biological functions of accelerating aging even in midlife [29].pws can be easily determined in each individual, so that a practical use of the pws in our daily lives may contribute to monitor our locomotion ability.other notable intervention studies reported that the pws was slower on rough terrain compared to smooth terrain, and that the pws was significantly slower than the es on both terrains [30].
number of words= 919
[{'rouge-1': {'f': 0.386976927951681, 'p': 0.794,'r': 0.2558316221765914}, 'rouge-2': {'f': 0.24618000779999946, 'p': 0.4555421686746988,'r': 0.1686639260020555}, 'rouge-l': {'f': 0.36680360893970176, 'p': 0.6113533834586466,'r': 0.262}}]
-----------------------------------------------------------------------------------------------------------------------------------
p392:
Extractive Summary:
background as wrestling is a weight-categorised sport, wrestlers and coaches take advantage of body size and physical strength using rapid weight loss (rwl) strategies.until 2017, a rule of united world wrestling stated that an official weigh-in had to be conducted on the evening before each competition [1].in 2018, this rule changed to requiring a weigh-in each morning [2].other sports, such as professional boxing and mixed martial arts, still perform a weigh-in the day before the competition.thus, there are some rules depend on the type of sports, and optimal weigh-in rules are still under argument.traditionally, many male wrestlers (38–69%), particularly in the lightweight and middleweight classes, attempt to “make weight” by losing > 5% of their body mass approximately one week before their competition and then ingest fluids and food to recover their physical condition as quickly as possible [4–6].common rwl methods include food and fluid restriction, fasting, eating less food during each meal, decreasing carbohydrate intake, increasing exercise, saunas and baths, and training with rubber suits [4, 5, 7].these methods induce dehydration [8, 9], and decrease muscle glycogen (mgly) [10, 11].as a result, aerobic and anaerobic performance decrease [12] and immune [13, 14] and cognitive functions are impaired [15].however, wrestlers and coaches have considered that even if rwl reduces strength or endurance performance, these can be recovered through sufficient fluid and nutrition intake because the weigh-in was conducted in the evening before the day of each tournament.in addition, a recent review reported that an rwl of 5–8% body mass with a small impact on health and performance has remained acceptable practice [16].thus, if we accept rlw within a suitable range, the best nutrition strategies for recovery must be elucidated to achieve the best performance.mgly is a major energy source of moderate- to highintensity exercise [17].wrestling is an intermittent, combative sport requiring technical, tactical and psychological skill that demands absolute muscle strength and power on both the upper and lower body [18, 19].each match lasts approximately 6 min (two, 3 min rounds with a 30 s rest) including tackling, pushout, lifting, throwing, and blocking, which are high-intensity movements [20], and wrestlers must compete in up to five matches [21].therefore, the strategies of mgly recovery in the morning before the wrestling matches are important.to the best of our knowledge, few studies, with only one manipulated carbohydrate intake after rwl, have investigated nutrient strategies to recover mgly overnight, which is generally the time between weigh-in and the first match.only one study of rankin et al. [22] reported the effects of carbohydrate intake from a highcarbohydrate meal (75% energy) compared with those of a moderate-carbohydrate meal (47% energy) for a 5 h recovery period on anaerobic performance.in the study, anaerobic performance tended to be higher after a high-carbohydrate meal than a moderate-carbohydrate meal, which was 99.1% and 91.5% of baseline after recovery, respectively, although mgly concentration was not obtained.as a nutrition guideline, the carbohydrate recommendation after or during exercise is described to be dependent on the exercise situation, such as intensity and duration [23].however, the carbohydrates required to recover mgly between rwl and the match on the next morning remain to be elucidated.the aim of this study was to examine the effect of a high-carbohydrate meal of 7.1 g kg− 1 on the mgly concentration after rwl and an overnight recovery meal.to obtain the recovery process of mgly and short-term mgly synthesis, we measured these at 2 and 4 h after initiating the recovery meal.we hypothesized that a highcarbohydrate meal induces recovery of the mgly deficit after an overnight recovery phase.methods participants ten male collegiate wrestlers (age, 20.9 ± 0.5 year; height, 168.9 ± 4.3 cm; body mass, 73.2 ± 8.2 kg; % body fat, 11.2% ± 2.0%) were recruited from two teams in japanese colleage after receiving information about this study from their team coaches.all subjects belonged to the east japan collegiate league and had competed in wrestling matches on an international (n = 3), national level (n = 5) and regional level (n = 2).eligibility into the study required participants: (1) ≥ 18 years of age, (2) had experienced losing over 6% of their body mass before a major competition, (3) free of any metabolic, thyroid, or heart diseases.all participants submitted their written informed consent before the experiment began.this study was approved by the institutional review board of the japan institute of sports sciences (036 in 2014).experimental design all participants were instructed to complete their daily life dietary and training records before the experiment.participants visited our institute on the day before baseline (bl) measurements and ate their usual amount of dinner in a buffet restaurant by 21:00.they were given instructions to abstained from taking alcohol or stimulant beverages and to refrain from hard exercise for at least 12 h prior to bl measurement.they were allowed to drink mineral water after 23:00.we obtained anthropometric measurements, body composition, and mgly concentration, and took blood and urine samples at 06:30 (bl).after the measurements, we instructed them to lose 6% of their body mass.the methods of weight loss is described in detail elsewhere [8].at 53 h after bl measurements, they visited the laboratory for measurements after 6% rwl (r0).after the r0 measurements had been collected, the participants were provided with three prescribed meals (table 1) to consume between 17:30 and 23:00, which was similar to previous study [24].mgly concentrations were obtained 2 h (r2) and 4 h (r4) after initiating the prescribed meal.measurements were taken again on the next morning at 06:30, 13 h after initiating the prescribed meal (r13).conclusions we conclude that a high-carbohydrate meal of 7.1 g kg− 1 body mass after 6% rwl was insufficient to recover the mgly during a 13 h recovery phase.
number of words= 942
[{'rouge-1': {'f': 0.34612520722024, 'p': 0.7187603305785124,'r': 0.22794768611670022}, 'rouge-2': {'f': 0.20048667077623888, 'p': 0.3563070539419087,'r': 0.13948640483383687}, 'rouge-l': {'f': 0.3542301417113478, 'p': 0.6671223021582733,'r': 0.2411340206185567}}]
-----------------------------------------------------------------------------------------------------------------------------------
p393:
Extractive Summary:
consequently, the use of recovery interventions purported to accelerate recovery has become increasingly prevalent.there is an emerging interest in the effects of the non-nutritive compounds (poly)phenols as recovery aids following strenuous exercise.as such their popularity as a nutritional aid has increased in athletes and recreational exercisers, likely because these plant-based bioactive compounds have numerous additional health benefits [3].the term (poly)phenol refers to a variety of bioactive compounds including flavonoids, stilbenes, phenolic acids and lignans [4].the largest subclass, flavonoids, can be further classified into flavonols, flavanols, flavanones, anthocyanins, flavones and isoflavones. of these subclasses, the majority of research has focused on flavanols with particular attention on cocoa, not only because of the palatability of chocolate [5] but due to the high proportion of monomers such as catechin, epicatechin and gallocatechin; collectively referred to as cocoa flavanols (cf).these monomers are found in the largest quantities in cocoa when compared with other flavanol containing foodstuffs such as tea and fruits; however, the amounts vary considerably. for cocoa, the flavanol content depends on the bean, such as the seed it grows from [6], the manufacturing process, such as heavy alkalisation and temperature of roasting [7], and the final product, (e.g., milk chocolate versus dark chocolate) [8].cocoa flavanols have been shown to possess antiinflammatory and antioxidant effects, with epicatechin the most potent monomer of the flavanol group [9].currently, cardiovascular benefits, such as improved flow mediated dilation and reduced blood pressure, have been observed following various doses of cf, such as, 918 mg [10], 701 mg [11], 750 mg [12], and 917 mg [13] and epicatechin intakes as low as 25 mg [14] and 46 mg [15].regarding epicatechin, greater efficacy has been reported at higher epicatechin doses (see review [16]).these benefits have been observed following supplementation periods ranging from the same day of testing [11, 13], to seven days [10], and 30 days [12].additionally, cf may be beneficial for reducing markers of oxidative stress (defined as an imbalance in the generation of various reactive species and antioxidants [17]) and inflammation [18, 19].the role of cf in modulating inflammation may stem from their capacity to influence signalling cascades, i.e., via an alteration to eicosanoid production [20], and reducing the activation of certain inflammatory transcription factors, e.g., nuclear factor kappa-beta [21].given that exercise-induced muscle damage (eimd) is thought to partly stem from inflammation and oxidative stress, cf may be able to attenuate functional symptoms that impede athlete recovery, such as muscular soreness and deficits in muscle function [19, 22].reactive oxygen species (ros) are produced as part of normal metabolic processes, such as cellular respiration, and in certain scenarios, such as exercise, ros are produced in high amounts [2].various ros molecules are involved in a plethora of functions at a cellular level, including, growth and proliferation [23], immune response [24] and apoptosis [25].additionally, it is believed that ros act as signalling molecules in various tissues; however, this is still not fully understood due to the numerous ros produced at rest and during exercise [26].antioxidant defence systems maintain a balance between ros production and neutralisation; if the production of ros outweighs their neutralisation, then proteins, lipids and dna may be oxidised altering their function [27].this process is typically referred to as oxidative stress.alternatively, if cells are exposed to low levels of ros, such as during moderate intensity exercise, they may act as signalling molecules for skeletal muscle adaptations [28].such adaptations include an increase in endogenous antioxidants such as superoxide dismutase, glutathione peroxidase and catalase, reduced oxidative damage from exercise and an improved resistance to oxidative stress [29].the mechanisms by which cf modulate redox metabolism and oxidative stress are not entirely clear, but activation of the nuclear factor erythroid 2-related factor 2 (nrf2) transcription pathway, which activates a battery of cytoprotective protein with antioxidant and anti-inflammatory functions is a potential candidate [30].for example, it has been observed that supplementation with catechin results in an increase in the expression of heme-oxygenase 1, an enzyme with antioxidant and anti-inflammatory functions (31), via upregulation of nrf2 activity [30].moreover, cells treated with cf induced an increase in glutathione peroxidase and glutathione reductase, likely via nrf2 activation [32].in addition, cf treatment has been shown to prevent a depletion in reduced glutathione and replenish glutathione peroxidase, as well as effectively limiting lipid and protein peroxidation [33].collectively, these studies suggest cf may modulate oxidative stress, at least partly via redox sensitive pathways, e.g., stimulating nrf2 which in turn leads to an increase in redox enzyme expression.strenuous exercise may generate large amounts of ros that leads to oxidative stress.the ros produced is thought to stem from the increase in cellular respiration, and/or immune cells like neutrophils [1,34].leukocytes that accumulate in the muscle after eimd evoke a respiratory burst, whereby macrophages and neutrophils produce large amounts of ros to lyse cellular debris and begin regeneration.however, it has been proposed that during this process ros may also induce lipid peroxidation in nearby healthy tissues [35].it is thought that this damage to neighbouring cells might contribute to eimd, and at least partly explain why decrements in muscle function and increased muscle soreness can persist for several days after strenuous exercise [36].therefore, the aim of this narrative review was to critically examine research on the effects of cf on oxidative stress, inflammation, muscle function, perceived soreness, and exercise performance.this review builds on previous work by decroix, soares 19] that reviewed the effects of cf on exercise performance.the present review includes research completed since the aforementioned article and unlike decroix and colleagues focuses on cf and eimd.to identify the appropriate literature, an in-depth, systematic search of five separate databases was performed (pubmed, scopus, web of science, scienceopen and medline) using specific search terms (‘cocoa flavanols,’ or ‘dark chocolate,’ and ‘muscle damage,’ or ‘muscle recovery,’ or ‘exercise recovery,’ or ‘exercise-induced muscle damage,’ or ‘exercise’).all reviewed studies conducted the investigations in humans.impact of cocoa flavanols on exercise‑induced oxidative stress « table 1» antioxidants maintain redox status by neutralising ros produced by metabolic reactions [37].comparisons between different doses and thus establishing of an optimal dose to elicit benefits is needed before concrete recommendations can be made.it is also important that studies investigating eimd should use protocols that evoke sufficient muscle damage (e.g., inflammation, muscle soreness).although, such protocols may not be applicable to real world sport, they will be useful for determining the potential mechanisms by which cf might alter physiology and enhance exercise performance and recovery.nevertheless, studies should also investigate the effect of cf supplementation on recovery following real world exercise or movements that can induce muscle damage (e.g., repeated sprint protocols) instead of solely laboratory-based protocols that may not replicate the demands or damage response that follows sporting performances.this may lead to greater practical application within sport settings.utilising both variants of eimd protocol approaches will aid understanding of the potential ergogenic value of supplementing cf in an athlete’s diet.it may be pertinent to investigate prolonged flavanol supplementation on repeated bouts of exercise, with a focus on performance and recovery.moreover, investigating the impact that cf may have on exercising muscle is required to develop greater understanding of the mechanisms in which cf exert any effects, such as their influence on endogenous antioxidant enzymes and survival signalling proteins.indeed, future research should also look to further the knowledge of cf and their role in signalling pathways such as nuclear factor-kappa beta and nuclear erythroid 2-related factor 2, and how the regulation of these pathways may attenuate muscle damage.conclusion few studies have examined the effects of cf on recovery following eimd.of the available data sub-chronic (~ 7–14 day) supplementation of cf via dark chocolate solids or in the form of a high flavanol beverage reduces exercise-induced oxidative stress and has potential for delaying fatigue during exercise allowing for prolonged performance.however, data on recovery of muscle function, and the analgesic and anti-inflammatory effects of cf is limited.
number of words= 1296
[{'rouge-1': {'f': 0.2653779805080133, 'p': 0.8768965517241378,'r': 0.1563468634686347}, 'rouge-2': {'f': 0.188074463538456, 'p': 0.5005555555555556,'r': 0.11579025110782866}, 'rouge-l': {'f': 0.2668877566679967, 'p': 0.761358024691358,'r': 0.1618032786885246}}]
-----------------------------------------------------------------------------------------------------------------------------------
p394:
Extractive Summary:
the study flowchart is illustrated in fig. 1.body composition, upper body muscular strength, muscular endurance, cardiovascular endurance, flexibility, pa enjoyment, and quality of life were measured as outcome variables.the inclusion criteria were as follows: (a) adults aged ≥ 18 years, (b) pwpd registered in the central registry of rehabilitation, (c) presence of at least one functional arm, (d) no diagnosis of cognitive impairment, and (e) no participation in a structured pa program in the 6 months preceding the study.the exclusion criteria were as follows: (a) history of cardiovascular disease that would hinder study participation; (b) severe body pains, dizziness, and uncontrollable high blood pressure (> 160/100 mmhg); (c) seeing and hearing difficulties; (d) physician disapproval to participate in the study due to a limiting medical and physical conditions; and (e) inability to communicate in chinese.all eligible participants received a form providing a brief description of the study and were asked to sign a consent form.this study was approved by the institutional review board of university (human research ethics committee of education university of hong kong, a2019-2020–0303).the total distance travelled was recorded.the validity (predictive to maximum oxygen uptake: r = .59) and reliability (test–retest reliability: icc = .99) of the mft have been demonstrated in a study involving adults using wheelchairs [23].the short, chinese version of the physical activity enjoyment scale (paces) [24] was used to measure pa enjoyment in this study.the eight-item paces has been demonstrated to be a reliable (test–retest reliability: icc = .614) and valid (convergent validity: r = .43) instrument for assessing pa enjoyment among chinese adults [24].seven polarised rating items include “i find it pleasurable” (one end) to “i find it unpleasurable” (the other end).the mean score of the eight items was calculated to determine the pa enjoyment score.data analysis data were analyzed using spss 26.0.the t test was used to evaluate baseline differences between the groups.a series of 2 (group: intervention group vs control group) by 2 (time: pretest vs posttest) ancova for each outcome physical and psychological attribute were used to examine the effects of the intervention on measures of physical and psychological attributes in the groups.all effect sizes are reported as partial η2 and they are interpreted as medium: η2 = .06 and large: η2 = .14 [25].the alpha was set at p < .05 for all statistical tests.excluding data from individuals who withdrew from the slvb group, the average attendance rate was 71.81%.the mean age was 48.89 years [standard deviation (sd) = 14.42].over 50% of them aged between 50–58 years old.approximately half were either homemakers (18.75%) or retirees (37.5%).the most common health conditions were poliomyelitis (n = 28), limb deficiency (n = 4), cerebral palsy (n = 2), and others (n = 6).outcome measures the differences in pretest scores between the cg and slvb group were not statistically different for all outcome measures, except for pa enjoyment (t (30) = 2.07, p = .047).slightly higher levels of enjoyment were reported by the slvb group (5.21, sd = .96) than the cg (4.58, sd = .70).the means and sds of physical and psychological measures of the groups at pretests and posttest are presented in table 2.after the intervention, the slvb group had statistically significant lower adjusted fat mass levels [triceps, f(1,29) = 4.17, p = .050, η2 = .126] and higher adjusted aerobic endurance levels [f(1,29) = 4.27, p = .048, η2 = .128] compared with the cg.no significant interactive effects were found in other physical measures (see table 2).psychologically, a significant interaction effect was found for pa enjoyment [f(1,28) = 5.34, p = .028, partial η2 = .156].following a 16-week intervention that involved participation in slvb, compared with cg group, there were improvements in cardiovascular endurance, body composition (lower fat mass), and pa enjoyment in slvb group.there were no differences in flexibility, upper body strength, and endurance between groups.increased cardiovascular endurance resulting from the intervention may have contributed to lower fat mass of the slvb group.this result partially corroborates prior findings that exercises that improve cardiovascular endurance, improve fat distribution, and reduce overweight and obesity [30].lack of improvements in flexibility, upper body muscular strength, and muscular endurance.a previous study [27] confirmed the relationships between flexibility, muscular strength, muscular endurance, and performance of svb skills.for example, serving and blocking were correlated with flexibility (i.e., shoulder stretch test), whereas defense, service, and grip strength were correlated.in addition, higher levels of muscular endurance predicted higher performance in serving, blocking, and overall performance of svb players.however, our findings contrast with these results.in addition, tests such as the grip strength test (forearm strength), might not fully reflect participant’s upper body muscle endurance improvement resulting from the intervention.other slvb movement-specific measurements should be considered in the future.altogether, these factors explain the nonsignificant improvement in upper body muscular ability and flexibility.slvb is a team sport in which players cooperate and work with their teammates.players must collaborate and communicate with their team members in the processes of passing, setting, and spiking.this low dropout rate might be because our intervention entailed fun, self-efficacy, and social contacts, which encouraged participation, and this also helped overcome barriers such as the lack of opportunities for participation in sports and accessibility problems, as suggested in a previous study [38].therefore, future studies should include follow-up assessment with a more effective plan to monitor adherence to the intervention and changes in physical and psychological measures through study designs such as the wait-list control trial design.conclusion and practical implication this is the first study to examine the effects of an intervention involving an adapted sport, slvb, on the health outcomes of pwpd in hong kong.our results showed that slvb was beneficial to physical health (i.e., cardiovascular endurance and body composition) and psychological heath (i.e., pa enjoyment) among pwpd.moreover, it is playable on badminton courts, which avoids the problem of space constraints.our study therefore provides evidence-based information to the government and related practitioners for the promotion of adapted pa (such as slvb) among pw
number of words= 985
[{'rouge-1': {'f': 0.32505987595419844, 'p': 0.7675609756097561,'r': 0.2061904761904762}, 'rouge-2': {'f': 0.19363565538679897, 'p': 0.3788235294117647,'r': 0.13005719733079124}, 'rouge-l': {'f': 0.3194290052101372, 'p': 0.6448031496062991,'r': 0.21230019493177388}}]
-----------------------------------------------------------------------------------------------------------------------------------
p395:
Extractive Summary:
background nowadays, it is well known that exercise performed adequately and regularly  has physiological [1], psychological [2] and immunological [3] benefts to human health.people with cystic fbrosis (cf) can beneft from regular exercise participation not only in terms of increased respiratory capacity but also the prevention of comorbidities, as the current therapies and new medications have markedly prolonged the lifespans of people with cf [5].ward et  al. confrmed that exercise is a fundamental therapeutic intervention for people with cf [10].radtke et al. observed no adverse side efects of exercising [8].methods study design tis observational, web-based exercise intervention lasted 52  weeks and is a part of the cystic fbrosis online mentoring for microbiome, exercise and diet (commed) study.within commed a cf physician, a dietician and a sport scientist could personally interact with the participants via a website and directly at three specifc time points.te cf physician was present at each testing timepoint.te dietician could provide nutritional recommendations at each testing timepoint.whereas the information exchange with the medical doctor and the dietician was voluntary, the participants were obligated to exchange information with the sports scientist weekly.te exclusion criteria were orthopedic, rheumatic, cardiovascular or neurological contraindications; no internet access; and an acute exacerbation.written informed consent was obtained from all participants (or their parents or guardian).afterwards, the participants were registered on the website and received training material, including three resistance bands (pinoft®), a heartrate monitor (polar ft7), a redondo ball and two tennis balls.at the start of every week, the participants received a message on the website containing a tailored training plan for the following week.tis training plan comprised endurance, strength, fexibility and coordination training sessions and integrated preferred physical activities.te recommended exercise intensity was mainly based on the individual anaerobic threshold (iat) and the variation in oxygen saturation (spo2) measured during the exercise test.as a third additional option, the participants could pursue their preferred physical activities, such as horseback riding, tennis or climbing.at the end of each week, the participants had to upload their hand-written exercise protocol (jpeg/pdf) to the website.rated perceived discomfort (rpd 0–10) was the indicator for exercise tolerance.figure  2 shows the decision strategy employed by the sports scientist to adapt the training for the following week.te exercise participation parameters were the time spent training and training frequency over 52  weeks for the entire group and for each individual.terefore, 52 hand-written training protocols for each participant (n=572) were evaluated.table  1 presents the exercise participation-related parameters.te number of minutes spent training and frequency reported in domain four of the ipaq, “leisure” (vigorous; moderate), were evaluated.tese parameters were measured at baseline (t0), after 12 weeks (t1) and after 52 weeks (t2) of the intervention.te 6mwtd was the output of the 6mwt.te participants had to achieve their maximum distance within 6  min while moving at their fastest possible walking speed.ventilation of o2 (vo2) and co2 (vco2) (ergostick, geratherm®), lactate (l) (ekf diagnostics®; winlactat 3.0 mesics gmbh), heart rate (hr) (ecgpro, amedtec®), peripheral capillary oxygen saturation (spo2) and rate of perceived exertion (rpe) [22] were measured during the 6mwt and the cpet.l samples were taken from the ear lobe after each increment of the cpet.te individual anaerobic threshold (iat) was determined according to the dickhuth model [23].vo2peak was determined to be highest vo2 during the last 30 s of the cpet.statistical analysis microsoft excel (2013) and ibm spss statistics version 23.0 were utilized.we performed repeated-measures anova for the analysis of the primary and secondary outcomes.mauchly’s w-test was performed to test for sphericity.te greenhouse–geisser correction (ε) was applied when the assumption of sphericity was not met.tis participant group had a mean age of 32.9  years (sd±11.4), mean fev1%pred.of 72.3 (sd±17.3) and a mean bmi of 22.7 kg/m2  (sd±2.18) (table  2).of the 11 included subjects, six were males and fve were females.table  2 and fig.  4 show the descriptive analysis of the groups’ exercise participation during the 52 weeks of the intervention.terefore, subject 8 had a higher overall exercise intensity than subject 11 had.in the frst training section, the participants participated in 137.31 (sd: 95.7) training minutes.in the frst training section, the participants participated in 3.2 (sd: 1.4) training sessions.only in section nine was the number of mean weekly training sessions lower.missing data resulted from training sections for which no mean value could be calculated for a participant, because in at least one of the four weeks the training was interrupted.repeated-measures anova revealed no signifcant diference between the training sections with regard to training minutes (p=0.366) or training sessions (p=0.478).te distinct range of the confdence intervals, i.e., the heterogeneous participation in training in the group, led to the non-signifcant diferences between each training section.self‑reported physical activity at t0, t1, t2 te self-reported physical activity of eight participants who completed the ipaq at t0, t1 and t2, could be included in the repeated measures anova due to the listwise exclusion of missing data.participants 5 and 6 experienced decreases in their vo2peak of more than 5%.participants 1, 2, 3, 4, 5, and 10 improved their fvc by more than 5%.discussion taking into account that according to the who guidelines for physical activity, 150 min/week is recommended for healthy adults [25] and that overall adherence of young people to exercise recommendations is generally poor, the overall achieved mean training time of 178 min per week is noteworthy.however, a progressive increasing trend in exercise participation could be observed only until the seventh four-week training section.as shown in the results, the self-reported number of minutes spent training and sessions in domain 4 of the ipaq were also higher at t1 than before the intervention, which strengthens the assumption that the exercise intervention could support exercise participation in people with cf.notably, the exercise participation assessed at t0 and t1 on the ipaq was higher than the reported exercise participation reported in our protocols.te patients completed 85% of the training sessions, and they reported enjoying the training and gave the platform a high rating.te patients completed a 30- to 60-min training session fve days per week, but they included very young patients (mean 13  years old) with a mean fev1 of 82.7% predicted.nine patients completed eight weeks of this intervention, during which an exercise therapist supervised all sessions.finally, if integrated into usual care, this approach could facilitate the prescription of regular personalized exercise and promote exercise participation in the daily lives of people with
number of words= 1050
[{'rouge-1': {'f': 0.41499802034547767, 'p': 0.7459002770083103,'r': 0.28746880570409983}, 'rouge-2': {'f': 0.22258226360902675, 'p': 0.3561111111111111,'r': 0.16188224799286352}, 'rouge-l': {'f': 0.37536308951168723, 'p': 0.596829268292683,'r': 0.2737735849056604}}]
-----------------------------------------------------------------------------------------------------------------------------------
p396:
Extractive Summary:
the concept of testing biomarkers of the ans for estimating thresholds is based on the idea that the ans exerts distinct influences on the cardiorespiratory system during exercise to regulate heart rate, cardiac contractility and blood pressure.recently, different study groups implemented the evaluation of autonomic biomarkers to control training intensities and to detect training-induced states of fatigue [4].heart rate variability (hrv)-derived parameters reflecting ans activity showed an association with lt in healthy and diseased cohorts [5–8].the non-invasive assessment via highresolution ecg is based on beat-to-beat changes of the t wave vector (dt°) with periodic components of repolarization in the low-frequency range (≤ 0.1 hz).this pattern at the anaerobic threshold is characterized by a maximal discordance of dt° and heart rate and this point highly significantly correlated with lactate thresholds measured by the methods of mader and dickhuth [15].in the present study we validated this non-invasive ecg-based assessment of the anaerobic threshold by our previously described methods in a cohort of 65 team sport athletes to check if this method can be transferred to professional athletes as well as well-trained amateur athletes who are able to achieve markedly higher maximal workloads during exercise and have increased workloads at lactate thresholds.methods study population we included 65 healthy team sport athletes (14 women, 51 men, mean age 22.3 ± 5.2 year.exclusion criteria were acute or chronic infections, presence of pacemakers or implantable cardioverter defibrillators (icds), history of cardiovascular diseases or risk factors and other contraindications for performing exercise testing [16].assessment of ecg-based cardiac repolarization instability and detection of anaerobic threshold via dt° (at(dt°)) determination of at via dt° signals was carried out analogously to a previous study by our group [15].in brief, we analyzed high-resolution data from frank’s orthogonal lead ecg (1000 hz, schiller medilog ar4 plus, schiller diagnostics, ch) which was recorded throughout the entire exercise test including a 5 min resting phase prior and after the end of each cycle test with smartlab computer and r peak and t wave detection algorithms [17, 18].during exercise this dt° signal shows a characteristic three-phasic pattern and atdt° is defined as the point of maximal discordance between dt° signal and heart rate [15].figure 1 illustrates an exemplary dt° signal and corresponding heart rate signal during graded exercise test and shows the moment of maximal discordance between these signals defined as atdt°.the calculation of lt was determined according to the methods by mader (fixed threshold at 4 mmol/l) and dickhuth [1, 19].kruskall-wallis test was performed to detect statistical differences between mean determined thresholds (atdt°, ltmader and ltdickhuth).results baseline characteristics of study participants table 1 shows the baseline characteristics of all study participants (a) as well as of professional athletes (b) and amateur athletes (c) as subgroups.altogether 65 healthy team sport athletes (14 women, 51 men, mean age 22.3 ± 5.2 yrs.) were included in this study.all participants finished exercise testing until exhaustion.at atdt°, dt° and heart rate showed minimal correlation.then dt° transiently declines before increasing again until the end of the exercise test (exemplary signal: see fig. 1).atdt° highly significantly correlated with ltdickhuth (r = 0.96, r2 = 0.92, p < 0.001) and ltmader (r = 0.98, r2 = 0.96, p < 0.001) (fig. 2a and c, respectively) investigating power output.we further investigated thresholds for subgroups of professional athletes (n = 32) and amateur athletes (n = 33).in professional athletes mean atdt° was at 204.1 ± 33.9 w, ltdickhuth at 196.2 ± 37.0 w and ltmader at 196.2 ± 37.0 w (icc = 0.91).figure 3 visualizes the strong correlation between atdt° and ltdickhuth for both subgroups of amateur (r = 0.97, r2 = 0.94, p < 0.001, fig. 3a and b) and professional athletes (r = 0.93, r2 = 0.86, p < 0.001, fig. 3c and d) regarding power output.therefore, this pattern has been shown to be characteristic in both young and healthy average-trained athletes as well as professional athletes and very welltrained amateur athletes undergoing standardized incremental cycle exercise tests.while the exact physiological mechanisms underlying atdt° remain unclear, our study provides several relevant practical implications for future sports research and training physiology: (i) we were able to present a method which is non-invasive and does not require any puncture to gain blood samples.additionally, we performed graded exercise tests.however, the gold standard to determine lactate thresholds is the assessment of the maximal steady state during multiple rectangular exercise protocols.conclusions we demonstrated that atdt° is a reliable and noninvasive measurement to assess at.it correlates with established methods of lt assessment in a large cohort of professional and well-trained amateur athletes.the results of this validation study indicate that atdt° might represent a promising tool for future routine applicati
number of words= 771
[{'rouge-1': {'f': 0.4633212140373063, 'p': 0.8094366197183098,'r': 0.3245454545454545}, 'rouge-2': {'f': 0.24887517726109012, 'p': 0.3950883392226148,'r': 0.18165048543689322}, 'rouge-l': {'f': 0.4650366018356831, 'p': 0.6906896551724138,'r': 0.3505194805194805}}]
-----------------------------------------------------------------------------------------------------------------------------------
p397:
Extractive Summary:
new findings what is the central question of this study?what is the main finding and its importance? spa, but not exercise capacity, declines from early adulthood to the transition to middle age;  acute exercise decreases spa quantity and intensity in the following 24-48 h;  the magnitude of the fall in spa is similar from early adulthood to the transition to middle age;  the fall in spa can contribute to both body mass increase as a result of aging and to the below-thanexpected effects of exercise on energy balance.in rodents, the results are more consistent.we [9] and others [10–12] have shown that exercise decreases spa.lark et al. [12] found that this reduction was predicted to attenuate the expected change in energy balance by approximately 45 %.food intake food intake was determined individually in the 48 h before and in the 48 h immediately after the acute exercise session, simultaneously with the measurement of spa.the acclimatization consisted of placing the mice for 30 min on a static treadmill on day 1 and, in the following days, the speed was set at 8 cm/s, which is a very low speed aimed to make mice familiar with any noise a turned-on treadmill can produce.on days 4 and 5 the mice spent an additional 10 min in the treadmill at 12 cm/s.no electrical shock was applied for either the mect or the acute exercise session.acute exercise session the acute exercise session was done one week after the mect.it was performed for 30 min at 50 % of the maximal speed (cm/s) at which mice completed the 45 s stage (adapted from de [16]).when necessary, mice were gently prodded by hand.two mice at 4 and 9 months of age were not included in the posterior analysis of spontaneous physical activity and food intake for either refusing to run for the stipulated time or for getting a small injury in the finger during the session, avoiding any interference on locomotor parameters.as a control experiment, a separated set of mice was placed in the treadmill for the same 30 min but without exercising (non-moving treadmill).spa was recorded in the 48 h before and in the 48 h immediately after the acute exercise session at 4 and 9 months of age.average speed of displacement (cm/s) is calculated as locomotion (expressed in centimeter instead of counts) divided by the time mice spent in locomotion.for basal spa and average speed, we calculated the mean of the two 24 h periods (48 h) before the acute exercise session.there was a significant effect of age on spa (p = 0.01), which reduced significantly at 9m compared to 4m (fig. 1a).food intake was also similar from 4 to 9 months of age (fig. 1c).in an additional analysis, mice remained for 5 consecutive and uninterrupted days in the actimeter and again, no difference was observed for either locomotor parameters or food intake (supplementary fig. 1d-f).the effects of an acute exercise session on spa, average speed of displacement, and food intake in 4-monthold mice are shown in fig. 2.exercise also had a significant effect on average speed of displacement (p = 0.0001).for food intake, despite gee analysis indicated a significant effect of exercise (p = 0.041), no difference among the times (basal, day 1, and day 2) was found in the pairwise comparisons (fig. 2c).spa reduced significantly on day 1 compared to basal (p = 0.0001) but was similar to pre-exercise levels on day 2 (fig. 3a).likewise, there was a significant effect of exercise on average speed of displacement (p = 0.0001).the speed fell significantly on day 1 in relation to basal (p = 0.0001) but, even though it increased on day 2, the average speed was still significantly lower (p = 0.001) than before exercise (fig. 3b).there was no effect of age on the magnitude of change for spa and average speed of locomotion either on day 1 (fig. 4a) or day 2 (fig. 4b) after exercise.whereas food consumption increased to values higher than pre-exercise at 4 m, it decreased at 9 m (fig. 4b).discussion in our study, we found that an acute exercise session at individualized intensity consistently decreased both spa and average speed of displacement and that the magnitude of the fall did not increase from young adulthood to the transition to middle age.different mechanisms participate in body mass gain as age increases, including decreased activity of the somatotropic axis which begins as early as after puberty, increased fat to lean muscle mass ratio [17], and reduction in resting energy expenditure that cannot be fully explained by changes in body composition [18].the results of the maximal exercise capacity test indicate that mice at the transition to middle age have similar exercise capacity than at the beginning of adult life.spa, as previously said, was lower at 9 m compared to 4 m, thus, suggesting that, at least in mice, the mechanism controlling spa is more sensitive to aging and declines earlier than the function of cardiorespiratory/musculoskeletal systems.in humans, a compensatory decrease in spa in response to exercise training has been observed in individuals with 56 years or more [25, 26].a limitation of our study is that we evaluated spa only in response to a single bout of exercise, and not after training.the magnitude of this compensatory reduction in spa is similar from early adulthood to the transition to middle age.
number of words= 889
[{'rouge-1': {'f': 0.4882084385835874, 'p': 0.7854471544715447,'r': 0.35417653390742737}, 'rouge-2': {'f': 0.2573644657144994, 'p': 0.3825,'r': 0.19392241379310346}, 'rouge-l': {'f': 0.4665531011314296, 'p': 0.6486516853932585,'r': 0.36428571428571427}}]
-----------------------------------------------------------------------------------------------------------------------------------
p398:
Extractive Summary:
background muscular asymmetry, especially in professional athletes, increases the risk of injury, as pointed out by various authors examining asymmetry of muscles in football players, basketball players, and people with spinal pains [1–4].in this kind of analysis, the most often used parameters of the semg signal are changes in amplitude scope and in the mean or median frequency of total capacity spectrum.in some studies however physical effort did not decrease the median frequency semg signal [5–8].the asymmetry of gluteus maximus muscles has been described in the literature most often in the contexts of walk and isolated positions [11, 14].felser et al. studied athletes skating in a straight line and in curves [15].neuromuscular activation was higher in the right leg, while with the reduction in skating speed decreased neuromuscular activity, but only when skating in a straight line.this indicates that the right leg has higher activity during skating in curves.studying athletes skating in a straight line and in curves during the subsequent laps, hesford et al. found considerable asymmetry in oxygen supply to the two legs.the authors did not report effects of this asymmetry, but offered suggestions for training [16].stoter et al. showed that the bioelectric tension of muscles of speed skaters was correlated with speed at different sections of the track, but they did not analyse whether muscular asymmetry affected this phenomenon [17].at present, the literature of the examined phenomenon contains analyses of muscle myolectrical manifestations of fatigue in many variants [18].analysis of the semg signal frequency of the power spectrum provides useful information concerning local muscle myolectrical manifestations of fatigue [7, 19].the idea behind the study was initiated by the coaches of the polish national team, worried about the asymmetry in the skaters and the related increased injury risk.even though the coaches try to focus as much of the training as possible on symmetrical work, they stress that training on ice accounts for about 60% of the training volume and skating to the left is a typical asymmetrical work.since the level of asymmetry varies from athlete to athlete, the coaches stress the importance of customised training, which would help them improve muscular symmetry in each athlete in an optimal way.the main research hypothesis is that intensive shorttrack training leads to asymmetry of the gluteus maximus muscles.the experimental group included eight female members of the polish national team in short track, with a mean age of 18.7 ± 2.9 standard deviation, mean height of 162.4 ± 2.4 cm, and mean body weight of 57.2 ± 5.9 kg.the tests were approved by the bioethical commission of the chamber of physicians in opole, poland.in interviews conducted before the tests, all the respondents declared they were right-handed and right-legged in daily and sports activities (e. g., tossing a ball, kicking a ball, supporting with a foot during swinging).furthermore, a kick-a-ball test (with three attempts) confirmed all the participants were right-legged, while the modified edinburgh questionnaire confirmed they were righthanded [20].during the test, the subjects were lying on a horizontal table on the abdomen, with the iliac crests aligned to the edge of the table and the lower limbs attached to the straps around the ankle joints.they were instructed to hold the body (head, shoulders, and torso), without support, horizontally to the ground as long as they could, with the arms crossed at the chest (fig. 1).the emg measurement in the test, a 16-channel emg system (produced by noraxon dts) was used, which recorded signals with an accuracy of 16 bits at a sampling rate of 1500 hz.statistical analysis the slopes representing the subjects’ myolectrical manifestations of fatigue were analyzed with mixed anova within-between interactions.we see thus that the interaction was significant because the groups differed in these differences (representing asymmetry) in the myolectrical manifestations of fatigue of the right and the left muscles.while the subjects from the two groups did not differ in the myolectrical manifestations of fatigue of the left muscle, they did in the myolectrical manifestations of fatigue of the right muscle: the elite speed-track skaters had higher myolectrical manifestations of fatigue in the right muscle, and higher than the non-training subjects (p = 0.001; table 1 and figs. 4 and 5).what was surprising, however, was so high asymmetry of muscle myolectrical manifestations of fatigue observed in the members of the polish women’s national team in short-track speed skating.even though experiments without the control group are difficult to interpret, most studies on muscle myolectrical manifestations of fatigue did not include nontraining subjects [15, 16].
number of words= 743
[{'rouge-1': {'f': 0.520071371614642, 'p': 0.8071601208459214,'r': 0.38362467866323907}, 'rouge-2': {'f': 0.3140038619100215, 'p': 0.46393939393939393,'r': 0.23731016731016732}, 'rouge-l': {'f': 0.3597138562651659, 'p': 0.5804895104895105,'r': 0.2606005221932115}}]
-----------------------------------------------------------------------------------------------------------------------------------
p399:
Extractive Summary:
background health needs of the local community there is a continuous ageing trend in the population of hkg.according to the prediction report on the development trend of population aging in chn, there were 241 million chinese citizens aged 60 or above in 2017, about 17.3% of china’s total population.owing to the effects of net medical inflation, population growth and ageing, and assuming the service enhancement continues with the historical trend, hong kong’s recurrent social welfare and health expenditure as a percentage of nominal gdp would increase from $56.9 billion and $52.4 billion in 2014 and 2015, respectively, and to $523.3 billion and $563.6 billion in 2041 and 2042, respectively [4].the limitations of previously reviewed studies suggest that larger scale interventions with prospective follow-ups, rcts with representative and sufficiently powered samples, and the use of valid and reliable outcome measures are recommended for future research.these lvb features reduce the older adults’ likelihood of injury, such as falling, and these features address the recommendations of franco [14] regarding factors that facilitate older adult’s participation in pa.moreover, with the popularity of the china national volleyball team and annual hkg professional volleyball competition (i.e., fivb volleyball nations league), volleyball is a traditional and popular sport in hkg.in 2018, leung et al. [17] conducted a quasi-experimental intervention to evaluate the effects of a 15-week lvb intervention on physical and psychological attributes among 78 hong kong older adults (≥ 60 years) by comparing lvb to a modified form of physical activity, rouliqiu (rlq), and to a control group.this pilot study suggested that the effectiveness of lvb on promoting older adult’s health should be further investigated using a larger-scale rct, combined with follow-up measures in the community.objectives in response to the promising results of the lvb pilot study and the priority of allocating resources to the prevention of age-related fitness degradation in hkg, the present project aims to extend the aforementioned work, which is to investigate the effectiveness of a lvb intervention on physical and psychological health attributes among older adults in hkg.specifically, the objectives of this study are: (1) compare the effects of a 16-week light volleyball (lvb) intervention program, with a tc program in terms of improving functional fitness among chinese older adults aged 65 years or above; (2) compare the effects of a 16-week lvb intervention program, with a tc program in terms of improving psychological attributes (i.e., resilience, physical activity enjoyment) among chinese older adults aged 65 years or above; (3) compare the effects of a 16-week lvb intervention program, with a tc program in terms of improving quality of life and balance among chinese older adults aged 65 years or above; and.next, compared to team-based lvb, tc is an individual pa that may impact the health and quality of life of older adults differently (e.g. perhaps psychologically).others [20, 21] demonstrated that increasing amounts of social support in community based group pa interventions were associated with increasingly beneficial effects (e.g., social functioning) relative to pa intervention and program adherence.when lvb is promoted or becomes popular in hong kong communities on a larger scale during the later phases of this project, tc is a good candidate for the pa utilized by the active control group in the current study.participants inclusion criteria for participants will be (a) aged 65 years and above; (b) living in the community independently; (c) absence of diagnosed cognitive impairment; and will have (d) no participation in a structured pa program for two years preceding the study; and (e) passing score on the timed-up-and-go test (tug) and abbreviated mental test (amt).recruitment and procedures participants will be recruited via a presentation offered by the research team and via advertisements in the local neighborhood elderly centers (necs).an information session (e.g., aims and procedures of the intervention) will be delivered to groups of potential participants.as suggested by fjeldsoe et al. [27], two follow-up tests will be done 6 months and 12 months after completion of the intervention.all participants will receive a hk$100 (us$12) supermarket cash voucher as incentive for their participation.*see fig. 1 for the design of the study using the consort guidelines for rct.the tests consist of seven items: chair stand test (lower body strength), arm curl test (upper body strength), chair sit and reach test (lower body flexibility), back scratch (upper body flexibility), 8-ft up-and-go test (agility and balance), 2-min step test (aerobic endurance), and body mass index (bmi; kg/m2).these tests were found to be reliable (icc: .80–.98 for participants in trials) and valid through content, construct and criterion-related analyses [28].balance test the balance system sd (biodex) will be used to measure balance ability of participants in our intervention study.this system has previously been used to assess balance in a group of hong kong chinese older adults [29].in the test, participants will be asked to maintain the vertical projection with their centre of gravity in the centre of the platform by observing a vertical screen located 30 cm in front of their face.the average of the results from three trials will be obtained.reliability (test-retest reliability = .69–.80) and validity of the balance test was supported in the studies of parraca et al. [30] and finn et al. [31].this scale is reliable (test-retest reliability = .8; cronbach’s α = .95) for chinese elderly populations [33].the reliability and validity of the sf-36 questionnaire have been confirmed for chinese individuals [34].this survey contains eight domains: physical functioning (pf), physical role, bodily pain, general health (gh), vitality, social functioning (sf), emotional role, and mental health.in the present study, only pf, sf and gh will be included in order to lower the participants’ burden to complete the questionnaires.examples of the items include asking participants if they encountered any restrictions or limitations while performing moderate pa, such as moving a table, using a vacuum cleaner, bowling, playing golf, and walking.the score ranges from 0 to 100 and a higher score indicates better physical functioning.pa enjoyment the short version (8 items) of the pa enjoyment scale (paces) [35], translated to chinese, will be used to measure participants’ pa enjoyment throughout the study.the scale was originally developed by kendzierski and decarlo [36].this short version the paces questionnaire is a reliable and valid instrument for assessing pa enjoyment in chinese older adults [35].in specific, the test-retest reliability and internal consistency was moderate (intraclass correlation coefficient = 0.614) and high (cronbach’s alpha = .91–.92), respectively.data analysis data will be analysed using spss 24.0 with a significance level of .05.to answer study objectives (a)-(c), generalized estimating equations (gee) models will be used to analyse mean changes in outcomes over time among the three groups with adjustment for baseline characteristics showing statistically significant differences among the groups.the expected and direct beneficiaries are expected to be: older adults in hkg will benefit from enhancing their physical and psychological health after participating in a 16-week lvb intervention.the results of the qualitative study will also provide evidence-based information (e.g., workforce, service delivery) to practitioners to achieve a more effective organization of lvb and other pa programs in the future.policymakers will also have a better understanding of the effectiveness and efficiency of a pa promotion program such as which elements would work better for older adults.on a macro scale, this project may inform hkg government about the ways of promoting healthy ageing using promotion of lvb as an example of moving from mass participation, to regular practice and training, and to sport competitions (e.g. in the master games) in the future (long-term impact).
number of words= 1235
[{'rouge-1': {'f': 0.3705378113888701, 'p': 0.8182993197278912,'r': 0.2394915254237288}, 'rouge-2': {'f': 0.25442630090463136, 'p': 0.5102730375426621,'r': 0.1694602929838088}, 'rouge-l': {'f': 0.36015775396364536, 'p': 0.6598876404494383,'r': 0.24766497461928935}}]
-----------------------------------------------------------------------------------------------------------------------------------
p400:
Extractive Summary:
background competitive dinghy sailing is practiced at varying levels, from amateur competitions to the olympics.dinghies range in size and complexity from single-handed 2.4 m optimist, intended for use by children, to 49er, the olympic two-handed skiff-type high-performance sailing dinghy with a total sail area of 59.2 m2 and capable of speeds easily exceeding 20 knots.according to the recent review by nathason, sailors at risk of acute injuries, overuse injuries, environmental injuries, and sailing-related illnesses [1].injury rates at an international 2014 olympic-class regatta and among elite dinghy racers in new zealand have been reported as 0.59 per 1000 h and 0.2 injuries per year, respectively; while in dinghies mixed population, injuries were 4.6 per 1000 days of practice [2–4].in dinghies, most acute injuries are contusions (9–55%) commonly caused by falls and collision with the boat during mismanaged maneuvers [1].head trauma is among the most severe reported injuries and is often caused by the boom’s impact.hand injuries are also common (6–31%) and mainly include lacerations, cuts and fractures from handling lines or the center-board.capsizing and collisions with other boats or objects are other frequent mechanisms of injury.overuse injuries especially are mainly represented by low back pain (29–45%) and knee pain (13–22%) and are common especially among olympic- class sailors.high winds, turning maneuvers (i.e. tacks, jibes), operator inattention and inexperience have been identified as leading contributing factors for injuries [1].over the last decade, hydrofoil technology has had an enormous impact on sailing, providing high speed and spectacle that attract media attention and increase sailing popularity.while foil sailing was introduced with the nacra 17 class in the 2016 rio de janeiro summer olympic games, hydrofoil technology is also used in various amateur sailing classes.despite the advantages offered by hydrofoils, the technology exposes crews to high speeds and accelerations (over 40 knots, 46 mph, 74 km/h) that increases the risk of high-energy trauma, especially in the case of collisions between boats or boat capsizing [6].in hydrofoil technology, the shape of the appendices’ lamina diverts the water flow downwards, thus exerting an upward force on the lamina.this upward force lifts the boat’s keel, decreasing water drag and allowing for an increase in speed.the speed, in turn, increases the apparent wind, contributing to the possibility of exceeding the actual wind speed and achieve significantly higher final velocities than traditional boats.high speed can expose participants to a risk of high-energy trauma, as some events reported by the media seem to confirm.for example, in 2013, artemis racing’s ac72 jumped (somersaulted) and broke apart while training for the 34th america’s cup.the accident resulted in a crewmember’s death who suffered blunt trauma, severe head injuries and cuts and drowned after being trapped underwater [7].in 2017, during a practice race for the nacra 17 world championships in france, the us crew pitchpoled and capsized at a speed of 17 knots resulting in the helmsman partially losing three fingers of the right hand [8].however, the media often highlight outdoor sports risks after a severe accident or fatality [9].however, it is possible that most injuries are minor and that while the speed and maneuverability increase the risk of injuries, they are not necessarily serious.sailing on foiling boats requires fine coordination between the crewmembers to maintain precarious flight.furthermore, wind shifts and water movements also affect the flying boat’s speed and balance, continually challenging the flight.a recent study by terrien et al. [10] showed that the natural environmental condition influences the interaction between the crewmembers and the nacra-17 class boat.moreover, extra-personal and interpersonal coordination processes are also distinct cognitive and interactive process influences to maintain or restore flight stability, and reflective interpersonal coordination processes to anticipate environmental events or understand past events [10].since hydrofoil has only recently been introduced into the sport of sailing, to date, no scientific studies are investigating the risk of acute injuries, overuse injuries, environmental injuries, and sailing-related illnesses associated with this technology.therefore, this study aimed to provide preliminary information about the most common injuries related to hydrofoil sailing.the primary outcome was to classify the lesions and identify which anatomical parts may be affected.the secondary outcome was to determine the traumatic dynamics most often involved and the main risk factors to guide specific recommendations.methods study design this descriptive epidemiology study was conducted following the strengthening the reporting of observational studies in epidemiology (strobe) [11] during three different regattas: 2018 and 2019 foiling week editions held in malcesine (italy) and the moth world championship held in perth (australia) in december 2019.the recruitment of participants was extensive in foiling week 2018.we preliminarily sent an informative e-mail to all pre-registered sailors; furthermore, upon completion of regatta registration, participation in the study was proposed in person by one of the researchers; finally, during the regatta briefing, the organizers explained the aims of the research and encouraged participation.as for the other two regattas, the participants were allowed to participate, but the information campaign was in charge of the organizing committee only and took place in a sparse and non-systematic way.before entering the study, the participants were fully informed about its aims and procedures.furthermore, each participant released written informed consent before the inclusion in the study.the study protocol was approved by the ethical committee of politecnico di milano (prot.2/2018), with current national and international laws and regulations governing the use of human subjects (declaration of helsinki ii).data collection the study protocol required completing the same questionnaire pre and post the sailing event to distinguish between injuries sustained during the regatta from those before the regatta.however, we speculate that this may have limited effect on the present study because all sailors participated in international regattas, with sailing instructions and regulations in english.good knowledge of the language is therefore assumed.in conclusion, most of the reported musculoskeletal injuries were to the upper limbs (34.6%), among which one third were represented by hand contusions and lacerations, followed by shoulder strains and contusions.this study provides potentially helpful information for regatta organizers, boat builders, athletes, and their coaches and doctors.the results also provide the basis for new studies.in hydrofoil sailing, much more than in traditional sailing, there should be a focus above all on the ergonomics of boats and specific physical exercises adapted for the crew training and rehabilitation of cre
number of words= 1027
[{'rouge-1': {'f': 0.3488730833715773, 'p': 0.7264885496183207,'r': 0.22955473098330242}, 'rouge-2': {'f': 0.19296639185262135, 'p': 0.3381992337164751,'r': 0.13499535747446612}, 'rouge-l': {'f': 0.31702542783976173, 'p': 0.555207100591716,'r': 0.22185185185185186}}]
-----------------------------------------------------------------------------------------------------------------------------------
AVERAGE FOR one set (400 PAPERS):

ROUGE-1=  0.7812171407445393
ROUGE-2=  0.3894537503910574
ROUGE-l=  0.6347531438566493




